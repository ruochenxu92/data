nohup: ignoring input
2015-04-10 02:32:03+0000 [scrapy] INFO: Scrapy 0.24.5 started (bot: superqq_spider)
2015-04-10 02:32:03+0000 [scrapy] INFO: Optional features available: ssl, http11, boto
2015-04-10 02:32:03+0000 [scrapy] INFO: Overridden settings: {'NEWSPIDER_MODULE': 'superqq_spider.spiders', 'SPIDER_MODULES': ['superqq_spider.spiders'], 'COOKIES_ENABLED': False, 'DOWNLOAD_DELAY': 53, 'BOT_NAME': 'superqq_spider'}
2015-04-10 02:32:03+0000 [scrapy] INFO: Enabled extensions: LogStats, TelnetConsole, CloseSpider, WebService, CoreStats, SpiderState
2015-04-10 02:32:03+0000 [scrapy] INFO: Enabled downloader middlewares: HttpAuthMiddleware, DownloadTimeoutMiddleware, UserAgentPoolMiddleware, RetryMiddleware, DefaultHeadersMiddleware, MetaRefreshMiddleware, HttpCompressionMiddleware, RedirectMiddleware, ChunkedTransferMiddleware, DownloaderStats
2015-04-10 02:32:03+0000 [scrapy] INFO: Enabled spider middlewares: HttpErrorMiddleware, OffsiteMiddleware, RefererMiddleware, UrlLengthMiddleware, DepthMiddleware
2015-04-10 02:32:03+0000 [scrapy] INFO: Enabled item pipelines: JsonWriterPipeline
2015-04-10 02:32:03+0000 [xxu46_10] INFO: Spider opened
2015-04-10 02:32:03+0000 [xxu46_10] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2015-04-10 02:32:03+0000 [scrapy] DEBUG: Telnet console listening on 127.0.0.1:6024
2015-04-10 02:32:03+0000 [scrapy] DEBUG: Web service listening on 127.0.0.1:6081
2015-04-10 02:32:59+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/list/cs/15?skip=3000&show=1000> (referer: None)
2015-04-10 02:33:03+0000 [xxu46_10] INFO: Crawled 1 pages (at 1 pages/min), scraped 0 items (at 0 items/min)
2015-04-10 02:33:56+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/list/cs/15?skip=2000&show=1000> (referer: None)
2015-04-10 02:34:03+0000 [xxu46_10] INFO: Crawled 2 pages (at 1 pages/min), scraped 0 items (at 0 items/min)
2015-04-10 02:34:07+0000 [xxu46_10] DEBUG: Filtered duplicate request: <GET http://arxiv.org/abs/1502.06910> - no more duplicates will be shown (see DUPEFILTER_DEBUG to show all duplicates)
2015-04-10 02:34:52+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/list/cs/15?skip=1000&show=1000> (referer: None)
2015-04-10 02:35:03+0000 [xxu46_10] INFO: Crawled 3 pages (at 1 pages/min), scraped 0 items (at 0 items/min)
2015-04-10 02:36:03+0000 [xxu46_10] INFO: Crawled 3 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2015-04-10 02:36:31+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/list/cs/14?skip=16000&show=1000> (referer: None)
2015-04-10 02:36:42+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/list/cs/15?skip=0&show=1000> (referer: None)
2015-04-10 02:37:03+0000 [xxu46_10] INFO: Crawled 5 pages (at 2 pages/min), scraped 0 items (at 0 items/min)
2015-04-10 02:37:44+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/list/cs/14?skip=15000&show=1000> (referer: None)
2015-04-10 02:38:03+0000 [xxu46_10] INFO: Crawled 6 pages (at 1 pages/min), scraped 0 items (at 0 items/min)
2015-04-10 02:39:03+0000 [xxu46_10] INFO: Crawled 6 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2015-04-10 02:39:08+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/list/cs/14?skip=14000&show=1000> (referer: None)
2015-04-10 02:39:22+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1503.01180> (referer: http://arxiv.org/list/cs/15?skip=3000&show=1000)
2015-04-10 02:39:22+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1503.01180>
	{'abstract': u'Although analyzing user behavior within individual communities is an active and rich research domain, people usually interact with multiple communities both on- and off-line. How do users act in such multi-community environments? Although there are a host of intriguing aspects to this question, it has received much less attention in the research community in comparison to the intra-community case. In this paper, we examine three aspects of multi-community engagement: the sequence of communities that users post to, the language that users employ in those communities, and the feedback that users receive, using longitudinal posting behavior on Reddit as our main data source, and DBLP for auxiliary experiments. We also demonstrate the effectiveness of features drawn from these aspects in predicting users\' future level of activity. One might expect that a user\'s trajectory mimics the "settling-down" process in real life: an initial exploration of sub-communities before settling down into a few niches. However, we find that the users in our data continually post in new communities; moreover, as time goes on, they post increasingly evenly among a more diverse set of smaller communities. Interestingly, it seems that users that eventually leave the community are "destined" to do so from the very beginning, in the sense of showing significantly different "wandering" patterns very early on in their trajectories; this finding has potentially important design implications for community maintainers. Our multi-community perspective also allows us to investigate the "situation vs. personality" debate from language usage across different communities.',
	 'authors': u'Chenhao Tan, Lillian Lee,',
	 'category': u'Computer Science ',
	 'date': '2015-3-4',
	 'pdflink': u'http://arxiv.org/pdf/1503.01180',
	 'subjects': u'Social and Information Networks (cs.SI)',
	 'title': u'\nAll Who Wander: On the Prevalence and Characteristics of Multi-community  Engagement',
	 'urllink': u'http://arxiv.org/abs/1503.01180'}
2015-04-10 02:40:03+0000 [xxu46_10] INFO: Crawled 8 pages (at 2 pages/min), scraped 1 items (at 1 items/min)
2015-04-10 02:40:31+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1503.01185> (referer: http://arxiv.org/list/cs/15?skip=3000&show=1000)
2015-04-10 02:40:31+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1503.01185>
	{'abstract': u'In this paper, we propose two novel p-norm penalty least mean square (Lp-LMS) algorithms as supplements of the conventional Lp-LMS algorithm established for sparse adaptive filtering recently. A gradient comparator is employed to selectively apply the zero attractor of p-norm constraint for only those taps that have the same polarity as that of the gradient of the squared instantaneous error, which leads to the new proposed gradient compared p-norm constraint LMS algorithm (LpGC-LMS). We explain that the LpGC-LMS can achieve lower mean square error than the standard Lp-LMS algorithm theoretically and experimentally. To further improve the performance of the filter, the LpNGC-LMS algorithm is derived using a new gradient comparator which takes the sign-smoothed version of the previous one. The performance of the LpNGC-LMS is superior to that of the LpGC-LMS in theory and in simulations. Moreover, these two comparators can be easily applied to other norm constraint LMS algorithms to derive some new approaches for sparse adaptive filtering. The numerical simulation results show that the two proposed algorithms achieve better performance than the standard LMS algorithm and Lp-LMS algorithm in terms of convergence rate and steady-state behavior in sparse system identification settings.',
	 'authors': u'Yong Feng, Jiasong Wu, Rui Zeng, Limin Luo, Huazhong Shu,',
	 'category': u'Computer Science ',
	 'date': '2015-3-4',
	 'pdflink': u'http://arxiv.org/pdf/1503.01185',
	 'subjects': u'Systems and Control (cs.SY)',
	 'title': u'\nGradient Compared Lp-LMS Algorithms for Sparse System Identification',
	 'urllink': u'http://arxiv.org/abs/1503.01185'}
2015-04-10 02:41:03+0000 [xxu46_10] INFO: Crawled 9 pages (at 1 pages/min), scraped 2 items (at 1 items/min)
2015-04-10 02:41:45+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1503.01186> (referer: http://arxiv.org/list/cs/15?skip=3000&show=1000)
2015-04-10 02:41:45+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1503.01186>
	{'abstract': u"Threats from the internet, particularly malicious software (i.e., malware) often use cryptographic algorithms to disguise their actions and even to take control of a victim's system (as in the case of ransomware). Malware and other threats proliferate too quickly for the time-consuming traditional methods of binary analysis to be effective. By automating detection and classification of cryptographic algorithms, we can speed program analysis and more efficiently combat malware. This thesis will present several methods of leveraging machine learning to automatically discover and classify cryptographic algorithms in compiled binary programs. While further work is necessary to fully evaluate these methods on real-world binary programs, the results in this paper suggest that machine learning can be used successfully to detect and identify cryptographic primitives in compiled code. Currently, these techniques successfully detect and classify cryptographic algorithms in small single-purpose programs, and further work is proposed to apply them to real-world examples.",
	 'authors': u'Diane Duros Hosfelt,',
	 'category': u'Computer Science ',
	 'date': '2015-3-4',
	 'pdflink': u'http://arxiv.org/pdf/1503.01186',
	 'subjects': u'Cryptography and Security (cs.CR)',
	 'title': u'\nAutomated detection and classification of cryptographic algorithms in  binary programs through machine learning',
	 'urllink': u'http://arxiv.org/abs/1503.01186'}
2015-04-10 02:42:03+0000 [xxu46_10] INFO: Crawled 10 pages (at 1 pages/min), scraped 3 items (at 1 items/min)
2015-04-10 02:42:46+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1503.01187> (referer: http://arxiv.org/list/cs/15?skip=3000&show=1000)
2015-04-10 02:42:46+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1503.01187>
	{'abstract': u'As a promising paradigm for fifth generation (5G) wireless communication systems, cloud radio access networks (C-RANs) have been shown to reduce both capital and operating expenditures, as well as to provide high spectral efficiency (SE) and energy efficiency (EE). The fronthaul in such networks, defined as the transmission link between a baseband unit (BBU) and a remote radio head (RRH), requires high capacity, but is often constrained. This article comprehensively surveys recent advances in fronthaul-constrained C-RANs, including system architectures and key techniques. In particular, key techniques for alleviating the impact of constrained fronthaul on SE/EE and quality of service for users, including compression and quantization, large-scale coordinated processing and clustering, and resource allocation optimization, are discussed. Open issues in terms of software-defined networking, network function virtualization, and partial centralization are also identified.',
	 'authors': u'M. Peng, C. Wang, V. Lau, H. V. Poor,',
	 'category': u'Computer Science ',
	 'date': '2015-3-4',
	 'pdflink': u'http://arxiv.org/pdf/1503.01187',
	 'subjects': u'Information Theory (cs.IT)',
	 'title': u'\nFronthaul-Constrained Cloud Radio Access Networks: Insights and  Challenges',
	 'urllink': u'http://arxiv.org/abs/1503.01187'}
2015-04-10 02:43:03+0000 [xxu46_10] INFO: Crawled 11 pages (at 1 pages/min), scraped 4 items (at 1 items/min)
2015-04-10 02:43:51+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1503.01189> (referer: http://arxiv.org/list/cs/15?skip=3000&show=1000)
2015-04-10 02:43:51+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1503.01189>
	{'abstract': u'This paper presents some physical interpretations of recent stability results on the feedback interconnection of negative imaginary systems. These interpretations involve spring mass damper systems coupled together by springs or RLC electrical networks coupled together via inductors or capacitors.',
	 'authors': u'Ian R. Petersen,',
	 'category': u'Computer Science ',
	 'date': '2015-3-4',
	 'pdflink': u'http://arxiv.org/pdf/1503.01189',
	 'subjects': u'Systems and Control (cs.SY)',
	 'title': u'\nPhysical Interpretations of Negative Imaginary Systems Theory',
	 'urllink': u'http://arxiv.org/abs/1503.01189'}
2015-04-10 02:44:03+0000 [xxu46_10] INFO: Crawled 12 pages (at 1 pages/min), scraped 5 items (at 1 items/min)
2015-04-10 02:44:36+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1503.01190> (referer: http://arxiv.org/list/cs/15?skip=3000&show=1000)
2015-04-10 02:44:36+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1503.01190>
	{'abstract': u'We explore training an automatic modality tagger. Modality is the attitude that a speaker might have toward an event or state. One of the main hurdles for training a linguistic tagger is gathering training data. This is particularly problematic for training a tagger for modality because modality triggers are sparse for the overwhelming majority of sentences. We investigate an approach to automatically training a modality tagger where we first gathered sentences based on a high-recall simple rule-based modality tagger and then provided these sentences to Mechanical Turk annotators for further annotation. We used the resulting set of training data to train a precise modality tagger using a multi-class SVM that delivers good performance.',
	 'authors': u'Vinodkumar Prabhakaran, Michael Bloodgood, Mona Diab, Bonnie Dorr, Lori Levin, Christine D. Piatko, Owen Rambow, Benjamin Van Durme,',
	 'category': u'Computer Science ',
	 'date': '2015-3-4',
	 'pdflink': u'http://arxiv.org/pdf/1503.01190',
	 'subjects': u'Computation and Language (cs.CL)',
	 'title': u'\nStatistical modality tagging from rule-based annotations and  crowdsourcing',
	 'urllink': u'http://arxiv.org/abs/1503.01190'}
2015-04-10 02:45:03+0000 [xxu46_10] INFO: Crawled 13 pages (at 1 pages/min), scraped 6 items (at 1 items/min)
2015-04-10 02:45:40+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1503.01191> (referer: http://arxiv.org/list/cs/15?skip=3000&show=1000)
2015-04-10 02:45:40+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1503.01191>
	{'abstract': u'Heterogeneous cloud radio access networks (HCRANs) are potential solutions to improve both spectral and energy efficiencies by embedding cloud computing into heterogeneous networks (HetNets). The interference among remote radio heads (RRHs) can be suppressed with centralized cooperative processing in the base band unit (BBU) pool, while the intertier interference between RRHs and macro base stations (MBSs) is still challenging in H-CRANs. In this paper, to mitigate this inter-tier interference, a contract-based interference coordination framework is proposed, where three scheduling schemes are involved, and the downlink transmission interval is divided into three phases accordingly. The core idea of the proposed framework is that the BBU pool covering all RRHs is selected as the principal that would offer a contract to the MBS, and the MBS as the agent decides whether to accept the contract or not according to an individual rational constraint. An optimal contract design that maximizes the rate-based utility is derived when perfect channel state information (CSI) is acquired at both principal and agent. Furthermore, contract optimization under the situation where only the partial CSI can be obtained from practical channel estimation is addressed as well. Monte Carlo simulations are provided to confirm the analysis, and simulation results show that the proposed framework can significantly increase the transmission data rates over baselines, thus demonstrating the effectiveness of the proposed contract-based solution.',
	 'authors': u'Mugen Peng, Xinqian Xie, Qiang Hu, Jie Zhang, H. Vincent Poor,',
	 'category': u'Computer Science ',
	 'date': '2015-3-4',
	 'pdflink': u'http://arxiv.org/pdf/1503.01191',
	 'subjects': u'Information Theory (cs.IT)',
	 'title': u'\nContract-Based Interference Coordination in Heterogeneous Cloud Radio  Access Networks',
	 'urllink': u'http://arxiv.org/abs/1503.01191'}
2015-04-10 02:46:03+0000 [xxu46_10] INFO: Crawled 14 pages (at 1 pages/min), scraped 7 items (at 1 items/min)
2015-04-10 02:46:46+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1503.01192> (referer: http://arxiv.org/list/cs/15?skip=3000&show=1000)
2015-04-10 02:46:46+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1503.01192>
	{'abstract': u'We give a simple and efficient algorithm for adaptively counting inversions in a sequence of integers. Our algorithm runs in time in the word-RAM model of computation, where is the number of inversions.',
	 'authors': u'Amr Elmasry,',
	 'category': u'Computer Science ',
	 'date': '2015-3-4',
	 'pdflink': u'http://arxiv.org/pdf/1503.01192',
	 'subjects': u'Data Structures and Algorithms (cs.DS)',
	 'title': u'\nCounting Inversions Adaptively',
	 'urllink': u'http://arxiv.org/abs/1503.01192'}
2015-04-10 02:47:03+0000 [xxu46_10] INFO: Crawled 15 pages (at 1 pages/min), scraped 8 items (at 1 items/min)
2015-04-10 02:47:59+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1503.01203> (referer: http://arxiv.org/list/cs/15?skip=3000&show=1000)
2015-04-10 02:47:59+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1503.01203>
	{'abstract': u'We consider the largest number of minimal separators a graph on n vertices can have at most. We give a new proof that this number is in . We prove that this number is in , improving on the previous best lower bound of . This gives also an improved lower bound on the number of potential maximal cliques in a graph. We would like to emphasize that our proofs are short, simple, and elementary.',
	 'authors': u'Serge Gaspers, Simon Mackenzie,',
	 'category': u'Computer Science ',
	 'date': '2015-3-4',
	 'pdflink': u'http://arxiv.org/pdf/1503.01203',
	 'subjects': u'Data Structures and Algorithms (cs.DS)',
	 'title': u'\nOn the Number of Minimal Separators in Graphs',
	 'urllink': u'http://arxiv.org/abs/1503.01203'}
2015-04-10 02:48:03+0000 [xxu46_10] INFO: Crawled 16 pages (at 1 pages/min), scraped 9 items (at 1 items/min)
2015-04-10 02:48:57+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1503.01205> (referer: http://arxiv.org/list/cs/15?skip=3000&show=1000)
2015-04-10 02:48:57+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1503.01205>
	{'abstract': u'In a diffusion-based molecular communication network, transmitters and receivers communicate by using signalling molecules (or ligands) in a fluid medium. This paper proposes a novel modulation mechanism for molecular communication called Reaction Shift Keying (RSK). In RSK, the transmitter uses different chemical reactions to generate different time-varying functions of concentration of signalling molecules to represent different transmission symbols. We consider the problem of demodulating the RSK symbols assuming that the transmitter and receiver are synchronised. We assume the receiver consists of receptors and signalling molecules may react with these receptors to form ligand-receptor complexes. We derive an optimal RSK demodulator using the continuous history of the number of complexes at the receiver as the input to the demodulator. We do that by first deriving a communication model which includes the chemical reactions in the transmitter, diffusion in the transmission medium and the ligand-receptor process in the receiver. This model, which takes the form of a continuous-time Markov process, captures the noise in the receiver signal due to the stochastic nature of chemical reactions and diffusion. We then adopt a maximum posterior framework and use Bayesian filtering to derive the optimal demodulator for RSK signals. We use numerical examples to illustrate the properties of the RSK demodulator.',
	 'authors': u'Chun Tung Chou,',
	 'category': u'Computer Science ',
	 'date': '2015-3-4',
	 'pdflink': u'http://arxiv.org/pdf/1503.01205',
	 'subjects': u'Information Theory (cs.IT)',
	 'title': u'\nOptimal Demodulation of Reaction Shift Keying Signals in Diffusion-based  Molecular Communication Networks',
	 'urllink': u'http://arxiv.org/abs/1503.01205'}
2015-04-10 02:49:03+0000 [xxu46_10] INFO: Crawled 17 pages (at 1 pages/min), scraped 10 items (at 1 items/min)
2015-04-10 02:50:03+0000 [xxu46_10] INFO: Crawled 17 pages (at 0 pages/min), scraped 10 items (at 0 items/min)
2015-04-10 02:50:11+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1503.07790> (referer: http://arxiv.org/list/cs/15?skip=3000&show=1000)
2015-04-10 02:50:11+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1503.07790>
	{'abstract': u'Zero-shot learning has received increasing interest as a means to alleviate the often prohibitive expense of annotating training data for large scale recognition problems. These methods have achieved great success via learning intermediate semantic representations in the form of attributes and more recently, semantic word vectors. However, they have thus far been constrained to the single-label case, in contrast to the growing popularity and importance of more realistic multi-label data. In this paper, for the first time, we investigate and formalise a general framework for multi-label zero-shot learning, addressing the unique challenge therein: how to exploit multi-label correlation at test time with no training data for those classes? In particular, we propose (1) a multi-output deep regression model to project an image into a semantic word space, which explicitly exploits the correlations in the intermediate semantic layer of word vectors; (2) a novel zero-shot learning algorithm for multi-label data that exploits the unique compositionality property of semantic word vector representations; and (3) a transductive learning strategy to enable the regression model learned from seen classes to generalise well to unseen classes. Our zero-shot learning experiments on a number of standard multi-label datasets demonstrate that our method outperforms a variety of baselines.',
	 'authors': u'Yanwei Fu, Yongxin Yang, Tim Hospedales, Tao Xiang, Shaogang Gong,',
	 'category': u'Computer Science ',
	 'date': '2015-3-26',
	 'pdflink': u'http://arxiv.org/pdf/1503.07790',
	 'subjects': u'Learning (cs.LG)',
	 'title': u'\nTransductive Multi-label Zero-shot Learning',
	 'urllink': u'http://arxiv.org/abs/1503.07790'}
2015-04-10 02:51:03+0000 [xxu46_10] INFO: Crawled 18 pages (at 1 pages/min), scraped 11 items (at 1 items/min)
2015-04-10 02:51:21+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1503.01173> (referer: http://arxiv.org/list/cs/15?skip=2000&show=1000)
2015-04-10 02:51:21+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1503.01173>
	{'abstract': u'The global movement of people and goods has increased the risk of biosecurity threats and their potential to incur large economic, social, and environmental costs. Conventional manual biosecurity surveillance methods are limited by their scalability in space and time. This article focuses on autonomous surveillance systems, comprising sensor networks, robots, and intelligent algorithms, and their applicability to biosecurity threats. We discuss the spatial and temporal attributes of autonomous surveillance technologies and map them to three broad categories of biosecurity threat: (i) vector-borne diseases; (ii) plant pests; and (iii) aquatic pests. Our discussion reveals a broad range of opportunities to serve biosecurity needs through autonomous surveillance.',
	 'authors': u'Raja Jurdak, Alberto Elfes, Branislav Kusy, Ashley Tews, Wen Hu, Emili Hernandez, Navinda Kottege, Pavan Sikka,',
	 'category': u'Computer Science ',
	 'date': '2015-3-4',
	 'pdflink': u'http://arxiv.org/pdf/1503.01173',
	 'subjects': u'Robotics (cs.RO)',
	 'title': u'\nAutonomous surveillance for biosecurity',
	 'urllink': u'http://arxiv.org/abs/1503.01173'}
2015-04-10 02:52:03+0000 [xxu46_10] INFO: Crawled 19 pages (at 1 pages/min), scraped 12 items (at 1 items/min)
2015-04-10 02:52:27+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1502.04341> (referer: http://arxiv.org/list/cs/15?skip=1000&show=1000)
2015-04-10 02:52:27+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1502.04341>
	{'abstract': u'We prove that the height of any algebraic computation tree for deciding membership in a semialgebraic set is bounded from below (up to a multiplicative constant) by the logarithm of m-th Betti number (with respect to singular homology) of the set, divided by m+1. This result complements the well known lower bound by Yao for locally closed semialgebraic sets in terms of the total Borel-Moore Betti number. We also prove that the height is bounded from below by the logarithm of m-th Betti number of a projection of the set onto a coordinate subspace, divided by (m+1)^2. We illustrate these general results by examples of lower complexity bounds for some specific computational problems.',
	 'authors': u'Nicolai Vorobjov, Andrei Gabrielov,',
	 'category': u'Computer Science ',
	 'date': '2015-2-15',
	 'pdflink': u'http://arxiv.org/pdf/1502.04341',
	 'subjects': u'Computational Complexity (cs.CC)',
	 'title': u'\nOn topological lower bounds for algebraic computation trees',
	 'urllink': u'http://arxiv.org/abs/1502.04341'}
2015-04-10 02:53:03+0000 [xxu46_10] INFO: Crawled 20 pages (at 1 pages/min), scraped 13 items (at 1 items/min)
2015-04-10 02:53:24+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1412.2227> (referer: http://arxiv.org/list/cs/14?skip=16000&show=1000)
2015-04-10 02:53:24+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1412.2227>
	{'abstract': u'The reciprocity relations for the relative Fisher information (RFI, hereafter) and a generalized RFI-Euler theorem, are self-consistently derived from the Hellmann-Feynman theorem. These new reciprocity relations generalize the RFI-Euler theorem and constitute the basis for building up a mathematical Legendre transform structure (LTS, hereafter), akin to that of thermodynamics, that underlies the RFI scenario. This demonstrates the possibility of translating the entire mathematical structure of thermodynamics into a RFI-based theoretical framework. Virial theorems play a prominent role in this endeavor, as a Schr "odinger-like equation can be associated to the RFI. Lagrange multipliers are determined invoking the RFI-LTS link and the quantum mechanical virial theorem. An appropriate ansatz allows for the inference of probability density functions (pdf\'s, hereafter) and energy-eigenvalues of the above mentioned Schr "odinger-like equation. The energy-eigenvalues obtained here via inference are benchmarked against established theoretical and numerical results. A principled theoretical basis to reconstruct the RFI-framework from the FIM framework is established. Numerical examples for exemplary cases are provided.',
	 'authors': u'R. C. Venkatesan, A. Plastino,',
	 'category': u'Computer Science ',
	 'date': '2014-12-6',
	 'pdflink': u'http://arxiv.org/pdf/1412.2227',
	 'subjects': u'Statistical Mechanics (cond-mat.stat-mech)',
	 'title': u'\nHellmann-Feynman connection for the relative Fisher information',
	 'urllink': u'http://arxiv.org/abs/1412.2227'}
2015-04-10 02:54:03+0000 [xxu46_10] INFO: Crawled 21 pages (at 1 pages/min), scraped 14 items (at 1 items/min)
2015-04-10 02:54:35+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1501.06851> (referer: http://arxiv.org/list/cs/15?skip=0&show=1000)
2015-04-10 02:54:35+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1501.06851>
	{'abstract': u'LTE release 12 proposes the use of dual connectivity in heterogeneous cellular networks, where a user equipment (UE) maintains parallel connections to a macro-cell node (base station) and to a low-tier node (pico base station or relay). In this paper, we propose a distributed multi-objective power control scheme where each UE independently adapts its transmit power on its dual connections, possibly of unequal bandwidth, with non-ideal backhaul links. In the proposed scheme, the UEs can dynamically switch their objectives between data rate maximization and transmit power minimization as the backhaul load varies. Given the coupling between interference and the backhaul load, we propose a low-overhead convergence mechanism which does not require explicit coordination between autonomous nodes and also derive a closed-form expression of the transmit power levels at equilibrium. We illustrate a higher aggregate end-to-end data rate and significant power saving for our scheme over when the optimization is implemented through a greedy algorithm or when UEs only perform waterfilling.',
	 'authors': u'Syed Amaar Ahmad, Dinesh Datla,',
	 'category': u'Computer Science ',
	 'date': '2015-1-27',
	 'pdflink': u'http://arxiv.org/pdf/1501.06851',
	 'subjects': u'Information Theory (cs.IT)',
	 'title': u'\nDistributed Power Allocations in Heterogeneous Networks with Dual  Connectivity using Backhaul State Information',
	 'urllink': u'http://arxiv.org/abs/1501.06851'}
2015-04-10 02:55:03+0000 [xxu46_10] INFO: Crawled 22 pages (at 1 pages/min), scraped 15 items (at 1 items/min)
2015-04-10 02:55:40+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1412.8313> (referer: http://arxiv.org/list/cs/14?skip=15000&show=1000)
2015-04-10 02:55:40+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1412.8313>
	{'abstract': u'This paper investigates the system achievable rate and optimization for the multiple-input multiple-output (MIMO)-orthogonal frequency division multiplexing (OFDM) system with an energy harvesting (EH) relay. Firstly we propose a time switchingbased relaying (TSR) protocol to enable the simultaneous information processing and energy harvesting at the relay. Then, we discuss its achievable rate performance theoretically and formulated an optimization problem to maximize the system achievable rate. As the problem is difficult to solve, we design an Augmented Lagrangian Penalty Function (ALPF) method for it. Extensive simulation results are provided to demonstrate the accuracy of the analytical results and the effectiveness of the ALPF method.',
	 'authors': u'Guanyao Du, Zhilong Dong, Ke Xiong, Zhengding Qiu,',
	 'category': u'Computer Science ',
	 'date': '2014-12-29',
	 'pdflink': u'http://arxiv.org/pdf/1412.8313',
	 'subjects': u'Information Theory (cs.IT)',
	 'title': u'\nWireless Information and Energy Transfer for Decode-and-Forward Relaying  MIMO-OFDM Networks',
	 'urllink': u'http://arxiv.org/abs/1412.8313'}
2015-04-10 02:56:03+0000 [xxu46_10] INFO: Crawled 23 pages (at 1 pages/min), scraped 16 items (at 1 items/min)
2015-04-10 02:56:34+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1412.1372> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 02:56:34+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1412.1372>
	{'abstract': u'In this paper, the effects of oscillator phase-noise with arbitrary spectral characteristics on self-interference cancellation capability of a full-duplex radio transceiver are addressed, and design considerations are given for oscillator designers for optimized PLL design for full-duplex radio application. The paper first gives a full-duplex transceiver model that inherently mitigates most of the phase-noise effect from the self-interference signal. The remaining effect of the phase noise is then analysed. Closed-form solutions for the self-interference power are then derived. In the simulations part, a practical phase-locked loop type oscillator is used, which is based on the arbitrary mask phase-noise model. Analytical derivations are verified with the simulations, and the self-interference cancellation performance is thoroughly studied with various parameters. Design considerations are finally given for oscillator design for full-duplex radio transceivers, with the help of tangible parameters of the phase-locked loop type oscillators.',
	 'authors': u'Ville Syrj\xe4l\xe4, Koji Yamamoto, Mikko Valkama,',
	 'category': u'Computer Science ',
	 'date': '2014-12-3',
	 'pdflink': u'http://arxiv.org/pdf/1412.1372',
	 'subjects': u'Information Theory (cs.IT)',
	 'title': u'\nAnalysis and Design Specifications for Full-Duplex Radio Transceivers  under RF Oscillator Phase-Noise with Arbitrary Spectral Shape',
	 'urllink': u'http://arxiv.org/abs/1412.1372'}
2015-04-10 02:57:03+0000 [xxu46_10] INFO: Crawled 24 pages (at 1 pages/min), scraped 17 items (at 1 items/min)
2015-04-10 02:57:44+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1412.2122> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 02:57:44+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1412.2122>
	{'abstract': u'In this paper we present a non-invasive ambient intelligence framework for the semi-automatic analysis of non-verbal communication applied to the restorative justice field. In particular, we propose the use of computer vision and social signal processing technologies in real scenarios of Victim-Offender Mediations, applying feature extraction techniques to multi-modal audio-RGB-depth data. We compute a set of behavioral indicators that define communicative cues from the fields of psychology and observational methodology. We test our methodology on data captured in real world Victim-Offender Mediation sessions in Catalonia in collaboration with the regional government. We define the ground truth based on expert opinions when annotating the observed social responses. Using different state-of-the-art binary classification approaches, our system achieves recognition accuracies of 86% when predicting satisfaction, and 79% when predicting both agreement and receptivity. Applying a regression strategy, we obtain a mean deviation for the predictions between 0.5 and 0.7 in the range [1-5] for the computed social signals.',
	 'authors': u'V\xedctor Ponce-L\xf3pez, Sergio Escalera, Marc P\xe9rez, Oriol Jan\xe9s, Xavier Bar\xf3,',
	 'category': u'Computer Science ',
	 'date': '2014-11-25',
	 'pdflink': u'http://arxiv.org/pdf/1412.2122',
	 'subjects': u'Human-Computer Interaction (cs.HC)',
	 'title': u'\nNon-Verbal Communication Analysis in Victim-Offender Mediations',
	 'urllink': u'http://arxiv.org/abs/1412.2122'}
2015-04-10 02:58:03+0000 [xxu46_10] INFO: Crawled 25 pages (at 1 pages/min), scraped 18 items (at 1 items/min)
2015-04-10 02:58:47+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1412.2118> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 02:58:47+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1412.2118>
	{'abstract': u'We study normalisation of multistep strategies, strategies that reduce a set of redexes at a time, focussing on the notion of necessary sets, those which contain at least one redex that cannot be avoided in order to reach a normal form. This is particularly appealing in the setting of non-sequential rewrite systems, in which terms that are not in normal form may not have any emph redex. We first prove a normalisation theorem for abstract rewrite systems or ARS, a general rewriting framework encompassing many rewriting systems developed by P-A.Mellies. The theorem states that multistep strategies reducing so called necessary and non-gripping sets of redexes at a time are normalising in any ARS. Gripping refers to an abstract property reflecting the behavior of higher-order substitution. We then apply this result to the particular case of the Pure Pattern Calculus, a calculus of patterns and to the lambda-calculus with parallel-or.',
	 'authors': u'Eduardo Bonelli, Delia Kesner, Carlos Lombardi, Alejandro Rios,',
	 'category': u'Computer Science ',
	 'date': '2014-12-5',
	 'pdflink': u'http://arxiv.org/pdf/1412.2118',
	 'subjects': u'Logic in Computer Science (cs.LO)',
	 'title': u'\nAn abstract normalisation result with applications to non-sequential  calculi',
	 'urllink': u'http://arxiv.org/abs/1412.2118'}
2015-04-10 02:59:03+0000 [xxu46_10] INFO: Crawled 26 pages (at 1 pages/min), scraped 19 items (at 1 items/min)
2015-04-10 03:00:00+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1412.2115> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 03:00:00+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1412.2115>
	{'abstract': u'In recent years, crowdfunding has become a popular method of funding new technology or entertainment products, or artistic projects. The idea is that people or projects ask for many small donations from individuals who support the proposed work, rather than a large amount from a single source. Crowdfunding is usually done via an online portal or platform which handles the financial transactions involved. The Universe Awareness (UNAWE) programme decided to undertake a Kickstarter crowdfunding campaign centring on the resource Universe in a Box2. In this article we present the lessons learned and best practices from that campaign.',
	 'authors': u'Abi J. Ashton, Pedro Russo, Thilina Heenatigala,',
	 'category': u'Computer Science ',
	 'date': '2014-12-4',
	 'pdflink': u'http://arxiv.org/pdf/1412.2115',
	 'subjects': u'Computers and Society (cs.CY)',
	 'title': u'\nCrowdfunding Astronomy Outreach Projects: Lessons Learned from the UNAWE  Crowdfunding Campaign',
	 'urllink': u'http://arxiv.org/abs/1412.2115'}
2015-04-10 03:00:03+0000 [xxu46_10] INFO: Crawled 27 pages (at 1 pages/min), scraped 20 items (at 1 items/min)
2015-04-10 03:00:57+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1412.2114> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 03:00:57+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1412.2114>
	{'abstract': u'We propose a new approach for solving combinatorial optimization problem by utilizing the mechanism of chases and escapes, which has a long history in mathematics. In addition to the well-used steepest descent and neighboring search, we perform a chase and escape game on the "landscape" of the cost function. We have created a concrete algorithm for the Traveling Salesman Problem. Our preliminary test indicates a possibility that this new fusion of chases and escapes problem into combinatorial optimization search is fruitful.',
	 'authors': u'Toru Ohira,',
	 'category': u'Computer Science ',
	 'date': '2014-12-1',
	 'pdflink': u'http://arxiv.org/pdf/1412.2114',
	 'subjects': u'Artificial Intelligence (cs.AI)',
	 'title': u'\nChases and Escapes, and Optimization Problems',
	 'urllink': u'http://arxiv.org/abs/1412.2114'}
2015-04-10 03:01:03+0000 [xxu46_10] INFO: Crawled 28 pages (at 1 pages/min), scraped 21 items (at 1 items/min)
2015-04-10 03:02:03+0000 [xxu46_10] INFO: Crawled 28 pages (at 0 pages/min), scraped 21 items (at 0 items/min)
2015-04-10 03:02:13+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1412.2109> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 03:02:13+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1412.2109>
	{'abstract': u'While algebrisation constitutes a powerful technique in the design and analysis of centralised algorithms, to date there have been hardly any applications of algebraic techniques in the context of distributed graph algorithms. This work is a case study that demonstrates the potential of algebrisation in the distributed context. We will focus on distributed graph algorithms in the congested clique model; the graph problems that we will consider include, e.g., the triangle detection problem and the all-pairs shortest path problem (APSP). There is plenty of prior work on combinatorial algorithms in the congested clique model: for example, Dolev et al. (DISC 2012) gave an algorithm for triangle detection with a running time of , and Nanongkai (STOC 2014) gave an approximation algorithm for APSP with a running time of . In this work, we will use algebraic techniques -- in particular, algorithms based on fast matrix multiplication -- to solve both triangle detection and the unweighted APSP in time ; for weighted APSP, we give a -approximation with this running time, as well as an exact solution.',
	 'authors': u'Petteri Kaski, Janne H. Korhonen, Christoph Lenzen, Jukka Suomela,',
	 'category': u'Computer Science ',
	 'date': '2014-12-5',
	 'pdflink': u'http://arxiv.org/e-print/1412.2109',
	 'subjects': u'Distributed, Parallel, and Cluster Computing (cs.DC)',
	 'title': u'\nAlgebrisation in Distributed Graph Algorithms: Fast Matrix  Multiplication in the Congested Clique',
	 'urllink': u'http://arxiv.org/abs/1412.2109'}
2015-04-10 03:03:03+0000 [xxu46_10] INFO: Crawled 29 pages (at 1 pages/min), scraped 22 items (at 1 items/min)
2015-04-10 03:03:11+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1412.2106> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 03:03:11+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1412.2106>
	{'abstract': u'In this paper, we theoretically justify an approach popular among participants of the Higgs Boson Machine Learning Challenge to optimize approximate median significance (AMS). The approach is based on the following two-stage procedure. First, a real-valued function is learned by minimizing a surrogate loss for binary classification, such as logistic loss, on the training sample. Then, a threshold is tuned on a separate validation sample, by direct optimization of AMS. We show that the regret of the resulting (thresholded) classifier measured with respect to the squared AMS, is upperbounded by the regret of the underlying real-valued function measured with respect to the logistic loss. Hence, we prove that minimizing logistic surrogate is a consistent method of optimizing AMS.',
	 'authors': u'Wojciech Kot\u0142owski,',
	 'category': u'Computer Science ',
	 'date': '2014-12-5',
	 'pdflink': u'http://arxiv.org/pdf/1412.2106',
	 'subjects': u'Learning (cs.LG)',
	 'title': u'\nConsistent optimization of AMS by logistic loss minimization',
	 'urllink': u'http://arxiv.org/abs/1412.2106'}
2015-04-10 03:04:03+0000 [xxu46_10] INFO: Crawled 30 pages (at 1 pages/min), scraped 23 items (at 1 items/min)
2015-04-10 03:04:13+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1412.2105> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 03:04:13+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1412.2105>
	{'abstract': u'In Martin-L "of\'s Intensional Type Theory, identity type is a heavily used and studied concept. The reason for that is the fact that it\'s responsible for the recently discovered connection between Type Theory and Homotopy Theory. The main problem with identity types, as originally formulated, is that they are complex to understand and use. Using that fact as motivation, a much simpler formulation for the identity type was proposed by Queiroz &amp; Gabbay (1994) and further developed by de Queiroz &amp; de Oliveira (2013). In this formulation, an element of an identity type is seen as a sequence of rewrites (or computational paths). Together with the logical rules of this new entity, there exists a system of reduction rules between sequence of rewrites called LND_-RWS. This system is constructed using the labelled natural deduction (i.e. Prawitz\' Natural Deduction plus derivations-as-terms) and is responsible for establishing how a sequence of rewrites can be rewritten, resulting in a new sequence of rewrites. In this context, we propose a categorical interpretation for this new entity, using the types as objects and the rules of rewrites as morphisms. Moreover, we show that our interpretation is in accordance with some known results, like that types have a groupoidal structure. We also interpret more complicated structures, like the one formed by a rewrite of a sequence of rewrites.',
	 'authors': u'Arthur Ramos, Ruy J. G. B. de Queiroz, Anjolina G. de Oliveira,',
	 'category': u'Computer Science ',
	 'date': '2014-12-5',
	 'pdflink': u'http://arxiv.org/pdf/1412.2105',
	 'subjects': u'Logic in Computer Science (cs.LO)',
	 'title': u'\nSequences of Rewrites: A Categorical Interpretation',
	 'urllink': u'http://arxiv.org/abs/1412.2105'}
2015-04-10 03:05:03+0000 [xxu46_10] INFO: Crawled 31 pages (at 1 pages/min), scraped 24 items (at 1 items/min)
2015-04-10 03:05:09+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1412.2087> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 03:05:09+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1412.2087>
	{'abstract': u'Although the Poisson point process (PPP) has been widely used to model base station (BS) locations in cellular networks, it is an idealized model that neglects the spatial correlation among BSs. The present paper proposes the use of determinantal point process (DPP) to take into account these correlations; in particular the repulsiveness among macro base station locations. DPPs are demonstrated to be analytically tractable by leveraging several unique computational properties. Specifically, we show that the empty space function, the nearest neighbor function, the mean interference and the signal-to-interference ratio (SIR) distribution have explicit analytical representations and can be numerically evaluated for cellular networks with DPP configured BSs. In addition, the modeling accuracy of DPPs is investigated by fitting three DPP models to real BS location data sets from two major U.S. cities. Using hypothesis testing for various performance metrics of interest, we show that these fitted DPPs are significantly more accurate than popular choices such as the PPP and the perturbed hexagonal grid model.',
	 'authors': u'Yingzhe Li, Fran\xe7ois Baccelli, Harpreet S. Dhillon, Jeffrey G. Andrews,',
	 'category': u'Computer Science ',
	 'date': '2014-12-5',
	 'pdflink': u'http://arxiv.org/pdf/1412.2087',
	 'subjects': u'Information Theory (cs.IT)',
	 'title': u'\nStatistical Modeling and Probabilistic Analysis of Cellular Networks  with Determinantal Point Processes',
	 'urllink': u'http://arxiv.org/abs/1412.2087'}
2015-04-10 03:06:00+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1412.2078> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 03:06:00+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1412.2078>
	{'abstract': u'Daring predictions of the proximate future can establish shared discursive frameworks, mobilize capital, and steer complex processes. Among the prophetic visions that encouraged and accompanied the development of new communication technologies was the "Digital Earth," described in a 1998 speech by Al Gore as a high-resolution representation of the planet to share and analyze detailed information about its state. This article traces a genealogy of the Digital Earth as a techno-scientific myth, locating it in a constellation of media futures, arguing that a common subtext of these envisionments consists of a dream of wholeness, an afflatus to overcome perceived fragmentation among humans, and between humans and the Earth.',
	 'authors': u'Andrea Ballatore,',
	 'category': u'Computer Science ',
	 'date': '2014-12-3',
	 'pdflink': u'http://arxiv.org/pdf/1412.2078',
	 'subjects': u'Computers and Society (cs.CY)',
	 'title': u'\nThe myth of the Digital Earth between fragmentation and wholeness',
	 'urllink': u'http://arxiv.org/abs/1412.2078'}
2015-04-10 03:06:03+0000 [xxu46_10] INFO: Crawled 33 pages (at 2 pages/min), scraped 26 items (at 2 items/min)
2015-04-10 03:06:56+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1412.2070> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 03:06:56+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1412.2070>
	{'abstract': u'People treat smartphones as a second skin, having them around nearly 24/7 and constantly interacting with them. Although smartphones are used mainly for personal communication, social networking and web browsing, they have many connectivity capabilities, and are at the same time equipped with a wide range of embedded sensors. Additionally, bluetooth connectivity can be leveraged to collect data from external sensors, greatly extending the sensing capabilities. However, massive data-gathering using smartphones still poses many architectural challenges, such as limited battery and processing power, and possibly connectivity costs. This article describes SenseMyCity (SMC), an Internet of Things mobile urban sensor that is extensible and fully configurable. The platform consists of an app, a backoffice and a frontoffice. The SMC app can collect data from embedded sensors, like GPS, wifi, accelerometer, magnetometer, etc, as well as from external bluetooth sensors, ranging from On-Board Diagnostics gathering data from vehicles, to wearable cardiac sensors. Adding support for new internal or external sensors is straightforward due to the modular architecture. Data transmission to our servers can occur either on-demand or in real-time, while keeping costs down by only using the configured type of Internet connectivity. We discuss our experience implementing the platform and using it to make longitudinal studies with many users. Further, we present results on bandwidth utilization and energy consumption for different sensors and sampling rates. Finally, we show two use cases: mapping fuel consumption and user stress extracted from cardiac sensors.',
	 'authors': u'Jo\xe3o G. P. Rodrigues, Ana Aguiar, Jo\xe3o Barros,',
	 'category': u'Computer Science ',
	 'date': '2014-12-5',
	 'pdflink': u'http://arxiv.org/pdf/1412.2070',
	 'subjects': u'Computers and Society (cs.CY)',
	 'title': u'\nSenseMyCity: Crowdsourcing an Urban Sensor',
	 'urllink': u'http://arxiv.org/abs/1412.2070'}
2015-04-10 03:07:03+0000 [xxu46_10] INFO: Crawled 34 pages (at 1 pages/min), scraped 27 items (at 1 items/min)
2015-04-10 03:08:03+0000 [xxu46_10] INFO: Crawled 34 pages (at 0 pages/min), scraped 27 items (at 0 items/min)
2015-04-10 03:08:08+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1412.2067> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 03:08:08+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1412.2067>
	{'abstract': u'We present a method for improving a Non Local Means operator by computing its low-rank approximation. The low-rank operator is constructed by applying a filter to the spectrum of the original Non Local Means operator. This results in an operator which is less sensitive to noise while preserving important properties of the original operator. The method is efficiently implemented based on Chebyshev polynomials and is demonstrated on the application of natural images denoising. For this application, we provide a comprehensive comparison of our method with leading denoising methods.',
	 'authors': u'Victor May, Yosi Keller, Nir Sharon, Yoel Shkolnisky,',
	 'category': u'Computer Science ',
	 'date': '2014-11-20',
	 'pdflink': u'http://arxiv.org/pdf/1412.2067',
	 'subjects': u'Computer Vision and Pattern Recognition (cs.CV)',
	 'title': u'\nAn algorithm for improving Non-Local Means operators via low-rank  approximation',
	 'urllink': u'http://arxiv.org/abs/1412.2067'}
2015-04-10 03:09:03+0000 [xxu46_10] INFO: Crawled 35 pages (at 1 pages/min), scraped 28 items (at 1 items/min)
2015-04-10 03:09:13+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1412.2066> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 03:09:13+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1412.2066>
	{'abstract': u'We describe a model for multi-target tracking based on associating collections of candidate detections across frames of a video. In order to model pairwise interactions between different tracks, such as suppression of overlapping tracks and contextual cues about co-occurence of different objects, we augment a standard min-cost flow objective with quadratic terms between detection variables. We learn the parameters of this model using structured prediction and a loss function which approximates the multi-target tracking accuracy. We evaluate two different approaches to finding an optimal set of tracks under model objective based on an LP relaxation and a novel greedy extension to dynamic programming that handles pairwise interactions. We find the greedy algorithm achieves equivalent performance to the LP relaxation while being 2-7x faster than a commercial solver. The resulting model with learned parameters outperforms existing methods across several categories on the KITTI tracking benchmark.',
	 'authors': u'Shaofei Wang, Charless C. Fowlkes,',
	 'category': u'Computer Science ',
	 'date': '2014-12-5',
	 'pdflink': u'http://arxiv.org/pdf/1412.2066',
	 'subjects': u'Computer Vision and Pattern Recognition (cs.CV)',
	 'title': u'\nLearning Multi-target Tracking with Quadratic Object Interactions',
	 'urllink': u'http://arxiv.org/abs/1412.2066'}
2015-04-10 03:09:48+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1412.2064> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 03:09:48+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1412.2064>
	{'abstract': u'The use of multivalued controls derived from a special maximal monotone operator are studied in this note. Starting with a strictly passive linear system (with possible parametric uncertainty and external disturbances) a multivalued control law is derived, ensuring regulation of the output to a desired value. The methodology used falls in a passivity-based control context, where we study how the multivalued control affects the dissipation equation of the closed-loop system, from which we derive its robustness properties. Finally, some numerical examples together with implementation issues are presented to support the main result.',
	 'authors': u'F\xe9lix A. Miranda, Fernando Casta\xf1os,',
	 'category': u'Computer Science ',
	 'date': '2014-12-5',
	 'pdflink': u'http://arxiv.org/pdf/1412.2064',
	 'subjects': u'Systems and Control (cs.SY)',
	 'title': u'\nRobust Output Regulation of Linear Passive Systems with Multivalued  Upper Semicontinuous Controls',
	 'urllink': u'http://arxiv.org/abs/1412.2064'}
2015-04-10 03:10:03+0000 [xxu46_10] INFO: Crawled 37 pages (at 2 pages/min), scraped 30 items (at 2 items/min)
2015-04-10 03:10:39+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1412.2062> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 03:10:39+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1412.2062>
	{'abstract': u"A fundamental decision faced by a firm hiring employees - and a familiar one to anyone who has dealt with the academic job market, for example - is deciding what caliber of candidates to pursue. Should the firm try to increase its reputation by making offers to higher-quality candidates, despite the risk that the candidates might reject the offers and leave the firm empty-handed? Or should it concentrate on weaker candidates who are more likely to accept the offer? The question acquires an added level of complexity once we take into account the effect one hiring cycle has on the next: hiring better employees in the current cycle increases the firm's reputation, which in turn increases its attractiveness for higher-quality candidates in the next hiring cycle. These considerations introduce an interesting temporal dynamic aspect to the rich line of research on matching models for job markets, in which long-range planning and evolving reputational effects enter into the strategic decisions made by competing firms. We develop a model based on two competing firms to try capturing as cleanly as possible the elements that we believe constitute the strategic tension at the core of the problem: the trade-off between short-term recruiting success and long-range reputation-building; the inefficiency that results from underemployment of people who are not ranked highest; and the influence of earlier accidental outcomes on long-term reputations. Our model exhibits all these phenomena in a stylized setting, governed by a parameter q that captures the difference in strength between the two top candidates in each hiring cycle. We show that when q is relatively low the efficiency of the job market is improved by long-range reputational effects, but when q is relatively high, taking future reputations into account can sometimes reduce the efficiency.",
	 'authors': u'Jon Kleinberg, Sigal Oren,',
	 'category': u'Computer Science ',
	 'date': '2014-12-5',
	 'pdflink': u'http://arxiv.org/pdf/1412.2062',
	 'subjects': u'Computer Science and Game Theory (cs.GT)',
	 'title': u'\nDynamic Models of Reputation and Competition in Job-Market Matching',
	 'urllink': u'http://arxiv.org/abs/1412.2062'}
2015-04-10 03:11:03+0000 [xxu46_10] INFO: Crawled 38 pages (at 1 pages/min), scraped 31 items (at 1 items/min)
2015-04-10 03:11:52+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1412.2013> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 03:11:52+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1412.2013>
	{'abstract': u'In this work, we propose online traffic engineering as a novel approach to detect and mitigate an emerging class of stealthy Denial of Service (DoS) link-flooding attacks. Our approach exploits the Software Defined Networking (SDN) paradigm, which renders the management of network traffic more flexible through centralised flow-level control and monitoring. We implement a full prototype of our solution on an emulated SDN environment using OpenFlow to interface with the network devices. We further discuss useful insights gained from our preliminary experiments as well as a number of open research questions which constitute work in progress.',
	 'authors': u'Dimitrios Gkounis, Vasileios Kotronis, Xenofontas Dimitropoulos,',
	 'category': u'Computer Science ',
	 'date': '2014-12-5',
	 'pdflink': u'http://arxiv.org/pdf/1412.2013',
	 'subjects': u'Networking and Internet Architecture (cs.NI)',
	 'title': u'\nTowards Defeating the Crossfire Attack using SDN',
	 'urllink': u'http://arxiv.org/abs/1412.2013'}
2015-04-10 03:12:03+0000 [xxu46_10] INFO: Crawled 39 pages (at 1 pages/min), scraped 32 items (at 1 items/min)
2015-04-10 03:12:34+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1412.2007> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 03:12:34+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1412.2007>
	{'abstract': u'Neural machine translation, a recently proposed approach to machine translation based purely on neural networks, has shown promising results compared to the existing approaches such as phrase-based statistical machine translation. Despite its recent success, neural machine translation has its limitation in handling a larger vocabulary, as training complexity as well as decoding complexity increase proportionally to the number of target words. In this paper, we propose a method that allows us to use a very large target vocabulary without increasing training complexity, based on importance sampling. We show that decoding can be efficiently done even with the model having a very large target vocabulary by selecting only a small subset of the whole target vocabulary. The models trained by the proposed approach are empirically found to outperform the baseline models with a small vocabulary as well as the LSTM-based neural machine translation models. Furthermore, when we use the ensemble of a few models with very large target vocabularies, we achieve the state-of-the-art translation performance (measured by BLEU) on the English-&gt;German translation and almost as high performance as state-of-the-art English-&gt;French translation system.',
	 'authors': u'S\xe9bastien Jean, Kyunghyun Cho, Roland Memisevic, Yoshua Bengio,',
	 'category': u'Computer Science ',
	 'date': '2014-12-5',
	 'pdflink': u'http://arxiv.org/pdf/1412.2007',
	 'subjects': u'Computation and Language (cs.CL)',
	 'title': u'\nOn Using Very Large Target Vocabulary for Neural Machine Translation',
	 'urllink': u'http://arxiv.org/abs/1412.2007'}
2015-04-10 03:13:03+0000 [xxu46_10] INFO: Crawled 40 pages (at 1 pages/min), scraped 33 items (at 1 items/min)
2015-04-10 03:13:37+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1412.2005> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 03:13:37+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1412.2005>
	{'abstract': u'The generalized approximate message passing (GAMP) algorithm is an efficient method of MAP or approximate-MMSE estimation of observed from a noisy version of the transform coefficients . In fact, for large zero-mean i.i.d sub-Gaussian , GAMP is characterized by a state evolution whose fixed points, when unique, are optimal. For generic , however, GAMP may diverge. In this paper, we propose adaptive damping and mean-removal strategies that aim to prevent divergence. Numerical results demonstrate significantly enhanced robustness to non-zero-mean, rank-deficient, column-correlated, and ill-conditioned .',
	 'authors': u'Jeremy Vila, Philip Schniter, Sundeep Rangan, Florent Krzakala, Lenka Zdeborova,',
	 'category': u'Computer Science ',
	 'date': '2014-12-5',
	 'pdflink': u'http://arxiv.org/pdf/1412.2005',
	 'subjects': u'Information Theory (cs.IT)',
	 'title': u'\nAdaptive Damping and Mean Removal for the Generalized Approximate  Message Passing Algorithm',
	 'urllink': u'http://arxiv.org/abs/1412.2005'}
2015-04-10 03:14:03+0000 [xxu46_10] INFO: Crawled 41 pages (at 1 pages/min), scraped 34 items (at 1 items/min)
2015-04-10 03:14:46+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1412.1993> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 03:14:46+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1412.1993>
	{'abstract': u'This paper studies networks with N half-duplex relays assisting the communication between a source and a destination. In ISIT\'12 Brahma, "zg "r and Fragouli conjectured that in Gaussian half-duplex diamond networks (i.e., without a direct link between the source and the destination, and with N non-interfering relays) an approximately optimal relay scheduling policy (i.e., achieving the cut-set upper bound to within a constant gap) has at most N+1 active states (i.e., at most N+1 out of the possible relay listen-transmit states have a strictly positive probability). Such relay scheduling policies were referred to as simple. In ITW\'13 we conjectured that simple approximately optimal relay scheduling policies exist for any Gaussian half-duplex multi-relay network irrespectively of the topology. This paper formally proves this more general version of the conjecture and shows it holds beyond Gaussian noise networks. In particular, for any memoryless half-duplex N-relay network with independent noises and for which independent inputs are approximately optimal in the cut-set upper bound, an approximately optimal simple relay scheduling policy exists. A convergent iterative polynomial-time algorithm, which alternates between minimizing a submodular function and maximizing a linear program, is proposed to find the approximately optimal simple relay schedule. As an example, for N-relay Gaussian networks with independent noises, where each node in equipped with multiple antennas and where each antenna can be configured to listen or transmit irrespectively of the others, the existence of an approximately optimal simple relay scheduling policy with at most N+1 active states is proved. Through a line-network example it is also shown that independently switching the antennas at each relay can provide a strictly larger multiplexing gain compared to using the antennas for the same purpose.',
	 'authors': u'Martina Cardone, Daniela Tuninetti, Raymond Knopp,',
	 'category': u'Computer Science ',
	 'date': '2014-12-5',
	 'pdflink': u'http://arxiv.org/pdf/1412.1993',
	 'subjects': u'Information Theory (cs.IT)',
	 'title': u'\nOn the Optimality of Simple Schedules for Networks with Multiple  Half-Duplex Relays',
	 'urllink': u'http://arxiv.org/abs/1412.1993'}
2015-04-10 03:15:03+0000 [xxu46_10] INFO: Crawled 42 pages (at 1 pages/min), scraped 35 items (at 1 items/min)
2015-04-10 03:15:53+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1412.1990> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 03:15:53+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1412.1990>
	{'abstract': u'We study asymptotic dynamical patterns that emerge among a set of nodes interacting in a dynamically evolving signed random network, where positive links carry out standard consensus and negative links induce relative-state flipping. A sequence of deterministic signed graphs define potential node interactions that take place independently. Each node receives a positive recommendation consistent with the standard consensus algorithm from its positive neighbors, and a negative recommendation defined by relative-state flipping from its negative neighbors. After receiving these recommendations, each node puts a deterministic weight to each recommendation, and then encodes these weighted recommendations in its state update through stochastic attentions defined by two Bernoulli random variables. We establish a number of conditions regarding almost sure convergence and divergence of the node states. We also propose a condition for almost sure state clustering for essentially weakly balanced graphs, with the help of several martingale convergence lemmas. Some fundamental differences on the impact of the deterministic weights and stochastic attentions to the node state evolution are highlighted between the current relative-state-flipping model and the state-flipping model considered in Altafini 2013 and Shi et al. 2014.',
	 'authors': u'Guodong Shi, Alexandre Proutiere, Mikael Johansson, John. S. Baras, Karl H. Johansson,',
	 'category': u'Computer Science ',
	 'date': '2014-12-5',
	 'pdflink': u'http://arxiv.org/pdf/1412.1990',
	 'subjects': u'Social and Information Networks (cs.SI)',
	 'title': u'\nEmergent Behaviors over Signed Random Dynamical Networks:  Relative-State-Flipping Model',
	 'urllink': u'http://arxiv.org/abs/1412.1990'}
2015-04-10 03:16:03+0000 [xxu46_10] INFO: Crawled 43 pages (at 1 pages/min), scraped 36 items (at 1 items/min)
2015-04-10 03:16:41+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1412.1964> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 03:16:41+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1412.1964>
	{'abstract': u'We consider the problem of erasure/list decoding using certain classes of simplified decoders. Specifically, we assume a class of erasure/list decoders, such that a codeword is in the list if its likelihood is larger than a threshold. This class of decoders both approximates the optimal decoder of Forney, and also includes the following simplified subclasses of decoding rules: The first is a function of the output vector only, but not the codebook (which is most suitable for high rates), and the second is a scaled version of the maximum likelihood decoder (which is most suitable for low rates). We provide single-letter expressions for the exact random coding exponents of any decoder in these classes, operating over a discrete memoryless channel. For each class of decoders, we find the optimal decoder within the class, in the sense that it maximizes the erasure/list exponent, under a given constraint on the error exponent. We establish the optimality of the simplified decoders of the first and second kind for low and high rates, respectively.',
	 'authors': u'Nir Weinberger, Neri Merhav,',
	 'category': u'Computer Science ',
	 'date': '2014-12-5',
	 'pdflink': u'http://arxiv.org/pdf/1412.1964',
	 'subjects': u'Information Theory (cs.IT)',
	 'title': u'\nSimplified Erasure/List Decoding',
	 'urllink': u'http://arxiv.org/abs/1412.1964'}
2015-04-10 03:17:03+0000 [xxu46_10] INFO: Crawled 44 pages (at 1 pages/min), scraped 37 items (at 1 items/min)
2015-04-10 03:17:31+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1412.1961> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 03:17:31+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1412.1961>
	{'abstract': u'This paper presents an approach for defining Unmanned Aerial Vehicle (UAV) missions on a high level. Current methods for UAV mission specification are evaluated and their deficiencies are analyzed. From these findings, a new graphical specification language for UAV missions is proposed, which is targeted towards typical UAV users from various domains rather than computer science experts. The research is ongoing, but a first prototype is presented.',
	 'authors': u'Benjamin Schwartz, Ludwig N\xe4gele, Andreas Angerer, Bruce A. MacDonald,',
	 'category': u'Computer Science ',
	 'date': '2014-12-5',
	 'pdflink': u'http://arxiv.org/pdf/1412.1961',
	 'subjects': u'Robotics (cs.RO)',
	 'title': u'\nTowards a graphical language for quadrotor missions',
	 'urllink': u'http://arxiv.org/abs/1412.1961'}
2015-04-10 03:18:03+0000 [xxu46_10] INFO: Crawled 45 pages (at 1 pages/min), scraped 38 items (at 1 items/min)
2015-04-10 03:18:33+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1412.1957> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 03:18:33+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1412.1957>
	{'abstract': u'Feature or interest points typically use information aggregation in 2D patches which does not remain stable at object boundaries when there is object motion against a significantly varying background. Level or iso-intensity curves are much more stable under such conditions, especially the longer ones. In this paper, we identify stable portions on long iso-curves and detect corners on them. Further, the iso-curve associated with a corner is used to discard portions from the background and improve matching. Such CoMIC (Corners on Maximally-stable Iso-intensity Curves) points yield superior results at the object boundary regions compared to state-of-the-art detectors while performing comparably at the interior regions as well. This is illustrated in exhaustive matching experiments for both boundary and non-boundary regions in applications such as stereo and point tracking for structure from motion in video sequences.',
	 'authors': u'Swarna Kamlam Ravindran, Anurag Mittal,',
	 'category': u'Computer Science ',
	 'date': '2014-12-5',
	 'pdflink': u'http://arxiv.org/pdf/1412.1957',
	 'subjects': u'Computer Vision and Pattern Recognition (cs.CV)',
	 'title': u'\nCoMIC: Good features for detection and matching at object boundaries',
	 'urllink': u'http://arxiv.org/abs/1412.1957'}
2015-04-10 03:19:03+0000 [xxu46_10] INFO: Crawled 46 pages (at 1 pages/min), scraped 39 items (at 1 items/min)
2015-04-10 03:19:50+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1412.1952> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 03:19:50+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1412.1952>
	{'abstract': u'Vehicular energy network (VEN) is a vehicular network which can transport energy over a large geographical area by means of electric vehicles (EVs). In the near future, an abundance of EVs, plentiful generation of the renewables, and mature wireless energy transfer and vehicular communication technologies will expedite the realization of VEN. To transmit energy from a source to a destination, we need to establish energy paths, which are composed of segments of vehicular routes, while satisfying various design objectives. In this paper, we develop a method to construct all energy paths for a particular energy source-destination pair, followed by some analytical results of the method. We describe how to utilize the energy paths to develop optimization models for different design goals and propose two solutions. We also develop a heuristic for the power loss minimization problem. We compare the performance of the three solution methods with artificial and real-world traffic networks and provide a comprehensive comparison in terms of solution quality, computation time, solvable problem size, and applicability. This paper lays the foundations of VEN routing.',
	 'authors': u'Albert Y.S. Lam, Victor O.K. Li,',
	 'category': u'Computer Science ',
	 'date': '2014-12-5',
	 'pdflink': u'http://arxiv.org/pdf/1412.1952',
	 'subjects': u'Systems and Control (cs.SY)',
	 'title': u'\nOpportunistic Routing for the Vehicular Energy Network',
	 'urllink': u'http://arxiv.org/abs/1412.1952'}
2015-04-10 03:20:03+0000 [xxu46_10] INFO: Crawled 47 pages (at 1 pages/min), scraped 40 items (at 1 items/min)
2015-04-10 03:20:42+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1412.1947> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 03:20:42+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1412.1947>
	{'abstract': u'The problem of automatically clustering data is an age old problem. People have created numerous algorithms to tackle this problem. The execution time of any of this algorithm grows with the number of input points and the number of cluster centers required. To reduce the number of input points we could average the points locally and use the means or the local centers as the input for clustering. However since the required number of local centers is very high, running the clustering algorithm on the entire dataset to obtain these representational points is very time consuming. To remedy this problem, in this paper we are proposing two subclustering schemes where by we subdivide the dataset into smaller sets and run the clustering algorithm on the smaller datasets to obtain the required number of datapoints to run our clustering algorithm with. As we are subdividing the given dataset, we could run clustering algorithm on each smaller piece of the dataset in parallel. We found that both parallel and serial execution of this method to be much faster than the original clustering algorithm and error in running the clustering algorithm on a reduced set to be very less.',
	 'authors': u'Aditya AV Sastry, Kalyan Netti,',
	 'category': u'Computer Science ',
	 'date': '2014-12-5',
	 'pdflink': u'http://arxiv.org/pdf/1412.1947',
	 'subjects': u'Learning (cs.LG)',
	 'title': u'\nA parallel sampling based clustering',
	 'urllink': u'http://arxiv.org/abs/1412.1947'}
2015-04-10 03:21:03+0000 [xxu46_10] INFO: Crawled 48 pages (at 1 pages/min), scraped 41 items (at 1 items/min)
2015-04-10 03:22:01+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1412.1945> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 03:22:01+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1412.1945>
	{'abstract': u'By assuming that the most frequently occuring color in a video or a region of a video I propose a new algorithm for detecting foreground objects in a video. The process of detecting the foreground objects is complicated because of the fact that there may be swaying trees, objects of the background being moved around or lighting changes in the video. To deal with such complexities many have come up with solutions which heavily rely on expensive floating point operations. In this paper I used a data structure called Octree which is implemented only using binary operations. Traditionally octrees were used for color quantization but here in this paper I used it as a data structure to store the most frequently occuring colors in a video as well. For each of the starting few video frames, I constructed a Octree using all the colors of that frame. Next I pruned all the trees by removing nodes below a certain height and gave the leaf nodes a color which is dependant on the topological path from that node to its parent. Hence any two leaf nodes in two different octrees with the same topological path from themselves to the root will represent the same color. Next I merged all these individual trees into a single tree retaining only those nodes whose topological path to itself from the root is most common among all the trees. The colors represented by the leaf nodes in the resultant tree will be the most frequently occuring colors in the starting video frames of the video. Hence any color of an incomming frame that is not close to any of the colors represented by the leaf node of the merged tree can be regarded as belonging to a foreground object. As an Octree is constructed using only binary operations, it is very fast compared to other leading algorithms.',
	 'authors': u'Aditya A.V. Sastry,',
	 'category': u'Computer Science ',
	 'date': '2014-12-5',
	 'pdflink': u'http://arxiv.org/pdf/1412.1945',
	 'subjects': u'Computer Vision and Pattern Recognition (cs.CV)',
	 'title': u'\nBackground Modelling using Octree Color Quantization',
	 'urllink': u'http://arxiv.org/abs/1412.1945'}
2015-04-10 03:22:03+0000 [xxu46_10] INFO: Crawled 49 pages (at 1 pages/min), scraped 42 items (at 1 items/min)
2015-04-10 03:23:03+0000 [xxu46_10] INFO: Crawled 49 pages (at 0 pages/min), scraped 42 items (at 0 items/min)
2015-04-10 03:23:17+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1412.1913> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 03:23:17+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1412.1913>
	{'abstract': u'It has been widely known that performance of algorithms for NP-Hard problems varies from instance to instance. This phenomenon has been observed, when we comprehensively studied multi-objective evolutionary algorithms (MOEAs) on a six benchmark instances of discrete time-cost trade-off problem (DTCTP). Instead of using single algorithm to solve DTCTP, we use a portfolio approach that takes multiple algorithms as its constituent. In this paper, we proposed portfolio comprising of four MOEAs, Non-dominated sorting genetic algorithm 2 (NSGA 2), the strength Pareto EA 2 (SPEA 2), Pareto archive evolutionary strategy (PAES) and Niched Pareto Genetic Algorithm 2 (NPGA 2) to solve DTCTP. The result shows that the portfolio approach is computationally fast and qualitatively superior than its constituent algorithms for all benchmark instances. Moreover, portfolio approach provides an insight in selecting the best algorithm for all instances of DTCTP.',
	 'authors': u'Santosh Mungle,',
	 'category': u'Computer Science ',
	 'date': '2014-12-5',
	 'pdflink': u'http://arxiv.org/pdf/1412.1913',
	 'subjects': u'Artificial Intelligence (cs.AI)',
	 'title': u'\nA Portfolio Approach to Algorithm Selection for Discrete Time-Cost  Trade-off Problem',
	 'urllink': u'http://arxiv.org/abs/1412.1913'}
2015-04-10 03:24:03+0000 [xxu46_10] INFO: Crawled 50 pages (at 1 pages/min), scraped 43 items (at 1 items/min)
2015-04-10 03:24:20+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1412.1908> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 03:24:20+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1412.1908>
	{'abstract': u'Human eyes can recognize person identities based on small salient regions, i.e. human saliency is distinctive and reliable in pedestrian matching across disjoint camera views. However, such valuable information is often hidden when computing similarities of pedestrian images with existing approaches. Inspired by our user study result of human perception on human saliency, we propose a novel perspective for person re-identification based on learning human saliency and matching saliency distribution. The proposed saliency learning and matching framework consists of four steps: (1) To handle misalignment caused by drastic viewpoint change and pose variations, we apply adjacency constrained patch matching to build dense correspondence between image pairs. (2) We propose two alternative methods, i.e. K-Nearest Neighbors and One-class SVM, to estimate a saliency score for each image patch, through which distinctive features stand out without using identity labels in the training procedure. (3) saliency matching is proposed based on patch matching. Matching patches with inconsistent saliency brings penalty, and images of the same identity are recognized by minimizing the saliency matching cost. (4) Furthermore, saliency matching is tightly integrated with patch matching in a unified structural RankSVM learning framework. The effectiveness of our approach is validated on the VIPeR dataset and the CUHK01 dataset. Our approach outperforms the state-of-the-art person re-identification methods on both datasets.',
	 'authors': u'Rui Zhao, Wanli Ouyang, Xiaogang Wang,',
	 'category': u'Computer Science ',
	 'date': '2014-12-5',
	 'pdflink': u'http://arxiv.org/pdf/1412.1908',
	 'subjects': u'Computer Vision and Pattern Recognition (cs.CV)',
	 'title': u'\nPerson Re-identification by Saliency Learning',
	 'urllink': u'http://arxiv.org/abs/1412.1908'}
2015-04-10 03:25:03+0000 [xxu46_10] INFO: Crawled 51 pages (at 1 pages/min), scraped 44 items (at 1 items/min)
2015-04-10 03:25:19+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1412.1898> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 03:25:19+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1412.1898>
	{'abstract': u'Load balancing by proactively offloading users onto small and otherwise lightly-loaded cells is critical for tapping the potential of dense heterogeneous cellular networks (HCNs). Offloading has mostly been studied for the downlink, where it is generally assumed that a user offloaded to a small cell will communicate with it on the uplink as well. The impact of coupled downlink-uplink offloading is not well understood. Uplink power control and spatial interference correlation further complicate the mathematical analysis as compared to the downlink. We propose an accurate and tractable model to characterize the uplink SINR and rate distribution in a multi-tier HCN as a function of the association rules and power control parameters. Joint uplink-downlink rate coverage is also characterized. Using the developed analysis, it is shown that the optimal degree of channel inversion (for uplink power control) increases with load imbalance in the network. In sharp contrast to the downlink, minimum path loss association is shown to be optimal for uplink rate. Moreover, with minimum path loss association and full channel inversion, uplink SIR is shown to be invariant of infrastructure density. It is further shown that a decoupled association---employing differing association strategies for uplink and downlink---leads to significant improvement in joint uplink-downlink rate coverage over the standard coupled association in HCNs.',
	 'authors': u'Sarabjot Singh, Xinchen Zhang, Jeffrey G. Andrews,',
	 'category': u'Computer Science ',
	 'date': '2014-12-5',
	 'pdflink': u'http://arxiv.org/pdf/1412.1898',
	 'subjects': u'Information Theory (cs.IT)',
	 'title': u'\nJoint Rate and SINR Coverage Analysis for Decoupled Uplink-Downlink  Biased Cell Associations in HetNets',
	 'urllink': u'http://arxiv.org/abs/1412.1898'}
2015-04-10 03:26:03+0000 [xxu46_10] INFO: Crawled 52 pages (at 1 pages/min), scraped 45 items (at 1 items/min)
2015-04-10 03:26:38+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1412.1897> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 03:26:38+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1412.1897>
	{'abstract': u'Deep neural networks (DNNs) have recently been achieving state-of-the-art performance on a variety of pattern-recognition tasks, most notably visual classification problems. Given that DNNs are now able to classify objects in images with near-human-level performance, questions naturally arise as to what differences remain between computer and human vision. A recent study revealed that changing an image (e.g. of a lion) in a way imperceptible to humans can cause a DNN to label the image as something else entirely (e.g. mislabeling a lion a library). Here we show a related result: it is easy to produce images that are completely unrecognizable to humans, but that state-of-the-art DNNs believe to be recognizable objects with 99.99% confidence (e.g. labeling with certainty that white noise static is a lion). Specifically, we take convolutional neural networks trained to perform well on either the ImageNet or MNIST datasets and then find images with evolutionary algorithms or gradient ascent that DNNs label with high confidence as belonging to each dataset class. It is possible to produce images totally unrecognizable to human eyes that DNNs believe with near certainty are familiar objects, which we call "fooling images" (more generally, fooling examples). Our results shed light on interesting differences between human vision and current DNNs, and raise questions about the generality of DNN computer vision.',
	 'authors': u'Anh Nguyen, Jason Yosinski, Jeff Clune,',
	 'category': u'Computer Science ',
	 'date': '2014-12-5',
	 'pdflink': u'http://arxiv.org/pdf/1412.1897',
	 'subjects': u'Computer Vision and Pattern Recognition (cs.CV)',
	 'title': u'\nDeep Neural Networks are Easily Fooled: High Confidence Predictions for  Unrecognizable Images',
	 'urllink': u'http://arxiv.org/abs/1412.1897'}
2015-04-10 03:27:03+0000 [xxu46_10] INFO: Crawled 53 pages (at 1 pages/min), scraped 46 items (at 1 items/min)
2015-04-10 03:27:39+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1412.1888> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 03:27:39+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1412.1888>
	{'abstract': u'Document clustering is an unsupervised approach in which a large collection of documents (corpus) is subdivided into smaller, meaningful, identifiable, and verifiable sub-groups (clusters). Meaningful representation of documents and implicitly identifying the patterns, on which this separation is performed, is the challenging part of document clustering. We have proposed a document clustering technique using graph based document representation with constraints. A graph data structure can easily capture the non-linear relationships of nodes, document contains various feature terms that can be non-linearly connected hence a graph can easily represents this information. Constrains, are explicit conditions for document clustering where background knowledge is use to set the direction for Linking or Not-Linking a set of documents for a target clusters, thus guiding the clustering process. We deemed clustering is an ill-define problem, there can be many clustering results. Background knowledge can be used to drive the clustering algorithm in the right direction. We have proposed three different types of constraints, Instance level, corpus level and cluster level constraints. A new algorithm Constrained HAC is also proposed which will incorporate Instance level constraints as prior knowledge; it will guide the clustering process leading to better results. Extensive set of experiments have been performed on both synthetic and standard document clustering datasets, results are compared on standard clustering measures like: purity, entropy and F-measure. Results clearly establish that our proposed approach leads to improvement in cluster quality.',
	 'authors': u'Muhammad Rafi, Farnaz Amin, Mohammad Shahid Shaikh,',
	 'category': u'Computer Science ',
	 'date': '2014-12-5',
	 'pdflink': u'http://arxiv.org/pdf/1412.1888',
	 'subjects': u'Information Retrieval (cs.IR)',
	 'title': u'\nDocument clustering using graph based document representation with  constraints',
	 'urllink': u'http://arxiv.org/abs/1412.1888'}
2015-04-10 03:28:03+0000 [xxu46_10] INFO: Crawled 54 pages (at 1 pages/min), scraped 47 items (at 1 items/min)
2015-04-10 03:28:36+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1412.1885> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 03:28:36+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1412.1885>
	{'abstract': u'Tensor decompositions are promising tools for big data analytics as they bring multiple modes and aspects of data to a unified framework, which allows us to discover complex internal structures and correlations of data. Unfortunately most existing approaches are not designed to meet the major challenges posed by big data analytics. This paper attempts to improve the scalability of tensor decompositions and provides two contributions: A flexible and fast algorithm for the CP decomposition (FFCP) of tensors based on their Tucker compression; A distributed randomized Tucker decomposition approach for arbitrarily big tensors but with relatively low multilinear rank. These two algorithms can deal with huge tensors, even if they are dense. Extensive simulations provide empirical evidence of the validity and efficiency of the proposed algorithms.',
	 'authors': u'Guoxu Zhou, Andrzej Cichocki, Shengli Xie,',
	 'category': u'Computer Science ',
	 'date': '2014-12-5',
	 'pdflink': u'http://arxiv.org/pdf/1412.1885',
	 'subjects': u'Numerical Analysis (cs.NA)',
	 'title': u'\nDecomposition of Big Tensors With Low Multilinear Rank',
	 'urllink': u'http://arxiv.org/abs/1412.1885'}
2015-04-10 03:29:03+0000 [xxu46_10] INFO: Crawled 55 pages (at 1 pages/min), scraped 48 items (at 1 items/min)
2015-04-10 03:29:29+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1412.1870> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 03:29:29+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1412.1870>
	{'abstract': u"The single-source shortest path problem is a classical problem in the research field of graph algorithm. In this paper, a new single-source shortest path algorithm for nonnegative weight graph is proposed. The algorithm can compress multi-round Fibonacci heap operations to one round to save running time relative to Dijkstra's algorithm using Fibonacci heap. The time complexity of the algorithm is also O(m+nlogn) in the worst case, where m is the number of edges and n is the number of nodes. However, the bound can be linear in some case, for example, when edge weights of a graph are all the same and the hop count of the longest shortest path is much less than n. We use c to represent the actual execution round number of the algorithm when the algorithm solves the single-source shortest path problem for a class of nonnegative weight graph. Based on the theoretical analyses, we demonstrate that the algorithm can perform better than Dijkstra's algorithm using Fibonacci heap in average situation only if n is large enough and the expectation of c is no more than en, where e is a positive constant which is less than 1. Because it is so difficult to accurately describe the probability distribution of c for all possible problem instances, it is still an open problem whether the algorithm of this paper is faster than Dijkstra's algorithm using Fibonacci heap in average situation only if n is large enough.",
	 'authors': u'Yunpeng Li, Yichuan Jiang, Yong Zhang,',
	 'category': u'Computer Science ',
	 'date': '2014-12-5',
	 'pdflink': u'http://arxiv.org/pdf/1412.1870',
	 'subjects': u'Data Structures and Algorithms (cs.DS)',
	 'title': u'\nA New Single-Source Shortest Path Algorithm for Nonnegative Weight Graph',
	 'urllink': u'http://arxiv.org/abs/1412.1870'}
2015-04-10 03:30:03+0000 [xxu46_10] INFO: Crawled 56 pages (at 1 pages/min), scraped 49 items (at 1 items/min)
2015-04-10 03:30:47+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1412.1866> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 03:30:47+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1412.1866>
	{'abstract': u'Understanding temporal expressions among events described in text is a major challenge in natural language processing. In longer texts, processing on sentence-by-sentence or expression-by-expression basis often fails, in part due to the disregard for the consistency of the processed data, e.g., temporal relations. We present an ensemble method, which reconciles the output of multiple classifiers for temporal relations, subject to consistency constraints across the whole text. Using integer programming to enforce the consistency constraints globally, we improve the best published F1 score from the TempEval-3 Challenge by 3 percentage points to 0.3899.',
	 'authors': u'Catherine Kerr, Terri Hoare, Jakub Marecek, Paula Carroll,',
	 'category': u'Computer Science ',
	 'date': '2014-12-5',
	 'pdflink': u'http://arxiv.org/pdf/1412.1866',
	 'subjects': u'Computation and Language (cs.CL)',
	 'title': u'\nInteger Programming Ensemble of Classifiers for Temporal Relations',
	 'urllink': u'http://arxiv.org/abs/1412.1866'}
2015-04-10 03:31:03+0000 [xxu46_10] INFO: Crawled 57 pages (at 1 pages/min), scraped 50 items (at 1 items/min)
2015-04-10 03:31:49+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1412.1862> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 03:31:49+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1412.1862>
	{'abstract': u'Is knowledge definable as justified true belief ("JTB")? We argue that one can legitimately answer positively or negatively, depending on how the notion of justification is understood. To facilitate our argument, we introduce a simple propositional logic of reason-based belief. We show that this logic is sufficiently flexible to accommodate various useful features, including quantification over reasons. We use our framework to contrast two notions of JTB: one internalist, the other externalist. We argue that Gettier cases essentially challenge the internalist notion but not the externalist one. In particular, we may equate knowledge and JTB if the latter is grounded in what we call "adequate" reasons.',
	 'authors': u'Paul Egr\xe9, Paul Marty, Bryan Renne,',
	 'category': u'Computer Science ',
	 'date': '2014-12-4',
	 'pdflink': u'http://arxiv.org/pdf/1412.1862',
	 'subjects': u'Logic in Computer Science (cs.LO)',
	 'title': u'\nKnowledge, Justification, and Reason-Based Belief',
	 'urllink': u'http://arxiv.org/abs/1412.1862'}
2015-04-10 03:32:03+0000 [xxu46_10] INFO: Crawled 58 pages (at 1 pages/min), scraped 51 items (at 1 items/min)
2015-04-10 03:33:03+0000 [xxu46_10] INFO: Crawled 58 pages (at 0 pages/min), scraped 51 items (at 0 items/min)
2015-04-10 03:33:05+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1412.1859> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 03:33:05+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1412.1859>
	{'abstract': u"This paper argues that one of the most important decisions in designing and deploying censorship resistance systems is whether one set of system options should be selected (the best), or whether there should be several sets of good ones. We model the problem of choosing these options as a cat-and-mouse game and show that the best strategy depends on the value the censor associates with total system censorship versus partial, and the tolerance of false positives. If the censor has a low tolerance to false positives then choosing one censorship resistance system is best. Otherwise choosing several systems is the better choice, but the way traffic should be distributed over the systems depends on the tolerance of the censor to false negatives. We demonstrate that establishing the censor's utility function is critical to discovering the best strategy for censorship resistance.",
	 'authors': u'Tariq Elahi, Steven J. Murdoch, Ian Goldberg,',
	 'category': u'Computer Science ',
	 'date': '2014-12-4',
	 'pdflink': u'http://arxiv.org/pdf/1412.1859',
	 'subjects': u'Cryptography and Security (cs.CR)',
	 'title': u'\nCensorship Resistance: Let a Thousand Flowers Bloom?',
	 'urllink': u'http://arxiv.org/abs/1412.1859'}
2015-04-10 03:33:53+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1412.1842> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 03:33:53+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1412.1842>
	{'abstract': u'In this work we present an end-to-end system for text spotting -- localising and recognising text in natural scene images -- and text based image retrieval. This system is based on a region proposal mechanism for detection and deep convolutional neural networks for recognition. Our pipeline uses a novel combination of complementary proposal generation techniques to ensure high recall, and a fast subsequent filtering stage for improving precision. For the recognition and ranking of proposals, we train very large convolutional neural networks to perform word recognition on the whole proposal region at the same time, departing from the character classifier based systems of the past. These networks are trained solely on data produced by a synthetic text generation engine, requiring no human labelled data. Analysing the stages of our pipeline, we show state-of-the-art performance throughout. We perform rigorous experiments across a number of standard end-to-end text spotting benchmarks and text-based image retrieval datasets, showing a large improvement over all previous methods. Finally, we demonstrate a real-world application of our text spotting system to allow thousands of hours of news footage to be instantly searchable via a text query.',
	 'authors': u'Max Jaderberg, Karen Simonyan, Andrea Vedaldi, Andrew Zisserman,',
	 'category': u'Computer Science ',
	 'date': '2014-12-4',
	 'pdflink': u'http://arxiv.org/pdf/1412.1842',
	 'subjects': u'Computer Vision and Pattern Recognition (cs.CV)',
	 'title': u'\nReading Text in the Wild with Convolutional Neural Networks',
	 'urllink': u'http://arxiv.org/abs/1412.1842'}
2015-04-10 03:34:03+0000 [xxu46_10] INFO: Crawled 60 pages (at 2 pages/min), scraped 53 items (at 2 items/min)
2015-04-10 03:35:03+0000 [xxu46_10] INFO: Crawled 60 pages (at 0 pages/min), scraped 53 items (at 0 items/min)
2015-04-10 03:35:08+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1412.1841> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 03:35:08+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1412.1841>
	{'abstract': u"We develop a model of phonological contrast in natural language. Specifically, the model describes the maintenance of contrast between different words in a language, and the elimination of such contrast when sounds in the words merge. An example of such a contrast is that provided by the two vowel sounds 'i' and 'e', which distinguish pairs of words such as 'pin' and 'pen' in most dialects of English. We model language users' knowledge of the pronunciation of a word as consisting of collections of labeled exemplars stored in memory. Each exemplar is a detailed memory of a particular utterance of the word in question. In our model an exemplar is represented by one or two phonetic variables along with a weight indicating how strong the memory of the utterance is. Starting from an exemplar-level model we derive integro-differential equations for the evolution of exemplar density fields in phonetic space. Using these latter equations we investigate under what conditions two sounds merge, thus eliminating the contrast. Our main conclusion is that for the preservation of phonological contrast, it is necessary that anomalous utterances of a given word are discarded, and not merely stored in memory as an exemplar of another word.",
	 'authors': u'P. F. Tupper,',
	 'category': u'Computer Science ',
	 'date': '2014-12-2',
	 'pdflink': u'http://arxiv.org/pdf/1412.1841',
	 'subjects': u'Computation and Language (cs.CL)',
	 'title': u'\nExemplar Dynamics and Sound Merger in Language',
	 'urllink': u'http://arxiv.org/abs/1412.1841'}
2015-04-10 03:36:01+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1412.1820> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 03:36:01+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1412.1820>
	{'abstract': u'Entity type tagging is the task of assigning category labels to each mention of an entity in a document. While standard systems focus on a small set of types, recent work (Ling and Weld, 2012) suggests that using a large fine-grained label set can lead to dramatic improvements in downstream tasks. In the absence of labeled training data, existing fine-grained tagging systems obtain examples automatically, using resolved entities and their types extracted from a knowledge base. However, since the appropriate type often depends on context (e.g. Washington could be tagged either as city or government), this procedure can result in spurious labels, leading to poorer generalization. We propose the task of context-dependent fine type tagging, where the set of acceptable labels for a mention is restricted to only those deducible from the local context (e.g. sentence or document). We introduce new resources for this task: 11,304 mentions annotated with their context-dependent fine types, and we provide baseline experimental results on this data.',
	 'authors': u'Dan Gillick, Nevena Lazic, Kuzman Ganchev, Jesse Kirchner, David Huynh,',
	 'category': u'Computer Science ',
	 'date': '2014-12-3',
	 'pdflink': u'http://arxiv.org/pdf/1412.1820',
	 'subjects': u'Computation and Language (cs.CL)',
	 'title': u'\nContext-Dependent Fine-Grained Entity Type Tagging',
	 'urllink': u'http://arxiv.org/abs/1412.1820'}
2015-04-10 03:36:03+0000 [xxu46_10] INFO: Crawled 62 pages (at 2 pages/min), scraped 55 items (at 2 items/min)
2015-04-10 03:37:03+0000 [xxu46_10] INFO: Crawled 62 pages (at 0 pages/min), scraped 55 items (at 0 items/min)
2015-04-10 03:37:15+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1412.1798> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 03:37:15+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1412.1798>
	{'abstract': u'The multitask diffusion LMS is an efficient strategy to simultaneously infer, in a collaborative manner, multiple parameter vectors. Existing works on multitask problems assume that all agents respond to data synchronously. In several applications, agents may not be able to act synchronously because networks can be subject to several sources of uncertainties such as changing topology, random link failures, or agents turning on and off for energy conservation. In this work, we describe a model for the solution of multitask problems over asynchronous networks and carry out a detailed mean and mean-square error analysis. Results show that sufficiently small step-sizes can still ensure both stability and performance. Simulations and illustrative examples are provided to verify the theoretical findings. The framework is applied to a particular application involving spectral sensing.',
	 'authors': u'Roula Nassif, C\xe9dric Richard, Andr\xe9 Ferrari, Ali H. Sayed,',
	 'category': u'Computer Science ',
	 'date': '2014-12-4',
	 'pdflink': u'http://arxiv.org/pdf/1412.1798',
	 'subjects': u'Systems and Control (cs.SY)',
	 'title': u'\nMultitask diffusion adaptation over asynchronous networks',
	 'urllink': u'http://arxiv.org/abs/1412.1798'}
2015-04-10 03:38:03+0000 [xxu46_10] INFO: Crawled 63 pages (at 1 pages/min), scraped 56 items (at 1 items/min)
2015-04-10 03:38:34+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1412.1792> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 03:38:34+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1412.1792>
	{'abstract': u"Computing the Euler genus of a graph is a fundamental problem in graph theory and topology. It has been shown to be NP-hard by [Thomassen '89] and a linear-time fixed-parameter algorithm has been obtained by [Mohar '99]. Despite extensive study, the approximability of the Euler genus remains wide open. While the existence of an -approximation is not ruled out, the currently best-known upper bound is a trivial -approximation that follows from bounds on the Euler characteristic. In this paper, we give the first non-trivial approximation algorithm for this problem. Specifically, we present a polynomial-time algorithm which given a graph of Euler genus outputs an embedding of into a surface of Euler genus . Combined with the above -approximation, our result also implies a -approximation, for some universal constant . Our approximation algorithm also has implications for the design of algorithms on graphs of small genus. Several of these algorithms require that an embedding of the graph into a surface of small genus is given as part of the input. Our result implies that many of these algorithms can be implemented even when the embedding of the input graph is unknown.",
	 'authors': u'Ken-ichi Kawarabayashi, Anastasios Sidiropoulos,',
	 'category': u'Computer Science ',
	 'date': '2014-12-4',
	 'pdflink': u'http://arxiv.org/pdf/1412.1792',
	 'subjects': u'Data Structures and Algorithms (cs.DS)',
	 'title': u'\nBeyond the Euler characteristic: Approximating the genus of general  graphs',
	 'urllink': u'http://arxiv.org/abs/1412.1792'}
2015-04-10 03:39:03+0000 [xxu46_10] INFO: Crawled 64 pages (at 1 pages/min), scraped 57 items (at 1 items/min)
2015-04-10 03:39:44+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1412.1790> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 03:39:44+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1412.1790>
	{'abstract': u'We introduce Teegi, a Tangible ElectroEncephaloGraphy (EEG) Interface that enables novice users to get to know more about something as complex as brain signals, in an easy, en- gaging and informative way. To this end, we have designed a new system based on a unique combination of spatial aug- mented reality, tangible interaction and real-time neurotech- nologies. With Teegi, a user can visualize and analyze his or her own brain activity in real-time, on a tangible character that can be easily manipulated, and with which it is possible to interact. An exploration study has shown that interacting with Teegi seems to be easy, motivating, reliable and infor- mative. Overall, this suggests that Teegi is a promising and relevant training and mediation tool for the general public.',
	 'authors': u'J\xe9r\xe9my Frey, Renaud Gervais, St\xe9phanie Fleck, Fabien Lotte, Martin Hachet,',
	 'category': u'Computer Science ',
	 'date': '2014-12-4',
	 'pdflink': u'http://arxiv.org/pdf/1412.1790',
	 'subjects': u'Human-Computer Interaction (cs.HC)',
	 'title': u'\nTeegi: Tangible EEG Interface',
	 'urllink': u'http://arxiv.org/abs/1412.1790'}
2015-04-10 03:40:03+0000 [xxu46_10] INFO: Crawled 65 pages (at 1 pages/min), scraped 58 items (at 1 items/min)
2015-04-10 03:40:50+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1412.1788> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 03:40:50+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1412.1788>
	{'abstract': u'Non-negative matrix factorization (NMF) approximates a given matrix as a product of two non-negative matrices. Multiplicative algorithms deliver reliable results, but they show slow convergence for high-dimensional data and may be stuck away from local minima. Gradient descent methods have better behavior, but only apply to smooth losses such as the least-squares loss. In this article, we propose a first-order primal-dual algorithm for non-negative decomposition problems (where one factor is fixed) with the KL divergence, based on the Chambolle-Pock algorithm. All required computations may be obtained in closed form and we provide an efficient heuristic way to select step-sizes. By using alternating optimization, our algorithm readily extends to NMF and, on synthetic examples, face recognition or music source separation datasets, it is either faster than existing algorithms, or leads to improved local optima, or both.',
	 'authors': u'Felipe Yanez, Francis Bach,',
	 'category': u'Computer Science ',
	 'date': '2014-12-4',
	 'pdflink': u'http://arxiv.org/pdf/1412.1788',
	 'subjects': u'Learning (cs.LG)',
	 'title': u'\nPrimal-Dual Algorithms for Non-negative Matrix Factorization with the  Kullback-Leibler Divergence',
	 'urllink': u'http://arxiv.org/abs/1412.1788'}
2015-04-10 03:41:03+0000 [xxu46_10] INFO: Crawled 66 pages (at 1 pages/min), scraped 59 items (at 1 items/min)
2015-04-10 03:41:43+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1412.1787> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 03:41:43+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1412.1787>
	{'abstract': u'We investigate the computational complexity of the exponential random graph model (ERGM) commonly used in social network analysis. This model represents a probability distribution on graphs by setting the log-likelihood of generating a graph to be a weighted sum of feature counts. These log-likelihoods must be exponentiated and then normalized to produce probabilities, and the normalizing constant is called the emph. We show that the problem of computing the partition function is -hard, and inapproximable in polynomial time to within an exponential ratio, assuming . Furthermore, there is no randomized polynomial time algorithm for generating random graphs whose distribution is within total variation distance of a given ERGM. Our proofs use standard feature types based on the sociological theories of assortative mixing and triadic closure.',
	 'authors': u'Michael J. Bannister, William E. Devanny, David Eppstein,',
	 'category': u'Computer Science ',
	 'date': '2014-12-4',
	 'pdflink': u'http://arxiv.org/pdf/1412.1787',
	 'subjects': u'Data Structures and Algorithms (cs.DS)',
	 'title': u'\nERGMs are Hard',
	 'urllink': u'http://arxiv.org/abs/1412.1787'}
2015-04-10 03:42:03+0000 [xxu46_10] INFO: Crawled 67 pages (at 1 pages/min), scraped 60 items (at 1 items/min)
2015-04-10 03:42:42+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1412.1780> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 03:42:42+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1412.1780>
	{'abstract': u'A wealth of Open Educational Resources is now available, and beyond the first and evident problem of finding them, the issue of articulating a set of resources is arising. When using audiovisual resources, among different possibilities, annotating a video resource with additional resources linked to specific fragments can constitute one of the articulation modalities. Annotating a video is a complex task, and in a pedagogical context, intermediary activities should be proposed in order to mitigate this complexity. In this paper, we describe a tool dedicated to supporting video annotation activities. It aims at improving learner engagement, by having students be more active when watching videos by offering a progressive annotation process, first guided by providing predefined resources, then more freely, to accompany users in the practice of annotating videos.',
	 'authors': u'Olivier Aubert, Joscha Jaeger,',
	 'category': u'Computer Science ',
	 'date': '2014-12-4',
	 'pdflink': u'http://arxiv.org/pdf/1412.1780',
	 'subjects': u'Computers and Society (cs.CY)',
	 'title': u'\nAnnotating Video with Open Educational Resources in a Flipped Classroom  Scenario',
	 'urllink': u'http://arxiv.org/abs/1412.1780'}
2015-04-10 03:43:03+0000 [xxu46_10] INFO: Crawled 68 pages (at 1 pages/min), scraped 61 items (at 1 items/min)
2015-04-10 03:43:48+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1412.1772> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 03:43:48+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1412.1772>
	{'abstract': u'Physiological sensors are gaining the attention of manufacturers and users. As denoted by devices such as smartwatches or the newly released Kinect 2 -- which can covertly measure heartbeats -- or by the popularity of smartphone apps that track heart rate during fitness activities. Soon, physiological monitoring could become widely accessible and transparent to users. We demonstrate how one could take advantage of this situation to increase users\' engagement and enhance user experience in human-agent interaction. We created an experimental protocol involving embodied agents -- "virtual avatars". Those agents were displayed alongside a beating heart. We compared a condition in which this feedback was simply duplicating the heart rates of users to another condition in which it was set to an average heart rate. Results suggest a superior social presence of agents when they display feedback similar to users\' internal state. This physiological "similarity-attraction" effect may lead, with little effort, to a better acceptance of agents and robots by the general public.',
	 'authors': u'J\xe9r\xe9my Frey,',
	 'category': u'Computer Science ',
	 'date': '2014-12-4',
	 'pdflink': u'http://arxiv.org/pdf/1412.1772',
	 'subjects': u'Human-Computer Interaction (cs.HC)',
	 'title': u'\nHeart Rate Monitoring as an Easy Way to Increase Engagement in  Human-Agent Interaction',
	 'urllink': u'http://arxiv.org/abs/1412.1772'}
2015-04-10 03:44:03+0000 [xxu46_10] INFO: Crawled 69 pages (at 1 pages/min), scraped 62 items (at 1 items/min)
2015-04-10 03:44:53+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1412.1763> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 03:44:53+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1412.1763>
	{'abstract': u'The traditional requirement for a randomized streaming algorithm is just , i.e., algorithm should be correct (within the stated -error bound) at the end of the stream. In this paper, we study the problem, where the output should be correct at all times. The standard approach for solving the tracking problem is to run independent instances of the one-shot algorithm and apply the union bound to all time instances. In this paper, we study if this standard approach can be improved, for the classical frequency moment problem. We show that for the problem for any , we actually only need copies to achieve the tracking guarantee in the cash register model, where is the universe size. Meanwhile, we present a lower bound of bits for all linear sketches achieving this guarantee. This shows that our upper bound is tight when . We also present an lower bound in the turnstile model, showing that the standard approach by using the union bound is essentially optimal.',
	 'authors': u'Zengfeng Huang, Wai Ming Tai, Ke Yi,',
	 'category': u'Computer Science ',
	 'date': '2014-12-4',
	 'pdflink': u'http://arxiv.org/pdf/1412.1763',
	 'subjects': u'Data Structures and Algorithms (cs.DS)',
	 'title': u'\nTracking the Frequency Moments at All Times',
	 'urllink': u'http://arxiv.org/abs/1412.1763'}
2015-04-10 03:45:03+0000 [xxu46_10] INFO: Crawled 70 pages (at 1 pages/min), scraped 63 items (at 1 items/min)
2015-04-10 03:45:59+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1412.1741> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 03:45:59+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1412.1741>
	{'abstract': u'Regular expression matching is essential for many applications, such as finding patterns in text, exploring substrings in large DNA sequences, or lexical analysis. However, sequential regular expression matching may be time-prohibitive for large problem sizes. In this paper, we describe a novel algorithm for parallel regular expression matching via deterministic finite automata. Furthermore, we present our tool PaREM that accepts regular expressions and finite automata as input and automatically generates the corresponding code for our algorithm that is amenable for parallel execution on shared-memory systems. We evaluate our parallel algorithm empirically by comparing it with a commonly used algorithm for sequential regular expression matching. Experiments on a dual-socket shared-memory system with 24 physical cores show speed-ups of up to 21x for 48 threads.',
	 'authors': u'Suejb Memeti, Sabri Pllana,',
	 'category': u'Computer Science ',
	 'date': '2014-12-4',
	 'pdflink': u'http://arxiv.org/pdf/1412.1741',
	 'subjects': u'Formal Languages and Automata Theory (cs.FL)',
	 'title': u'\nPaREM: A Novel Approach for Parallel Regular Expression Matching',
	 'urllink': u'http://arxiv.org/abs/1412.1741'}
2015-04-10 03:46:03+0000 [xxu46_10] INFO: Crawled 71 pages (at 1 pages/min), scraped 64 items (at 1 items/min)
2015-04-10 03:47:03+0000 [xxu46_10] INFO: Crawled 71 pages (at 0 pages/min), scraped 64 items (at 0 items/min)
2015-04-10 03:47:14+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1412.1736> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 03:47:14+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1412.1736>
	{'abstract': u'Looking at the automata defined over a group alphabet as a nearring, we see that they are a highly complicated structure. As with ring theory, one method to deal with complexity is to look at semisimplicity modulo radical structures. We find some bounds on the Jacobson 2-radical and show that in certain groups, this radical can be explicitly found and the semisimple image determined.',
	 'authors': u'Tim Boykett, Gerhard Wendt,',
	 'category': u'Computer Science ',
	 'date': '2014-12-4',
	 'pdflink': u'http://arxiv.org/pdf/1412.1736',
	 'subjects': u'Formal Languages and Automata Theory (cs.FL)',
	 'title': u'\nJ2 Radical in Automata Nearrings',
	 'urllink': u'http://arxiv.org/abs/1412.1736'}
2015-04-10 03:48:03+0000 [xxu46_10] INFO: Crawled 72 pages (at 1 pages/min), scraped 65 items (at 1 items/min)
2015-04-10 03:48:21+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1412.1712> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 03:48:21+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1412.1712>
	{'abstract': u'Simultaneous wireless information and power transfer (SWIPT) is a promising solution to increase the lifetime of wireless nodes and hence alleviate the energy bottleneck of energy constrained wireless networks. As an alternative to conventional energy harvesting techniques, SWIPT relies on the use of radio frequency signals, and is expected to bring some fundamental changes to the design of wireless communication networks. This article focuses on the application of advanced smart antenna technologies, including multiple-input multiple-output and relaying techniques, to SWIPT. These smart antenna technologies have the potential to significantly improve the energy efficiency and also the spectral efficiency of SWIPT. Different network topologies with single and multiple users are investigated, along with some promising solutions to achieve a favorable trade-off between system performance and complexity. A detailed discussion of future research challenges for the design of SWIPT systems is also provided.',
	 'authors': u'Zhiguo Ding, Caijun Zhong, Derrick Wing Kwan Ng, Mugen Peng, Himal A. Suraweera, Robert Schober, H. Vincent Poor,',
	 'category': u'Computer Science ',
	 'date': '2014-12-4',
	 'pdflink': u'http://arxiv.org/pdf/1412.1712',
	 'subjects': u'Information Theory (cs.IT)',
	 'title': u'\nApplication of Smart Antenna Technologies in Simultaneous Wireless  Information and Power Transfer',
	 'urllink': u'http://arxiv.org/abs/1412.1712'}
2015-04-10 03:49:03+0000 [xxu46_10] INFO: Crawled 73 pages (at 1 pages/min), scraped 66 items (at 1 items/min)
2015-04-10 03:49:35+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1412.1710> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 03:49:35+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1412.1710>
	{'abstract': u'Though recent advanced convolutional neural networks (CNNs) have been improving the image recognition accuracy, the models are getting more complex and time-consuming. For real-world applications in industrial and commercial scenarios, engineers and developers are often faced with the requirement of constrained time budget. In this paper, we investigate the accuracy of CNNs under constrained time cost. Under this constraint, the designs of the network architectures should exhibit as trade-offs among the factors like depth, numbers of filters, filter sizes, etc. With a series of controlled comparisons, we progressively modify a baseline model while preserving its time complexity. This is also helpful for understanding the importance of the factors in network designs. We present an architecture that achieves very competitive accuracy in the ImageNet dataset (11.8% top-5 error, 10-view test), yet is 20% faster than "AlexNet" (16.0% top-5 error, 10-view test).',
	 'authors': u'Kaiming He, Jian Sun,',
	 'category': u'Computer Science ',
	 'date': '2014-12-4',
	 'pdflink': u'http://arxiv.org/pdf/1412.1710',
	 'subjects': u'Computer Vision and Pattern Recognition (cs.CV)',
	 'title': u'\nConvolutional Neural Networks at Constrained Time Cost',
	 'urllink': u'http://arxiv.org/abs/1412.1710'}
2015-04-10 03:50:03+0000 [xxu46_10] INFO: Crawled 74 pages (at 1 pages/min), scraped 67 items (at 1 items/min)
2015-04-10 03:50:54+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1412.1683> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 03:50:54+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1412.1683>
	{'abstract': u'The approximate nearest neighbor problem (-ANN) in Euclidean settings is a fundamental question, which has been addressed by two main approaches: Data-dependent space partitioning techniques perform well when the dimension is relatively low, but are affected by the curse of dimensionality. On the other hand, locality sensitive hashing has polynomial dependence in the dimension, sublinear query time with an exponent inversely proportional to the error factor , and subquadratic space requirement. We generalize the Johnson-Lindenstrauss lemma to define "low-quality" mappings to a Euclidean space of significantly lower dimension, such that they satisfy a requirement weaker than approximately preserving all distances or even preserving the nearest neighbor. This mapping guarantees, with arbitrarily high probability, that an approximate nearest neighbor lies among the approximate nearest neighbors in the projected space. This leads to a randomized tree based data structure that avoids the curse of dimensionality for -ANN. Our algorithm, given points in dimension , achieves space usage in , preprocessing time in , and query time in , where is proportional to , for fixed . It employs a data structure, such as BBD-trees, that efficiently finds approximate nearest neighbors. The dimension reduction is larger if one assumes that pointsets possess some structure, namely bounded expansion rate. We implement our method and present experimental results in up to 500 dimensions and points, which show that the practical performance is better than predicted by the theoretical analysis. In addition, we compare our approach with E2LSH.',
	 'authors': u'Evangelos Anagnostopoulos, Ioannis Z. Emiris, Ioannis Psarros,',
	 'category': u'Computer Science ',
	 'date': '2014-12-4',
	 'pdflink': u'http://arxiv.org/pdf/1412.1683',
	 'subjects': u'Computational Geometry (cs.CG)',
	 'title': u'\nLow-quality dimension reduction and high-dimensional Approximate Nearest  Neighbor',
	 'urllink': u'http://arxiv.org/abs/1412.1683'}
2015-04-10 03:51:03+0000 [xxu46_10] INFO: Crawled 75 pages (at 1 pages/min), scraped 68 items (at 1 items/min)
2015-04-10 03:51:53+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1412.1680> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 03:51:53+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1412.1680>
	{'abstract': u'Given a real-valued function defined over a manifold embedded in , we are interested in recovering structural information about from the sole information of its values on a finite sample . Existing methods provide approximation to the persistence diagram of when geometric noise and functional noise are bounded. However, they fail in the presence of aberrant values, also called outliers, both in theory and practice. We propose a new algorithm that deals with outliers. We handle aberrant functional values with a method inspired from the k-nearest neighbors regression and the local median filtering, while the geometric outliers are handled using the distance to a measure. Combined with topological results on nested filtrations, our algorithm performs robust topological analysis of scalar fields in a wider range of noise models than handled by current methods. We provide theoretical guarantees and experimental results on the quality of our approximation of the sampled scalar field.',
	 'authors': u'Micka\xebl Buchet, Fr\xe9d\xe9ric Chazal, Tamal K. Dey, Fengtao Fan, Steve Y. Oudot, Yusu Wang,',
	 'category': u'Computer Science ',
	 'date': '2014-12-4',
	 'pdflink': u'http://arxiv.org/pdf/1412.1680',
	 'subjects': u'Computational Geometry (cs.CG)',
	 'title': u'\nTopological analysis of scalar fields with outliers',
	 'urllink': u'http://arxiv.org/abs/1412.1680'}
2015-04-10 03:52:03+0000 [xxu46_10] INFO: Crawled 76 pages (at 1 pages/min), scraped 69 items (at 1 items/min)
2015-04-10 03:52:44+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1412.1671> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 03:52:44+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1412.1671>
	{'abstract': u'In this paper we show that the chase technique is powerful enough to capture the bag-set semantics of conjunctive queries over IDBs and IDs and TGDs. In addition, we argue that in such cases it provides efficient (LogSpace) query evaluation algorithms and that, moreover, it can serve as a basis for evaluating some restricted classes of aggregate queries under incomplete information.',
	 'authors': u'Camilo Thorne,',
	 'category': u'Computer Science ',
	 'date': '2014-12-4',
	 'pdflink': u'http://arxiv.org/pdf/1412.1671',
	 'subjects': u'Databases (cs.DB)',
	 'title': u'\nChases and Bag-Set Certain Answers',
	 'urllink': u'http://arxiv.org/abs/1412.1671'}
2015-04-10 03:53:03+0000 [xxu46_10] INFO: Crawled 77 pages (at 1 pages/min), scraped 70 items (at 1 items/min)
2015-04-10 03:53:57+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1412.1665> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 03:53:57+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1412.1665>
	{'abstract': u'In this paper, randomly-directional beamforming (RDB) is considered for millimeter-wave (mmwave) multi-user (MU) multiple-input multiple-output (MIMO) downlink systems. By using asymptotic techniques, the performance of RDB and the MU gain in mm-wave MIMO are analyzed based on the uniform random line-of-sight (UR-LoS) channel model suitable for highly directional mm-wave radio propagation channels. It is shown that there exists a transition point on the number of users relative to the number of antenna elements for non-trivial performance of the RDB scheme, and sum rate scaling arbitrarily close to linear scaling with respect to the number of antenna elements can be achieved under the UR-LoS channel model by proper user scheduling if the number of users increases linearly with respect to the number of antenna elements. The provided results yield insights into the most effective beamforming and scheduling choices for mm-wave MU-MIMO in various operating conditions. Simulation results validate our analysis based on asymptotic techniques for finite cases.',
	 'authors': u'Gilwon Lee, Youngchul Sung, Junyeong Seo,',
	 'category': u'Computer Science ',
	 'date': '2014-12-4',
	 'pdflink': u'http://arxiv.org/pdf/1412.1665',
	 'subjects': u'Information Theory (cs.IT)',
	 'title': u'\nRandomly-Directional Beamforming in Millimeter-Wave Multi-User MIMO  Downlink',
	 'urllink': u'http://arxiv.org/abs/1412.1665'}
2015-04-10 03:54:03+0000 [xxu46_10] INFO: Crawled 78 pages (at 1 pages/min), scraped 71 items (at 1 items/min)
2015-04-10 03:55:03+0000 [xxu46_10] INFO: Crawled 78 pages (at 0 pages/min), scraped 71 items (at 0 items/min)
2015-04-10 03:55:05+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1412.1652> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 03:55:05+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1412.1652>
	{'abstract': u'This paper analyzes two-tier heterogeneous cellular network with decoupled downlink and uplink access. The basic performance benefits of uplink/downlink decoupling have been recently introduced. Here we provide a more elaborate treatment of the decoupling mechanism by analyzing spectral and energy efficiency of the system, as the joint improvement of these two features is crucial for the upcoming 5G systems. Contrary to the common assumption of a homogeneous user domain, we analyze two-tier user domain, whose transmit powers depend purely on the association process. The derived joint association probabilities and the distributions of the distance to the serving base station give deeper insight into the fundamentals of a system with decoupled access. The rigorous theoretical analysis shows that decoupling of downlink and uplink with two-level uplink power adaptation improves both, spectral and energy efficiency of the system.',
	 'authors': u'Katerina Smiljkovikj, Liljana Gavrilovska, Petar Popovski,',
	 'category': u'Computer Science ',
	 'date': '2014-12-4',
	 'pdflink': u'http://arxiv.org/pdf/1412.1652',
	 'subjects': u'Information Theory (cs.IT)',
	 'title': u'\nEfficiency Analysis of Decoupled Downlink and Uplink Access in  Heterogeneous Networks',
	 'urllink': u'http://arxiv.org/abs/1412.1652'}
2015-04-10 03:55:59+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1412.1641> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 03:55:59+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1412.1641>
	{'abstract': u"A language is -piecewise testable if it is a finite boolean combination of languages of the form , where and . We investigate the problem, given a minimal DFA recognizing a piecewise testable language, what is the minimal for which the language is -piecewise testable? It was shown by Kl 'ima and Pol 'ak that such a is bounded by the depth of the minimal DFA. We first provide the complexity bound to decide whether a given minimal DFA represents a -piecewise testable language for a fixed , which then results in an algorithm that is single exponential with respect to the size of the DFA and double exponential with respect to the resulting . We provide a detailed complexity analysis for . Finally, we generalize a result valid for DFAs to NFAs and show that the upper bound given by the depth of the minimal DFA can be exponentially far from the minimal .",
	 'authors': u'Tom\xe1\u0161 Masopust, Micha\xebl Thomazo,',
	 'category': u'Computer Science ',
	 'date': '2014-12-4',
	 'pdflink': u'http://arxiv.org/pdf/1412.1641',
	 'subjects': u'Formal Languages and Automata Theory (cs.FL)',
	 'title': u'\nOn $k$-piecewise testability (preliminary report)',
	 'urllink': u'http://arxiv.org/abs/1412.1641'}
2015-04-10 03:56:03+0000 [xxu46_10] INFO: Crawled 80 pages (at 2 pages/min), scraped 73 items (at 2 items/min)
2015-04-10 03:57:03+0000 [xxu46_10] INFO: Crawled 80 pages (at 0 pages/min), scraped 73 items (at 0 items/min)
2015-04-10 03:57:14+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1412.1639> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 03:57:14+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1412.1639>
	{'abstract': u'Let be a directed graph with vertices and edges. The graph is called singly-connected if for each pair of vertices there is at most one simple path from to in . Buchsbaum and Carlisle (1993) gave an algorithm for testing whether is singly-connected in time. In this paper we describe a refined version of this algorithm with running time , where and are the number of sources and sinks, respectively, in the reduced graph obtained by first contracting each strongly connected component of into one vertex and then eliminating vertices of indegree or outdegree by a contraction operation. Moreover, we show that the problem of finding a minimum cardinality edge subset (respectively, vertex subset ) whose removal from leaves a singly-connected graph is NP-hard.',
	 'authors': u'Martin Dietzfelbinger, Raed Jaberi,',
	 'category': u'Computer Science ',
	 'date': '2014-12-4',
	 'pdflink': u'http://arxiv.org/pdf/1412.1639',
	 'subjects': u'Data Structures and Algorithms (cs.DS)',
	 'title': u'\nOn testing single connectedness in directed graphs and some related  problems',
	 'urllink': u'http://arxiv.org/abs/1412.1639'}
2015-04-10 03:58:03+0000 [xxu46_10] INFO: Crawled 81 pages (at 1 pages/min), scraped 74 items (at 1 items/min)
2015-04-10 03:58:16+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1412.1632> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 03:58:16+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1412.1632>
	{'abstract': u'Answer sentence selection is the task of identifying sentences that contain the answer to a given question. This is an important problem in its own right as well as in the larger context of open domain question answering. We propose a novel approach to solving this task via means of distributed representations, and learn to match questions with answers by considering their semantic encoding. This contrasts prior work on this task, which typically relies on classifiers with large numbers of hand-crafted syntactic and semantic features and various external resources. Our approach does not require any feature engineering nor does it involve specialist linguistic data, making this model easily applicable to a wide range of domains and languages. Experimental results on a standard benchmark dataset from TREC demonstrate that---despite its simplicity---our model matches state of the art performance on the answer sentence selection task.',
	 'authors': u'Lei Yu, Karl Moritz Hermann, Phil Blunsom, Stephen Pulman,',
	 'category': u'Computer Science ',
	 'date': '2014-12-4',
	 'pdflink': u'http://arxiv.org/pdf/1412.1632',
	 'subjects': u'Computation and Language (cs.CL)',
	 'title': u'\nDeep Learning for Answer Sentence Selection',
	 'urllink': u'http://arxiv.org/abs/1412.1632'}
2015-04-10 03:59:03+0000 [xxu46_10] INFO: Crawled 82 pages (at 1 pages/min), scraped 75 items (at 1 items/min)
2015-04-10 03:59:30+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1412.1628> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 03:59:30+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1412.1628>
	{'abstract': u'Compared to image representation based on low-level local descriptors, deep neural activations of Convolutional Neural Networks (CNNs) are richer in mid-level representation, but poorer in geometric invariance properties. In this paper, we present a straightforward framework for better image representation by combining the two approaches. To take advantages of both representations, we propose an efficient method to extract a fair amount of multi-scale dense local activations from a pre-trained CNN. We then aggregate the activations by Fisher kernel framework, which has been modified with a simple scale-wise normalization essential to make it suitable for CNN activations. Replacing the direct use of a single activation vector with our representation demonstrates significant performance improvements: +17.76 (Acc.) on MIT Indoor 67 and +7.18 (mAP) on PASCAL VOC 2007. The results suggest that our proposal can be used as a primary image representation for better performances in visual recognition tasks.',
	 'authors': u'Donggeun Yoo, Sunggyun Park, Joon-Young Lee, In So Kweon,',
	 'category': u'Computer Science ',
	 'date': '2014-12-4',
	 'pdflink': u'http://arxiv.org/pdf/1412.1628',
	 'subjects': u'Computer Vision and Pattern Recognition (cs.CV)',
	 'title': u'\nFisher Kernel for Deep Neural Activations',
	 'urllink': u'http://arxiv.org/abs/1412.1628'}
2015-04-10 04:00:03+0000 [xxu46_10] INFO: Crawled 83 pages (at 1 pages/min), scraped 76 items (at 1 items/min)
2015-04-10 04:00:47+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1412.1623> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 04:00:47+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1412.1623>
	{'abstract': u'Single Sign-On (SSO) systems simplify login procedures by using an an Identity Provider (IdP) to issue authentication tokens which can be consumed by Service Providers (SPs). Traditionally, IdPs are modeled as trusted third parties. This is reasonable for SSO systems like Kerberos, MS Passport and SAML, where each SP explicitely specifies which IdP he trusts. However, in open systems like OpenID and OpenID Connect, each user may set up his own IdP, and a discovery phase is added to the protocol flow. Thus it is easy for an attacker to set up its own IdP. In this paper we use a novel approach for analyzing SSO authentication schemes by introducing a malicious IdP. With this approach we evaluate one of the most popular and widely deployed SSO protocols - OpenID. We found four novel attack classes on OpenID, which were not covered by previous research, and show their applicability to real-life implementations. As a result, we were able to compromise 11 out of 16 existing OpenID implementations like Sourceforge, Drupal and ownCloud. We automated discovery of these attacks in a open source tool OpenID Attacker, which additionally allows fine-granular testing of all parameters in OpenID implementations. Our research helps to better understand the message flow in the OpenID protocol, trust assumptions in the different components of the system, and implementation issues in OpenID components. It is applicable to other SSO systems like OpenID Connect and SAML. All OpenID implementations have been informed about their vulnerabilities and we supported them in fixing the issues.',
	 'authors': u'Christian Mainka, Vladislav Mladenov, J\xf6rg Schwenk,',
	 'category': u'Computer Science ',
	 'date': '2014-12-4',
	 'pdflink': u'http://arxiv.org/pdf/1412.1623',
	 'subjects': u'Cryptography and Security (cs.CR)',
	 'title': u'\nDo not trust me: Using malicious IdPs for analyzing and attacking Single  Sign-On',
	 'urllink': u'http://arxiv.org/abs/1412.1623'}
2015-04-10 04:01:03+0000 [xxu46_10] INFO: Crawled 84 pages (at 1 pages/min), scraped 77 items (at 1 items/min)
2015-04-10 04:01:57+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1412.1619> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 04:01:57+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1412.1619>
	{'abstract': u'In this work we consider the learning setting where in addition to the training set, the learner receives a collection of auxiliary hypotheses originating from other tasks. This paradigm, known as Hypothesis Transfer Learning (HTL), has been successfully exploited in empirical works, but only recently has received a theoretical attention. Here, we try to understand when HTL facilitates accelerated generalization -- the goal of the transfer learning paradigm. Thus, we study a broad class of algorithms, a Hypothesis Transfer Learning through Regularized ERM, that can be instantiated with any non-negative smooth loss function and any strongly convex regularizer. We establish generalization and excess risk bounds, showing that if the algorithm is fed with a good source hypotheses combination, generalization happens at the fast rate instead of usual . We also observe that if the combination is perfect, our theory formally backs up the intuition that learning is not necessary. On the other hand, if the source hypotheses combination is a misfit for the target task, we recover the usual learning rate. As a byproduct of our study, we also prove a new bound on the Rademacher complexity of the smooth loss class under weaker assumptions compared to previous works.',
	 'authors': u'Ilja Kuzborskij, Francesco Orabona,',
	 'category': u'Computer Science ',
	 'date': '2014-12-4',
	 'pdflink': u'http://arxiv.org/pdf/1412.1619',
	 'subjects': u'Learning (cs.LG)',
	 'title': u'\nLearning by Transferring from Auxiliary Hypotheses',
	 'urllink': u'http://arxiv.org/abs/1412.1619'}
2015-04-10 04:02:03+0000 [xxu46_10] INFO: Crawled 85 pages (at 1 pages/min), scraped 78 items (at 1 items/min)
2015-04-10 04:03:03+0000 [xxu46_10] INFO: Crawled 85 pages (at 0 pages/min), scraped 78 items (at 0 items/min)
2015-04-10 04:03:13+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1412.1602> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 04:03:13+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1412.1602>
	{'abstract': u'We replace the Hidden Markov Model (HMM) which is traditionally used in in continuous speech recognition with a bi-directional recurrent neural network encoder coupled to a recurrent neural network decoder that directly emits a stream of phonemes. The alignment between the input and output sequences is established using an attention mechanism: the decoder emits each symbol based on a context created with a subset of input symbols elected by the attention mechanism. We report initial results demonstrating that this new approach achieves phoneme error rates that are comparable to the state-of-the-art HMM-based decoders, on the TIMIT dataset.',
	 'authors': u'Jan Chorowski, Dzmitry Bahdanau, Kyunghyun Cho, Yoshua Bengio,',
	 'category': u'Computer Science ',
	 'date': '2014-12-4',
	 'pdflink': u'http://arxiv.org/pdf/1412.1602',
	 'subjects': u'Neural and Evolutionary Computing (cs.NE)',
	 'title': u'\nEnd-to-end Continuous Speech Recognition using Attention-based Recurrent  NN: First Results',
	 'urllink': u'http://arxiv.org/abs/1412.1602'}
2015-04-10 04:04:01+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1412.1591> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 04:04:01+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1412.1591>
	{'abstract': u'The rapid advance of DNA sequencing technologies has yielded databases of thousands of genomes. To search and index these databases effectively, it is important that we take advantage of the similarity between those genomes. Several authors have recently suggested searching or indexing only one reference genome and the parts of the other genomes where they differ. In this paper we survey the twenty-year history of this idea and discuss its relation to kernelization in parameterized complexity.',
	 'authors': u'Travis Gagie, Simon J. Puglisi,',
	 'category': u'Computer Science ',
	 'date': '2014-12-4',
	 'pdflink': u'http://arxiv.org/pdf/1412.1591',
	 'subjects': u'Data Structures and Algorithms (cs.DS)',
	 'title': u'\nSearching and Indexing Genomic Databases via Kernelization',
	 'urllink': u'http://arxiv.org/abs/1412.1591'}
2015-04-10 04:04:03+0000 [xxu46_10] INFO: Crawled 87 pages (at 2 pages/min), scraped 80 items (at 2 items/min)
2015-04-10 04:05:03+0000 [xxu46_10] INFO: Crawled 87 pages (at 0 pages/min), scraped 80 items (at 0 items/min)
2015-04-10 04:05:16+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1412.1574> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 04:05:16+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1412.1574>
	{'abstract': u'As an important and challenging problem in computer vision and graphics, keypoint-based object tracking is typically formulated in a spatio-temporal statistical learning framework. However, most existing keypoint trackers are incapable of effectively modeling and balancing the following three aspects in a simultaneous manner: temporal model coherence across frames, spatial model consistency within frames, and discriminative feature construction. To address this issue, we propose a robust keypoint tracker based on spatio-temporal multi-task structured output optimization driven by discriminative metric learning. Consequently, temporal model coherence is characterized by multi-task structured keypoint model learning over several adjacent frames, while spatial model consistency is modeled by solving a geometric verification based structured learning problem. Discriminative feature construction is enabled by metric learning to ensure the intra-class compactness and inter-class separability. Finally, the above three modules are simultaneously optimized in a joint learning scheme. Experimental results have demonstrated the effectiveness of our tracker.',
	 'authors': u'Liming Zhao, Xi Li, Jun Xiao, Fei Wu, Yueting Zhuang,',
	 'category': u'Computer Science ',
	 'date': '2014-12-4',
	 'pdflink': u'http://arxiv.org/pdf/1412.1574',
	 'subjects': u'Computer Vision and Pattern Recognition (cs.CV)',
	 'title': u'\nMetric Learning Driven Multi-Task Structured Output Optimization for  Robust Keypoint Tracking',
	 'urllink': u'http://arxiv.org/abs/1412.1574'}
2015-04-10 04:06:03+0000 [xxu46_10] INFO: Crawled 88 pages (at 1 pages/min), scraped 81 items (at 1 items/min)
2015-04-10 04:06:32+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1412.1565> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 04:06:32+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1412.1565>
	{'abstract': u'We study the recovery of sparse signals from underdetermined linear measurements when a potentially erroneous support estimate is available. Our results are twofold. First, we derive necessary and sufficient conditions for signal recovery from compressively sampled measurements using weighted -norm minimization. These conditions, which depend on the choice of weights as well as the size and accuracy of the support estimate, are on the null space of the measurement matrix. They can guarantee recovery even when standard minimization fails. Second, we derive bounds on the number of Gaussian measurements for these conditions to be satisfied, i.e., for weighted minimization to successfully recover all sparse signals whose support has been estimated sufficiently accurately. Our bounds show that weighted minimization requires significantly fewer measurements than standard minimization when the support estimate is relatively accurate.',
	 'authors': u'Hassan Mansour, Rayan Saab,',
	 'category': u'Computer Science ',
	 'date': '2014-12-4',
	 'pdflink': u'http://arxiv.org/pdf/1412.1565',
	 'subjects': u'Information Theory (cs.IT)',
	 'title': u'\nRecovery Analysis for Weighted $\\ell_1$-Minimization Using a Null Space  Property',
	 'urllink': u'http://arxiv.org/abs/1412.1565'}
2015-04-10 04:07:03+0000 [xxu46_10] INFO: Crawled 89 pages (at 1 pages/min), scraped 82 items (at 1 items/min)
2015-04-10 04:07:34+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1412.1547> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 04:07:34+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1412.1547>
	{'abstract': u'Tightness is a generalisation of the notion of convexity: a space is tight if and only if it is "as convex as possible", given its topological constraints. For a simplicial complex, deciding tightness has a straightforward exponential time algorithm, but efficient methods to decide tightness are only known in the trivial setting of triangulated surfaces. In this article, we present a new polynomial time procedure to decide tightness for triangulations of -manifolds -- a problem which previously was thought to be hard. Furthermore, we describe an algorithm to decide general tightness in the case of -dimensional combinatorial manifolds which is fixed parameter tractable in the treewidth of the -skeletons of their vertex links, and we present an algorithm to decide -tightness for weak pseudomanifolds of arbitrary but fixed dimension which is fixed parameter tractable in the treewidth of the dual graph of .',
	 'authors': u'Bhaskar Bagchi, Benjamin A. Burton, Basudeb Datta, Nitin Singh, Jonathan Spreer,',
	 'category': u'Computer Science ',
	 'date': '2014-12-4',
	 'pdflink': u'http://arxiv.org/pdf/1412.1547',
	 'subjects': u'Computational Geometry (cs.CG)',
	 'title': u'\nEfficient algorithms to decide tightness',
	 'urllink': u'http://arxiv.org/abs/1412.1547'}
2015-04-10 04:08:03+0000 [xxu46_10] INFO: Crawled 90 pages (at 1 pages/min), scraped 83 items (at 1 items/min)
2015-04-10 04:08:49+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1412.1543> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 04:08:49+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1412.1543>
	{'abstract': u'Tolerance graphs model interval relations in such a way that intervals can tolerate a certain amount of overlap without being in conflict. In one of the most natural generalizations of tolerance graphs with direct applications in the comparison of DNA sequences from different organisms, namely emph graphs, two tolerances are allowed for each interval - one from the left and one from the right side. Several efficient algorithms for optimization problems that are NP-hard in general graphs have been designed for tolerance and multitolerance graphs. In spite of this progress, the complexity status of some fundamental algorithmic problems on tolerance and multitolerance graphs, such as the emph problem, remained unresolved until now, three decades after the introduction of tolerance graphs. In this article we introduce two new geometric representations for tolerance and multitolerance graphs, given by points and line segments in the plane. Apart from being important on their own, these new representations prove to be a powerful tool for deriving both hardness results and polynomial time algorithms. Using them, we surprisingly prove that the dominating set problem can be solved in polynomial time on tolerance graphs and that it is APX-hard on multitolerance graphs, solving thus a longstanding open problem. This problem is the first one that has been discovered with a different complexity status in these two graph classes. Furthermore we present an algorithm that solves the independent dominating set problem on multitolerance graphs in polynomial time, thus demonstrating the potential of this new representation for further exploitation via sweep line algorithms.',
	 'authors': u'Archontia C. Giannopoulou, George B. Mertzios,',
	 'category': u'Computer Science ',
	 'date': '2014-12-4',
	 'pdflink': u'http://arxiv.org/pdf/1412.1543',
	 'subjects': u'Computational Complexity (cs.CC)',
	 'title': u'\nNew Geometric Representations and Domination Problems on Tolerance and  Multitolerance Graphs',
	 'urllink': u'http://arxiv.org/abs/1412.1543'}
2015-04-10 04:09:03+0000 [xxu46_10] INFO: Crawled 91 pages (at 1 pages/min), scraped 84 items (at 1 items/min)
2015-04-10 04:09:37+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1412.1538> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 04:09:37+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1412.1538>
	{'abstract': u'Let be an unknown linear evolution process on driving an unknown initial state and producing the states at different time levels. The problem under consideration in this paper is to find as much information as possible about and from the measurements , , , . If is a "low-pass" convolution operator, we show that we can recover both and , almost surely, as long as we double the amount of temporal samples needed in cite to recover the signal propagated by a known operator . For a general operator , we can recover parts or even all of its spectrum from . As a special case of our method, we derive the centuries old Prony\'s method cite which recovers a vector with an -sparse Fourier transform from of its consecutive components.',
	 'authors': u'Akram Aldroubi, Ilya Krishtal,',
	 'category': u'Computer Science ',
	 'date': '2014-12-4',
	 'pdflink': u'http://arxiv.org/pdf/1412.1538',
	 'subjects': u'Information Theory (cs.IT)',
	 'title': u'\nKrylov Subspace Methods in Dynamical Sampling',
	 'urllink': u'http://arxiv.org/abs/1412.1538'}
2015-04-10 04:10:03+0000 [xxu46_10] INFO: Crawled 92 pages (at 1 pages/min), scraped 85 items (at 1 items/min)
2015-04-10 04:10:31+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1412.1526> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 04:10:31+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1412.1526>
	{'abstract': u'This paper presents an approach to parsing humans when there is significant occlusion. We model humans using a graphical model which has a tree structure building on recent work [27, 5] and exploit the prior that, even in presence of occlusion, the visible nodes form a connected subtree of the graphical model. We call each connected subtree a flexible composition of object parts. This involves a novel method for learning occlusion cues. During inference we need to search over a mixture of different flexible models. By exploiting part sharing, we show that this inference can be done extremely efficiently requiring only twice as many computations as searching for the entire object (i.e., not modeling occlusion). We evaluate our model on the standard benchmarked "We Are Family" Stickmen dataset and obtain significant performance improvements over the best alternative algorithms.',
	 'authors': u'Xianjie Chen, Alan Yuille,',
	 'category': u'Computer Science ',
	 'date': '2014-12-4',
	 'pdflink': u'http://arxiv.org/pdf/1412.1526',
	 'subjects': u'Computer Vision and Pattern Recognition (cs.CV)',
	 'title': u'\nParsing Occluded People by Flexible Compositions',
	 'urllink': u'http://arxiv.org/abs/1412.1526'}
2015-04-10 04:11:03+0000 [xxu46_10] INFO: Crawled 93 pages (at 1 pages/min), scraped 86 items (at 1 items/min)
2015-04-10 04:11:31+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1412.1523> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 04:11:31+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1412.1523>
	{'abstract': u'The paper examines the learning mechanism of adaptive agents over weakly-connected graphs and reveals an interesting behavior on how information flows through such topologies. The results clarify how asymmetries in the exchange of data can mask local information at certain agents and make them totally dependent on other agents. A leader-follower relationship develops with the performance of some agents being fully determined by the performance of other agents that are outside their domain of influence. This scenario can arise, for example, due to intruder attacks by malicious agents or as the result of failures by some critical links. The findings in this work help explain why strong-connectivity of the network topology, adaptation of the combination weights, and clustering of agents are important ingredients to equalize the learning abilities of all agents against such disturbances. The results also clarify how weak-connectivity can be helpful in reducing the effect of outlier data on learning performance.',
	 'authors': u'Bicheng Ying, Ali H. Sayed,',
	 'category': u'Computer Science ',
	 'date': '2014-12-4',
	 'pdflink': u'http://arxiv.org/pdf/1412.1523',
	 'subjects': u'Multiagent Systems (cs.MA)',
	 'title': u'\nInformation Exchange and Learning Dynamics over Weakly-Connected  Adaptive Networks',
	 'urllink': u'http://arxiv.org/abs/1412.1523'}
2015-04-10 04:12:03+0000 [xxu46_10] INFO: Crawled 94 pages (at 1 pages/min), scraped 87 items (at 1 items/min)
2015-04-10 04:12:15+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1412.1520> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 04:12:15+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1412.1520>
	{'abstract': u'Index coding studies multiterminal source-coding problems where a set of receivers are required to decode multiple (different) messages from a common broadcast, and they each know some messages a priori. In this paper, we consider a class of index-coding problems, which we term single uniprior, where at the receiver end, each receiver knows one of the messages a priori. At the broadcasting end, we consider a generalized setting where they are multiple senders, each sender only knows a subset of messages, and all senders are required to collectively transmit the index code. The aim is to find the minimum number of total coded bits the senders need to transmit. For the single-sender setup, we propose a pruning algorithm to find the optimal (i.e., minimum) index codelength, and establish that linear index codes are optimal. For the multi-sender setup, the pruning technique is used in conjunction with a proposed appending technique to give a lower bound to the optimal index codelength. An upper bound is derived based on cyclic codes. While the two bounds do not match in general, for the special case where no two senders know any message bit in common, the bounds match, giving the optimal index codelength. The results are derived based on graph theory, and are expressed in terms of strongly connected components.',
	 'authors': u'Lawrence Ong, Chin Keong Ho, Fabian Lim,',
	 'category': u'Computer Science ',
	 'date': '2014-12-3',
	 'pdflink': u'http://arxiv.org/pdf/1412.1520',
	 'subjects': u'Information Theory (cs.IT)',
	 'title': u'\nThe Single-Uniprior Index-Coding Problem: The Single-Sender Case and The  Multi-Sender Extension',
	 'urllink': u'http://arxiv.org/abs/1412.1520'}
2015-04-10 04:13:03+0000 [xxu46_10] INFO: Crawled 95 pages (at 1 pages/min), scraped 88 items (at 1 items/min)
2015-04-10 04:13:25+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1412.1506> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 04:13:25+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1412.1506>
	{'abstract': u'Mass abnormality segmentation is a vital step for the medical diagnostic process and is attracting more and more the interest of many research groups. Currently, most of the works achieved in this area have used the Gray Level Co-occurrence Matrix (GLCM) as texture features with a region-based approach. These features come in previous phase for segmentation stage or are using as inputs to classification stage. The work discussed in this paper attempts to experiment the GLCM method under a contour-based approach. Besides, we experiment the proposed approach on various tissues densities to bring more significant results. At this end, we explored some challenging breast images from BIRADS medical Data Base. Our first experimentations showed promising results with regard to the edges mass segmentation methods. This paper discusses first the main works achieved in this area. Sections 2 and 3 present materials and our methodology. The main results are showed and evaluated before concluding our paper.',
	 'authors': u'Khamsa Djaroudib, Abdelmalik Taleb Ahmed, Abdelmadjid Zidani,',
	 'category': u'Computer Science ',
	 'date': '2014-12-3',
	 'pdflink': u'http://arxiv.org/pdf/1412.1506',
	 'subjects': u'Computer Vision and Pattern Recognition (cs.CV)',
	 'title': u'\nTextural Approach for Mass Abnormality Segmentation in Mammographic  Images',
	 'urllink': u'http://arxiv.org/abs/1412.1506'}
2015-04-10 04:14:03+0000 [xxu46_10] INFO: Crawled 96 pages (at 1 pages/min), scraped 89 items (at 1 items/min)
2015-04-10 04:14:38+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1412.1505> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 04:14:38+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1412.1505>
	{'abstract': u'The FO Model Counting problem (FOMC) is the following: given a sentence in FO and a number , compute the number of models of over a domain of size ; the Weighted variant (WFOMC) generalizes the problem by associating a weight to each tuple and defining the weight of a model to be the product of weights of its tuples. In this paper we study the complexity of the symmetric WFOMC, where all tuples of a given relation have the same weight. Our motivation comes from an important application, inference in Knowledge Bases with soft constraints, like Markov Logic Networks, but the problem is also of independent theoretical interest. We study both the data complexity, and the combined complexity of FOMC and WFOMC. For the data complexity we prove the existence of an FO formula for which FOMC is #P-complete, and the existence of a Conjunctive Query for which WFOMC is #P-complete. We also prove that all -acyclic queries have polynomial time data complexity. For the combined complexity, we prove that, for every fragment FO, , the combined complexity of FOMC (or WFOMC) is #P-complete.',
	 'authors': u'Paul Beame, Guy Van den Broeck, Eric Gribkoff, Dan Suciu,',
	 'category': u'Computer Science ',
	 'date': '2014-12-3',
	 'pdflink': u'http://arxiv.org/pdf/1412.1505',
	 'subjects': u'Databases (cs.DB)',
	 'title': u'\nSymmetric Weighted First-Order Model Counting',
	 'urllink': u'http://arxiv.org/abs/1412.1505'}
2015-04-10 04:15:03+0000 [xxu46_10] INFO: Crawled 97 pages (at 1 pages/min), scraped 90 items (at 1 items/min)
2015-04-10 04:15:32+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1412.1470> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 04:15:32+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1412.1470>
	{'abstract': u'Mining frequent tree patterns has many practical applications in different areas such as XML data, bioinformatics and World Wide Web. The crucial step in frequent pattern mining is frequency counting which involves performing a matching operator to find occurrences (instances) of a pattern tree in database trees. A widely used matching operator for tree-structured data is subtree homeomorphism, where an edge in the pattern tree is mapped onto an ancestor-descendant relationship in the database tree. Tree patterns that are frequent under subtree homeomorphism are usually called embedded patterns. In this paper, we present an efficient algorithm for subtree homeomorphism with application to frequent pattern mining. We propose a compact data-structure, called occ, which can encode and represent several occurrences of a pattern tree. We then define efficient join operations on the occ data-structure, which help us count occurrences of tree patterns according to occurrences of their proper subtrees. Based on the proposed subtree homeomorphism method, we develop an effective pattern mining algorithm, called TPMiner. We evaluate the efficiency of TPMiner on several real-world and synthetic datasets. Our extensive experiments confirm that TPMiner always outperforms well-known existing algorithms, and in several cases the improvement with respect to a specific existing algorithm is significant.',
	 'authors': u'Mostafa Haghir Chehreghani, Maurice Bruynooghe,',
	 'category': u'Computer Science ',
	 'date': '2014-12-3',
	 'pdflink': u'http://arxiv.org/pdf/1412.1470',
	 'subjects': u'Databases (cs.DB)',
	 'title': u'\nMining Rooted Ordered Trees under Homeomorphism',
	 'urllink': u'http://arxiv.org/abs/1412.1470'}
2015-04-10 04:16:03+0000 [xxu46_10] INFO: Crawled 98 pages (at 1 pages/min), scraped 91 items (at 1 items/min)
2015-04-10 04:16:41+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1412.1468> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 04:16:41+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1412.1468>
	{'abstract': u"We examine the behavior of multi-agent networks where information-sharing is subject to a positive communications cost over the edges linking the agents. We consider a general mean-square-error formulation where all agents are interested in estimating the same target vector. We first show that, in the absence of any incentives to cooperate, the optimal strategy for the agents is to behave in a selfish manner with each agent seeking the optimal solution independently of the other agents. Pareto inefficiency arises as a result of the fact that agents are not using historical data to predict the behavior of their neighbors and to know whether they will reciprocate and participate in sharing information. Motivated by this observation, we develop a reputation protocol to summarize the opponent's past actions into a reputation score, which can then be used to form a belief about the opponent's subsequent actions. The reputation protocol entices agents to cooperate and turns their optimal strategy into an action-choosing strategy that enhances the overall social benefit of the network. In particular, we show that when the communications cost becomes large, the expected social benefit of the proposed protocol outperforms the social benefit that is obtained by cooperative agents that always share data. We perform a detailed mean-square-error analysis of the evolution of the network over three domains: far field, near-field, and middle-field, and show that the network behavior is stable for sufficiently small step-sizes. The various theoretical results are illustrated by numerical simulations.",
	 'authors': u'Chung-Kai Yu, Mihaela van der Schaar, Ali H. Sayed,',
	 'category': u'Computer Science ',
	 'date': '2014-12-3',
	 'pdflink': u'http://arxiv.org/pdf/1412.1468',
	 'subjects': u'Multiagent Systems (cs.MA)',
	 'title': u'\nInformation-Sharing over Adaptive Networks with Self-interested Agents',
	 'urllink': u'http://arxiv.org/abs/1412.1468'}
2015-04-10 04:17:03+0000 [xxu46_10] INFO: Crawled 99 pages (at 1 pages/min), scraped 92 items (at 1 items/min)
2015-04-10 04:17:52+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1412.1463> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 04:17:52+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1412.1463>
	{'abstract': u'The pre-image problem has to be solved during inference by most structured output predictors. For string kernels, this problem corresponds to finding the string associated to a given input. An algorithm capable of solving or finding good approximations to this problem would have many applications in computational biology and other fields. This work uses a recent result on combinatorial optimization of linear predictors based on string kernels to develop, for the pre-image, a low complexity upper bound valid for many string kernels. This upper bound is used with success in a branch and bound searching algorithm. Applications and results in the discovery of druggable peptides are presented and discussed.',
	 'authors': u'S\xe9bastien Gigu\xe8re, Am\xe9lie Rolland, Fran\xe7ois Laviolette, Mario Marchand,',
	 'category': u'Computer Science ',
	 'date': '2014-12-3',
	 'pdflink': u'http://arxiv.org/pdf/1412.1463',
	 'subjects': u'Learning (cs.LG)',
	 'title': u'\nOn the String Kernel Pre-Image Problem with Applications in Drug  Discovery',
	 'urllink': u'http://arxiv.org/abs/1412.1463'}
2015-04-10 04:18:03+0000 [xxu46_10] INFO: Crawled 100 pages (at 1 pages/min), scraped 93 items (at 1 items/min)
2015-04-10 04:18:40+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1412.1462> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 04:18:40+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1412.1462>
	{'abstract': u'In this paper, we study the problem of allocating ads to users through the viral-marketing lens. Advertisers approach the host with a budget in return for the marketing campaign service provided by the host. We show that allocation that takes into account the propensity of ads for viral propagation can achieve significantly better performance. However, uncontrolled virality could be undesirable for the host as it creates room for exploitation by the advertisers: hoping to tap uncontrolled virality, an advertiser might declare a lower budget for its marketing campaign, aiming at the same large outcome with a smaller cost. This creates a challenging trade-off: on the one hand, the host aims at leveraging virality and the network effect to improve advertising efficacy, while on the other hand the host wants to avoid giving away free service due to uncontrolled virality. We formalize this as the problem of ad allocation with minimum regret, which we show is NP-hard and inapproximable w.r.t. any factor. However, we devise an algorithm that provides approximation guarantees w.r.t. the total budget of all advertisers. We develop a scalable version of our approximation algorithm, which we extensively test on four real-world data sets, confirming that our algorithm delivers high quality solutions, is scalable, and significantly outperforms several natural baselines.',
	 'authors': u'Cigdem Aslay, Wei Lu, Francesco Bonchi, Amit Goyal, Laks V.S. Lakshmanan,',
	 'category': u'Computer Science ',
	 'date': '2014-12-3',
	 'pdflink': u'http://arxiv.org/pdf/1412.1462',
	 'subjects': u'Social and Information Networks (cs.SI)',
	 'title': u'\nViral Marketing Meets Social Advertising: Ad Allocation with Minimum  Regret',
	 'urllink': u'http://arxiv.org/abs/1412.1462'}
2015-04-10 04:19:03+0000 [xxu46_10] INFO: Crawled 101 pages (at 1 pages/min), scraped 94 items (at 1 items/min)
2015-04-10 04:19:31+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1412.1455> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 04:19:31+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1412.1455>
	{'abstract': u'We introduce a simple and effective method for retrieval of videos showing a specific event, even when the videos of that event were captured from significantly different viewpoints. Appearance-based methods fail in such cases, as appearances change with large changes of viewpoints. Our method is based on a pixel-based feature, "motion barcode", which records the existence/non-existence of motion as a function of time. While appearance, motion magnitude, and motion direction can vary greatly between disparate viewpoints, the existence of motion is viewpoint invariant. Based on the motion barcode, a similarity measure is developed for videos of the same event taken from very different viewpoints. This measure is robust to occlusions common under different viewpoints, and can be computed efficiently. Event retrieval is demonstrated using challenging videos from stationary and hand held cameras.',
	 'authors': u'Gil Ben-Artzi, Michael Werman, Shmuel Peleg,',
	 'category': u'Computer Science ',
	 'date': '2014-12-3',
	 'pdflink': u'http://arxiv.org/pdf/1412.1455',
	 'subjects': u'Computer Vision and Pattern Recognition (cs.CV)',
	 'title': u'\nEvent Retrieval Using Motion Barcodes',
	 'urllink': u'http://arxiv.org/abs/1412.1455'}
2015-04-10 04:20:03+0000 [xxu46_10] INFO: Crawled 102 pages (at 1 pages/min), scraped 95 items (at 1 items/min)
2015-04-10 04:20:23+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1412.1454> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 04:20:23+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1412.1454>
	{'abstract': u'We present a novel family of language model (LM) estimation techniques named Sparse Non-negative Matrix (SNM) estimation. A first set of experiments empirically evaluating it on the One Billion Word Benchmark shows that SNM -gram LMs perform almost as well as the well-established Kneser-Ney (KN) models. When using skip-gram features the models are able to match the state-of-the-art recurrent neural network (RNN) LMs; combining the two modeling techniques yields the best known result on the benchmark. The computational advantages of SNM over both maximum entropy and RNN LM estimation are probably its main strength, promising an approach that has the same flexibility in combining arbitrary features effectively and yet should scale to very large amounts of data as gracefully as -gram LMs do.',
	 'authors': u'Noam Shazeer, Joris Pelemans, Ciprian Chelba,',
	 'category': u'Computer Science ',
	 'date': '2014-12-3',
	 'pdflink': u'http://arxiv.org/pdf/1412.1454',
	 'subjects': u'Learning (cs.LG)',
	 'title': u'\nSkip-gram Language Modeling Using Sparse Non-negative Matrix Probability  Estimation',
	 'urllink': u'http://arxiv.org/abs/1412.1454'}
2015-04-10 04:21:03+0000 [xxu46_10] INFO: Crawled 103 pages (at 1 pages/min), scraped 96 items (at 1 items/min)
2015-04-10 04:21:35+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1412.1442> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 04:21:35+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1412.1442>
	{'abstract': u'In this work, we investigate the use of sparsity-inducing regularizers during training of Convolution Neural Networks (CNNs). These regularizers encourage that fewer connections in the convolution and fully connected layers take non-zero values and in effect result in sparse connectivity between hidden units in the deep network. This in turn reduces the memory and runtime cost involved in deploying the learned CNNs. We show that training with such regularization can still be performed using stochastic gradient descent implying that it can be used easily in existing codebases. Experimental evaluation of our approach on MNIST, CIFAR, and ImageNet datasets shows that our regularizers can result in dramatic reductions in memory requirements. For instance, when applied on AlexNet, our method can reduce the memory consumption by a factor of four with minimal loss in accuracy.',
	 'authors': u'Maxwell D. Collins, Pushmeet Kohli,',
	 'category': u'Computer Science ',
	 'date': '2014-12-3',
	 'pdflink': u'http://arxiv.org/pdf/1412.1442',
	 'subjects': u'Computer Vision and Pattern Recognition (cs.CV)',
	 'title': u'\nMemory Bounded Deep Convolutional Networks',
	 'urllink': u'http://arxiv.org/abs/1412.1442'}
2015-04-10 04:22:03+0000 [xxu46_10] INFO: Crawled 104 pages (at 1 pages/min), scraped 97 items (at 1 items/min)
2015-04-10 04:22:38+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1412.1441> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 04:22:38+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1412.1441>
	{'abstract': u'Most high quality object detection approaches use the same scheme: salience-based object proposal methods followed by post-classification using deep convolutional features. In this work, we demonstrate that fully learnt, data-driven proposal generation methods can effectively match the accuracy of their hand engineered counterparts, while allowing for very efficient runtime-quality trade-offs. This is achieved by making several key improvements to the MultiBox method [4], among which are an improved neural network architecture, use of contextual features and a new loss function that is robust to missing groundtruth labels. We show that our proposal generation method can closely match the performance of Selective Search [22] at a fraction of the cost. We report new single model state-ofthe-art on the ILSVRC 2014 detection challenge data set, with 0.431 mean average precision when combining both Selective Search and MultiBox proposals with our postclassification model. Finally, our approach allows the training of single class detectors that can process 50 images per second on a Xeon workstation, using CPU only, rivaling the quality of the current best performing methods.',
	 'authors': u'Christian Szegedy, Scott Reed, Dumitru Erhan, Dragomir Anguelov,',
	 'category': u'Computer Science ',
	 'date': '2014-12-3',
	 'pdflink': u'http://arxiv.org/pdf/1412.1441',
	 'subjects': u'Computer Vision and Pattern Recognition (cs.CV)',
	 'title': u'\nScalable, High-Quality Object Detection',
	 'urllink': u'http://arxiv.org/abs/1412.1441'}
2015-04-10 04:23:03+0000 [xxu46_10] INFO: Crawled 105 pages (at 1 pages/min), scraped 98 items (at 1 items/min)
2015-04-10 04:23:43+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1412.1424> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 04:23:43+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1412.1424>
	{'abstract': u"People regularly share items using online social media. However, people's decisions around sharing---who shares what to whom and why---are not well understood. We present a user study involving 87 pairs of Facebook users to understand how people make their sharing decisions. We find that even when sharing to a specific individual, people's own preference for an item (individuation) dominates over the recipient's preferences (altruism). People's open-ended responses about how they share, however, indicate that they do try to personalize shares based on the recipient. To explain these contrasting results, we propose a novel process model of sharing that takes into account people's preferences and the salience of an item. We also present encouraging results for a sharing prediction model that incorporates both the senders' and the recipients' preferences. These results suggest improvements to both algorithms that support sharing in social media and to information diffusion models.",
	 'authors': u'Amit Sharma, Dan Cosley,',
	 'category': u'Computer Science ',
	 'date': '2014-12-3',
	 'pdflink': u'http://arxiv.org/pdf/1412.1424',
	 'subjects': u'Human-Computer Interaction (cs.HC)',
	 'title': u"\nStudying and Modeling the Connection between People's Preferences and  Content Sharing",
	 'urllink': u'http://arxiv.org/abs/1412.1424'}
2015-04-10 04:24:03+0000 [xxu46_10] INFO: Crawled 106 pages (at 1 pages/min), scraped 99 items (at 1 items/min)
2015-04-10 04:24:59+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1412.1419> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 04:24:59+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1412.1419>
	{'abstract': u'A timeline is presented which shows the stages involved in converting a bioinformatics software application from a set of standalone algorithms through to a simple web based tool then to a web based portal harnessing Grid technologies and on to its latest inception as a Cloud based bioinformatics web tool. The nature of the software is discussed together with a description of its development at various stages including a detailed account of the Cloud service. An outline of user results is also included.',
	 'authors': u'John Allen, David Scott, Malcolm Illingworth, Bartek Dobrzelecki, Davy Virdee, Steve Thorn, Sara Knott,',
	 'category': u'Computer Science ',
	 'date': '2014-12-3',
	 'pdflink': u'http://arxiv.org/pdf/1412.1419',
	 'subjects': u'Distributed, Parallel, and Cluster Computing (cs.DC)',
	 'title': u'\nCloudQTL: Evolving a Bioinformatics Application to the Cloud',
	 'urllink': u'http://arxiv.org/abs/1412.1419'}
2015-04-10 04:25:03+0000 [xxu46_10] INFO: Crawled 107 pages (at 1 pages/min), scraped 100 items (at 1 items/min)
2015-04-10 04:26:03+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1412.1402> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 04:26:03+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1412.1402>
	{'abstract': u'Qubit models and methods for improving the performance of software and hardware for analyzing digital devices through increasing the dimension of the data structures and memory are proposed. The basic concepts, terminology and definitions necessary for the implementation of quantum computing when analyzing virtual computers are introduced. The investigation results concerning design and modeling computer systems in a cyberspace based on the use of two-component structure &lt;memory - transactions&gt; are presented.',
	 'authors': u'Vladimir Hahanov, Wajeb Gharibi, Svetlana Chumachenko, Eugenia Litvinova,',
	 'category': u'Computer Science ',
	 'date': '2014-12-2',
	 'pdflink': u'http://arxiv.org/pdf/1412.1402',
	 'subjects': u'Other Computer Science (cs.OH)',
	 'title': u'\nQubit Data Structures for Analyzing Computing Systems',
	 'urllink': u'http://arxiv.org/abs/1412.1402'}
2015-04-10 04:26:03+0000 [xxu46_10] INFO: Crawled 108 pages (at 1 pages/min), scraped 101 items (at 1 items/min)
2015-04-10 04:27:03+0000 [xxu46_10] INFO: Crawled 108 pages (at 0 pages/min), scraped 101 items (at 0 items/min)
2015-04-10 04:27:22+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1412.1401> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 04:27:22+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1412.1401>
	{'abstract': u'The three-dimensional geometry and connectivity of pore space determines the flow of single-phase incompressible flow. Herein I report on new throat finding algorithms that contribute to finding the exact flow-relevant geometrical properties of the void space, including high porosity samples of X2B images, three-dimensional synchrotron X-ray computed microtomographic images, and amounting to over 20% porosity. These new algorithms use the modified medial axis that comes from the 3DMA-Rock software package. To find accurate throats, we classify three major throat types: mostly planar and simply connected type, non-planar and simply connected type, and non-planar and non-simply connected type. For each type, we make at least one algorithm to find the throats. Here I introduce an example that has a non-planar and simply connected throat, and my solution indicated by one of my algorithms. My five algorithms each calculate the throat for each path. It selects one of them, which has the smallest inner area. New algorithms find accurate throats at least 98% among 12 high porosity samples (over 20%). Also, I introduce a new length calculation in the digitized image. The new calculation uses three mathematical concepts: i) differentiability, ii) implicit function theorem, iii) line integral. The result can convert the discrete boundary of the XMCT image to the real boundary. When the real boundary has an arc shape, the new calculation has less than 1% relative error.',
	 'authors': u'Kyung-Taek Jun,',
	 'category': u'Computer Science ',
	 'date': '2014-10-28',
	 'pdflink': u'http://arxiv.org/pdf/1412.1401',
	 'subjects': u'Computational Geometry (cs.CG)',
	 'title': u'\nThroat Finding Algorithms based on Throat Types',
	 'urllink': u'http://arxiv.org/abs/1412.1401'}
2015-04-10 04:28:03+0000 [xxu46_10] INFO: Crawled 109 pages (at 1 pages/min), scraped 102 items (at 1 items/min)
2015-04-10 04:28:28+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1412.1398> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 04:28:28+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1412.1398>
	{'abstract': u'We investigate what computational tasks can be performed on a point set in , if we are only given black-box access to it via nearest-neighbor search. This is a reasonable assumption if the underlying point set is either provided implicitly, or it is stored in a data structure that can answer such queries. In particular, we show the following: (A) One can compute an approximate bi-criteria -center clustering of the point set, and more generally compute a greedy permutation of the point set. (B) One can decide if a query point is (approximately) inside the convex-hull of the point set. We also investigate the problem of clustering the given point set, such that meaningful proximity queries can be carried out on the centers of the clusters, instead of the whole point set.',
	 'authors': u'Sariel Har-Peled, Nirman Kumar, David M. Mount, Benjamin Raichel,',
	 'category': u'Computer Science ',
	 'date': '2014-12-3',
	 'pdflink': u'http://arxiv.org/pdf/1412.1398',
	 'subjects': u'Computational Geometry (cs.CG)',
	 'title': u'\nSpace Exploration via Proximity Search',
	 'urllink': u'http://arxiv.org/abs/1412.1398'}
2015-04-10 04:29:03+0000 [xxu46_10] INFO: Crawled 110 pages (at 1 pages/min), scraped 103 items (at 1 items/min)
2015-04-10 04:29:10+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1412.1395> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 04:29:10+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1412.1395>
	{'abstract': u"Collisions are a main cause of throughput degradation in WLANs. The current contention mechanism used in IEEE 802.11 networks is called Carrier Sense Multiple Access with Collision Avoidance (CSMA/CA). It uses a Binary Exponential Backoff (BEB) technique to randomise each contender attempt of transmitting, effectively reducing the collision probability. Nevertheless, CSMA/CA relies on a random backoff that while effective and totally distributed, in principle is unable to completely eliminate collisions, therefore degrading the network throughput as more contenders attempt to share the channel. Carrier Sense Multiple Access with Enhanced Collision Avoidance (CSMA/ECA) is able to create a collision-free schedule in a totally distributed manner using a deterministic backoff after successful transmissions. Hysteresis and Fair Share are two extensions of CSMA/ECA to support a large number of contenders in a collision-free schedule. CSMA/ECA offers better throughput than CSMA/CA and short-term throughput fairness. This work describes CSMA/ECA and its extensions. Additionally, it provides the first evaluation results of CSMA/ECA in non-saturated traffic conditions as well as its performance when coexisting with CSMA/CA nodes. Furthermore, the effects of imperfect clocks over CSMA/ECA's deterministic backoff mechanism and its consequences when attempting to implement the protocol in real hardware are also analysed.",
	 'authors': u'Luis Sanabria-Russo, Jaume Barcelo, Boris Bellalta,',
	 'category': u'Computer Science ',
	 'date': '2014-12-3',
	 'pdflink': u'http://arxiv.org/pdf/1412.1395',
	 'subjects': u'Networking and Internet Architecture (cs.NI)',
	 'title': u'\nA High Efficiency MAC Protocol for WLANs: Providing Fairness in Dense  Scenarios',
	 'urllink': u'http://arxiv.org/abs/1412.1395'}
2015-04-10 04:30:03+0000 [xxu46_10] INFO: Crawled 111 pages (at 1 pages/min), scraped 104 items (at 1 items/min)
2015-04-10 04:30:29+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1412.1393> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 04:30:29+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1412.1393>
	{'abstract': u'This document contains a description of a Common Lisp extension that allows a programmer to write functional programs that use "normal order" evaluation, as in "non-strict" languages like Haskell. The extension is relatively straightforward, and it appears to be the first one such that is integrated in the overall Common Lisp framework.',
	 'authors': u'Marco Antoniotti,',
	 'category': u'Computer Science ',
	 'date': '2014-12-3',
	 'pdflink': u'http://arxiv.org/pdf/1412.1393',
	 'subjects': u'Programming Languages (cs.PL)',
	 'title': u'\nCLAZY: Lazy Calling for Common Lisp',
	 'urllink': u'http://arxiv.org/abs/1412.1393'}
2015-04-10 04:31:03+0000 [xxu46_10] INFO: Crawled 112 pages (at 1 pages/min), scraped 105 items (at 1 items/min)
2015-04-10 04:31:45+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1412.1342> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 04:31:45+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1412.1342>
	{'abstract': u'Comment on "Approaching human language with complex networks" by Cong and Liu (Physics of Life Reviews, Volume 11, Issue 4, December 2014, Pages 598-618).',
	 'authors': u'Diego R. Amancio,',
	 'category': u'Computer Science ',
	 'date': '2014-12-3',
	 'pdflink': u'http://arxiv.org/pdf/1412.1342',
	 'subjects': u'Computation and Language (cs.CL)',
	 'title': u'\nA perspective on the advancement of natural language processing tasks  via topological analysis of complex networks',
	 'urllink': u'http://arxiv.org/abs/1412.1342'}
2015-04-10 04:32:03+0000 [xxu46_10] INFO: Crawled 113 pages (at 1 pages/min), scraped 106 items (at 1 items/min)
2015-04-10 04:33:01+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1412.1330> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 04:33:01+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1412.1330>
	{'abstract': u'This paper presents an application of photogrammetry on ceramic fragments from two excavation sites located north-west of France. The restitution by photogrammetry of these different fragments allowed reconstructions of the potteries in their original state or at least to get to as close as possible. We used the 3D reconstructions to compute some metrics and to generate a presentation support by using a 3D printer. This work is based on affordable tools and illustrates how 3D technologies can be quite easily integrated in archaeology process with limited financial resources. 1. INTRODUCTION Today, photogrammetry and 3D modelling are an integral part of the methods used in archeology and heritage management. They provide answers to scientific needs in the fields of conservation, preservation, restoration and mediation of architectural, archaeological and cultural heritage [2] [6] [7] [9]. Photogrammetry on ceramic fragments was one of the first applications contemporary of the development of this technique applied in the archaeological community [3]. More recently and due to its democratization, it was applied more generally to artifacts [5]. Finally joined today by the rise of 3D printing [8] [10], it can restore fragmented artifacts [1] [12]. These examples target one or several particular objects and use different types of equipment that can be expensive. These aspects can put off uninitiated archaeologists. So it would be appropriate to see if these techniques could be generalized to a whole class of geometrically simple and common artifacts, such as ceramics. From these observations, associated to ceramics specialists with fragments of broken ceramics, we aimed at arranging different tools and methods, including photogrammetry, to explore opportunities for a cheap and attainable reconstruction methodology and its possible applications. Our first objective was to establish a protocol for scanning fragments with photogrammetry, and for reconstruction of original ceramics. We used the digital reconstitutions of the ceramics we got following our process to calculate some metrics and to design and 3D print a display for the remaining fragments of one pottery.',
	 'authors': u'Jean-Baptiste Barreau, Th\xe9ophane Nicolas, G Bruniaux, E Petit, Q Petit, Y Bernard, Ronan Gaugne, Val\xe9rie Gouranton,',
	 'category': u'Computer Science ',
	 'date': '2014-12-3',
	 'pdflink': u'http://arxiv.org/pdf/1412.1330',
	 'subjects': u'Graphics (cs.GR)',
	 'title': u'\nCeramics Fragments Digitization by Photogrammetry, Reconstructions and  Applications',
	 'urllink': u'http://arxiv.org/abs/1412.1330'}
2015-04-10 04:33:03+0000 [xxu46_10] INFO: Crawled 114 pages (at 1 pages/min), scraped 107 items (at 1 items/min)
2015-04-10 04:34:03+0000 [xxu46_10] INFO: Crawled 114 pages (at 0 pages/min), scraped 107 items (at 0 items/min)
2015-04-10 04:34:12+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1412.1318> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 04:34:12+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1412.1318>
	{'abstract': u"We present the first deterministic data structures for maintaining approximate minimum vertex cover and maximum matching in a fully dynamic graph , with and , in time per update. In particular, for minimum vertex cover we provide deterministic data structures for maintaining a approximation in amortized time per update. For maximum matching, we show how to maintain a approximation in time per update, and a approximation in time per update. Our data structure for fully dynamic minimum vertex cover is essentially near-optimal and settles an open problem by Onak and Rubinfeld from STOC' 2010.",
	 'authors': u'Sayan Bhattacharya, Monika Henzinger, Giuseppe F. Italiano,',
	 'category': u'Computer Science ',
	 'date': '2014-12-3',
	 'pdflink': u'http://arxiv.org/pdf/1412.1318',
	 'subjects': u'Data Structures and Algorithms (cs.DS)',
	 'title': u'\nDeterministic Fully Dynamic Data Structures for Vertex Cover and  Matching',
	 'urllink': u'http://arxiv.org/abs/1412.1318'}
2015-04-10 04:35:03+0000 [xxu46_10] INFO: Crawled 115 pages (at 1 pages/min), scraped 108 items (at 1 items/min)
2015-04-10 04:35:24+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1412.1297> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 04:35:24+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1412.1297>
	{'abstract': u'High Performance Distributed Computing is essential to boost scientific progress in many areas of science and to efficiently deploy a number of complex scientific applications. These applications have different characteristics that require distinct computational resources too. In this work we propose a systematic performance evaluation methodology. The focus of our methodology begins on scientific application characteristics, and then considers how these characteristics interact with the problem size, with the programming language and finally with a specific computational architecture. The computational experiments developed highlight this model of evaluation and indicate that optimal performance is found when we evaluate a combination of application class, program language, problem size and architecture model.',
	 'authors': u'Mariza Ferro, Antonio R. Mury, Laion F. Manfroi, Bruno Schlze,',
	 'category': u'Computer Science ',
	 'date': '2014-12-3',
	 'pdflink': u'http://arxiv.org/pdf/1412.1297',
	 'subjects': u'Distributed, Parallel, and Cluster Computing (cs.DC)',
	 'title': u'\nHigh Performance Computing Evaluation A methodology based on Scientific  Application Requirements',
	 'urllink': u'http://arxiv.org/abs/1412.1297'}
2015-04-10 04:36:03+0000 [xxu46_10] INFO: Crawled 116 pages (at 1 pages/min), scraped 109 items (at 1 items/min)
2015-04-10 04:36:25+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1412.1283> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 04:36:25+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1412.1283>
	{'abstract': u'The topic of semantic segmentation has witnessed considerable progress due to the powerful features learned by convolutional neural networks (CNNs). The current leading approaches for semantic segmentation exploit shape information by extracting CNN features from masked image regions. This strategy introduces artificial boundaries on the images and may impact the quality of the extracted features. Besides, the operations on the raw image domain require to compute thousands of networks on a single image, which is time-consuming. In this paper, we propose to exploit shape information via masking convolutional features. The proposal segments (e.g., super-pixels) are treated as masks on the convolutional feature maps. The CNN features of segments are directly masked out from these maps and used to train classifiers for recognition. We further propose a joint method to handle objects and "stuff" (e.g., grass, sky, water) in the same framework. State-of-the-art results are demonstrated on benchmarks of PASCAL VOC and new PASCAL-CONTEXT, with a compelling computational speed.',
	 'authors': u'Jifeng Dai, Kaiming He, Jian Sun,',
	 'category': u'Computer Science ',
	 'date': '2014-12-3',
	 'pdflink': u'http://arxiv.org/pdf/1412.1283',
	 'subjects': u'Computer Vision and Pattern Recognition (cs.CV)',
	 'title': u'\nConvolutional Feature Masking for Joint Object and Stuff Segmentation',
	 'urllink': u'http://arxiv.org/abs/1412.1283'}
2015-04-10 04:37:03+0000 [xxu46_10] INFO: Crawled 117 pages (at 1 pages/min), scraped 110 items (at 1 items/min)
2015-04-10 04:37:33+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1412.1271> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 04:37:33+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1412.1271>
	{'abstract': u'In ( cite), we have viewed machine learning as a coding and dimensionality reduction problem, and further proposed a simple unsupervised dimensionality reduction method, entitled deep distributed random samplings (DDRS). In this paper, we further extend it to supervised learning incrementally. The key idea here is to incorporate label information into the coding process by reformulating that each center in DDRS has multiple output units indicating which class the center belongs to. The supervised learning method seems somewhat similar with random forests ( cite), here we emphasize their differences as follows. (i) Each layer of our method considers the relationship between part of the data points in training data with all training data points, while random forests focus on building each decision tree on only part of training data points independently. (ii) Our method builds gradually-narrowed network by sampling less and less data points, while random forests builds gradually-narrowed network by merging subclasses. (iii) Our method is trained more straightforward from bottom layer to top layer, while random forests build each tree from top layer to bottom layer by splitting. (iv) Our method encodes output targets implicitly in sparse codes, while random forests encode output targets by remembering the class attributes of the activated nodes. Therefore, our method is a simpler, more straightforward, and maybe a better alternative choice, though both methods use two very basic elements---randomization and nearest neighbor optimization---as the core. This preprint is used to protect the incremental idea from ( cite). Full empirical evaluation will be announced carefully later.',
	 'authors': u'Xiao-Lei Zhang,',
	 'category': u'Computer Science ',
	 'date': '2014-12-3',
	 'pdflink': u'http://arxiv.org/e-print/1412.1271',
	 'subjects': u'Learning (cs.LG)',
	 'title': u'\nDeep Distributed Random Samplings for Supervised Learning: An  Alternative to Random Forests?',
	 'urllink': u'http://arxiv.org/abs/1412.1271'}
2015-04-10 04:38:03+0000 [xxu46_10] INFO: Crawled 118 pages (at 1 pages/min), scraped 111 items (at 1 items/min)
2015-04-10 04:38:47+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1412.1267> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 04:38:47+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1412.1267>
	{'abstract': u'In this paper, we consider an energy harvesting (EH) node which harvests energy from a radio frequency (RF) signal broadcasted by an access point (AP) in the downlink (DL). The node stores the harvested energy in an energy buffer and uses the stored energy to transmit data to the AP in the uplink (UL). We consider a simple transmission policy, which accounts for the fact that, in practice, the EH node may not have knowledge of the EH profile nor of the UL channel state information. In particular, in each time slot, the EH node transmits with either a constant desired power or remains silent if not enough energy is available in its energy buffer. For this simple policy, we use the theory of discrete-time continuous-state Markov chains to analyze the limiting distribution of the stored energy for finite- and infinite-size energy buffers. Moreover, we take into account imperfections of the energy buffer and the circuit power consumption of the EH node. For a Rayleigh fading DL channel, we provide the limiting distribution of the energy buffer content in closed form. In addition, we analyze the average error rate and the outage probability of a Rayleigh faded UL channel and show that the diversity order is not affected by the finite capacity of the energy buffer. Our results reveal that, for medium to high signal-to-noise ratio (SNRs), the optimal target transmit power of the EH node is less than the average harvested power and increases with the capacity of the energy buffer.',
	 'authors': u'Rania Morsi, Diomidis S. Michalopoulos, Robert Schober,',
	 'category': u'Computer Science ',
	 'date': '2014-12-3',
	 'pdflink': u'http://arxiv.org/pdf/1412.1267',
	 'subjects': u'Information Theory (cs.IT)',
	 'title': u'\nOn-Off Transmission Policy for Wireless Powered Communication with  Energy Storage',
	 'urllink': u'http://arxiv.org/abs/1412.1267'}
2015-04-10 04:39:03+0000 [xxu46_10] INFO: Crawled 119 pages (at 1 pages/min), scraped 112 items (at 1 items/min)
2015-04-10 04:39:23+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1412.1265> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 04:39:23+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1412.1265>
	{'abstract': u'This paper designs a high-performance deep convolutional network (DeepID2+) for face recognition. It is learned with the identification-verification supervisory signal. By increasing the dimension of hidden representations and adding supervision to early convolutional layers, DeepID2+ achieves new state-of-the-art on LFW and YouTube Faces benchmarks. Through empirical studies, we have discovered three properties of its deep neural activations critical for the high performance: sparsity, selectiveness and robustness. (1) It is observed that neural activations are moderately sparse. Moderate sparsity maximizes the discriminative power of the deep net as well as the distance between images. It is surprising that DeepID2+ still can achieve high recognition accuracy even after the neural responses are binarized. (2) Its neurons in higher layers are highly selective to identities and identity-related attributes. We can identify different subsets of neurons which are either constantly excited or inhibited when different identities or attributes are present. Although DeepID2+ is not taught to distinguish attributes during training, it has implicitly learned such high-level concepts. (3) It is much more robust to occlusions, although occlusion patterns are not included in the training set.',
	 'authors': u'Yi Sun, Xiaogang Wang, Xiaoou Tang,',
	 'category': u'Computer Science ',
	 'date': '2014-12-3',
	 'pdflink': u'http://arxiv.org/pdf/1412.1265',
	 'subjects': u'Computer Vision and Pattern Recognition (cs.CV)',
	 'title': u'\nDeeply learned face representations are sparse, selective, and robust',
	 'urllink': u'http://arxiv.org/abs/1412.1265'}
2015-04-10 04:40:03+0000 [xxu46_10] INFO: Crawled 120 pages (at 1 pages/min), scraped 113 items (at 1 items/min)
2015-04-10 04:40:14+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1412.1261> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 04:40:14+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1412.1261>
	{'abstract': u'Maximum Common Induced Subgraph (henceforth MCIS) is among the most studied classical NP-hard problems. MCIS remains NP-hard on many graph classes including bipartite graphs, planar graphs and -trees. Little is known, however, about the parameterized complexity of the problem. When parameterized by the vertex cover number of the input graphs, the problem was recently shown to be fixed-parameter tractable. Capitalizing on this result, we show that the problem does not have a polynomial kernel when parameterized by vertex cover unless . We also show that mccis (MCCIS), which is a variant where the solution must be connected, is also fixed-parameter tractable when parameterized by the vertex cover number of input graphs. Both problems are shown to be W[1]-complete on bipartite graphs and graphs of girth five and, unless P = NP, they do not belong to the class XP when parameterized by a bound on the size of the minimum feedback vertex sets of the input graphs, that is solving them in polynomial time is very unlikely when this parameter is a constant.',
	 'authors': u'Faisal N. Abu-Khzam, \xc9douard Bonnet, Florian Sikora,',
	 'category': u'Computer Science ',
	 'date': '2014-12-3',
	 'pdflink': u'http://arxiv.org/pdf/1412.1261',
	 'subjects': u'Data Structures and Algorithms (cs.DS)',
	 'title': u'\nOn the Complexity of Various Parameterizations of Common Induced  Subgraph Isomorphism',
	 'urllink': u'http://arxiv.org/abs/1412.1261'}
2015-04-10 04:41:03+0000 [xxu46_10] INFO: Crawled 121 pages (at 1 pages/min), scraped 114 items (at 1 items/min)
2015-04-10 04:41:28+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1412.1257> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 04:41:28+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1412.1257>
	{'abstract': u'In this article, the first general constructions of fast-decodable, more specifically (conditionally) -group decodable, space-time block codes for the Nonorthogonal Amplify and Forward (NAF) Multiple-Input Multiple-Output (MIMO) relay channel under the half-duplex constraint are proposed. In this scenario, the source and the intermediate relays used for data amplification are allowed to employ multiple antennas for data transmission and reception. The worst-case decoding complexity of the obtained codes is reduced by up to . In addition to being fast-decodable, the proposed codes achieve full-diversity and have nonvanishing determinants, which has been shown to be useful for achieving the optimal Diversity-Multiplexing Tradeoff (DMT) of the NAF channel. Further, it is shown that the same techniques as in the cooperative scenario can be utilized to achieve fast-decodability for -user MIMO Multiple-Access Channel (MAC) space-time block codes. The resulting codes in addition exhibit the conditional nonvanishing determinant property which, for its part, has been shown to be useful for achieving the optimal MAC-DMT.',
	 'authors': u'Amaro Barreal, Camilla Hollanti, Nadya Markin,',
	 'category': u'Computer Science ',
	 'date': '2014-12-3',
	 'pdflink': u'http://arxiv.org/pdf/1412.1257',
	 'subjects': u'Information Theory (cs.IT)',
	 'title': u'\nFast-Decodable Space-Time Codes for the $N$-Relay and Multiple-Access  MIMO Channel',
	 'urllink': u'http://arxiv.org/abs/1412.1257'}
2015-04-10 04:42:03+0000 [xxu46_10] INFO: Crawled 122 pages (at 1 pages/min), scraped 115 items (at 1 items/min)
2015-04-10 04:42:45+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1412.1254> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 04:42:45+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1412.1254>
	{'abstract': u'The longest common extension (LCE) of two indices in a string is the length of the longest identical substrings starting at these two indices. The LCE problem asks to preprocess a string into a compact data structure that supports fast LCE queries. In this paper we generalize the LCE problem to trees and suggest a few applications of LCE in trees to tries and XML databases. Given a labeled and rooted tree of size , the goal is to preprocess into a compact data structure that support the following LCE queries between subpaths and subtrees in . Let , , , and be nodes of such that and are descendants of and respectively. begin item : (path-path ) return the longest common prefix of the paths and . item : (path-tree ) return maximal path-path LCE of the path and any path from to a descendant leaf. item : (tree-tree ) return a maximal path-path LCE of any pair of paths from and to descendant leaves. end We present the first non-trivial bounds for supporting these queries. For queries, we present a linear-space solution with query time. For queries, we present a linear-space solution with query time, and complement this with a lower bound showing that any path-tree LCE structure of size must necessarily use time to answer queries. For queries, we present a time-space trade-off, that given any parameter , , leads to an space and query-time solution. This is complemented with a reduction to the the set intersection problem implying that a fast linear space solution is not likely to exist.',
	 'authors': u'Philip Bille, Pawel Gawrychowski, Inge Li Goertz, Gad M. Landau, Oren Weimann,',
	 'category': u'Computer Science ',
	 'date': '2014-12-3',
	 'pdflink': u'http://arxiv.org/pdf/1412.1254',
	 'subjects': u'Data Structures and Algorithms (cs.DS)',
	 'title': u'\nLongest Common Extensions in Trees',
	 'urllink': u'http://arxiv.org/abs/1412.1254'}
2015-04-10 04:43:03+0000 [xxu46_10] INFO: Crawled 123 pages (at 1 pages/min), scraped 116 items (at 1 items/min)
2015-04-10 04:43:53+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1412.1251> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 04:43:53+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1412.1251>
	{'abstract': u'Human-Robot Social Interaction became one of active research fields in which researchers from different areas propose solutions and directives leading robots to improve their interactions with humans. In this paper we propose to introduce works in both human robot interaction and human computer interaction and to make a bridge between them, i.e. to integrate emotions and capabilities concepts of the robot in human computer model to become adequate for human robot interaction and discuss challenges related to the proposed model. Finally an illustration through real case of this model will be presented.',
	 'authors': u'Tarek Toumi, Abdelmadjid Zidani,',
	 'category': u'Computer Science ',
	 'date': '2014-12-3',
	 'pdflink': u'http://arxiv.org/pdf/1412.1251',
	 'subjects': u'Robotics (cs.RO)',
	 'title': u'\nFrom Human-Computer Interaction to Human-Robot Social Interaction',
	 'urllink': u'http://arxiv.org/abs/1412.1251'}
2015-04-10 04:44:03+0000 [xxu46_10] INFO: Crawled 124 pages (at 1 pages/min), scraped 117 items (at 1 items/min)
2015-04-10 04:45:03+0000 [xxu46_10] INFO: Crawled 124 pages (at 0 pages/min), scraped 117 items (at 0 items/min)
2015-04-10 04:45:10+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1412.1241> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 04:45:10+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1412.1241>
	{'abstract': u'The problem of detecting and removing redundant constraints is fundamental in optimization. We focus on the case of linear programs (LPs) in dictionary form, given by equality constraints in variables, where the variables are constrained to be nonnegative. A variable is called redundant, if after removing the LP still has the same feasible region. The time needed to solve such an LP is denoted by . It is easy to see that solving LPs of the above size is sufficient to detect all redundancies. The currently fastest practical method is the one by Clarkson: it solves linear programs, but each of them has at most variables, where is the number of nonredundant constraints. In the first part we show that knowing all of the finitely many dictionaries of the LP is sufficient for the purpose of redundancy detection. A dictionary is a matrix that can be thought of as an enriched encoding of a vertex in the LP. Moreover - and this is the combinatorial aspect - it is enough to know only the signs of the entries, the actual values do not matter. Concretely we show that for any variable one can find a dictionary, such that its sign pattern is either a redundancy or nonredundancy certificate for . In the second part we show that considering only the sign patterns of the dictionary, there is an output sensitive algorithm of running time to detect all redundancies. In the case where all constraints are in general position, the running time is , which is essentially the running time of the Clarkson method. Our algorithm extends naturally to a more general setting of arrangements of oriented topological hyperplane arrangements.',
	 'authors': u'Komei Fukuda, Bernd G\xe4rtner, May Szedl\xe1k,',
	 'category': u'Computer Science ',
	 'date': '2014-12-3',
	 'pdflink': u'http://arxiv.org/pdf/1412.1241',
	 'subjects': u'Computational Geometry (cs.CG)',
	 'title': u'\nCombinatorial Redundancy Detection',
	 'urllink': u'http://arxiv.org/abs/1412.1241'}
2015-04-10 04:46:03+0000 [xxu46_10] INFO: Crawled 125 pages (at 1 pages/min), scraped 118 items (at 1 items/min)
2015-04-10 04:46:24+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1412.1229> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 04:46:24+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1412.1229>
	{'abstract': u"In this paper the reason why entropy reduction (negentropy) can be used to measure the complexity of any computation was first elaborated both in the aspect of mathematics and informational physics. In the same time the equivalence of computation and information was clearly stated. Then the complexities of three specific problems: logical compare, sorting and SAT, were analyzed and measured. The result showed SAT was a problem with exponential complexity which naturally leads to the conclusion that no efficient algorithm exists to solve it. That's to say: NP!=P.",
	 'authors': u'Feng Pan,',
	 'category': u'Computer Science ',
	 'date': '2014-12-3',
	 'pdflink': u'http://arxiv.org/e-print/1412.1229',
	 'subjects': u'Computational Complexity (cs.CC)',
	 'title': u'\nSAT is a problem with exponential complexity measured by negentropy',
	 'urllink': u'http://arxiv.org/abs/1412.1229'}
2015-04-10 04:47:03+0000 [xxu46_10] INFO: Crawled 126 pages (at 1 pages/min), scraped 119 items (at 1 items/min)
2015-04-10 04:47:34+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1412.1227> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 04:47:35+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1412.1227>
	{'abstract': u'This article presents an overview of the challenges of increasing development of wireless links to enable a more consistent and transparent interconnection between people and the digital world. These issues are in the domain of high performance architectures for conventional applications of wireless Internet, but also in the field of sensor networks and connected objects. Beyond the constraints of the various applications push to develop architectures with high digital capabilities like software defined radio. In each of these categories, examples of approaches proposed by INRIA Socrate team are presented.',
	 'authors': u'Guillaume Villemaud, Florin Hutu, Tanguy Risset, Jean-Marie Gorce,',
	 'category': u'Computer Science ',
	 'date': '2014-12-3',
	 'pdflink': u'http://arxiv.org/pdf/1412.1227',
	 'subjects': u'Networking and Internet Architecture (cs.NI)',
	 'title': u"\nEnjeux et propositions sur les architectures RF pour l'homme connect\xe9  \xe0 la soci\xe9t\xe9 num\xe9rique",
	 'urllink': u'http://arxiv.org/abs/1412.1227'}
2015-04-10 04:48:03+0000 [xxu46_10] INFO: Crawled 127 pages (at 1 pages/min), scraped 120 items (at 1 items/min)
2015-04-10 04:48:53+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1412.1221> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 04:48:53+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1412.1221>
	{'abstract': u"Sequential tasks cannot be effectively handled in logic programming based on classical logic or linear logic. This limitation can be addressed by using a fragment of Japaridze'sSequential tasks cannot be effectively handled in logic programming based on classical logic or linear logic. This limitation can be addressed by using a fragment of Japaridze's computability logic. We propose seqweb, an extension to LogicWeb with sequential goal formulas. SeqWeb extends the LogicWeb by allowing goals of the form and where is a goal. These goals allow us to specify both sequential-conjunctive and sequential-disjunctive tasks. computability logic. We propose seqweb, an extension to LogicWeb with sequential goal formulas. SeqWeb extends the LogicWeb by allowing goals of the form and where is a goal. These goals allow us to specify both sequential-conjunctive and sequential-disjunctive tasks.",
	 'authors': u'Daeseong Kang, Keehang Kwon, Zulkarnine Mahmud,',
	 'category': u'Computer Science ',
	 'date': '2014-12-3',
	 'pdflink': u'http://arxiv.org/pdf/1412.1221',
	 'subjects': u'Programming Languages (cs.PL)',
	 'title': u'\nSequential Operations in LogicWeb',
	 'urllink': u'http://arxiv.org/abs/1412.1221'}
2015-04-10 04:49:03+0000 [xxu46_10] INFO: Crawled 128 pages (at 1 pages/min), scraped 121 items (at 1 items/min)
2015-04-10 04:50:03+0000 [xxu46_10] INFO: Crawled 128 pages (at 0 pages/min), scraped 121 items (at 0 items/min)
2015-04-10 04:50:06+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1412.1219> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 04:50:06+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1412.1219>
	{'abstract': u'We present here a real time mobile mapping system mounted on a vehicle. The terrestrial acquisition system is based on a geolocation system and two sensors, namely, a laser scanner and a camera with a fish-eye lens. We produce 3D colored points cloud and textured models of the environment. Once the system has been calibrated, the data acquisition and processing are done "on the way". This article mainly presents our methods of colorization of point cloud, triangulation and texture mapping.',
	 'authors': u'Jean-Emmanuel Deschaud, Xavier Brun, Fran\xe7ois Goulette,',
	 'category': u'Computer Science ',
	 'date': '2014-12-3',
	 'pdflink': u'http://arxiv.org/pdf/1412.1219',
	 'subjects': u'Robotics (cs.RO)',
	 'title': u"\nColorisation et texturation temps r\xe9el d'environnements urbains par  syst\xe8me mobile avec scanner laser et cam\xe9ra fish-eye",
	 'urllink': u'http://arxiv.org/abs/1412.1219'}
2015-04-10 04:51:03+0000 [xxu46_10] INFO: Crawled 129 pages (at 1 pages/min), scraped 122 items (at 1 items/min)
2015-04-10 04:51:08+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1412.1216> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 04:51:08+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1412.1216>
	{'abstract': u'The visual observation and tracking of cells and other micrometer-sized objects has many different biomedical applications. The automation of those tasks based on computer methods helps in the evaluation of such measurements. In this work, we present a general purpose algorithm that excels at evaluating deterministic behavior of micrometer-sized objects. Our concrete application is the tracking of fast moving objects over large distances along deterministic trajectories in a microscopic video. Thereby, we are able to determine characteristic properties of the objects. For this purpose, we use a set of basic algorithms, including blob recognition, feature-based shape recognition and a graph algorithm, and combined them in a novel way. An evaluation of the algorithms performance shows a high accuracy in the recognition of objects as well as of complete trajectories. Moreover, a direct comparison to a similar algorithm shows superior recognition rates.',
	 'authors': u'Alexandra Heidsieck,',
	 'category': u'Computer Science ',
	 'date': '2014-12-3',
	 'pdflink': u'http://arxiv.org/pdf/1412.1216',
	 'subjects': u'Computer Vision and Pattern Recognition (cs.CV)',
	 'title': u'\nSimple Two-Dimensional Object Tracking based on a Graph Algorithm',
	 'urllink': u'http://arxiv.org/abs/1412.1216'}
2015-04-10 04:52:03+0000 [xxu46_10] INFO: Crawled 130 pages (at 1 pages/min), scraped 123 items (at 1 items/min)
2015-04-10 04:52:10+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1412.1215> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 04:52:10+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1412.1215>
	{'abstract': u'In the following article we elected to study with NooJ the lexis of a 17 th century text, Mary Astell\'s seminal essay, A Serious Proposal to the Ladies, part I, published in 1694. We first focused on the semantics to see how Astell builds her vindication of the female sex, which words she uses to sensitise women to their alienated condition and promote their education. Then we studied the morphology of the lexemes (which is different from contemporary English) used by the author, thanks to the NooJ tools we have devised for this purpose. NooJ has great functionalities for lexicographic work. Its commands and graphs prove to be most efficient in the spotting of archaic words or variants in spelling. Introduction In our previous articles, we have studied the singularities of 17 th century English within the framework of a diachronic analysis thanks to syntactical and morphological graphs and thanks to the dictionaries we have compiled from a corpus that may be expanded overtime. Our early work was based on a limited corpus of English travel literature to Greece in the 17 th century. This article deals with a late seventeenth century text written by a woman philosopher and essayist, Mary Astell (1666--1731), considered as one of the first English feminists. Astell wrote her essay at a time in English history when women were "the weaker vessel" and their main business in life was to charm and please men by their looks and submissiveness. In this essay we will see how NooJ can help us analyse Astell\'s rhetoric (what point of view does she adopt, does she speak in her own name, in the name of all women, what is her representation of men and women and their relationships in the text, what are the goals of education?). Then we will turn our attention to the morphology of words in the text and use NooJ commands and graphs to carry out a lexicographic inquiry into Astell\'s lexemes.',
	 'authors': u'H\xe9l\xe8ne Pignot, Odile Piton,',
	 'category': u'Computer Science ',
	 'date': '2014-12-3',
	 'pdflink': u'http://arxiv.org/pdf/1412.1215',
	 'subjects': u'Computation and Language (cs.CL)',
	 'title': u"\nMary Astell's words in A Serious Proposal to the Ladies (part I), a  lexicographic inquiry with NooJ",
	 'urllink': u'http://arxiv.org/abs/1412.1215'}
2015-04-10 04:53:03+0000 [xxu46_10] INFO: Crawled 131 pages (at 1 pages/min), scraped 124 items (at 1 items/min)
2015-04-10 04:53:07+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1412.1205> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 04:53:07+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1412.1205>
	{'abstract': u'In this paper, we present a novel yet simple homotopy proximal mapping algorithm for compressive sensing. The algorithm adopts a simple proximal mapping for norm regularization at each iteration and gradually reduces the regularization parameter of the norm. We prove a global linear convergence for the proposed homotopy proximal mapping (HPM) algorithm for solving compressive sensing under three different settings (i) sparse signal recovery under noiseless measurements, (ii) sparse signal recovery under noise measurements, and (iii) nearly-sparse signal recovery under sub-gaussian noise measurements. In particular, we show that when the measurement matrix satisfies Restricted Isometric Properties (RIP), our theoretical results in settings (i) and (ii) almost recover the best condition on the RIP constants for compressive sensing. In addition, in setting (iii), our results for sparse signal recovery are better than the previous results, and furthermore our analysis explicitly exhibits that more observations lead to not only more accurate recovery but also faster convergence. Compared with previous studies on linear convergence for sparse signal recovery, our algorithm is simple and efficient, and our results are better and provide more insights.',
	 'authors': u'Tianbao Yang, Lijun Zhang, Rong Jin, Shenghuo Zhu,',
	 'category': u'Computer Science ',
	 'date': '2014-12-3',
	 'pdflink': u'http://arxiv.org/pdf/1412.1205',
	 'subjects': u'Information Theory (cs.IT)',
	 'title': u'\nA Simple Homotopy Proximal Mapping for Compressive Sensing',
	 'urllink': u'http://arxiv.org/abs/1412.1205'}
2015-04-10 04:54:03+0000 [xxu46_10] INFO: Crawled 132 pages (at 1 pages/min), scraped 125 items (at 1 items/min)
2015-04-10 04:54:24+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1412.1194> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 04:54:24+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1412.1194>
	{'abstract': u'This paper introduces a high efficient local spatiotemporal descriptor, called gradient boundary histograms (GBH). The proposed GBH descriptor is built on simple spatio-temporal gradients, which are fast to compute. We demonstrate that it can better represent local structure and motion than other gradient-based descriptors, and significantly outperforms them on large realistic datasets. A comprehensive evaluation shows that the recognition accuracy is preserved while the spatial resolution is greatly reduced, which yields both high efficiency and low memory usage.',
	 'authors': u'Feng Shi, Robert Laganiere, Emil Petriu,',
	 'category': u'Computer Science ',
	 'date': '2014-12-3',
	 'pdflink': u'http://arxiv.org/pdf/1412.1194',
	 'subjects': u'Computer Vision and Pattern Recognition (cs.CV)',
	 'title': u'\nGradient Boundary Histograms for Action Recognition',
	 'urllink': u'http://arxiv.org/abs/1412.1194'}
2015-04-10 04:55:03+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1412.1193> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 04:55:03+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1412.1193>
	{'abstract': u'In this report we review and discuss some theoretical aspects of Amari\'s natural gradient method, provide a unifying picture of the many different versions of it which have appeared over the years, and offer some new insights and perspectives regarding the method and its relationship to other optimization methods. Among our various contributions is the identification of a general condition under which the Fisher information matrix and Schraudolph\'s generalized Gauss-Newton matrix are equivalent. This equivalence implies that optimization methods which use the latter matrix, such as the Hessian-free optimization approach of Martens, are actually natural gradient methods in disguise. It also lets us view natural gradient methods as approximate Newton methods, justifying the application of various "update damping" techniques to them, which are designed to compensate for break-downs in local quadratic approximations. Additionally, we analyze the parameterization invariance possessed by the natural gradient method in the idealized setting of infinitesimally small update steps, and consider the extent to which it holds for practical versions of the method which take large discrete steps. We go on to show that parameterization invariance is not possessed by the classical Newton-Raphson method (even in the idealized setting), and then give a general characterization of gradient-based methods which do possess it.',
	 'authors': u'James Martens,',
	 'category': u'Computer Science ',
	 'date': '2014-12-3',
	 'pdflink': u'http://arxiv.org/pdf/1412.1193',
	 'subjects': u'Learning (cs.LG)',
	 'title': u'\nNew perspectives on the natural gradient method',
	 'urllink': u'http://arxiv.org/abs/1412.1193'}
2015-04-10 04:55:03+0000 [xxu46_10] INFO: Crawled 134 pages (at 2 pages/min), scraped 127 items (at 2 items/min)
2015-04-10 04:56:03+0000 [xxu46_10] INFO: Crawled 134 pages (at 0 pages/min), scraped 127 items (at 0 items/min)
2015-04-10 04:56:18+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1412.1189> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 04:56:18+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1412.1189>
	{'abstract': u'We consider the domination number for on-line social networks, both in a stochastic network model, and for real-world, networked data. Asymptotic sublinear bounds are rigorously derived for the domination number of graphs generated by the memoryless geometric protean random graph model. We establish sublinear bounds for the domination number of graphs in the Facebook 100 data set, and these bounds are well-correlated with those predicted by the stochastic model. In addition, we derive the asymptotic value of the domination number in classical random geometric graphs.',
	 'authors': u'Anthony Bonato, Marc Lozier, Dieter Mitsche, Xavier P\xe9rez-Gim\xe9nez, Pawe\u0142 Pra\u0142at,',
	 'category': u'Computer Science ',
	 'date': '2014-12-3',
	 'pdflink': u'http://arxiv.org/pdf/1412.1189',
	 'subjects': u'Social and Information Networks (cs.SI)',
	 'title': u'\nThe domination number of on-line social networks and random geometric  graphs',
	 'urllink': u'http://arxiv.org/abs/1412.1189'}
2015-04-10 04:57:03+0000 [xxu46_10] INFO: Crawled 135 pages (at 1 pages/min), scraped 128 items (at 1 items/min)
2015-04-10 04:57:33+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1412.1185> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 04:57:33+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1412.1185>
	{'abstract': u'The vast majority of YouTube videos never become popular, languishing in obscurity with few views, no likes, and no comments. We use information theoretical measures based on entropy to examine how time series distributions of common measures of popularity in videos from YouTube\'s "Trending videos" and "Most recent" video feeds relate to the theoretical concept of attention. While most of the videos in the "Most recent" feed are never popular, some 20% of them have distributions of attention metrics and measures of entropy that are similar to distributions for "Trending videos". We analyze how the 20% of "Most recent" videos that become somewhat popular differ from the 80% that do not, then compare these popular "Most recent" videos to different subsets of "Trending videos" to try to characterize and compare the attention each receives.',
	 'authors': u'Jonathan Scott Morgan, Iman Barjasteh, Cliff Lampe, Hayder Radha,',
	 'category': u'Computer Science ',
	 'date': '2014-12-3',
	 'pdflink': u'http://arxiv.org/pdf/1412.1185',
	 'subjects': u'Social and Information Networks (cs.SI)',
	 'title': u'\nThe Entropy of Attention and Popularity in YouTube Videos',
	 'urllink': u'http://arxiv.org/abs/1412.1185'}
2015-04-10 04:58:03+0000 [xxu46_10] INFO: Crawled 136 pages (at 1 pages/min), scraped 129 items (at 1 items/min)
2015-04-10 04:58:36+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1412.1180> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 04:58:36+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1412.1180>
	{'abstract': u'Current layouts for alphabetic input on mobile phone keypads are very inefficient. We propose a genetic algorithm (GA) to find a suitable keypad layout for each user, based on their personal text history. It incorporates codes for frequent multigrams, which may be directly input. This greatly reduces the average number of strokes required for typing. We optimize for two-handed use, the left thumb covering the leftmost rows and vice versa. The GA mini- mizes the number of strokes, consecutive use of the same key, and consecutive use of the same hand. Using these criteria, the algorithm re-arranges the 26 alphabetic characters, plus 14 additional multigrams, on the 10-key pad. We demonstrate that this arrangement can generate a more effective layout, especially for SMS-style messages. Substantial savings are verified by both computational analysis and human evaluation.',
	 'authors': u'Joonseok Lee, R. I., McKay,',
	 'category': u'Computer Science ',
	 'date': '2014-12-3',
	 'pdflink': u'http://arxiv.org/pdf/1412.1180',
	 'subjects': u'Human-Computer Interaction (cs.HC)',
	 'title': u'\nOptimizing a Personalized Multigram Cellphone Keypad',
	 'urllink': u'http://arxiv.org/abs/1412.1180'}
2015-04-10 04:59:03+0000 [xxu46_10] INFO: Crawled 137 pages (at 1 pages/min), scraped 130 items (at 1 items/min)
2015-04-10 04:59:46+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1412.1156> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 04:59:46+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1412.1156>
	{'abstract': u'In this paper we present a novel rule-based approach for Runtime Verification of FLTL properties over finite but expanding traces. Our system exploits Horn clauses in implication form and relies on a forward chaining-based monitoring algorithm. This approach avoids the branching structure and exponential complexity typical of tableaux-based formulations, creating monitors with a single state and a fixed number of rules. This allows for a fast and scalable tool for Runtime Verification: we present the technical details together with a working implementation.',
	 'authors': u"Alan Perotti, Guido Boella, Artur d'Avila Garcez,",
	 'category': u'Computer Science ',
	 'date': '2014-12-3',
	 'pdflink': u'http://arxiv.org/pdf/1412.1156',
	 'subjects': u'Logic in Computer Science (cs.LO)',
	 'title': u'\nRuntime Verification Through Forward Chaining',
	 'urllink': u'http://arxiv.org/abs/1412.1156'}
2015-04-10 05:00:03+0000 [xxu46_10] INFO: Crawled 138 pages (at 1 pages/min), scraped 131 items (at 1 items/min)
2015-04-10 05:00:57+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1412.1154> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 05:00:57+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1412.1154>
	{'abstract': u'We present an approach to constrained Horn clause (CHC) verification combining three techniques: abstract interpretation over a domain of convex polyhedra, specialisation of the constraints in CHCs using abstract interpretation of query-answer transformed clauses, and refinement by splitting predicates. The purpose of the work is to investigate how analysis and transformation tools developed for constraint logic programs (CLP) can be applied to the Horn clause verification problem. Abstract interpretation over convex polyhedra is capable of deriving sophisticated invariants and when used in conjunction with specialisation for propagating constraints it can frequently solve challenging verification problems. This is a contribution in itself, but refinement is needed when it fails, and the question of how to refine convex polyhedral analyses has not been studied much. We present a refinement technique based on interpolants derived from a counterexample trace; these are used to drive a property-based specialisation that splits predicates, leading in turn to more precise convex polyhedral analyses. The process of specialisation, analysis and splitting can be repeated, in a manner similar to the CEGAR and iterative specialisation approaches.',
	 'authors': u'Bishoksan Kafle, John P. Gallagher,',
	 'category': u'Computer Science ',
	 'date': '2014-12-3',
	 'pdflink': u'http://arxiv.org/pdf/1412.1154',
	 'subjects': u'Logic in Computer Science (cs.LO)',
	 'title': u'\nConvex polyhedral abstractions, specialisation and property-based  predicate splitting in Horn clause verification',
	 'urllink': u'http://arxiv.org/abs/1412.1154'}
2015-04-10 05:01:03+0000 [xxu46_10] INFO: Crawled 139 pages (at 1 pages/min), scraped 132 items (at 1 items/min)
2015-04-10 05:01:49+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1412.1153> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 05:01:49+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1412.1153>
	{'abstract': u'Languages based on the theory of timed automata are a well established approach for modelling and analysing real-time systems, with many applications both in industrial and academic context. Model checking for timed automata has been studied extensively during the last two decades; however, even now industrial-grade model checkers are available only for few timed automata dialects (in particular Uppaal timed automata), exhibit limited scalability for systems with large discrete state space, or cannot handle parametrised systems. We explore the use of Horn constraints and off-the-shelf model checkers for analysis of networks of timed automata. The resulting analysis method is fully symbolic and applicable to systems with large or infinite discrete state space, and can be extended to include various language features, for instance Uppaal-style communication/broadcast channels and BIP-style interactions, and systems with infinite parallelism. Experiments demonstrate the feasibility of the method.',
	 'authors': u'Hossein Hojjat, Philipp R\xfcmmer, Pavle Subotic, Wang Yi,',
	 'category': u'Computer Science ',
	 'date': '2014-12-3',
	 'pdflink': u'http://arxiv.org/pdf/1412.1153',
	 'subjects': u'Logic in Computer Science (cs.LO)',
	 'title': u'\nHorn Clauses for Communicating Timed Systems',
	 'urllink': u'http://arxiv.org/abs/1412.1153'}
2015-04-10 05:02:03+0000 [xxu46_10] INFO: Crawled 140 pages (at 1 pages/min), scraped 133 items (at 1 items/min)
2015-04-10 05:03:00+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1412.1152> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 05:03:00+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1412.1152>
	{'abstract': u'In this paper, we explore different techniques to synthesize modular invariants for synchronous code encoded as Horn clauses. Modular invariants are a set of formulas that characterizes the validity of predicates. They are very useful for different aspects of analysis, synthesis, testing and program transformation. We describe two techniques to generate modular invariants for code written in the synchronous dataflow language Lustre. The first technique directly encodes the synchronous code in a modular fashion. While in the second technique, we synthesize modular invariants starting from a monolithic invariant. Both techniques, take advantage of analysis techniques based on property-directed reachability. We also describe a technique to minimize the synthesized invariants.',
	 'authors': u'Pierre-Loic Garoche, Arie Gurfinkel, Temesghen Kahsai,',
	 'category': u'Computer Science ',
	 'date': '2014-12-3',
	 'pdflink': u'http://arxiv.org/pdf/1412.1152',
	 'subjects': u'Logic in Computer Science (cs.LO)',
	 'title': u'\nSynthesizing Modular Invariants for Synchronous Code',
	 'urllink': u'http://arxiv.org/abs/1412.1152'}
2015-04-10 05:03:03+0000 [xxu46_10] INFO: Crawled 141 pages (at 1 pages/min), scraped 134 items (at 1 items/min)
2015-04-10 05:04:03+0000 [xxu46_10] INFO: Crawled 141 pages (at 0 pages/min), scraped 134 items (at 0 items/min)
2015-04-10 05:04:17+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1412.1151> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 05:04:17+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1412.1151>
	{'abstract': u'We present a verification technique for program safety that combines Iterated Specialization and Interpolating Horn Clause Solving. Our new method composes together these two techniques in a modular way by exploiting the common Horn Clause representation of the verification problem. The Iterated Specialization verifier transforms an initial set of verification conditions by using unfold/fold equivalence preserving transformation rules. During transformation, program invariants are discovered by applying widening operators. Then the output set of specialized verification conditions is analyzed by an Interpolating Horn Clause solver, hence adding the effect of interpolation to the effect of widening. The specialization and interpolation phases can be iterated, and also combined with other transformations that change the direction of propagation of the constraints (forward from the program preconditions or backward from the error conditions). We have implemented our verification technique by integrating the VeriMAP verifier with the FTCLP Horn Clause solver, based on Iterated Specialization and Interpolation, respectively. Our experimental results show that the integrated verifier improves the precision of each of the individual components run separately.',
	 'authors': u'Emanuele De Angelis, Fabio Fioravanti, Jorge A. Navas, Maurizio Proietti,',
	 'category': u'Computer Science ',
	 'date': '2014-12-3',
	 'pdflink': u'http://arxiv.org/pdf/1412.1151',
	 'subjects': u'Logic in Computer Science (cs.LO)',
	 'title': u'\nVerification of Programs by Combining Iterated Specialization with  Interpolation',
	 'urllink': u'http://arxiv.org/abs/1412.1151'}
2015-04-10 05:05:03+0000 [xxu46_10] INFO: Crawled 142 pages (at 1 pages/min), scraped 135 items (at 1 items/min)
2015-04-10 05:05:15+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1412.1145> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 05:05:15+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1412.1145>
	{'abstract': u'Matrix multiplication (hereafter we use the acronym MM) is among the most fundamental operations of modern computations. The efficiency of its performance depends on various factors, in particular vectorization, data movement and arithmetic complexity of the computations, but here we focus just on the study of the arithmetic cost and the impact of this study on other areas of modern computing. In the early 1970s it was expected that the straightforward cubic time algorithm for MM will soon be accelerated to enable MM in nearly quadratic arithmetic time, with some far fetched implications. While pursuing this goal the mainstream research had its focus on the decrease of the classical exponent 3 of the complexity of MM towards its lower bound 2, disregarding the growth of the input size required to support this decrease. Eventually, surprising combinations of novel ideas and sophisticated techniques enabled the decrease of the exponent to its benchmark value of about 2.38, but the supporting MM algorithms improved the straightforward one only for the inputs of immense sizes. Meanwhile, the communication complexity, rather than the arithmetic complexity, has become the bottleneck of computations in linear algebra. This development may seem to undermine the value of the past and future research aimed at the decrease of the arithmetic cost of MM, but we feel that the study should be reassessed rather than closed and forgotten. We review the old and new work in this area in the present day context, recall some major techniques introduced in the study of MM, discuss their impact on the modern theory and practice of computations for MM and beyond MM, and link one of these techniques to some simple algorithms for inner product and summation.',
	 'authors': u'Victor Y. Pan,',
	 'category': u'Computer Science ',
	 'date': '2014-12-3',
	 'pdflink': u'http://arxiv.org/pdf/1412.1145',
	 'subjects': u'Data Structures and Algorithms (cs.DS)',
	 'title': u'\nMatrix Multiplication, Trilinear Decompositions, APA Algorithms, and  Summation',
	 'urllink': u'http://arxiv.org/abs/1412.1145'}
2015-04-10 05:06:03+0000 [xxu46_10] INFO: Crawled 143 pages (at 1 pages/min), scraped 136 items (at 1 items/min)
2015-04-10 05:06:25+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1412.1143> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 05:06:25+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1412.1143>
	{'abstract': u'Marcus, Spielman, and Srivastava in their seminal work [MSS13] resolved the Kadison-Singer conjecture by proving that the sum of any set of finitely supported independently distributed random vectors with "small" expected squared norm that are in isotropic position (in expectation) attains a "small" spectral norm with a nonzero probability. Their proof crucially employs real stability of polynomials which is the natural generalization of real-rootedness to multivariate polynomials. Strongly Rayleigh distributions are families of probability distributions whose generating polynomials are real stable [BBL09]. As independent distributions are just special cases of strongly Rayleigh measures, it is a natural question to see if the main theorem of [MSS13] can be extended to families of vectors assigned to the elements of a strongly Rayleigh distribution. In this paper we answer this question affirmatively; we show that for any homogeneous strongly Rayleigh distribution where the marginal probabilities are upper bounded by and any isotropic set of vectors assigned to the underlying elements whose norms are at most , there is a set in the support of the distribution such that the spectral norm of the sum of the vectors assigned to the elements of the set is at most . We employ our theorem to provide a sufficient condition for the existence of spectrally thin trees. This, together with a recent work of the authors, provide an improved upper bound on the integrality gap of the natural LP relaxation of the Asymmetric Traveling Salesman Problem.',
	 'authors': u'Nima Anari, Shayan Oveis Gharan,',
	 'category': u'Computer Science ',
	 'date': '2014-12-3',
	 'pdflink': u'http://arxiv.org/pdf/1412.1143',
	 'subjects': u'Data Structures and Algorithms (cs.DS)',
	 'title': u'\nThe Kadison-Singer Problem for Strongly Rayleigh Measures and  Applications to Asymmetric TSP',
	 'urllink': u'http://arxiv.org/abs/1412.1143'}
2015-04-10 05:07:03+0000 [xxu46_10] INFO: Crawled 144 pages (at 1 pages/min), scraped 137 items (at 1 items/min)
2015-04-10 05:07:09+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1412.1141> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 05:07:09+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1412.1141>
	{'abstract': u'This paper proposes an implementation framework that lays out the ground for a coherent, systematic, and comprehensive approach to implement the National Information Assurance and Cyber Security Strategy (NIACSS) of Jordan. The Framework 1). Suggests a methodology to analyze the NIACSS, 2). Illustrates how the NIACSS analysis can be utilized to design strategic moves and develop an appropriate functional structure, and 3). proposes a set of adaptable strategic controls that govern the NIACSS implementation and allow achieving excellence, innovation, efficiency, and quality.The framework, if adopted, is expected to harvest several advantages within the following areas: information security implementation management, control and guidance, efforts consolidation, resource utilization, productive collaboration, and completeness. The framework is flexible and expandable; therefore, it can be generalized.',
	 'authors': u'Ahmed Otoom, Issa Atoum,',
	 'category': u'Computer Science ',
	 'date': '2014-11-30',
	 'pdflink': u'http://arxiv.org/pdf/1412.1141',
	 'subjects': u'Software Engineering (cs.SE)',
	 'title': u'\nAn Implementation Framework (IF) for the National Information Assurance  and Cyber Security Strategy (NIACSS) of Jordan',
	 'urllink': u'http://arxiv.org/abs/1412.1141'}
2015-04-10 05:08:03+0000 [xxu46_10] INFO: Crawled 145 pages (at 1 pages/min), scraped 138 items (at 1 items/min)
2015-04-10 05:08:18+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1412.1140> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 05:08:18+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1412.1140>
	{'abstract': u'The Sphynx project was an exploratory study to discover what might be done to improve the heavy replication of in- structions in independent instruction caches for a massively parallel machine where a single program is executing across all of the cores. While a machine with only many cores (fewer than 50) might not have any issues replicating the instructions for each core, as we approach the era where thousands of cores can be placed on one chip, the overhead of instruction replication may become unacceptably large. We believe that a large amount of sharing should be possible when the ma- chine is configured for all of the threads to issue from the same set of instructions. We propose a technique that allows sharing an instruction cache among a number of independent processor cores to allow for inter-thread sharing and reuse of instruction memory. While we do not have test cases to demonstrate the potential magnitude of performance gains that could be achieved, the potential for sharing reduces the die area required for instruction storage on chip.',
	 'authors': u'Dong-hyeon Park, Akhil Bagaria, Fabiha Hannan, Eric Storm, Josef Spjut,',
	 'category': u'Computer Science ',
	 'date': '2014-12-3',
	 'pdflink': u'http://arxiv.org/pdf/1412.1140',
	 'subjects': u'Hardware Architecture (cs.AR)',
	 'title': u'\nSphynx: A Shared Instruction Cache Exporatory Study',
	 'urllink': u'http://arxiv.org/abs/1412.1140'}
2015-04-10 05:09:03+0000 [xxu46_10] INFO: Crawled 146 pages (at 1 pages/min), scraped 139 items (at 1 items/min)
2015-04-10 05:09:15+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1412.1138> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 05:09:15+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1412.1138>
	{'abstract': u"A database of fetal heart rate (FHR) time series measured from 7221 patients during labor is analyzed with the aim of learning the types of features of these recordings that are informative of low cord pH. Our 'highly comparative' analysis involves extracting over 9000 time-series analysis features from each FHR time series, including measures of autocorrelation, entropy, distribution, and various model fits. This diverse collection of features was developed in previous work, and is publicly available. We describe five features that most accurately classify a balanced training set of 59 'low pH' and 59 'normal pH' FHR recordings. We then describe five of the features with the strongest linear correlation to cord pH across the full dataset of FHR time series. The features identified in this work may be used as part of a system for guiding intervention during labor in future. This work successfully demonstrates the utility of comparing across a large, interdisciplinary literature on time-series analysis to automatically contribute new scientific results for specific biomedical signal processing challenges.",
	 'authors': u'B. D. Fulcher, A. E. Georgieva, C. W. G. Redman, Nick S. Jones,',
	 'category': u'Computer Science ',
	 'date': '2014-12-3',
	 'pdflink': u'http://arxiv.org/pdf/1412.1138',
	 'subjects': u'Learning (cs.LG)',
	 'title': u'\nHighly comparative fetal heart rate analysis',
	 'urllink': u'http://arxiv.org/abs/1412.1138'}
2015-04-10 05:10:03+0000 [xxu46_10] INFO: Crawled 147 pages (at 1 pages/min), scraped 140 items (at 1 items/min)
2015-04-10 05:10:29+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1412.1135> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 05:10:29+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1412.1135>
	{'abstract': u'We develop methods for detector learning which exploit joint training over both weak and strong labels and which transfer learned perceptual representations from strongly-labeled auxiliary tasks. Previous methods for weak-label learning often learn detector models independently using latent variable optimization, but fail to share deep representation knowledge across classes and usually require strong initialization. Other previous methods transfer deep representations from domains with strong labels to those with only weak labels, but do not optimize over individual latent boxes, and thus may miss specific salient structures for a particular category. We propose a model that subsumes these previous approaches, and simultaneously trains a representation and detectors for categories with either weak or strong labels present. We provide a novel formulation of a joint multiple instance learning method that includes examples from classification-style data when available, and also performs domain transfer learning to improve the underlying detector representation. Our model outperforms known methods on ImageNet-200 detection with weak labels.',
	 'authors': u'Judy Hoffman, Deepak Pathak, Trevor Darrell, Kate Saenko,',
	 'category': u'Computer Science ',
	 'date': '2014-12-2',
	 'pdflink': u'http://arxiv.org/pdf/1412.1135',
	 'subjects': u'Computer Vision and Pattern Recognition (cs.CV)',
	 'title': u'\nDetector Discovery in the Wild: Joint Multiple Instance and  Representation Learning',
	 'urllink': u'http://arxiv.org/abs/1412.1135'}
2015-04-10 05:11:03+0000 [xxu46_10] INFO: Crawled 148 pages (at 1 pages/min), scraped 141 items (at 1 items/min)
2015-04-10 05:11:47+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1412.1130> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 05:11:47+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1412.1130>
	{'abstract': u"In 1976, Knuth asked if the stable marriage problem (SMP) can be generalized to marriages consisting of 3 genders. In 1988, Alkan showed that the natural generalization of SMP to 3 genders (GSM) need not admit a stable marriage. Three years later, Ng and Hirschberg proved that it is NP-complete to determine if given preferences admit a stable marriage. They further prove an analogous result for the person stable assignment (PSA) problem. In light of Ng and Hirschberg's NP-hardness result for GSM and PSA, we initiate the study of approximate versions of these problems. In particular, we describe two optimization variants of GSM and PSA: maximally stable marriage/matching (MSM) and maximum stable submarriage/submatching (MSS). We show that both variants are NP-hard to approximate within some fixed constant factor. Conversely, we describe a simple polynomial time algorithm which computes constant factor approximations for the maximally stable marriage and matching problems. Thus both variants of MSM are APX-complete.",
	 'authors': u'Rafail Ostrovsky, Will Rosenbaum,',
	 'category': u'Computer Science ',
	 'date': '2014-12-2',
	 'pdflink': u'http://arxiv.org/pdf/1412.1130',
	 'subjects': u'Computational Complexity (cs.CC)',
	 'title': u"\nIt's Not Easy Being Three: The Approximability of Three-Dimensional  Stable Matching Problems",
	 'urllink': u'http://arxiv.org/abs/1412.1130'}
2015-04-10 05:12:03+0000 [xxu46_10] INFO: Crawled 149 pages (at 1 pages/min), scraped 142 items (at 1 items/min)
2015-04-10 05:12:19+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1412.1127> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 05:12:19+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1412.1127>
	{'abstract': u"In this paper we introduce IPMACC, a framework for translating OpenACC applications to CUDA or OpenCL. IPMACC is composed of set of translators translating OpenACC for C applications to CUDA or OpenCL. The framework uses the system compiler (e.g. nvcc) for generating final accelerator's binary. The framework can be used for extending the OpenACC API, executing OpenACC applications, or obtaining CUDA or OpenCL code which is equivalent to OpenACC code. We verify correctness of our framework under several benchmarks included from Rodinia Benchmark Suit and CUDA SDK. We also compare the performance of CUDA version of the benchmarks to OpenACC version which is compiled by our framework. By comparing CUDA and OpenACC versions, we discuss the limitations of OpenACC in achieving a performance near to highly-optimized CUDA version.",
	 'authors': u'Ahmad Lashgar, Alireza Majidi, Amirali Baniasadi,',
	 'category': u'Computer Science ',
	 'date': '2014-12-2',
	 'pdflink': u'http://arxiv.org/pdf/1412.1127',
	 'subjects': u'Programming Languages (cs.PL)',
	 'title': u'\nIPMACC: Open Source OpenACC to CUDA/OpenCL Translator',
	 'urllink': u'http://arxiv.org/abs/1412.1127'}
2015-04-10 05:13:03+0000 [xxu46_10] INFO: Crawled 150 pages (at 1 pages/min), scraped 143 items (at 1 items/min)
2015-04-10 05:13:31+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1412.1124> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 05:13:31+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1412.1124>
	{'abstract': u"We consider two CSP problems: the first CSP encodes 2D Sperner's lemma for the standard triangulation of the right triangle on small triangles; the second CSP encodes the fact that it is impossible to match cells of square to arrows (two horizontal, two vertical and four diagonal) such that arrows in two cells with a common edge differ by at most , and all arrows on the boundary of the square do not look outside (this fact is a corollary of the Brower's fixed point theorem). We prove that the tree-like resolution complexities of these CSPs are . For Sperner's lemma our result implies lower bound on the number of request to colors of vertices that is enough to make in order to find a trichromatic triangle; this lower bound was originally proved by Crescenzi and Silvestri. CSP based on Sperner's lemma is related with the -complete problem. We show that CSP corresponding to arrows is also related with a -complete problem.",
	 'authors': u'Dmitry Itsykson, Anna Malova, Vsevolod Oparin, Dmitry Sokolov,',
	 'category': u'Computer Science ',
	 'date': '2014-12-2',
	 'pdflink': u'http://arxiv.org/pdf/1412.1124',
	 'subjects': u'Computational Complexity (cs.CC)',
	 'title': u'\nTree-like resolution complexity of two planar problems',
	 'urllink': u'http://arxiv.org/abs/1412.1124'}
2015-04-10 05:14:03+0000 [xxu46_10] INFO: Crawled 151 pages (at 1 pages/min), scraped 144 items (at 1 items/min)
2015-04-10 05:14:42+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1412.1123> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 05:14:42+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1412.1123>
	{'abstract': u'Contour detection has been a fundamental component in many image segmentation and object detection systems. Most previous work utilizes low-level features such as texture or saliency to detect contours and then use them as cues for a higher-level task such as object detection. However, we claim that recognizing objects and predicting contours are two mutually related tasks. Contrary to traditional approaches, we show that we can invert the commonly established pipeline: instead of detecting contours with low-level cues for a higher-level recognition task, we exploit object-related features as high-level cues for contour detection. We achieve this goal by means of a multi-scale deep network that consists of five convolutional layers and a bifurcated fully-connected sub-network. The section from the input layer to the fifth convolutional layer is fixed and directly lifted from a pre-trained network optimized over a large-scale object classification task. This section of the network is applied to four different scales of the image input. These four parallel and identical streams are then attached to a bifurcated sub-network consisting of two independently-trained branches. One branch learns to predict the contour likelihood (with a classification objective) whereas the other branch is trained to learn the fraction of human labelers agreeing about the contour presence at a given point (with a regression criterion). We show that without any feature engineering our multi-scale deep learning approach achieves state-of-the-art results in contour detection.',
	 'authors': u'Gedas Bertasius, Jianbo Shi, Lorenzo Torresani,',
	 'category': u'Computer Science ',
	 'date': '2014-12-2',
	 'pdflink': u'http://arxiv.org/pdf/1412.1123',
	 'subjects': u'Computer Vision and Pattern Recognition (cs.CV)',
	 'title': u'\nDeepEdge: A Multi-Scale Bifurcated Deep Network for Top-Down Contour  Detection',
	 'urllink': u'http://arxiv.org/abs/1412.1123'}
2015-04-10 05:15:03+0000 [xxu46_10] INFO: Crawled 152 pages (at 1 pages/min), scraped 145 items (at 1 items/min)
2015-04-10 05:15:49+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1412.1114> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 05:15:49+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1412.1114>
	{'abstract': u'Optunity is a free software package dedicated to hyperparameter optimization. It contains various types of solvers, ranging from undirected methods to direct search, particle swarm and evolutionary optimization. The design focuses on ease of use, flexibility, code clarity and interoperability with existing software in all machine learning environments. Optunity is written in Python and contains interfaces to environments such as R and MATLAB. Optunity uses a BSD license and is freely available online at this http URL',
	 'authors': u'Marc Claesen, Jaak Simm, Dusan Popovic, Yves Moreau, Bart De Moor,',
	 'category': u'Computer Science ',
	 'date': '2014-12-2',
	 'pdflink': u'http://arxiv.org/pdf/1412.1114',
	 'subjects': u'Learning (cs.LG)',
	 'title': u'\nEasy Hyperparameter Search Using Optunity',
	 'urllink': u'http://arxiv.org/abs/1412.1114'}
2015-04-10 05:16:03+0000 [xxu46_10] INFO: Crawled 153 pages (at 1 pages/min), scraped 146 items (at 1 items/min)
2015-04-10 05:16:55+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1412.1069> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 05:16:55+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1412.1069>
	{'abstract': u'This paper proposes a new approach for approximate evaluation of #P-hard queries with probabilistic databases. In our approach, every query is evaluated entirely in the database engine by evaluating a fixed number of query plans, each providing an upper bound on the true probability, then taking their minimum. We provide an algorithm that takes into account important schema information to enumerate only the minimal necessary plans among all possible plans. Importantly, this algorithm is a strict generalization of all known results of PTIME self-join-free conjunctive queries: A query is safe if and only if our algorithm returns one single plan. We also apply three relational query optimization techniques to evaluate all minimal safe plans very fast. We give a detailed experimental evaluation of our approach and, in the process, provide a new way of thinking about the value of probabilistic methods over non-probabilistic methods for ranking query answers.',
	 'authors': u'Wolfgang Gatterbauer, Dan Suciu,',
	 'category': u'Computer Science ',
	 'date': '2014-12-2',
	 'pdflink': u'http://arxiv.org/pdf/1412.1069',
	 'subjects': u'Databases (cs.DB)',
	 'title': u'\nApproximate Lifted Inference with Probabilistic Databases',
	 'urllink': u'http://arxiv.org/abs/1412.1069'}
2015-04-10 05:17:03+0000 [xxu46_10] INFO: Crawled 154 pages (at 1 pages/min), scraped 147 items (at 1 items/min)
2015-04-10 05:17:57+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1412.1058> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 05:17:57+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1412.1058>
	{'abstract': u'Convolutional neural network (CNN) is a neural network that can make use of the internal structure of data such as the 2D structure of image data. This paper studies CNN on text categorization to exploit the 1D structure (namely, word order) of text data for accurate prediction. Instead of using low-dimensional word vectors as input as is often done, we directly apply CNN to high-dimensional text data, which leads to directly learning embedding of small text regions for use in classification. In addition to a straightforward adaptation of CNN from image to text, a simple but new variation which employs bag-of-word conversion in the convolution layer is proposed. An extension to combine multiple convolution layers is also explored for higher accuracy. The experiments demonstrate the effectiveness of our approach in comparison with state-of-the-art methods.',
	 'authors': u'Rie Johnson, Tong Zhang,',
	 'category': u'Computer Science ',
	 'date': '2014-12-1',
	 'pdflink': u'http://arxiv.org/pdf/1412.1058',
	 'subjects': u'Computation and Language (cs.CL)',
	 'title': u'\nEffective Use of Word Order for Text Categorization with Convolutional  Neural Networks',
	 'urllink': u'http://arxiv.org/abs/1412.1058'}
2015-04-10 05:18:03+0000 [xxu46_10] INFO: Crawled 155 pages (at 1 pages/min), scraped 148 items (at 1 items/min)
2015-04-10 05:19:03+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1412.1056> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 05:19:03+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1412.1056>
	{'abstract': u"In this paper, three laws are obtained for multiple-input multiple-output feedback systems, which are in entropy domain, frequency domain, and time domain, respectively. The system setup is that with causal plants and causal controllers. Those laws characterize the performance limitations of such systems imposed by the feedback mechanism. Some new notions are proposed to facilitate the analysis: negentropy rate, extended spectral flatness (extended Wiener entropy), Gaussianity-whiteness measure (joint Shannon-Wiener entropy), etc. Two approaches are adopted: the integrated approach and the divided approach. And 'uncertainty principles' are found in minimum variance control. Besides, performance limitations in minimum variance estimation and filtering are obtained. In the end, the special case of linear time-invariant feedback systems is discussed.",
	 'authors': u'Song Fang,',
	 'category': u'Computer Science ',
	 'date': '2014-12-1',
	 'pdflink': u'http://arxiv.org/e-print/1412.1056',
	 'subjects': u'Systems and Control (cs.SY)',
	 'title': u"\nThree Laws of Multivariable Feedback Systems, Extended Spectral Flatness  (Extended Wiener Entropy), 'Uncertainty Principles' in Variance Minimization,  and Performance Limitations in Minimum Variance Estimation/Filtering",
	 'urllink': u'http://arxiv.org/abs/1412.1056'}
2015-04-10 05:19:03+0000 [xxu46_10] INFO: Crawled 156 pages (at 1 pages/min), scraped 149 items (at 1 items/min)
2015-04-10 05:19:59+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1412.1044> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 05:19:59+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1412.1044>
	{'abstract': u'We define a problem theory from first principles. We investigate the objects of this theory: problems, resolutions, and solutions. We relate problem theory with set theory and with computing theory. We find taxonomies for resolutions and for problems. We build a hierarchy of resolvers: mechanism, adaptor, internalizer, learner, and subject. We show that the problem theory is complete, that is, that there are just three ways to resolve any problem: routine, trial, and analogy. Finally, we propose a thesis: We are Turing complete subjects because we are the result of an evolution of resolvers of the survival problem.',
	 'authors': u'Ram\xf3n Casares,',
	 'category': u'Computer Science ',
	 'date': '2014-12-1',
	 'pdflink': u'http://arxiv.org/pdf/1412.1044',
	 'subjects': u'Artificial Intelligence (cs.AI)',
	 'title': u'\nProblem Theory',
	 'urllink': u'http://arxiv.org/abs/1412.1044'}
2015-04-10 05:20:03+0000 [xxu46_10] INFO: Crawled 157 pages (at 1 pages/min), scraped 150 items (at 1 items/min)
2015-04-10 05:21:01+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1412.1042> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 05:21:01+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1412.1042>
	{'abstract': u'A crucial problem for many results and tools about bigraphs and bigraphical reactive systems is bigraph embedding. An embedding is more informative than a bigraph matching, since it keeps track of the correspondence between the various components of the redex (guest) within the agent (host). In this paper, we present an algorithm for computing embeddings based on a reduction to a constraint satisfaction problem. This algorithm, that we prove to be sound and complete, has been successfully implemented in LibBig, a library for manipulating bigraphical reactive systems. This library can be used for implementing a wide range of tools, and it can be adapted to various extensions of bigraphs.',
	 'authors': u'Marino Miculan, Marco Peressotti,',
	 'category': u'Computer Science ',
	 'date': '2014-12-1',
	 'pdflink': u'http://arxiv.org/pdf/1412.1042',
	 'subjects': u'Logic in Computer Science (cs.LO)',
	 'title': u'\nA CSP implementation of the bigraph embedding problem',
	 'urllink': u'http://arxiv.org/abs/1412.1042'}
2015-04-10 05:21:03+0000 [xxu46_10] INFO: Crawled 158 pages (at 1 pages/min), scraped 151 items (at 1 items/min)
2015-04-10 05:21:51+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1412.1039> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 05:21:51+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1412.1039>
	{'abstract': u'We introduce an O(n log n) time algorithm for the convex hull problem on points with locations determined by a continuous probability distribution. Such probabilistic points are common in applied settings such as location-based services using machine learning determined locations. Our algorithm finds the convex hull of such points to precision within some expected correctness determined by a user-given confidence value p. In order to precisely explain how correct the resulting structure is, we introduce a new certificate error model for calculating and understanding approximate geometric error based on the fundamental properties of a geometric structure. We show that this new error model implies correctness under a robust statistical error model, in which each point lies within the hull with probability at least p, for the convex hull problem. We additionally show that the error model that we introduce, which is based on certificates of the type used in kinetic data structures (KDS), allows for a translation of the KDS framework to this probabilistic points setting. We thus introduce a framework for calculating and maintaining geometric structures on moving objects, even when the precise location of those objects is known only probabilistically. Our framework maintains the geometric structures to within approximate correctness based on our new certificate error model. We show that, under our translation, any existing KDS is approximately correct and is efficient or close to efficient.',
	 'authors': u'F. Betul Atalay, Sorelle A. Friedler, Dianna Xu,',
	 'category': u'Computer Science ',
	 'date': '2014-12-2',
	 'pdflink': u'http://arxiv.org/pdf/1412.1039',
	 'subjects': u'Computational Geometry (cs.CG)',
	 'title': u'\nConvex Hull for Probabilistic Points',
	 'urllink': u'http://arxiv.org/abs/1412.1039'}
2015-04-10 05:22:03+0000 [xxu46_10] INFO: Crawled 159 pages (at 1 pages/min), scraped 152 items (at 1 items/min)
2015-04-10 05:23:03+0000 [xxu46_10] INFO: Crawled 159 pages (at 0 pages/min), scraped 152 items (at 0 items/min)
2015-04-10 05:23:11+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1412.1027> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 05:23:11+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1412.1027>
	{'abstract': u'Complexity is an inherent attribute of any project. The purpose of defining and documenting complexity is to have an early warning tool allowing a project team to focus on certain areas and aspects of the project in order to prevent and alleviate future risks and issues caused by this complexity. The main contribution of this paper is to present a systematic view of complexity in project management by identifying its key attributes and classifying complexity by these attributes. A "complexity taxonomy", based on a survey of the existing complexity literature, is developed and discussed including the product, project, and external environment dimensions. We show how complexity types are described through simple real life examples and business cases. Then we develop a framework (tool) for applying the notion of complexity as an early warning tool for a project manager in order to timely foresee future risks and problems. The paper is intended for researchers in complexity, project management, information systems, technology solutions and business management, and also for information specialists, project managers, program managers, financial staff and technology directors.',
	 'authors': u'Alexei Botchkarev, Patrick Finnigan,',
	 'category': u'Computer Science ',
	 'date': '2014-12-2',
	 'pdflink': u'http://arxiv.org/pdf/1412.1027',
	 'subjects': u'Software Engineering (cs.SE)',
	 'title': u'\nComplexity in the Context of Systems Approach to Project Management',
	 'urllink': u'http://arxiv.org/abs/1412.1027'}
2015-04-10 05:24:03+0000 [xxu46_10] INFO: Crawled 160 pages (at 1 pages/min), scraped 153 items (at 1 items/min)
2015-04-10 05:24:11+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1412.1023> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 05:24:11+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1412.1023>
	{'abstract': u'In this paper, we study the achievable degrees-of-freedom (DoF) regions of the -user multiple-input-single-output (MISO) time correlated broadcast channel (BC). The time correlation induces knowledge of the current channel state information at transmitter (CSIT) with an estimation error , where is the signal-to-noise ratio (SNR). We consider the following two scenarios: -user with -antenna base station (BS) and -user with -antenna BS. In case of symmetric DoF tuples, where all the users obtain the same DoF, we derive the total DoF equal to for the first scenario and for the second one. In particular, we provide the achievability schemes for these two DoF tuples. Nevertheless, we also consider the asymmetric case where one of the users is guaranteed DoF, and provide the achievability scheme. Notably, the consistency of the proposed DoF regions with an already published outer bound , as well as with the Maddah-Ali-Tse (MAT), which assumes only perfect delayed CSIT, and the ZF beamforming schemes (perfect current CSIT) consents to the optimality of the proposed achievability schemes.',
	 'authors': u'Yi Luo, Tharmalingam Ratnarajah, Anastasios K. Papazafeiropoulos,',
	 'category': u'Computer Science ',
	 'date': '2014-12-2',
	 'pdflink': u'http://arxiv.org/pdf/1412.1023',
	 'subjects': u'Information Theory (cs.IT)',
	 'title': u'\nDegrees-of-Freedom Regions for $K$-User MISO Time-Correlated Broadcast  Channel',
	 'urllink': u'http://arxiv.org/abs/1412.1023'}
2015-04-10 05:25:03+0000 [xxu46_10] INFO: Crawled 161 pages (at 1 pages/min), scraped 154 items (at 1 items/min)
2015-04-10 05:25:18+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1412.1001> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 05:25:18+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1412.1001>
	{'abstract': u'We study two fundamental problems in computational geometry: finding the maximum inscribed ball (MaxIB) inside a polytope defined by hyperplanes in a -dimensional space, and finding the minimum enclosing ball (MinEB) of a set of points in a -dimensional space. We translate both geometry problems into purely algebraic optimization questions, and apply ideas from first-order convex optimizations to obtain simple and nearly-linear-time algorithms. For MaxIB, our algorithm produces a -approximate solution in time , compared to the best known running time of Xie, Snoeyink, and Xu [XSX06], where is the aspect ratio of the polytope. For MinEB, our algorithm produces a -approximate solution in time . This result matches the previous best running time, while yielding a simpler algorithm. For both problems, we provide an empirical evaluation showing the improved performance of our algorithms.',
	 'authors': u'Zeyuan Allen-Zhu, Zhenyu Liao, Lorenzo Orecchia,',
	 'category': u'Computer Science ',
	 'date': '2014-12-2',
	 'pdflink': u'http://arxiv.org/pdf/1412.1001',
	 'subjects': u'Computational Geometry (cs.CG)',
	 'title': u'\nUsing Optimization to Find Maximum Inscribed Balls and Minimum Enclosing  Balls',
	 'urllink': u'http://arxiv.org/abs/1412.1001'}
2015-04-10 05:25:57+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1412.0985> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 05:25:57+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1412.0985>
	{'abstract': u'Classifying structural variability in noisy projections of biological macromolecules is a central problem in Cryo-EM. In this work, we build on a previous method for estimating the covariance matrix of the three-dimensional structure present in the molecules being imaged. Our proposed method allows for incorporation of contrast transfer function and non-uniform distribution of viewing angles, making it more suitable for real-world data. We evaluate its performance on a synthetic dataset and an experimental dataset obtained by imaging a 70S ribosome complex.',
	 'authors': u'Joakim and\xe9n, Eugene Katsevich, Amit Singer,',
	 'category': u'Computer Science ',
	 'date': '2014-12-2',
	 'pdflink': u'http://arxiv.org/pdf/1412.0985',
	 'subjects': u'Computer Vision and Pattern Recognition (cs.CV)',
	 'title': u'\nCovariance estimation using conjugate gradient for 3D classification in  Cryo-EM',
	 'urllink': u'http://arxiv.org/abs/1412.0985'}
2015-04-10 05:26:03+0000 [xxu46_10] INFO: Crawled 163 pages (at 2 pages/min), scraped 156 items (at 2 items/min)
2015-04-10 05:27:03+0000 [xxu46_10] INFO: Crawled 163 pages (at 0 pages/min), scraped 156 items (at 0 items/min)
2015-04-10 05:27:07+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1412.0981> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 05:27:07+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1412.0981>
	{'abstract': u'In this paper we propose a new approach to the description of a network of interacting processes in a traditional programming language. Special programming languages or extensions to sequential languages are usually designed to express the semantics of concurrent execution. Using libraries in C++, Java, C#, and other languages is more practical way of concurrent programming. However, this method leads to an increase in workload of a manual coding. Besides, stock compilers can not detect semantic errors related to the programming model in such libraries. The new markup language and a special technique of automatic programming based on the marked code can solve these problems. The article provides a detailed specification of the markup language without discussing its implementation details. The language is used for programming of current and prospective multi-core and many-core systems.',
	 'authors': u'Sergey Vostokin,',
	 'category': u'Computer Science ',
	 'date': '2014-12-2',
	 'pdflink': u'http://arxiv.org/pdf/1412.0981',
	 'subjects': u'Programming Languages (cs.PL)',
	 'title': u'\nTemplet: a Markup Language for Concurrent Programming',
	 'urllink': u'http://arxiv.org/abs/1412.0981'}
2015-04-10 05:28:03+0000 [xxu46_10] INFO: Crawled 164 pages (at 1 pages/min), scraped 157 items (at 1 items/min)
2015-04-10 05:28:09+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1412.0975> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 05:28:09+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1412.0975>
	{'abstract': u'We provide a counterexample to a lemma used in a recent tentative improvement of the the Pin-Frankl bound for synchronizing automata. This example naturally leads us to formulate an open question, whose answer could fix the line of proof, and improve the bound.',
	 'authors': u'Fran\xe7ois Gonze, Rapha\xebl M. Jungers, A. N. Trahtman,',
	 'category': u'Computer Science ',
	 'date': '2014-12-2',
	 'pdflink': u'http://arxiv.org/pdf/1412.0975',
	 'subjects': u'Formal Languages and Automata Theory (cs.FL)',
	 'title': u'\nA Note on a Recent Attempt to Improve the Pin-Frankl Bound',
	 'urllink': u'http://arxiv.org/abs/1412.0975'}
2015-04-10 05:29:03+0000 [xxu46_10] INFO: Crawled 165 pages (at 1 pages/min), scraped 158 items (at 1 items/min)
2015-04-10 05:29:11+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1412.0969> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 05:29:11+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1412.0969>
	{'abstract': u'Over the years, researchers have studied the complexity of several decision versions of Nash equilibrium in (symmetric) two-player games (bimatrix games). To the best of our knowledge, the last remaining open problem of this sort is the following; it was stated by Papadimitriou in 2007: find a non-symmetric Nash equilibrium (NE) in a symmetric game. We show that this problem is NP-complete and the problem of counting the number of non-symmetric NE in a symmetric game is #P-complete. In 2005, Kannan and Theobald defined the "rank of a bimatrix game" represented by matrices (A, B) to be rank(A+B) and asked whether a NE can be computed in rank 1 games in polynomial time. Observe that the rank 0 case is precisely the zero sum case, for which a polynomial time algorithm follows from von Neumann\'s reduction of such games to linear programming. In 2011, Adsul et. al. obtained an algorithm for rank 1 games; however, it does not solve the case of symmetric rank 1 games. We resolve this problem.',
	 'authors': u'Ruta Mehta, Vijay V. Vazirani, Sadra Yazdanbod,',
	 'category': u'Computer Science ',
	 'date': '2014-12-2',
	 'pdflink': u'http://arxiv.org/pdf/1412.0969',
	 'subjects': u'Computer Science and Game Theory (cs.GT)',
	 'title': u'\nSettling Some Open Problems on 2-Player Symmetric Nash Equilibria',
	 'urllink': u'http://arxiv.org/abs/1412.0969'}
2015-04-10 05:30:03+0000 [xxu46_10] INFO: Crawled 166 pages (at 1 pages/min), scraped 159 items (at 1 items/min)
2015-04-10 05:30:16+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1412.0967> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 05:30:16+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1412.0967>
	{'abstract': u'We describe a data structure that stores a string in space similar to that of its Lempel-Ziv encoding and efficiently supports access, rank and select queries. These queries are fundamental for implementing succinct and compressed data structures, such as compressed trees and graphs. We show that our data structure can be built in a scalable manner and is both small and fast in practice compared to other data structures supporting such queries.',
	 'authors': u'Djamal Belazzougui, Travis Gagie, Pawe\u0142 Gawrychowski, Juha K\xe4rkk\xe4inen, Alberto Ord\xf3\xf1ez, Simon J. Puglisi, Yasuo Tabei,',
	 'category': u'Computer Science ',
	 'date': '2014-12-2',
	 'pdflink': u'http://arxiv.org/pdf/1412.0967',
	 'subjects': u'Data Structures and Algorithms (cs.DS)',
	 'title': u'\nQueries on LZ-Bounded Encodings',
	 'urllink': u'http://arxiv.org/abs/1412.0967'}
2015-04-10 05:31:03+0000 [xxu46_10] INFO: Crawled 167 pages (at 1 pages/min), scraped 160 items (at 1 items/min)
2015-04-10 05:31:18+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1412.0963> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 05:31:18+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1412.0963>
	{'abstract': u'The paper analyzes the socio-economic impacts of dropshipping - a new form of organization of sales, is rapidly gaining the Russian market. Dropshipping opens up tremendous prospects not only for ordinary people, but also for the Russian manufacturers, which if used properly can dropshipping opportunities with minimal direct access to the world market.',
	 'authors': u'Mikhail Kaluzhsky,',
	 'category': u'Computer Science ',
	 'date': '2014-12-2',
	 'pdflink': u'http://arxiv.org/pdf/1412.0963',
	 'subjects': u'Computers and Society (cs.CY)',
	 'title': u'\nDropshipping - a revolutionary form movement of goods in the global  economic crisis',
	 'urllink': u'http://arxiv.org/abs/1412.0963'}
2015-04-10 05:32:03+0000 [xxu46_10] INFO: Crawled 168 pages (at 1 pages/min), scraped 161 items (at 1 items/min)
2015-04-10 05:32:25+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1412.0962> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 05:32:25+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1412.0962>
	{'abstract': u'The SINR model for the quality of wireless connections has been the subject of extensive recent study. It attempts to predict whether a particular transmitter is heard at a specific location, in a setting consisting of simultaneous transmitters and background noise. The SINR model gives rise to a natural geometric object, the SINR diagram, which partitions the space into regions where each of the transmitters can be heard and the remaining space where no transmitter can be heard. Efficient point location in the SINR diagram, i.e., being able to build a data structure that facilitates determining, for a query point, whether any transmitter is heard there, and if so, which one, has been recently investigated in several papers. These planar data structures are constructed in time at least quadratic in and support logarithmic-time approximate queries. Moreover, the performance of some of the proposed structures depends strongly not only on the number of transmitters and on the approximation parameter , but also on some geometric parameters that cannot be bounded a priori as a function of or . We address the question of batched point location queries, i.e., answering many queries simultaneously. Specifically, in one dimension, we can answer queries exactly in amortized polylogarithmic time per query, while in the plane we can do it approximately. We also show how to answer queries exactly in amortized polylogarithmic time per query, assuming the queries are located on a possibly non-uniform grid. All these results can handle arbitrary power assignments to the transmitters. Moreover, the amortized query time in these results depends only on and . These results demonstrate the (so far underutilized) power of combining algebraic tools with those of computational geometry and other fields.',
	 'authors': u'Boris Aronov, Matthew J. Katz,',
	 'category': u'Computer Science ',
	 'date': '2014-12-2',
	 'pdflink': u'http://arxiv.org/pdf/1412.0962',
	 'subjects': u'Computational Geometry (cs.CG)',
	 'title': u'\nBatched Point Location in SINR Diagrams via Algebraic Tools',
	 'urllink': u'http://arxiv.org/abs/1412.0962'}
2015-04-10 05:33:03+0000 [xxu46_10] INFO: Crawled 169 pages (at 1 pages/min), scraped 162 items (at 1 items/min)
2015-04-10 05:33:28+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1412.0961> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 05:33:28+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1412.0961>
	{'abstract': u'Avoiding access conflicts is a major challenge in the design of multi-threaded programs. In the context of real-time systems, the absence of conflicts can be guaranteed by ensuring that no two potentially conflicting accesses are ever scheduled concurrently.In this paper, we analyze programs that carry time annotations specifying the time for executing each statement. We propose a technique for verifying that a multi-threaded program with time annotations is free of access conflicts. In particular, we generate constraints that reflect the possible schedules for executing the program and the required properties. We then invoke an SMT solver in order to verify that no execution gives rise to concurrent conflicting accesses. Otherwise, we obtain a trace that exhibits the access conflict.',
	 'authors': u'Jingshu Chen, Marie Duflot, Stephan Merz,',
	 'category': u'Computer Science ',
	 'date': '2014-11-30',
	 'pdflink': u'http://arxiv.org/pdf/1412.0961',
	 'subjects': u'Programming Languages (cs.PL)',
	 'title': u'\nAnalyzing Conflict Freedom For Multi-threaded Programs With Time  Annotations',
	 'urllink': u'http://arxiv.org/abs/1412.0961'}
2015-04-10 05:34:03+0000 [xxu46_10] INFO: Crawled 170 pages (at 1 pages/min), scraped 163 items (at 1 items/min)
2015-04-10 05:34:47+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1412.0954> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 05:34:47+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1412.0954>
	{'abstract': u'In multiview video systems, multiple cameras generally acquire the same scene from different perspectives, such that users have the possibility to select their preferred viewpoint. This results in large amounts of highly redundant data, which needs to be properly handled during encoding and transmission over resource-constrained channels. In this work, we study coding and transmission strategies in multicamera systems, where correlated sources send data through a bottleneck channel to a central server, which eventually transmits views to different interactive users. We propose a dynamic correlation-aware packet scheduling optimization under delay, bandwidth, and interactivity constraints. The optimization relies both on a novel rate-distortion model, which captures the importance of each view in the 3D scene reconstruction, and on an objective function that optimizes resources based on a client navigation model. The latter takes into account the distortion experienced by interactive clients as well as the distortion variations that might be observed by clients during multiview navigation. We solve the scheduling problem with a novel trellis-based solution, which permits to formally decompose the multivariate optimization problem thereby significantly reducing the computation complexity. Simulation results show the gain of the proposed algorithm compared to baseline scheduling policies. More in details, we show the gain offered by our dynamic scheduling policy compared to static camera allocation strategies and to schemes with constant coding strategies. Finally, we show that the best scheduling policy consistently adapts to the most likely user navigation path and that it minimizes distortion variations that can be very disturbing for users in traditional navigation systems.',
	 'authors': u'Laura Toni, Thomas Maugey, Pascal Frossard,',
	 'category': u'Computer Science ',
	 'date': '2014-12-2',
	 'pdflink': u'http://arxiv.org/pdf/1412.0954',
	 'subjects': u'Multimedia (cs.MM)',
	 'title': u'\nOptimized Packet Scheduling in Multiview Video Navigation Systems',
	 'urllink': u'http://arxiv.org/abs/1412.0954'}
2015-04-10 05:35:03+0000 [xxu46_10] INFO: Crawled 171 pages (at 1 pages/min), scraped 164 items (at 1 items/min)
2015-04-10 05:35:52+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1412.0929> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 05:35:52+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1412.0929>
	{'abstract': u'Next-generation WLANs will support the use of wider channels, which is known as channel bonding, to achieve higher throughput. However, because both the channel center frequency and the channel width are autonomously selected by each WLAN, the use of wider channels may also increase the competition with other WLANs operating in the same area for the limited available spectrum, thus causing the opposite effect. In this paper, we analyse the interactions between a group of neighboring WLANs that use channel bonding and evaluate the impact of those interactions on the achievable throughput. A Continuous Time Markov Network (CTMN) model that is able to capture the coupled operation of a group of overlapping WLANs is introduced and validated. The results show that the use of channel bonding can provide significant performance gains even in scenarios with high densities of WLANs, though it may also cause unfair situations in which some WLANs cannot access the channel, while others receive most of the transmission opportunities.',
	 'authors': u'B. Bellalta, A. Checco, A. Zocca, J. Barcelo,',
	 'category': u'Computer Science ',
	 'date': '2014-12-2',
	 'pdflink': u'http://arxiv.org/pdf/1412.0929',
	 'subjects': u'Networking and Internet Architecture (cs.NI)',
	 'title': u'\nOn the Interactions between Multiple Overlapping WLANs using Channel  Bonding',
	 'urllink': u'http://arxiv.org/abs/1412.0929'}
2015-04-10 05:36:03+0000 [xxu46_10] INFO: Crawled 172 pages (at 1 pages/min), scraped 165 items (at 1 items/min)
2015-04-10 05:37:03+0000 [xxu46_10] INFO: Crawled 172 pages (at 0 pages/min), scraped 165 items (at 0 items/min)
2015-04-10 05:37:04+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1412.0914> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 05:37:04+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1412.0914>
	{'abstract': u'Full-duplex multi-way relaying is a potential solution for supporting high data rates in future Internet-of-Things (IoT) and 5G networks. Thus, in this paper the full-duplex MIMO multi-way channel consisting of 3 users (Y-channel) with antennas each and a common relay node with antennas is studied. Each user wants to exchange messages with all the other users via the relay. A transmission strategy is proposed based on channel diagonalization that decomposes the channel into parallel sub-channels, and physical-layer network coding over these sub-channels. It is shown that the proposed strategy achieves the optimal DoF region of the channel if . Furthermore, the proposed strategy that requires joint encoding over multiple sub-channels is compared to another strategy that encodes over each sub-channel separately. It turns out that coding jointly over sub-channels is necessary for an optimal transmission strategy. This shows that the MIMO Y-channel is inseparable.',
	 'authors': u'Anas Chaaban, Aydin Sezgin,',
	 'category': u'Computer Science ',
	 'date': '2014-12-2',
	 'pdflink': u'http://arxiv.org/pdf/1412.0914',
	 'subjects': u'Information Theory (cs.IT)',
	 'title': u'\nOn Channel Inseparability and the DoF Region of MIMO Multi-way Relay  Channels',
	 'urllink': u'http://arxiv.org/abs/1412.0914'}
2015-04-10 05:38:03+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1412.0885> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 05:38:03+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1412.0885>
	{'abstract': u'Many embedded and real-time systems have a inherent probabilistic behaviour (sensors data, unreliable hardware,...). In that context, it is crucial to evaluate system properties such as "the probability that a particular hardware fails". Such properties can be evaluated by using probabilistic model checking. However, this technique fails on models representing realistic embedded and real-time systems because of the state space explosion. To overcome this problem, we propose a verification framework based on Statistical Model Checking. Our framework is able to evaluate probabilistic and temporal properties on large systems modelled in SystemC, a standard system-level modelling language. It is fully implemented as an extension of the Plasma-lab statistical model checker. We illustrate our approach on a multi-lift system case study.',
	 'authors': u'Van Chan Ngo, Axel Legay, Jean Quilbeuf,',
	 'category': u'Computer Science ',
	 'date': '2014-12-2',
	 'pdflink': u'http://arxiv.org/pdf/1412.0885',
	 'subjects': u'Software Engineering (cs.SE)',
	 'title': u'\nDynamic Verification of SystemC with Statistical Model Checking',
	 'urllink': u'http://arxiv.org/abs/1412.0885'}
2015-04-10 05:38:03+0000 [xxu46_10] INFO: Crawled 174 pages (at 2 pages/min), scraped 167 items (at 2 items/min)
2015-04-10 05:39:03+0000 [xxu46_10] INFO: Crawled 174 pages (at 0 pages/min), scraped 167 items (at 0 items/min)
2015-04-10 05:39:14+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1412.0883> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 05:39:14+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1412.0883>
	{'abstract': u'In this paper, we focus on the ergodic downlink sum-rate performance of a system consisting of a set of heterogeneous users. We study three user selection schemes to group near-orthogonal users for simultaneous transmission. The first scheme is a random selection policy that achieves fairness, but does not exploit multi-user diversity. The second scheme is a greedy selection policy that fully exploits multi-user diversity, but does not achieve fairness, and the third scheme achieves fairness while partially exploiting multi-user diversity. We also consider two beamforming methods for data transmission, namely, maximum-ratio transmission and zero-forcing beamforming. In all scheduling schemes studied in the paper, there is a key parameter that controls the degrees of orthogonality of channel directions between co-scheduled users. We focus on optimally setting this parameter for each scheduling scheme such that the ergodic downlink sum-rate is maximized. To this end, we derive analytical expressions for the ergodic downlink sum-rate considering each scheduling scheme. Numerical results are also presented to provide further insights.',
	 'authors': u'Meng Wang, Tharaka Samarasinghe, Jamie S. Evans,',
	 'category': u'Computer Science ',
	 'date': '2014-12-2',
	 'pdflink': u'http://arxiv.org/pdf/1412.0883',
	 'subjects': u'Information Theory (cs.IT)',
	 'title': u'\nOptimizing user selection schemes in vector broadcast channels',
	 'urllink': u'http://arxiv.org/abs/1412.0883'}
2015-04-10 05:40:03+0000 [xxu46_10] INFO: Crawled 175 pages (at 1 pages/min), scraped 168 items (at 1 items/min)
2015-04-10 05:40:27+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1412.0880> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 05:40:27+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1412.0880>
	{'abstract': u'The added value of Device-to-Device (D2D) communication amounts to an efficient content discovery mechanism that enables users to steer their requests toward the node most likely to satisfy them. In this paper, we address the implementation of content-centric routing in a D2D architecture for Android devices based on WiFi Direct, a protocol recently standardised by the Wi-Fi Alliance. After discussing the creation of multiple D2D groups, we introduce novel paradigms featuring intra- and inter-group bidirectional communication. We then present the primitives involved in content advertising and requesting among members of the multi-group network. Finally, we evaluate the performance of our architecture in a real testbed involving Android devices in different group configurations. We also compare the results against the ones achievable exploiting Bluetooth technologies.',
	 'authors': u'Claudio Casetti, Carla-Fabiana Chiasserini, Luciano Curto Pelle, Carolina Del Valle, Yufeng Duan, Paolo Giaccone,',
	 'category': u'Computer Science ',
	 'date': '2014-12-2',
	 'pdflink': u'http://arxiv.org/pdf/1412.0880',
	 'subjects': u'Networking and Internet Architecture (cs.NI)',
	 'title': u'\nContent-centric Routing in Wi-Fi Direct Multi-group Networks',
	 'urllink': u'http://arxiv.org/abs/1412.0880'}
2015-04-10 05:41:03+0000 [xxu46_10] INFO: Crawled 176 pages (at 1 pages/min), scraped 169 items (at 1 items/min)
2015-04-10 05:41:41+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1412.0879> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 05:41:41+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1412.0879>
	{'abstract': u'The objective of the project is to design and run a system similar to Watson, designed to answer Jeopardy questions. In the course of a semester, we developed an open source question answering system using the Indri, Lucene, Bing and Google search engines, Apache UIMA, Open- and CoreNLP, and Weka among additional modules. By the end of the semester, we achieved 18% accuracy on Jeopardy questions, and work has not stopped since then.',
	 'authors': u'Sean Gallagher, Wlodek Zadrozny, Walid Shalaby, Adarsh Avadhani,',
	 'category': u'Computer Science ',
	 'date': '2014-12-2',
	 'pdflink': u'http://arxiv.org/pdf/1412.0879',
	 'subjects': u'Computation and Language (cs.CL)',
	 'title': u'\nWatsonsim: Overview of a Question Answering Engine',
	 'urllink': u'http://arxiv.org/abs/1412.0879'}
2015-04-10 05:42:03+0000 [xxu46_10] INFO: Crawled 177 pages (at 1 pages/min), scraped 170 items (at 1 items/min)
2015-04-10 05:42:43+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1412.0864> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 05:42:43+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1412.0864>
	{'abstract': u'In this paper, we study the parameterized complexity and inapproximability of the problem in hamiltonian bipartite graphs. We show that, given a hamiltonian cycle in a hamiltonian bipartite graph, the problem is W[1]-hard and cannot be solved in time unless W[1]=FPT, where is the number of vertices in the graph. In addition, we show that unless NP=P, the maximum induced matching in a hamiltonian graph cannot be approximated within a ratio of , where is the number of vertices in the graph. For a bipartite hamiltonian graph in vertices, it is NP-hard to approximate its maximum induced matching based on a hamiltonian cycle of the graph within a ratio of , where is the number of vertices in the graph and is any positive constant.',
	 'authors': u'Yinglei Song,',
	 'category': u'Computer Science ',
	 'date': '2014-12-2',
	 'pdflink': u'http://arxiv.org/pdf/1412.0864',
	 'subjects': u'Computational Complexity (cs.CC)',
	 'title': u'\nOn the Induced Matching Problem in Hamiltonian Bipartite Graphs',
	 'urllink': u'http://arxiv.org/abs/1412.0864'}
2015-04-10 05:43:03+0000 [xxu46_10] INFO: Crawled 178 pages (at 1 pages/min), scraped 171 items (at 1 items/min)
2015-04-10 05:43:48+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1412.0854> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 05:43:48+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1412.0854>
	{'abstract': u'Analyzing Big Data can help corporations to im-prove their efficiency. In this work we present a new vision to derive Value from Big Data using a Semantic Hierarchical Multi-label Classification called Semantic HMC based in a non-supervised Ontology learning process. We also proposea Semantic HMC process, using scalable Machine-Learning techniques and Rule-based reasoning.',
	 'authors': u'Thomas Hassan, Rafael Peixoto, Christophe Cruz, Aurlie Bertaux, Nuno Silva,',
	 'category': u'Computer Science ',
	 'date': '2014-12-2',
	 'pdflink': u'http://arxiv.org/pdf/1412.0854',
	 'subjects': u'Artificial Intelligence (cs.AI)',
	 'title': u'\nSemantic HMC for Big Data Analysis',
	 'urllink': u'http://arxiv.org/abs/1412.0854'}
2015-04-10 05:44:03+0000 [xxu46_10] INFO: Crawled 179 pages (at 1 pages/min), scraped 172 items (at 1 items/min)
2015-04-10 05:44:35+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1412.0845> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 05:44:35+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1412.0845>
	{'abstract': u"One of the main results shown through Roughgarden's notions of smooth games and robust price of anarchy is that, for any sum-bounded utilitarian social function, the worst-case price of anarchy of coarse correlated equilibria coincides with that of pure Nash equilibria in the class of weighted congestion games with non-negative and non-decreasing latency functions and that such a value can always be derived through the, so called, smoothness argument. We significantly extend this result by proving that, for a variety of (even non-sum-bounded) utilitarian and egalitarian social functions and for a broad generalization of the class of weighted congestion games with non-negative (and possibly decreasing) latency functions, the worst-case price of anarchy of -approximate coarse correlated equilibria still coincides with that of -approximate pure Nash equilibria, for any . As a byproduct of our proof, it also follows that such a value can always be determined by making use of the primal-dual method we introduced in a previous work. It is important to note that our scenario of investigation is beyond the scope of application of the robust price of anarchy (for as it is currently defined), so that our result seems unlikely to be alternatively proved via the smoothness framework.",
	 'authors': u'Vittorio Bil\xf2,',
	 'category': u'Computer Science ',
	 'date': '2014-12-2',
	 'pdflink': u'http://arxiv.org/pdf/1412.0845',
	 'subjects': u'Computer Science and Game Theory (cs.GT)',
	 'title': u'\nOn the Robustness of the Approximate Price of Anarchy in Generalized  Congestion Games',
	 'urllink': u'http://arxiv.org/abs/1412.0845'}
2015-04-10 05:45:03+0000 [xxu46_10] INFO: Crawled 180 pages (at 1 pages/min), scraped 173 items (at 1 items/min)
2015-04-10 05:45:43+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1412.0839> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 05:45:43+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1412.0839>
	{'abstract': u'The model of tree automata with equality and disequality constraints was introduced in 2007 by Filiot, Talbot and Tison. In this paper we show that if there is at least one disequality constraint, the emptiness problem is NP-hard.',
	 'authors': u'P.-C H\xe9am, V. Hugot, O. Kouchnarenko,',
	 'category': u'Computer Science ',
	 'date': '2014-12-2',
	 'pdflink': u'http://arxiv.org/pdf/1412.0839',
	 'subjects': u'Formal Languages and Automata Theory (cs.FL)',
	 'title': u'\nThe Emptiness Problem for Tree Automata with at Least One Disequality  Constraint is NP-hard',
	 'urllink': u'http://arxiv.org/abs/1412.0839'}
2015-04-10 05:46:03+0000 [xxu46_10] INFO: Crawled 181 pages (at 1 pages/min), scraped 174 items (at 1 items/min)
2015-04-10 05:46:49+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1412.0833> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 05:46:49+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1412.0833>
	{'abstract': u'In this paper, we analyze the problem of power control in a multiuser MIMO network, where the optimal linear precoder is employed in each user to achieve maximum point- to-point information rate. We design a distributed power control algorithm based on the concept of game theory and contractive functions that has a couple of advantages over the previous designs (e.g. more uniqueness probability of Nash equilibria and asynchronous implementation). Despite these improvements, the sum-rate of the users does not increase because the proposed algorithm can not lead the power control game to an efficient equilibrium point. We solve this issue by modifying our algorithm such that the game is led to the equilibrium that satisfies a particular criterion. This criterion can be chosen by the designer to achieve a certain optimality among the equilibria. Furthermore, we propose the inexact method that helps us to boost the convergence speed of our modified algorithms. Lastly, we show that pricing algorithms can also be a special case of our modified algorithms. Simulations show a noticeable improvement in the sum-rate when we modify our proposed algorithm.',
	 'authors': u'Peyman Siyari, Hassan Aghaeinia,',
	 'category': u'Computer Science ',
	 'date': '2014-12-2',
	 'pdflink': u'http://arxiv.org/pdf/1412.0833',
	 'subjects': u'Computer Science and Game Theory (cs.GT)',
	 'title': u'\nFast and Optimal Power Control Games in Multiuser MIMO Networks',
	 'urllink': u'http://arxiv.org/abs/1412.0833'}
2015-04-10 05:47:03+0000 [xxu46_10] INFO: Crawled 182 pages (at 1 pages/min), scraped 175 items (at 1 items/min)
2015-04-10 05:48:03+0000 [xxu46_10] INFO: Crawled 182 pages (at 0 pages/min), scraped 175 items (at 0 items/min)
2015-04-10 05:48:06+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1412.0826> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 05:48:06+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1412.0826>
	{'abstract': u'Learning based hashing methods have attracted considerable attention due to their ability to greatly increase the scale at which existing algorithms may operate. Most of these methods are designed to generate binary codes preserving the Euclidean similarity in the original space. Manifold learning techniques, in contrast, are better able to model the intrinsic structure embedded in the original high-dimensional data. The complexities of these models, and the problems with out-of-sample data, have previously rendered them unsuitable for application to large-scale embedding, however. In this work, how to learn compact binary embeddings on their intrinsic manifolds is considered. In order to address the above-mentioned difficulties, an efficient, inductive solution to the out-of-sample data problem, and a process by which non-parametric manifold learning may be used as the basis of a hashing method is proposed. The proposed approach thus allows the development of a range of new hashing techniques exploiting the flexibility of the wide variety of manifold learning approaches available. It is particularly shown that hashing on the basis of t-SNE outperforms state-of-the-art hashing methods on large-scale benchmark datasets, and is very effective for image classification with very short code lengths. The proposed hashing framework is shown to be easily improved, for example, by minimizing the quantization error with learned orthogonal rotations. In addition, a supervised inductive manifold hashing framework is developed by incorporating the label information, which is shown to greatly advance the semantic retrieval performance.',
	 'authors': u'Fumin Shen, Chunhua Shen, Qinfeng Shi, Anton van den Hengel, Zhenmin Tang, Heng Tao Shen,',
	 'category': u'Computer Science ',
	 'date': '2014-12-2',
	 'pdflink': u'http://arxiv.org/pdf/1412.0826',
	 'subjects': u'Computer Vision and Pattern Recognition (cs.CV)',
	 'title': u'\nHashing on Nonlinear Manifolds',
	 'urllink': u'http://arxiv.org/abs/1412.0826'}
2015-04-10 05:49:03+0000 [xxu46_10] INFO: Crawled 183 pages (at 1 pages/min), scraped 176 items (at 1 items/min)
2015-04-10 05:49:12+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1412.0825> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 05:49:12+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1412.0825>
	{'abstract': u'This volume contains the proceedings of HCVS 2014, the First Workshop on Horn Clauses for Verification and Synthesis which was held on July 17, 2014 in Vienna, Austria as a satellite event of the Federated Logic Conference (FLoC) and part of the Vienna Summer of Logic (VSL 2014). HCVS 2014 was affiliated to the 26th International Conference on Computer Aided Verification (CAV 2014) and to the 30th International Conference on Logic Programming (ICLP 2014). Most Program Verification and Synthesis problems of interest can be modeled directly using Horn clauses and many recent advances in the Constraint/Logic Programming and Program Verification communities have centered around efficiently solving problems presented as Horn clauses. Since Horn clauses for verification and synthesis have been advocated by these communities in different times and from different perspectives, the HCVS workshop was organized to stimulate interaction and a fruitful exchange and integration of experiences.',
	 'authors': u'Nikolaj Bj\xf8rner, Fabio Fioravanti, Andrey Rybalchenko, Valerio Senni,',
	 'category': u'Computer Science ',
	 'date': '2014-12-2',
	 'pdflink': u'http://arxiv.org/html/1412.0825',
	 'subjects': u'Logic in Computer Science (cs.LO)',
	 'title': u'\nProceedings First Workshop on Horn Clauses for Verification and  Synthesis',
	 'urllink': u'http://arxiv.org/abs/1412.0825'}
2015-04-10 05:50:03+0000 [xxu46_10] INFO: Crawled 184 pages (at 1 pages/min), scraped 177 items (at 1 items/min)
2015-04-10 05:50:27+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1412.0823> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 05:50:27+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1412.0823>
	{'abstract': u'Interference networks with no channel state information at the transmitter (CSIT) except for the knowledge of the connectivity graph have been recently studied under the topological interference management (TIM) framework. In this paper, we consider a similar problem with topological knowledge but in a distributed broadcast channel setting, i.e. a network where transmitter cooperation is enabled. We show that the topological information can also be exploited in this case to strictly improve the degrees of freedom (DoF) as long as the network is not fully connected, which is a reasonable assumption in practice. Achievability schemes based on selective graph coloring, interference alignment, and hypergraph covering, are proposed. Together with outer bounds built upon generator sequence, the concept of compound channel settings, and the relation to index coding, we characterize the symmetric DoF for so-called regular networks with constant number of interfering links, and identify the sufficient and/or necessary conditions for the arbitrary network topologies to achieve a certain amount of symmetric DoF.',
	 'authors': u'Xinping Yi, David Gesbert,',
	 'category': u'Computer Science ',
	 'date': '2014-12-2',
	 'pdflink': u'http://arxiv.org/pdf/1412.0823',
	 'subjects': u'Information Theory (cs.IT)',
	 'title': u'\nTopological Interference Management with Transmitter Cooperation',
	 'urllink': u'http://arxiv.org/abs/1412.0823'}
2015-04-10 05:51:03+0000 [xxu46_10] INFO: Crawled 185 pages (at 1 pages/min), scraped 178 items (at 1 items/min)
2015-04-10 05:51:03+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1412.0801> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 05:51:03+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1412.0801>
	{'abstract': u'Noise removal from images is a part of image restoration in which we try to reconstruct or recover an image that has been degraded by using apriori knowledge of the degradation phenomenon. Noises present in images can be of various types with their characteristic Probability Distribution Functions (PDF). Noise removal techniques depend on the kind of noise present in the image rather than on the image itself. This paper explores the effects of applying noise reduction filters having similar properties on noisy images with emphasis on Signal-to-Noise-Ratio (SNR) value estimation for comparing the results.',
	 'authors': u'Poorna Banerjee Dasgupta,',
	 'category': u'Computer Science ',
	 'date': '2014-12-2',
	 'pdflink': u'http://arxiv.org/pdf/1412.0801',
	 'subjects': u'Computer Vision and Pattern Recognition (cs.CV)',
	 'title': u'\nAnalytical Comparison of Noise Reduction Filters for Image Restoration  Using SNR Estimation',
	 'urllink': u'http://arxiv.org/abs/1412.0801'}
2015-04-10 05:52:03+0000 [xxu46_10] INFO: Crawled 186 pages (at 1 pages/min), scraped 179 items (at 1 items/min)
2015-04-10 05:52:18+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1412.0799> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 05:52:18+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1412.0799>
	{'abstract': u'By the Road Coloring Theorem (Trahtman, 2008), the edges of any aperiodic directed multigraph with a constant out-degree can be colored such that the resulting automaton admits a reset word. There may also be a need for a particular reset word to be admitted. For certain words it is NP-complete to decide whether there is a suitable coloring of a given multigraph. We present a classification of all words over the binary alphabet that separates such words from those that make the problem solvable in polynomial time. We show that the classification becomes different if we consider only strongly connected multigraphs. In this restricted setting the classification remains incomplete.',
	 'authors': u'Vojt\u011bch Vorel, Adam Roman,',
	 'category': u'Computer Science ',
	 'date': '2014-12-2',
	 'pdflink': u'http://arxiv.org/pdf/1412.0799',
	 'subjects': u'Formal Languages and Automata Theory (cs.FL)',
	 'title': u'\nComplexity of Road Coloring with Prescribed Reset Words',
	 'urllink': u'http://arxiv.org/abs/1412.0799'}
2015-04-10 05:53:03+0000 [xxu46_10] INFO: Crawled 187 pages (at 1 pages/min), scraped 180 items (at 1 items/min)
2015-04-10 05:53:30+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1412.0784> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 05:53:30+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1412.0784>
	{'abstract': u'Braid is a 2008 puzzle game centered around the ability to reverse time. We show that Braid can simulate an arbitrary computation. Our construction makes no use of Braid\'s unique time mechanics, and therefore may apply to many other video games. We also show that a plausible "bounded" variant of Braid lies within 2-EXPSPACE. Our proof relies on a technical lemma about Turing machines which may be of independent interest. Namely, define a braidlike Turing machine to be a Turing machine that, when it writes to the tape, deletes all data on the tape to the right of the head. We prove that deciding the behavior of such a machine lies in EXPSPACE.',
	 'authors': u'Linus Hamilton,',
	 'category': u'Computer Science ',
	 'date': '2014-12-2',
	 'pdflink': u'http://arxiv.org/pdf/1412.0784',
	 'subjects': u'Computational Complexity (cs.CC)',
	 'title': u'\nBraid is undecidable',
	 'urllink': u'http://arxiv.org/abs/1412.0784'}
2015-04-10 05:54:03+0000 [xxu46_10] INFO: Crawled 188 pages (at 1 pages/min), scraped 181 items (at 1 items/min)
2015-04-10 05:54:45+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1412.0781> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 05:54:45+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1412.0781>
	{'abstract': u'We introduce an algorithm that efficiently and accurately performs principal component analysis (PCA) for a large set of two-dimensional images, and, for each image, the set of its uniform rotations in the plane and its reflection. For a dataset consisting of images of size pixels, the computational complexity of our algorithm is which is times faster than existing algorithms. The new algorithm computes the Fourier-Bessel expansion coefficients more efficiently than its predecessor Fourier-Bessel steerable PCA (FBsPCA) using the non-uniform FFT. We compare the accuracy and efficiency of the new algorithm with traditional PCA and FBsPCA.',
	 'authors': u'Zhizhen Zhao, Yoel Shkolnisky, Amit Singer,',
	 'category': u'Computer Science ',
	 'date': '2014-12-2',
	 'pdflink': u'http://arxiv.org/pdf/1412.0781',
	 'subjects': u'Computer Vision and Pattern Recognition (cs.CV)',
	 'title': u'\nFast Steerable Principal Component Analysis',
	 'urllink': u'http://arxiv.org/abs/1412.0781'}
2015-04-10 05:55:03+0000 [xxu46_10] INFO: Crawled 189 pages (at 1 pages/min), scraped 182 items (at 1 items/min)
2015-04-10 05:55:36+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1412.0779> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 05:55:36+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1412.0779>
	{'abstract': u'We resolve an open problem due to Tetsuo Asano, showing how to compute the shortest path in a polygon, given in a read only memory, using sublinear space and subquadratic time. Specifically, given a simple polygon with vertices in a read only memory, and additional working memory of size , the new algorithm computes the shortest path (in ) in expected time. This requires several new tools, which we believe to be of independent interest.',
	 'authors': u'Sariel Har-Peled,',
	 'category': u'Computer Science ',
	 'date': '2014-12-2',
	 'pdflink': u'http://arxiv.org/pdf/1412.0779',
	 'subjects': u'Computational Geometry (cs.CG)',
	 'title': u'\nShortest Path in a Polygon using Sublinear Space',
	 'urllink': u'http://arxiv.org/abs/1412.0779'}
2015-04-10 05:56:03+0000 [xxu46_10] INFO: Crawled 190 pages (at 1 pages/min), scraped 183 items (at 1 items/min)
2015-04-10 05:56:32+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1412.0774> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 05:56:32+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1412.0774>
	{'abstract': u'We introduce a purely feed-forward architecture for semantic segmentation. We map small image elements (superpixels) to rich feature representations extracted from a sequence of nested regions of increasing extent. These regions are obtained by "zooming out" from the superpixel all the way to scene-level resolution. This approach exploits statistical structure in the image and in the label space without setting up explicit structured prediction mechanisms, and thus avoids complex and expensive inference. Instead superpixels are classified by a feedforward multilayer network. Our architecture achieves new state of the art performance in semantic segmentation, obtaining 64.4% average accuracy on the PASCAL VOC 2012 test set.',
	 'authors': u'Mohammadreza Mostajabi, Payman Yadollahpour, Gregory Shakhnarovich,',
	 'category': u'Computer Science ',
	 'date': '2014-12-2',
	 'pdflink': u'http://arxiv.org/pdf/1412.0774',
	 'subjects': u'Computer Vision and Pattern Recognition (cs.CV)',
	 'title': u'\nFeedforward semantic segmentation with zoom-out features',
	 'urllink': u'http://arxiv.org/abs/1412.0774'}
2015-04-10 05:57:03+0000 [xxu46_10] INFO: Crawled 191 pages (at 1 pages/min), scraped 184 items (at 1 items/min)
2015-04-10 05:57:40+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1412.0773> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 05:57:40+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1412.0773>
	{'abstract': u'The stable model semantics had been recently generalized to non-Herbrand structures by several works, which provides a unified framework and solid logical foundations for answer set programming. This paper focuses on the expressiveness of normal and disjunctive programs under the general stable model semantics. A translation from disjunctive programs to normal programs is proposed for infinite structures. Over finite structures, some disjunctive programs are proved to be intranslatable to normal programs if the arities of auxiliary predicates and functions are bounded in a certain way. The equivalence of the expressiveness of normal programs and disjunctive programs over arbitrary structures is also shown to coincide with that over finite structures, and coincide with whether NP is closed under complement. Moreover, to capture the exact expressiveness, some intertranslatability results between logic program classes and fragments of second-order logic are obtained.',
	 'authors': u'Heng Zhang, Yan Zhang,',
	 'category': u'Computer Science ',
	 'date': '2014-12-2',
	 'pdflink': u'http://arxiv.org/pdf/1412.0773',
	 'subjects': u'Artificial Intelligence (cs.AI)',
	 'title': u'\nExpressiveness of Logic Programs under General Stable Model Semantics',
	 'urllink': u'http://arxiv.org/abs/1412.0773'}
2015-04-10 05:58:03+0000 [xxu46_10] INFO: Crawled 192 pages (at 1 pages/min), scraped 185 items (at 1 items/min)
2015-04-10 05:58:54+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1412.0767> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 05:58:54+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1412.0767>
	{'abstract': u'Videos have become ubiquitous due to the ease of capturing and sharing via social platforms like Youtube, Facebook, Instagram, and others. The computer vision community has tried to tackle various video analysis problems independently. As a consequence, even though some really good hand-crafted features have been proposed there is a lack of generic features for video analysis. On the other hand, the image domain has progressed rapidly by using features from deep convolutional networks. These deep features are proving to be generic and perform well on variety of image tasks. In this work we propose Convolution 3D (C3D) feature, a generic spatio-temporal feature obtained by training a deep 3-dimensional convolutional network on a large annotated video dataset comprising objects, scenes, actions, and other frequently occurring concepts. We show that by using spatio-temporal convolutions the trained features encapsulate appearance and motion cues and perform well on different video classification tasks. C3D has three main advantages. First, it is generic: achieving state-of-the-art results on object recognition, scene classification, sport classification, and action similarity labeling in videos. Second, it is compact: obtaining better accuracy than best hand-crafted features and best deep image features with a lower dimensional feature descriptor. Third, it is efficient to compute: times faster than current hand-crafted features, and two orders of magnitude faster than current deep-learning based video classification methods.',
	 'authors': u'Du Tran, Lubomir Bourdev, Rob Fergus, Lorenzo Torresani, Manohar Paluri,',
	 'category': u'Computer Science ',
	 'date': '2014-12-2',
	 'pdflink': u'http://arxiv.org/pdf/1412.0767',
	 'subjects': u'Computer Vision and Pattern Recognition (cs.CV)',
	 'title': u'\nC3D: Generic Features for Video Analysis',
	 'urllink': u'http://arxiv.org/abs/1412.0767'}
2015-04-10 05:59:03+0000 [xxu46_10] INFO: Crawled 193 pages (at 1 pages/min), scraped 186 items (at 1 items/min)
2015-04-10 06:00:03+0000 [xxu46_10] INFO: Crawled 193 pages (at 0 pages/min), scraped 186 items (at 0 items/min)
2015-04-10 06:00:11+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1412.0765> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 06:00:11+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1412.0765>
	{'abstract': u'Ad hoc networks provide a flexible, infrastructure-free means to communicate between soldiers in war zones, aid workers in disaster areas, or consumers in device-to-device (D2D) applications. Ad hoc networks, however, are still plagued by interference caused by uncoordinated transmissions which leads to poor scaling due to distributed coordination. Communication with millimeter-wave (mmWave) devices offers hope to emph networks through higher bandwidth, reduced interference due to directional antennas, and weaker interference power due to building blockage. This paper uses a stochastic geometry approach to characterize the one-way and two-way signal-to-interference ratio (SINR) distribution of a mmWave ad hoc network with directional antennas, random blockages, and ALOHA channel access. The effect of random receiver location is quantified which shows that random receiver distances do not alter the SINR distribution beyond knowledge of the mean receiver position. A method for computing the distribution of mmWave ad hoc interference-to-noise ratio which shows that mmWave ad hoc networks can still be interference limited. Several reasonable simplifications are used to derive the transmission capacity and area spectral efficiency. The performance of mmWave is then analyzed in terms of rate coverage. The results show that mmWave networks can support higher densities and larger spectral efficiencies, even in the presence of blockage, compared with lower frequency communication for certain link distances. Due to the increased bandwidth, the rate coverage of mmWave can be much greater than lower frequency devices.',
	 'authors': u'Andrew Thornburg, Tianyang Bai, Robert W. Heath Jr,',
	 'category': u'Computer Science ',
	 'date': '2014-12-2',
	 'pdflink': u'http://arxiv.org/pdf/1412.0765',
	 'subjects': u'Information Theory (cs.IT)',
	 'title': u'\nPerformance Analysis of mmWave Ad Hoc Networks',
	 'urllink': u'http://arxiv.org/abs/1412.0765'}
2015-04-10 06:01:03+0000 [xxu46_10] INFO: Crawled 194 pages (at 1 pages/min), scraped 187 items (at 1 items/min)
2015-04-10 06:01:14+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1412.0760> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 06:01:14+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1412.0760>
	{'abstract': u"Let be a set of points in the plane and a set of non-crossing line segments between vertices in , called constraints. Two vertices are visible if the straight line segment connecting them does not properly intersect any constraints. The constrained -graph is constructed by partitioning the plane around each vertex into disjoint cones, each with aperture , and adding an edge to the `closest' visible vertex in each cone. We consider how to route on the constrained -graph. We first show that no deterministic 1-local routing algorithm is -competitive on all pairs of vertices of the constrained -graph. After that, we show how to route between any two visible vertices of the constrained -graph using only 1-local information. Our routing algorithm guarantees that the returned path has length at most 2 times the Euclidean distance between the source and destination. Additionally, we provide a 1-local 18-competitive routing algorithm for visible vertices in the constrained half--graph, a subgraph of the constrained -graph that is equivalent to the Delaunay graph where the empty region is an equilateral triangle. To the best of our knowledge, these are the first local routing algorithms in the constrained setting with guarantees on the length of the returned path.",
	 'authors': u'Prosenjit Bose, Rolf Fagerberg, Andr\xe9 van Renssen, Sander Verdonschot,',
	 'category': u'Computer Science ',
	 'date': '2014-12-2',
	 'pdflink': u'http://arxiv.org/pdf/1412.0760',
	 'subjects': u'Computational Geometry (cs.CG)',
	 'title': u'\nCompetitive Local Routing with Constraints',
	 'urllink': u'http://arxiv.org/abs/1412.0760'}
2015-04-10 06:02:03+0000 [xxu46_10] INFO: Crawled 195 pages (at 1 pages/min), scraped 188 items (at 1 items/min)
2015-04-10 06:02:27+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1412.0755> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 06:02:27+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1412.0755>
	{'abstract': u'"Big Data is the oil of the new economy" is the most famous citation during the three last years. It has even been adopted by the World Economic Forum in 2011. In fact, Big Data is like crude! It\'s valuable, but if unrefined it cannot be used. It must be broken down, analyzed for it to have value. But what about Big Data generated by the Petroleum Industry and particularly its upstream segment? Upstream is no stranger to Big Data. Understanding and leveraging data in the upstream segment enables firms to remain competitive throughout planning, exploration, delineation, and field development. Oil &amp; Gas Companies conduct advanced geophysics modeling and simulation to support operations where 2D, 3D &amp; 4D Seismic generate significant data during exploration phases. They closely monitor the performance of their operational assets. To do this, they use thousands sensors in subsurface wells and surface facilities to provide continuous and real-time monitoring of assets and environmental conditions. Unfortunately, this information comes in various and increasingly complex forms, making it a challenge to collect, interpret, and leverage the disparate data. Big Data technologies integrate common and disparate data sets to deliver the right information at the appropriate time to the correct decision-maker. These capabilities help firms act on large volumes of data, transforming decision-making from reactive to proactive and optimizing all phases of exploration, development and production. Furthermore, Big Data offers multiple opportunities to ensure safer, more responsible operations. Another invaluable effect of that would be shared learning. The aim of this paper is to explain how to use Big Data technologies to optimize operations. How can Big Data help experts to decision-making leading the desired outcomes?',
	 'authors': u'Abdelkader Baaziz, Luc Quoniam,',
	 'category': u'Computer Science ',
	 'date': '2014-12-1',
	 'pdflink': u'http://arxiv.org/pdf/1412.0755',
	 'subjects': u'Computers and Society (cs.CY)',
	 'title': u'\nHow to use Big Data technologies to optimize operations in Upstream  Petroleum Industry',
	 'urllink': u'http://arxiv.org/abs/1412.0755'}
2015-04-10 06:03:03+0000 [xxu46_10] INFO: Crawled 196 pages (at 1 pages/min), scraped 189 items (at 1 items/min)
2015-04-10 06:03:41+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1412.0751> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 06:03:41+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1412.0751>
	{'abstract': u'Many tasks in Natural Language Processing involve recognizing lexical entailment. Two different approaches to this problem have been proposed recently that are quite different from each other. The first is an asymmetric similarity measure designed to give high scores when the contexts of the narrower term in the entailment are a subset of those of the broader term. The second is a supervised approach where a classifier is learned to predict entailment given a concatenated latent vector representation of the word. Both of these approaches are vector space models that use a single context vector as a representation of the word. In this work, I study the effects of clustering words into senses and using these multiple context vectors to infer entailment using extensions of these two algorithms. I find that this approach offers some improvement to these entailment algorithms.',
	 'authors': u'John Wieting,',
	 'category': u'Computer Science ',
	 'date': '2014-12-2',
	 'pdflink': u'http://arxiv.org/pdf/1412.0751',
	 'subjects': u'Computation and Language (cs.CL)',
	 'title': u'\nTiered Clustering to Improve Lexical Entailment',
	 'urllink': u'http://arxiv.org/abs/1412.0751'}
2015-04-10 06:04:03+0000 [xxu46_10] INFO: Crawled 197 pages (at 1 pages/min), scraped 190 items (at 1 items/min)
2015-04-10 06:05:01+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1412.0721> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 06:05:01+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1412.0721>
	{'abstract': u'Wireless networks are very popular nowadays. Wireless Local Area Network (WLAN) that uses the IEEE 802.11 standard and WiMAX (Worldwide Interoperability for Microwave Access) that uses the IEEE 802.16 standard are networks that we want to explore. WiMAX has been developed over 10 years, but it is still unknown to most people. However compared to WLAN, it has many advantages in transmission speed and coverage area. This paper will introduce these two technologies and make comparisons between WiMAX and WiFi. In addition, wireless network coexistence of WLAN and WiMAX will be explored through simulation. Lastly we want to discuss the future of WiMAX in relation to WiFi.',
	 'authors': u'Shuang Song, Biju Issac,',
	 'category': u'Computer Science ',
	 'date': '2014-12-1',
	 'pdflink': u'http://arxiv.org/pdf/1412.0721',
	 'subjects': u'Networking and Internet Architecture (cs.NI)',
	 'title': u'\nAnalysis of WiFi and WiMAX and Wireless Network Coexistence',
	 'urllink': u'http://arxiv.org/abs/1412.0721'}
2015-04-10 06:05:03+0000 [xxu46_10] INFO: Crawled 198 pages (at 1 pages/min), scraped 191 items (at 1 items/min)
2015-04-10 06:06:03+0000 [xxu46_10] INFO: Crawled 198 pages (at 0 pages/min), scraped 191 items (at 0 items/min)
2015-04-10 06:06:10+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1412.0696> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 06:06:10+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1412.0696>
	{'abstract': u'We suggest an information-theoretic approach for measuring linguistic style coordination in dialogues. The proposed measure has a simple predictive interpretation and can account for various confounding factors through proper conditioning. We revisit some of the previous studies that reported strong sig- natures of stylistic accommodation, and find that a significant part of the observed coordination can be attributed to a simple confounding effect - length coordination. Specifically, longer utterances tend to be followed by longer responses, which gives rise to spurious correlations in the other stylistic features. We propose a test to distinguish correlations in length due to contextual factors (topic of conversation, user verbosity, etc.) and turn-by-turn coordination. We also suggest a test to identify whether stylistic coordination persists even after accounting for length coordination and contextual factors.',
	 'authors': u'Shuyang Gao, Greg Ver Steeg, Aram Galstyan,',
	 'category': u'Computer Science ',
	 'date': '2014-12-1',
	 'pdflink': u'http://arxiv.org/pdf/1412.0696',
	 'subjects': u'Computation and Language (cs.CL)',
	 'title': u'\nUnderstanding confounding effects in linguistic coordination: an  information-theoretic approach',
	 'urllink': u'http://arxiv.org/abs/1412.0696'}
2015-04-10 06:07:03+0000 [xxu46_10] INFO: Crawled 199 pages (at 1 pages/min), scraped 192 items (at 1 items/min)
2015-04-10 06:07:13+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1412.0691> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 06:07:13+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1412.0691>
	{'abstract': u'In this paper we introduce a knowledge engine, which learns and shares knowledge representations, for robots to carry out a variety of tasks. Building such an engine brings with it the challenge of dealing with multiple data modalities including symbols, natural language, haptic senses, robot trajectories, visual features and many others. The knowledge stored in the engine comes from multiple sources including physical interactions that robots have while performing tasks (perception, planning and control), knowledge bases from WWW and learned representations from leading robotics research groups. We discuss various technical aspects and associated challenges such as modeling the correctness of knowledge, inferring latent information and formulating different robotic tasks as queries to the knowledge engine. We describe the system architecture and how it supports different mechanisms for users and robots to interact with the engine. Finally, we demonstrate its use in three important research areas: grounding natural language, perception, and planning, which are the key building blocks for many robotic tasks. This knowledge engine is a collaborative effort and we call it RoboBrain.',
	 'authors': u'Ashutosh Saxena, Ashesh Jain, Ozan Sener, Aditya Jami, Dipendra K. Misra, Hema S. Koppula,',
	 'category': u'Computer Science ',
	 'date': '2014-12-1',
	 'pdflink': u'http://arxiv.org/pdf/1412.0691',
	 'subjects': u'Artificial Intelligence (cs.AI)',
	 'title': u'\nRoboBrain: Large-Scale Knowledge Engine for Robots',
	 'urllink': u'http://arxiv.org/abs/1412.0691'}
2015-04-10 06:08:03+0000 [xxu46_10] INFO: Crawled 200 pages (at 1 pages/min), scraped 193 items (at 1 items/min)
2015-04-10 06:08:04+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1412.0684> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 06:08:04+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1412.0684>
	{'abstract': u'In this paper we present a sufficient condition that guarantees identifiability of linear network dynamic systems exhibiting continuous-time weighted consensus protocols with acyclic structure. Each edge of the underlying network graph of the system is defined by a constant parameter, referred to as the weight of the edge, while each node is defined by a scalar state whose dynamics evolve as the weighted linear combination of its difference with the states of its neighboring nodes. Following the classical definitions of identifiability and indistinguishability, we first derive a condition that ensure the identifiability of the edge weights of in terms of the associated transfer function. Using this characterization, we propose a sensor placement algorithm that guarantees identifiability of the edge weights. We describe our results using several illustrative examples.',
	 'authors': u'Seyedbehzad Nabavi, Aranya Chakrabortty, Pramod P. Khargonekar,',
	 'category': u'Computer Science ',
	 'date': '2014-11-27',
	 'pdflink': u'http://arxiv.org/pdf/1412.0684',
	 'subjects': u'Systems and Control (cs.SY)',
	 'title': u'\nA Global Identifiability Condition for Consensus Networks with Tree  Graphs',
	 'urllink': u'http://arxiv.org/abs/1412.0684'}
2015-04-10 06:09:03+0000 [xxu46_10] INFO: Crawled 201 pages (at 1 pages/min), scraped 194 items (at 1 items/min)
2015-04-10 06:09:14+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1412.0683> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 06:09:14+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1412.0683>
	{'abstract': u'Dynamic response of loads has a significant effect on system stability and directly determines the stability margin of the operating point. Inherent uncertainty and natural variability of load models make the stability assessment especially difficult and may compromise the security of the system. We propose a novel mathematical "robust stability" criterion for the assessment of small-signal stability of operating points. Whenever the criterion is satisfied for a given operating point, it provides mathematical guarantees that the system will be stable for any dynamic response of the loads. The criterion can be naturally used for identification of operating regions secure from the occurrence of Hopf bifurcation. Several possible applications of the criterion are discussed, most importantly the concept of Robust Stability Assessment Tool (RSAT) that could be integrated in dynamic security assessment packages and used in contingency screening and other planning and operational studies.',
	 'authors': u'Hung D. Nguyen, Konstantin Turitsyn,',
	 'category': u'Computer Science ',
	 'date': '2014-11-26',
	 'pdflink': u'http://arxiv.org/pdf/1412.0683',
	 'subjects': u'Systems and Control (cs.SY)',
	 'title': u'\nRobust Stability Assessment in the Presence of Load Dynamics Uncertainty',
	 'urllink': u'http://arxiv.org/abs/1412.0683'}
2015-04-10 06:10:03+0000 [xxu46_10] INFO: Crawled 202 pages (at 1 pages/min), scraped 195 items (at 1 items/min)
2015-04-10 06:10:12+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1412.0681> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 06:10:12+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1412.0681>
	{'abstract': u"We give new rounding schemes for the standard linear programming relaxation of the correlation clustering problem, achieving approximation factors almost matching the integrality gaps: - For complete graphs our appoximation is for a fixed constant , which almost matches the previously known integrality gap of . - For complete -partite graphs our approximation is . We also show a matching integrality gap. - For complete graphs with edge weights satisfying triangle inequalities and probability constraints, our approximation is , and we show an integrality gap of . Our results improve a long line of work on approximation algorithms for correlation clustering in complete graphs, previously culminating in a ratio of for the complete case by Ailon, Charikar and Newman (JACM'08). In the weighted complete case satisfying triangle inequalities and probability constraints, the same authors give a -approximation; for the bipartite case, Ailon, Avigdor-Elgrabli, Liberty and van Zuylen give a -approximation (SICOMP'12).",
	 'authors': u'Shuchi Chawla, Konstantin Makarychev, Tselil Schramm, Grigory Yaroslavtsev,',
	 'category': u'Computer Science ',
	 'date': '2014-12-1',
	 'pdflink': u'http://arxiv.org/pdf/1412.0681',
	 'subjects': u'Data Structures and Algorithms (cs.DS)',
	 'title': u'\nNear Optimal LP Rounding Algorithm for Correlation Clustering on  Complete and Complete k-partite Graphs',
	 'urllink': u'http://arxiv.org/abs/1412.0681'}
2015-04-10 06:10:44+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1412.0680> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 06:10:44+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1412.0680>
	{'abstract': u'Sparse approximations using highly over-complete dictionaries is a state-of-the-art tool for many imaging applications including denoising, super-resolution, compressive sensing, light-field analysis, and object recognition. Unfortunately, the applicability of such methods is severely hampered by the computational burden of sparse approximation: these algorithms are linear or super-linear in both the data dimensionality and size of the dictionary. We propose a framework for learning the hierarchical structure of over-complete dictionaries that enables fast computation of sparse representations. Our method builds on tree-based strategies for nearest neighbor matching, and presents domain-specific enhancements that are highly efficient for the analysis of image patches. Contrary to most popular methods for building spatial data structures, out methods rely on shallow, balanced trees with relatively few layers. We show an extensive array of experiments on several applications such as image denoising/superresolution, compressive video/light-field sensing where we practically achieve 100-1000x speedup (with a less than 1dB loss in accuracy).',
	 'authors': u'Ali Ayremlou, Thomas Goldstein, Ashok Veeraraghavan, Richard Baraniuk,',
	 'category': u'Computer Science ',
	 'date': '2014-12-1',
	 'pdflink': u'http://arxiv.org/pdf/1412.0680',
	 'subjects': u'Computer Vision and Pattern Recognition (cs.CV)',
	 'title': u'\nFast Sublinear Sparse Representation using Shallow Tree Matching Pursuit',
	 'urllink': u'http://arxiv.org/abs/1412.0680'}
2015-04-10 06:11:03+0000 [xxu46_10] INFO: Crawled 204 pages (at 2 pages/min), scraped 197 items (at 2 items/min)
2015-04-10 06:11:52+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1412.0652> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 06:11:52+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1412.0652>
	{'abstract': u'Linear Programming is now included in Algorithm undergraduate and postgraduate courses for Computer Science majors. It is possible to teach interior-point methods directly with just minimal knowledge of Algebra and Matrices.',
	 'authors': u'Sanjeev Saxena,',
	 'category': u'Computer Science ',
	 'date': '2014-11-30',
	 'pdflink': u'http://arxiv.org/pdf/1412.0652',
	 'subjects': u'Data Structures and Algorithms (cs.DS)',
	 'title': u'\nStill Simpler Way of Introducing Interior-Point method for Linear  Programming',
	 'urllink': u'http://arxiv.org/abs/1412.0652'}
2015-04-10 06:12:03+0000 [xxu46_10] INFO: Crawled 205 pages (at 1 pages/min), scraped 198 items (at 1 items/min)
2015-04-10 06:12:48+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1412.0650> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 06:12:48+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1412.0650>
	{'abstract': u'The reviewed paper describes an analog device that empirically solves small instances of the NP-complete Subset Sum Problem (SSP). The authors claim that this device can solve the SSP in polynomial time using polynomial space, in principle, and observe no exponential scaling in resource requirements. We point out that (a) the properties ascribed by the authors to their device are insufficient to solve NP-complete problems in poly-time, (b) runtime analysis offered does not cover the spectral measurement step, (c) the overall technique requires exponentially increasing resources when scaled up because of the spectral measurement step.',
	 'authors': u'Igor L. Markov,',
	 'category': u'Computer Science ',
	 'date': '2014-11-29',
	 'pdflink': u'http://arxiv.org/pdf/1412.0650',
	 'subjects': u'Emerging Technologies (cs.ET)',
	 'title': u'\nA review of "Memcomputing NP-complete problems in polynomial time using  polynomial resources" (',
	 'urllink': u'http://arxiv.org/abs/1412.0650'}
2015-04-10 06:13:03+0000 [xxu46_10] INFO: Crawled 206 pages (at 1 pages/min), scraped 199 items (at 1 items/min)
2015-04-10 06:14:03+0000 [xxu46_10] INFO: Crawled 206 pages (at 0 pages/min), scraped 199 items (at 0 items/min)
2015-04-10 06:14:04+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1412.0644> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 06:14:04+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1412.0644>
	{'abstract': u'There is a growing demand for wireless services with different requirements in a dense and heterogeneous wireless environment. Handling this complex ecosystem is becoming a challenging issue, and wireless virtualization emerges as an efficient solution. Although the inclusion of virtualization in wireless networks ensures a better use of resources, current approaches adopted for wireless virtualization can cause an underutilization of resources, since the resource allocated to a virtual wireless network is not shared with other one. This problem can be overcome by combining wireless virtualization with the cognitive radio (CR) technology and dynamic access spectrum (DSA) techniques. Thus, virtual wireless networks with different access priorities to resources (e.g., primary and secondary) can be deployed in an overlay form and share the same substrate wireless network, where the secondary networks use the resources opportunistically. However, challenges emerge in this new scenario, ranging from the mapping to the operation of these networks. This paper is the first to propose the cognitive radio virtual network environment and to formulate the problem of mapping the secondary virtual networks (SVNs) onto the wireless substrate network based on CR. To this end, a multi-objective formulation is designed. Moreover, an analysis of the metrics defined in the problem is performed in order to give the reader useful assistance in designing schemes to solve the problem of mapping SVNs onto substrate networks.',
	 'authors': u'Andson Marreiros Balieiro, Kelvin Lopes Dias,',
	 'category': u'Computer Science ',
	 'date': '2014-12-1',
	 'pdflink': u'http://arxiv.org/pdf/1412.0644',
	 'subjects': u'Networking and Internet Architecture (cs.NI)',
	 'title': u'\nMapping of Secondary Virtual Networks onto Wireless Substrate based on  Cognitive Radio: multi-objective formulation and analysis',
	 'urllink': u'http://arxiv.org/abs/1412.0644'}
2015-04-10 06:14:34+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1412.0640> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 06:14:34+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1412.0640>
	{'abstract': u'An improved version of a recently developed stochastic cluster dynamics (SCD) method Marian, J. and Bulatov, V. V., textbf (2014) 84-95 is introduced as an alternative to rate theory (RT) methods for solving coupled ordinary differential equation (ODE) systems for irradiation damage simulations. SCD circumvents by design the curse of dimensionality of the variable space that renders traditional ODE-based RT approaches inefficient when handling complex defect population comprised of multiple (more than two) defect species. Several improvements introduced here enable efficient and accurate simulations of irradiated materials up to realistic (high) damage doses characteristic of next-generation nuclear systems. The first improvement is a procedure for efficiently updating the defect reaction-network and event selection in the context of a dynamically expanding reaction-network. Next is a novel implementation of the -leaping method that speeds up SCD simulations by advancing the state of the reaction network in large time increments when appropriate. Lastly, a volume rescaling procedure is introduced to control the computational complexity of the expanding reaction-network through occasional reductions of the defect population while maintaining accurate statistics. The enhanced SCD method is then applied to model defect cluster accumulation in iron thin films subjected to triple ion-beam (, and ) irradiations, for which standard RT or spatially-resolved kinetic Monte Carlo simulations are prohibitively expensive.',
	 'authors': u'Tuan L. Hoang, Jaime Marian, Vasily V. Bulatov, Peter Hosemann,',
	 'category': u'Computer Science ',
	 'date': '2014-12-1',
	 'pdflink': u'http://arxiv.org/pdf/1412.0640',
	 'subjects': u'Data Structures and Algorithms (cs.DS)',
	 'title': u'\nComputationally-efficient stochastic cluster dynamics method for  modeling damage accumulation in irradiated materials',
	 'urllink': u'http://arxiv.org/abs/1412.0640'}
2015-04-10 06:15:03+0000 [xxu46_10] INFO: Crawled 208 pages (at 2 pages/min), scraped 201 items (at 2 items/min)
2015-04-10 06:15:39+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1412.0639> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 06:15:39+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1412.0639>
	{'abstract': u"We consider the isomorphism problem for groups specified by their multiplication tables. Until recently, the best published bound for the worst-case was achieved by the n^(log_p n + O(1)) generator-enumeration algorithm. In previous work with Fabian Wagner, we showed an n^((1 / 2) log_p n + O(log n / log log n)) time algorithm for testing isomorphism of p-groups by building graphs with degree bounded by p + O(1) that represent composition series for the groups and applying Luks' algorithm for testing isomorphism of bounded degree graphs. In this work, we extend this improvement to the more general class of solvable groups to obtain an n^((1 / 2) log_p n + O(log n / log log n)) time algorithm. In the case of solvable groups, the composition factors can be large which prevents previous methods from outperforming the generator-enumeration algorithm. Using Hall's theory of Sylow bases, we define a new object that generalizes the notion of a composition series with small factors but exists even when the composition factors are large. By constructing graphs that represent these objects and running Luks' algorithm, we obtain our algorithm for solvable-group isomorphism. We also extend our algorithm to compute canonical forms of solvable groups while retaining the same complexity.",
	 'authors': u'David J. Rosenbaum,',
	 'category': u'Computer Science ',
	 'date': '2014-12-1',
	 'pdflink': u'http://arxiv.org/pdf/1412.0639',
	 'subjects': u'Data Structures and Algorithms (cs.DS)',
	 'title': u'\nBeating the Generator-Enumeration Bound for Solvable-Group Isomorphism',
	 'urllink': u'http://arxiv.org/abs/1412.0639'}
2015-04-10 06:16:03+0000 [xxu46_10] INFO: Crawled 209 pages (at 1 pages/min), scraped 202 items (at 1 items/min)
2015-04-10 06:16:55+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1412.0630> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 06:16:55+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1412.0630>
	{'abstract': u"In this paper, we revisit batch state estimation through the lens of Gaussian process (GP) regression. We consider continuous-discrete estimation problems wherein a trajectory is viewed as a one-dimensional GP, with time as the independent variable. Our continuous-time prior can be defined by any nonlinear, time-varying stochastic differential equation driven by white noise; this allows the possibility of smoothing our trajectory estimates using a variety of vehicle dynamics models (e.g., `constant-velocity'). We show that this class of prior results in an inverse kernel matrix (i.e., covariance matrix between all pairs of measurement times) that is exactly sparse (block-tridiagonal) and that this can be exploited to carry out GP regression (and interpolation) very efficiently. When the prior is based on a linear, time-varying stochastic differential equation and the measurement model is also linear, this GP approach is equivalent to classical, discrete-time smoothing (at the measurement times); when a nonlinearity is present, we iterate over the whole trajectory to maximize accuracy. We test the approach experimentally on a simultaneous trajectory estimation and mapping problem using a mobile robot dataset.",
	 'authors': u'Sean Anderson, Timothy D. Barfoot, Chi Hay Tong, Simo S\xe4rkk\xe4,',
	 'category': u'Computer Science ',
	 'date': '2014-12-1',
	 'pdflink': u'http://arxiv.org/pdf/1412.0630',
	 'subjects': u'Robotics (cs.RO)',
	 'title': u'\nBatch Nonlinear Continuous-Time Trajectory Estimation as Exactly Sparse  Gaussian Process Regression',
	 'urllink': u'http://arxiv.org/abs/1412.0630'}
2015-04-10 06:17:03+0000 [xxu46_10] INFO: Crawled 210 pages (at 1 pages/min), scraped 203 items (at 1 items/min)
2015-04-10 06:18:03+0000 [xxu46_10] INFO: Crawled 210 pages (at 0 pages/min), scraped 203 items (at 0 items/min)
2015-04-10 06:18:12+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1412.0625> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 06:18:12+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1412.0625>
	{'abstract': u'Despite the rich literature on quantum algorithms, there is a surprisingly small amount of coverage of their concrete logical design and implementation. Most resource estimation is done at the level of complexity analysis, but actual concrete numbers (of quantum gates, qubits, etc.) can differ by orders of magnitude. The line of work we present here is a formal framework to write, and reason about, quantum algorithms. Specifically, we designed a language, Quipper, with scalability in mind, and we are able to report actual resource counts for seven non-trivial algorithms found in the quantum computer science literature.',
	 'authors': u'Jonathan M. Smith, Neil J. Ross, Peter Selinger, Beno\xeet Valiron,',
	 'category': u'Computer Science ',
	 'date': '2014-12-1',
	 'pdflink': u'http://arxiv.org/pdf/1412.0625',
	 'subjects': u'Programming Languages (cs.PL)',
	 'title': u'\nQuipper: Concrete Resource Estimation in Quantum Algorithms',
	 'urllink': u'http://arxiv.org/abs/1412.0625'}
2015-04-10 06:19:03+0000 [xxu46_10] INFO: Crawled 211 pages (at 1 pages/min), scraped 204 items (at 1 items/min)
2015-04-10 06:19:30+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1412.0624> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 06:19:30+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1412.0624>
	{'abstract': u'Sparse signals can be recovered from a reduced set of samples by using compressive sensing algorithms. The case when available samples are a random subset of a uniformly or nonuniformly sampled signal is considered in this paper. A recalculation procedure is used to reconstruct the nonuniformly sampled signal. Signal recovery is done using an adaptive gradient-based algorithm in the time domain. A new criterion for the parameter adaptation in this algorithm, based on the gradient direction angles, is proposed. It improves the algorithm computational efficiency. The methods are illustrated on statistical examples.',
	 'authors': u'Ljubisa Stankovic, Milos Dakovic,',
	 'category': u'Computer Science ',
	 'date': '2014-12-1',
	 'pdflink': u'http://arxiv.org/pdf/1412.0624',
	 'subjects': u'Information Theory (cs.IT)',
	 'title': u'\nReconstruction of Randomly Sampled Sparse Signals Using an Adaptive  Gradient Algorithm',
	 'urllink': u'http://arxiv.org/abs/1412.0624'}
2015-04-10 06:20:03+0000 [xxu46_10] INFO: Crawled 212 pages (at 1 pages/min), scraped 205 items (at 1 items/min)
2015-04-10 06:20:25+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1412.0623> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 06:20:25+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1412.0623>
	{'abstract': u'Recognizing materials in images in the wild is a challenging task. Real-world materials have rich surface texture, geometry, lighting conditions, and clutter, which combine to make the problem particularly difficult. Recently, deep learning combined with large datasets is proving to be an effective approach to tackling hard problems in scene understanding. In this paper we introduce the Materials in Context Database (MINC), a new, large, open, annotated material database. MINC is an order of magnitude larger than previous databases, while being more diverse and well- sampled across all categories. We combine this database with an evaluation of recent convolutional neural networks (CNNs) for two tasks: classifying materials from patches, and simultaneous material recognition and segmentation in full images. For patch-based classification on our dataset we found that the best performing CNN architectures can achieve 88.4% mean class accuracy. We convert these trained CNN classifiers into an efficient fully convolutional framework combined with a fully connected conditional random field (CRF) to predict the material at every pixel in an image, achieving 85% mean class accuracy at label locations. Our experiments also demonstrate that it is crucial to have a large, well-sampled dataset for the material recognition task.',
	 'authors': u'Sean Bell, Paul Upchurch, Noah Snavely, Kavita Bala,',
	 'category': u'Computer Science ',
	 'date': '2014-12-1',
	 'pdflink': u'http://arxiv.org/pdf/1412.0623',
	 'subjects': u'Computer Vision and Pattern Recognition (cs.CV)',
	 'title': u'\nMaterial recognition in the wild with the Materials in Context Database',
	 'urllink': u'http://arxiv.org/abs/1412.0623'}
2015-04-10 06:21:03+0000 [xxu46_10] INFO: Crawled 213 pages (at 1 pages/min), scraped 206 items (at 1 items/min)
2015-04-10 06:21:26+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1412.0617> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 06:21:26+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1412.0617>
	{'abstract': u'National Telecommunications and Information Administration (NTIA) has proposed vast exclusions zones between radar and Worldwide Interoperability for Microwave Access (WiMAX) (WiMAX) systems which are also being considered as geographic separations between radars and 3.5 GHz Long Term Evolution (LTE) systems without investigating any changes induced by the distinct nature of LTE as opposed to WiMAX. This paper performs a detailed system-level analysis of the interference effects from shipborne radar systems into LTE systems. Even though the results reveal impacts of radar interference on LTE systems performance, they provide clear indications of conspicuously narrower exclusion zones for LTE vis- `a-vis those for WiMAX and pave the way toward deploying LTE at 3.5 GHz within the coastline populous areas.',
	 'authors': u'Mo Ghorbanzadeh, Eugene Visotsky, Weidong Yang, Prakash Moorut, Charles Clancy,',
	 'category': u'Computer Science ',
	 'date': '2014-12-1',
	 'pdflink': u'http://arxiv.org/pdf/1412.0617',
	 'subjects': u'Networking and Internet Architecture (cs.NI)',
	 'title': u'\nRadar In-Band and Out-of-Band Interference into LTE Macro and Small Cell  Uplinks in the 3.5 GHz Band',
	 'urllink': u'http://arxiv.org/abs/1412.0617'}
2015-04-10 06:22:03+0000 [xxu46_10] INFO: Crawled 214 pages (at 1 pages/min), scraped 207 items (at 1 items/min)
2015-04-10 06:22:41+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1412.0614> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 06:22:41+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1412.0614>
	{'abstract': u'This paper offers a characterization of fundamental limits in the classification and reconstruction of high-dimensional signals from low-dimensional features, in the presence of side information. In particular, we consider a scenario where a decoder has access both to noisy linear features of the signal of interest and to noisy linear features of the side information signal; while the side information may be in a compressed form, the objective is recovery or classification of the primary signal, not the side information. We assume the signal of interest and the side information signal are drawn from a correlated mixture of distributions/components, where each component associated with a specific class label follows a Gaussian mixture model (GMM). By considering bounds to the misclassification probability associated with the recovery of the underlying class label of the signal of interest, and bounds to the reconstruction error associated with the recovery of the signal of interest itself, we then provide sharp sufficient and/or necessary conditions for the phase transition of these quantities in the low-noise regime. These conditions, which are reminiscent of the well-known Slepian-Wolf and Wyner-Ziv conditions, are a function of the number of linear features extracted from the signal of interest, the number of linear features extracted from the side information signal, and the geometry of these signals and their interplay. Our framework, which also offers a principled mechanism to integrate side information in high-dimensional data problems, is also tested in the context of imaging applications. In particular, we report state-of-the-art results in compressive hyperspectral imaging applications, where the accompanying side information is a conventional digital photograph.',
	 'authors': u'Francesco Renna, Liming Wang, Xin Yuan, Jianbo Yang, Galen Reeves, Robert Calderbank, Lawrence Carin, Miguel R. D. Rodrigues,',
	 'category': u'Computer Science ',
	 'date': '2014-12-1',
	 'pdflink': u'http://arxiv.org/pdf/1412.0614',
	 'subjects': u'Information Theory (cs.IT)',
	 'title': u'\nClassification and Reconstruction of High-Dimensional Signals from  Low-Dimensional Noisy Features in the Presence of Side Information',
	 'urllink': u'http://arxiv.org/abs/1412.0614'}
2015-04-10 06:23:03+0000 [xxu46_10] INFO: Crawled 215 pages (at 1 pages/min), scraped 208 items (at 1 items/min)
2015-04-10 06:23:57+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1412.0611> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 06:23:57+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1412.0611>
	{'abstract': u'Despite all the progress of semiconductor integrated circuit technology, the extreme complexity of the human cerebral cortex makes the hardware implementation of neuromorphic networks with a comparable number of devices exceptionally challenging. One of the most prospective candidates to provide comparable complexity, while operating much faster and with manageable power dissipation, are so-called CrossNets based on hybrid CMOS/memristor circuits. In these circuits, the usual CMOS stack is augmented with one or several crossbar layers, with adjustable two-terminal memristors at each crosspoint. Recently, there was a significant progress in improvement of technology of fabrication of such memristive crossbars and their integration with CMOS circuits, including first demonstrations of their vertical integration. Separately, there have been several demonstrations of discrete memristors as artificial synapses for neuromorphic networks. Very recently such experiments were extended to crossbar arrays of phase-change memristive devices. The adjustment of such devices, however, requires an additional transistor at each crosspoint, and hence the prospects of their scaling are less impressive than those of metal-oxide memristors, whose nonlinear I-V curves enable transistor-free operation. Here we report the first experimental implementation of a transistor-free metal-oxide memristor crossbar with device variability lowered sufficiently to demonstrate a successful operation of a simple integrated neural network, a single layer-perceptron. The network could be taught in situ using a coarse-grain variety of the delta-rule algorithm to perform the perfect classification of 3x3-pixel black/white images into 3 classes. We believe that this demonstration is an important step towards the implementation of much larger and more complex memristive neuromorphic networks.',
	 'authors': u'Mirko Prezioso, Farnood Merrikh-Bayat, Brian Hoskins, Gina Adam, Konstantin K. Likharev, Dmitri B. Strukov,',
	 'category': u'Computer Science ',
	 'date': '2014-12-1',
	 'pdflink': u'http://arxiv.org/pdf/1412.0611',
	 'subjects': u'Emerging Technologies (cs.ET)',
	 'title': u'\nTraining and Operation of an Integrated Neuromorphic Network Based on  Metal-Oxide Memristors',
	 'urllink': u'http://arxiv.org/abs/1412.0611'}
2015-04-10 06:24:03+0000 [xxu46_10] INFO: Crawled 216 pages (at 1 pages/min), scraped 209 items (at 1 items/min)
2015-04-10 06:25:03+0000 [xxu46_10] INFO: Crawled 216 pages (at 0 pages/min), scraped 209 items (at 0 items/min)
2015-04-10 06:25:09+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1412.0600> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 06:25:09+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1412.0600>
	{'abstract': u'In this paper we study the existing CRT-RSA countermeasures against fault-injection at-tacks. In an attempt to classify them we get to achieve deep understanding of how they work. We show that the many countermeasures that we study (and their variations) actually share a number of common features, but optimize them in different ways. We also show that there is no conceptual distinction between test-based and infective countermeasures and how either one can be transformed into the other. Furthermore, we show that faults on the code (skipping instructions) can be captured by considering only faults on the data. These intermediate results allow us to improve the state of the art in several ways: (a) we fix an existing and that was known to be broken countermeasure (namely the one from Shamir); (b) we drastically optimize an existing countermeasure (namely the one from Vigilant) which we reduce to 3 tests instead of 9 in its original version, and prove that it resists not only one fault but also an arbitrary number of randomizing faults; (c) we also show how to upgrade countermeasures to resist any given number of faults: given a correct first-order countermeasure, we present a way to design a prov-able high-order countermeasure (for a well-defined and reasonable fault model). Finally, we pave the way for a generic approach against fault attacks for any modular arithmetic computations, and thus for the automatic insertion of countermeasures.',
	 'authors': u'Pablo Rauzy, Sylvain Guilley,',
	 'category': u'Computer Science ',
	 'date': '2014-12-1',
	 'pdflink': u'http://arxiv.org/pdf/1412.0600',
	 'subjects': u'Cryptography and Security (cs.CR)',
	 'title': u'\nCountermeasures Against High-Order Fault-Injection Attacks on CRT-RSA',
	 'urllink': u'http://arxiv.org/abs/1412.0600'}
2015-04-10 06:26:03+0000 [xxu46_10] INFO: Crawled 217 pages (at 1 pages/min), scraped 210 items (at 1 items/min)
2015-04-10 06:26:18+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1412.0595> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 06:26:18+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1412.0595>
	{'abstract': u'Simulation of spiking neural networks has been traditionally done on high-performance supercomputers or large-scale clusters. Utilizing the parallel nature of neural network computation algorithms, GeNN (GPU Enhanced Neural Network) provides a simulation environment that performs on General Purpose NVIDIA GPUs with a code generation based approach. GeNN allows the users to design and simulate neural networks by specifying the populations of neurons at different stages, their synapse connection densities and the model of individual neurons. In this report we describe work on how to scale synaptic weights based on the configuration of the user-defined network to ensure sufficient spiking and subsequent effective learning. We also discuss optimization strategies particular to GPU computing: sparse representation of synapse connections and occupancy based block-size determination.',
	 'authors': u'Naresh Balaji, Esin Yavuz, Thomas Nowotny,',
	 'category': u'Computer Science ',
	 'date': '2014-12-1',
	 'pdflink': u'http://arxiv.org/pdf/1412.0595',
	 'subjects': u'Distributed, Parallel, and Cluster Computing (cs.DC)',
	 'title': u'\nScalability and Optimization Strategies for GPU Enhanced Neural Networks  (GeNN)',
	 'urllink': u'http://arxiv.org/abs/1412.0595'}
2015-04-10 06:27:03+0000 [xxu46_10] INFO: Crawled 218 pages (at 1 pages/min), scraped 211 items (at 1 items/min)
2015-04-10 06:27:27+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1412.0591> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 06:27:27+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1412.0591>
	{'abstract': u'Accumulation of dust on the surface of solar panels reduces the amount of radiation reaching it. This leads to loss in generated electric power and formation of hotspots which would permanently damage the solar panel. This project aims at developing an autonomous vacuum cleaning method which can be used on a regular basis to maximize the lifetime and efficiency of a solar panel. This system is implemented using two subsystems namely a Robotic Vacuum Cleaner and a Docking Station. The Robotic Vacuum Cleaner uses a two stage cleaning process to remove the dust from the solar panel. It is designed to work on inclined and slippery surfaces. A control strategy is formulated to navigate the robot in the required path using an appropriate feedback mechanism. The battery voltage of the robot is determined periodically and if it goes below a threshold, it returns to the docking station and charges itself automatically using power drawn from the solar panels. The operation of the robotic vacuum cleaner has been verified and relevant results are presented. The DC Charging circuit in the docking station is simulated in Proteus environment and is implemented in hardware. An economical, robust Robotic Vacuum Cleaner which can clean arrays of Solar panels (with or without inclination) interlinked by rails and recharge itself automatically at a docking station is designed and implemented.',
	 'authors': u'G Aravind, Vasan Gautham, T.S.B Gowtham Kumar, Balaji Naresh,',
	 'category': u'Computer Science ',
	 'date': '2014-12-1',
	 'pdflink': u'http://arxiv.org/pdf/1412.0591',
	 'subjects': u'Robotics (cs.RO)',
	 'title': u'\nA Control Strategy for an Autonomous Robotic Vacuum Cleaner for Solar  Panels',
	 'urllink': u'http://arxiv.org/abs/1412.0591'}
2015-04-10 06:28:03+0000 [xxu46_10] INFO: Crawled 219 pages (at 1 pages/min), scraped 212 items (at 1 items/min)
2015-04-10 06:28:43+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1412.0588> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 06:28:43+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1412.0588>
	{'abstract': u'We give a simple algorithm to efficiently sample the rows of a matrix while preserving the p-norms of its product with vectors. Given an -by- matrix , we find with high probability and in input sparsity time an consisting of about rescaled rows of such that is close to for all vectors . We also show similar results for all that give nearly optimal sample bounds in input sparsity time. Our results are based on sampling by "Lewis weights", which can be viewed as statistical leverage scores of a reweighted matrix. We also give an elementary proof of the guarantees of this sampling process for .',
	 'authors': u'Michael B. Cohen, Richard Peng,',
	 'category': u'Computer Science ',
	 'date': '2014-12-1',
	 'pdflink': u'http://arxiv.org/pdf/1412.0588',
	 'subjects': u'Data Structures and Algorithms (cs.DS)',
	 'title': u'\n$\\ell_p$ Row Sampling by Lewis Weights',
	 'urllink': u'http://arxiv.org/abs/1412.0588'}
2015-04-10 06:29:03+0000 [xxu46_10] INFO: Crawled 220 pages (at 1 pages/min), scraped 213 items (at 1 items/min)
2015-04-10 06:29:52+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1412.0540> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 06:29:52+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1412.0540>
	{'abstract': u'We describe the missing class of the hierarchy of mixed unit interval graphs, generated by the intersection graphs of closed, open and one type of half-open intervals of the real line. This class lies strictly between unit interval graphs and mixed unit interval graphs. We give a complete characterization of this new class, as well as a polynomial time algorithm to recognize graphs from this class and to produce a corresponding interval representation if one exists.',
	 'authors': u'Alexandre Talon, Jan Kratochv\xedl,',
	 'category': u'Computer Science ',
	 'date': '2014-12-1',
	 'pdflink': u'http://arxiv.org/pdf/1412.0540',
	 'subjects': u'Discrete Mathematics (cs.DM)',
	 'title': u'\nCompletion of the mixed unit interval graphs hierarchy',
	 'urllink': u'http://arxiv.org/abs/1412.0540'}
2015-04-10 06:30:03+0000 [xxu46_10] INFO: Crawled 221 pages (at 1 pages/min), scraped 214 items (at 1 items/min)
2015-04-10 06:31:02+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1412.0538> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 06:31:02+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1412.0538>
	{'abstract': u'Conquerors of old (like, e.g., Alexander the Great or Ceasar) had to solve the following deployment problem. Sufficiently strong units had to be stationed at locations of strategic importance, and the moving forces had to be strong enough to advance to the next location. To the best of our knowledge we are the first to consider the (off-line) graph version of this problem. While being NP-hard for general graphs, for trees the minimum number of agents and an optimal deployment can be computed in optimal polynomial time. Moreover, the optimal solution for the minimum spanning tree of an arbitrary graph G results in a 2-approximation of the optimal solution for G.',
	 'authors': u'Elmar Langetepe, Andreas Lenerz, Bernd Br\xfcggemann,',
	 'category': u'Computer Science ',
	 'date': '2014-12-1',
	 'pdflink': u'http://arxiv.org/pdf/1412.0538',
	 'subjects': u'Data Structures and Algorithms (cs.DS)',
	 'title': u'\nStrategic deployment in graphs',
	 'urllink': u'http://arxiv.org/abs/1412.0538'}
2015-04-10 06:31:03+0000 [xxu46_10] INFO: Crawled 222 pages (at 1 pages/min), scraped 215 items (at 1 items/min)
2015-04-10 06:32:03+0000 [xxu46_10] INFO: Crawled 222 pages (at 0 pages/min), scraped 215 items (at 0 items/min)
2015-04-10 06:32:13+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1412.0537> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 06:32:13+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1412.0537>
	{'abstract': u'Copyless streaming string transducers (copyless SST) have been introduced by R. Alur and P. Cerny in 2010 as a one-way deterministic automata model to define transformations of finite strings. Copyless SST extend deterministic finite state automata with a set of registers in which to store intermediate output strings, and those registers can be combined and updated all along the run, in a linear manner, i.e., no register content can be copied on transitions. It is known that copyless SST capture exactly the class of MSO-definable string-to-string transformations, as defined by B. Courcelle, and are equi-expressive to deterministic two-way transducers. They enjoy good algorithmic properties. Most notably, they have decidable equivalence problem (in PSpace). In this paper, we show that they still have decidable equivalence problem even without the copyless restriction. The proof reduces to the HDT0L sequence equivalence problem, which is known to be decidable. We also show that this latter problem is as difficult as the SST equivalence problem, modulo linear time reduction.',
	 'authors': u'Emmanuel Filiot, Pierre-Alain Reynier,',
	 'category': u'Computer Science ',
	 'date': '2014-12-1',
	 'pdflink': u'http://arxiv.org/pdf/1412.0537',
	 'subjects': u'Formal Languages and Automata Theory (cs.FL)',
	 'title': u'\nOn Streaming String Transducers and HDT0L Systems',
	 'urllink': u'http://arxiv.org/abs/1412.0537'}
2015-04-10 06:33:03+0000 [xxu46_10] INFO: Crawled 223 pages (at 1 pages/min), scraped 216 items (at 1 items/min)
2015-04-10 06:33:28+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1412.0529> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 06:33:28+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1412.0529>
	{'abstract': u'We show how group discounts can be offered without forcing buyers to surrender their anonymity, as long as buyers can use their own computing devices (e.g. smartphone, tablet or computer) to perform a purchase. Specifically, we present a protocol for privacy-preserving group discounts. The protocol allows a group of buyers to prove how many they are without disclosing their identities. Coupled with an anonymous payment system, this makes group discounts compatible with buyer privacy (that is, buyer anonymity).',
	 'authors': u'Josep Domingo-Ferrer, Alberto Blanco-Justicia,',
	 'category': u'Computer Science ',
	 'date': '2014-12-1',
	 'pdflink': u'http://arxiv.org/pdf/1412.0529',
	 'subjects': u'Cryptography and Security (cs.CR)',
	 'title': u'\nGroup Discounts Compatible with Buyer Privacy',
	 'urllink': u'http://arxiv.org/abs/1412.0529'}
2015-04-10 06:34:03+0000 [xxu46_10] INFO: Crawled 224 pages (at 1 pages/min), scraped 217 items (at 1 items/min)
2015-04-10 06:34:35+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1412.0527> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 06:34:35+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1412.0527>
	{'abstract': u'Adaptation of software components is an important issue in Component Based Software Engineering (CBSE). Building a system from reusable or Commercial-Off-The-Shelf (COTS) components introduces a set of issues, mainly related to compatibility and communication aspects. Components may have incompatible interaction behavior. Moreover it might be necessary to enhance the current communication protocol to introduce more sophisticated interactions among components. We address these problems enhancing our architectural approach which allows for detection and recovery of integration mismatches by synthesizing a suitable coordinator. Starting from the specification of the system to be assembled and from the specification of the needed protocol enhancements, our framework automatically derives, in a compositional way, the glue code for the set of components. The synthesized glue code avoids interaction mismatches and provides a protocol-enhanced version of the composed system.',
	 'authors': u'Marco Autili, Paola Inverardi, Massimo Tivoli,',
	 'category': u'Computer Science ',
	 'date': '2014-12-1',
	 'pdflink': u'http://arxiv.org/pdf/1412.0527',
	 'subjects': u'Software Engineering (cs.SE)',
	 'title': u'\nAutomatic adaptor synthesis for protocol transformation',
	 'urllink': u'http://arxiv.org/abs/1412.0527'}
2015-04-10 06:35:03+0000 [xxu46_10] INFO: Crawled 225 pages (at 1 pages/min), scraped 218 items (at 1 items/min)
2015-04-10 06:35:53+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1412.0525> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 06:35:53+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1412.0525>
	{'abstract': u"There are many situations in which it would be beneficial for a robot to have predictive abilities similar to those of rational humans. Some of these situations include collaborative robots, robots in adversarial situations, and for dynamic obstacle avoidance. This paper presents an approach to modeling behaviors of dynamic agents in order to empower robots with the ability to predict the agent's actions and identify the behavior the agent is executing in real time. The method of behavior modeling implemented uses hidden Markov models (HMMs) to model the unobservable states of the dynamic agents. The background and theory of the behavior modeling is presented. Experimental results of realistic simulations of a robot predicting the behaviors and actions of a dynamic agent in a static environment are presented.",
	 'authors': u'Alan J. Hamlet, Carl D. Crane,',
	 'category': u'Computer Science ',
	 'date': '2014-12-1',
	 'pdflink': u'http://arxiv.org/pdf/1412.0525',
	 'subjects': u'Robotics (cs.RO)',
	 'title': u'\nRobotic Behavior Prediction Using Hidden Markov Models',
	 'urllink': u'http://arxiv.org/abs/1412.0525'}
2015-04-10 06:36:03+0000 [xxu46_10] INFO: Crawled 226 pages (at 1 pages/min), scraped 219 items (at 1 items/min)
2015-04-10 06:36:56+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1412.0501> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 06:36:56+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1412.0501>
	{'abstract': u'In this work, a new region-based, multipath-enabled packet routing is presented and called SmartPacket Routing. The proposed approach provides several opportunities to re-distribute the smartness and decision making among various elements of a network including the packets themselves toward providing a decentralized solution for SDNs. This would bring efficiency and scalability, and therefore also lower environmental footprint for the ever-growing networks. In particular, a region-based representation of the network topology is proposed which is then used to describe the routing actions along the possible paths for a packet flow. In addition to a region stack that expresses a partial or full regional path of a packet, QoS requirements of the packet (or its associated flow) is considered in the packet header in order to enable possible QoS-aware routing at region level without requiring a centralized controller.',
	 'authors': u'Reza Farrahi Moghaddam, Mohamed Cheriet,',
	 'category': u'Computer Science ',
	 'date': '2014-12-1',
	 'pdflink': u'http://arxiv.org/pdf/1412.0501',
	 'subjects': u'Networking and Internet Architecture (cs.NI)',
	 'title': u'\nSmartPacket: Re-Distributing the Routing Intelligence among Network  Components in SDNs',
	 'urllink': u'http://arxiv.org/abs/1412.0501'}
2015-04-10 06:37:03+0000 [xxu46_10] INFO: Crawled 227 pages (at 1 pages/min), scraped 220 items (at 1 items/min)
2015-04-10 06:37:50+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1412.0494> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 06:37:50+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1412.0494>
	{'abstract': u'In single particle reconstruction (SPR) from cryo-electron microscopy (cryo-EM), the 3D structure of a molecule needs to be determined from its 2D projection images taken at unknown viewing directions. Zvi Kam showed already in 1980 that the autocorrelation function of the 3D molecule over the rotation group SO(3) can be estimated from 2D projection images whose viewing directions are uniformly distributed over the sphere. The autocorrelation function determines the expansion coefficients of the 3D molecule in spherical harmonics up to an orthogonal matrix of size for each . In this paper we show how techniques for solving the phase retrieval problem in X-ray crystallography can be modified for the cryo-EM setup for retrieving the missing orthogonal matrices. Specifically, we present two new approaches that we term Orthogonal Extension and Orthogonal Replacement, in which the main algorithmic components are the singular value decomposition and semidefinite programming. We demonstrate the utility of these approaches through numerical experiments on simulated data.',
	 'authors': u'Tejal Bhamre, Teng Zhang, Amit Singer,',
	 'category': u'Computer Science ',
	 'date': '2014-12-1',
	 'pdflink': u'http://arxiv.org/pdf/1412.0494',
	 'subjects': u'Computer Vision and Pattern Recognition (cs.CV)',
	 'title': u'\nOrthogonal Matrix Retrieval in Cryo-Electron Microscopy',
	 'urllink': u'http://arxiv.org/abs/1412.0494'}
2015-04-10 06:38:03+0000 [xxu46_10] INFO: Crawled 228 pages (at 1 pages/min), scraped 221 items (at 1 items/min)
2015-04-10 06:39:03+0000 [xxu46_10] INFO: Crawled 228 pages (at 0 pages/min), scraped 221 items (at 0 items/min)
2015-04-10 06:39:07+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1412.0477> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 06:39:07+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1412.0477>
	{'abstract': u'Given unstructured videos of deformable objects (such as animals in the wild), we automatically recover spatiotemporal correspondences to map one object to another. In contrast to traditional methods based on appearance, which fail in such challenging conditions, we exploit consistency in observed object motion between instances. Our approach discovers pairs of short video intervals where the foreground moves in a consistent manner and uses these candidates as seeds for spatial alignment. We model the spatial correspondence between point trajectories generated by an object in one interval to those in the other using a homography. We find thousands of pairs of frames for which we successfully recover valid alignments without any human supervision.',
	 'authors': u'Luca Del Pero, Susanna Ricco, Rahul Sukthankar, Vittorio Ferrari,',
	 'category': u'Computer Science ',
	 'date': '2014-12-1',
	 'pdflink': u'http://arxiv.org/pdf/1412.0477',
	 'subjects': u'Computer Vision and Pattern Recognition (cs.CV)',
	 'title': u'\nRecovering Spatiotemporal Correspondence between Deformable Objects by  Exploiting Consistent Foreground Motion in Video',
	 'urllink': u'http://arxiv.org/abs/1412.0477'}
2015-04-10 06:40:03+0000 [xxu46_10] INFO: Crawled 229 pages (at 1 pages/min), scraped 222 items (at 1 items/min)
2015-04-10 06:40:21+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1412.0439> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 06:40:21+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1412.0439>
	{'abstract': u'Human Motion Analysis (HMA) is currently one of the most popularly active research domains as such significant research interests are motivated by a number of real world applications such as video surveillance, sports analysis, healthcare monitoring and so on. However, most of these real world applications face high levels of uncertainties that can affect the operations of such applications. Hence, the fuzzy set theory has been applied and showed great success in the recent past. In this paper, we aim at reviewing the fuzzy set oriented approaches for HMA, individuating how the fuzzy set may improve the HMA, envisaging and delineating the future perspectives. To the best of our knowledge, there is not found a single survey in the current literature that has discussed and reviewed fuzzy approaches towards the HMA. For ease of understanding, we conceptually classify the human motion into three broad levels: Low-Level (LoL), Mid-Level (MiL), and High-Level (HiL) HMA.',
	 'authors': u'Chern Hong Lim, Ekta Vats, Chee Seng Chan,',
	 'category': u'Computer Science ',
	 'date': '2014-12-1',
	 'pdflink': u'http://arxiv.org/pdf/1412.0439',
	 'subjects': u'Computer Vision and Pattern Recognition (cs.CV)',
	 'title': u'\nFuzzy human motion analysis: A review',
	 'urllink': u'http://arxiv.org/abs/1412.0439'}
2015-04-10 06:41:03+0000 [xxu46_10] INFO: Crawled 230 pages (at 1 pages/min), scraped 223 items (at 1 items/min)
2015-04-10 06:41:28+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1412.0436> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 06:41:28+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1412.0436>
	{'abstract': u'This document describes an infra-structure provided by the R package performanceEstimation that allows to estimate the predictive performance of different approaches (workflows) to predictive tasks. The infra-structure is generic in the sense that it can be used to estimate the values of any performance metrics, for any workflow on different predictive tasks, namely, classification, regression and time series tasks. The package also includes several standard workflows that allow users to easily set up their experiments limiting the amount of work and information they need to provide. The overall goal of the infra-structure provided by our package is to facilitate the task of estimating the predictive performance of different modeling approaches to predictive tasks in the R environment.',
	 'authors': u'Luis Torgo,',
	 'category': u'Computer Science ',
	 'date': '2014-12-1',
	 'pdflink': u'http://arxiv.org/pdf/1412.0436',
	 'subjects': u'Mathematical Software (cs.MS)',
	 'title': u'\nAn Infra-Structure for Performance Estimation and Experimental  Comparison of Predictive Models in R',
	 'urllink': u'http://arxiv.org/abs/1412.0436'}
2015-04-10 06:42:03+0000 [xxu46_10] INFO: Crawled 231 pages (at 1 pages/min), scraped 224 items (at 1 items/min)
2015-04-10 06:42:32+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1412.0426> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 06:42:32+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1412.0426>
	{'abstract': u'An undergraduate compilers course poses significant challenges to students, in both the conceptual richness of the major components and in the programming effort necessary to implement them. In this paper, I argue that a related architecture, the interpreter, serves as an effective conceptual framework in which to teach some of the later stages of the compiler pipeline. This framework can serve both to unify some of the major concepts that are taught in a typical undergraduate course and to structure the implementation of a semester-long compiler project.',
	 'authors': u'John H. E. Lasseter,',
	 'category': u'Computer Science ',
	 'date': '2014-12-1',
	 'pdflink': u'http://arxiv.org/pdf/1412.0426',
	 'subjects': u'Computers and Society (cs.CY)',
	 'title': u'\nThe Interpreter In An Undergraduate Compilers Course',
	 'urllink': u'http://arxiv.org/abs/1412.0426'}
2015-04-10 06:43:03+0000 [xxu46_10] INFO: Crawled 232 pages (at 1 pages/min), scraped 225 items (at 1 items/min)
2015-04-10 06:43:51+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1412.0423> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 06:43:51+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1412.0423>
	{'abstract': u"In this paper, we will show dichotomy theorems for the computation of polynomials corresponding to evaluation of graph homomorphisms in Valiant's model. We are given a fixed graph and want to find all graphs, from some graph class, homomorphic to this . These graphs will be encoded by a family of polynomials. We give dichotomies for the polynomials for cycles, cliques, trees, outerplanar graphs, planar graphs and graphs of bounded genus.",
	 'authors': u'Christian Engels,',
	 'category': u'Computer Science ',
	 'date': '2014-12-1',
	 'pdflink': u'http://arxiv.org/pdf/1412.0423',
	 'subjects': u'Computational Complexity (cs.CC)',
	 'title': u'\nDichotomy Theorems for Homomorphism Polynomials of Graph Classes',
	 'urllink': u'http://arxiv.org/abs/1412.0423'}
2015-04-10 06:44:03+0000 [xxu46_10] INFO: Crawled 233 pages (at 1 pages/min), scraped 226 items (at 1 items/min)
2015-04-10 06:45:03+0000 [xxu46_10] INFO: Crawled 233 pages (at 0 pages/min), scraped 226 items (at 0 items/min)
2015-04-10 06:45:08+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1412.0422> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 06:45:08+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1412.0422>
	{'abstract': u'A parameter space procedure for designing chosen parameters of a repetitive controller to satisfy a robust performance criterion is presented. Using this method, low order robust repetitive controllers can be designed and implemented for plants that possibly include time delay, poles on the imaginary axis and discontinuous weights. A design and simulation study based on a high speed atomic force microscope position control example is utilized to illustrate the method presented in this paper.',
	 'authors': u'Burak Demirel, Levent Guvenc,',
	 'category': u'Computer Science ',
	 'date': '2014-12-1',
	 'pdflink': u'http://arxiv.org/pdf/1412.0422',
	 'subjects': u'Systems and Control (cs.SY)',
	 'title': u'\nParameter Space Design of Repetitive Controllers for Satisfying a Robust  Performance Requirement',
	 'urllink': u'http://arxiv.org/abs/1412.0422'}
2015-04-10 06:46:03+0000 [xxu46_10] INFO: Crawled 234 pages (at 1 pages/min), scraped 227 items (at 1 items/min)
2015-04-10 06:46:25+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1412.0366> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 06:46:25+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1412.0366>
	{'abstract': u'A mobile sensor network is a wireless network of sensor nodes that move arbitrarily. In this paper, we explore the use of a maximum stability spanning tree-based data gathering (Max.Stability-DG) algorithm and a minimum-distance spanning tree-based data gathering (MST-DG) algorithm for mobile sensor networks. We analyze the impact of these two algorithms on the node failure times and the resulting coverage loss due to node failures. Both the Max.Stability-DG and MST-DG algorithms are based on a greedy strategy of determining a data gathering tree when one is needed and using that tree as long as it exists. The Max.Stability-DG algorithm assumes the availability of the complete knowledge of future topology changes and determines a data gathering tree whose corresponding spanning tree would exist for the longest time since the current time instant; whereas, the MST-DG algorithm determines a data gathering tree whose corresponding spanning tree is the minimum distance tree at the current time instant. We observe the Max.Stability-DG trees to incur a longer network lifetime (time of disconnection of the network of live sensor nodes due to node failures), a larger coverage loss time for a particular fraction of loss of coverage as well as a lower fraction of coverage loss at any time. The tradeoff is that the Max.Stability-DG trees incur a lower node lifetime (the time of first node failure) due to repeated use of a data gathering tree for a longer time.',
	 'authors': u'Natarajan Meghanathan, Philip Mumford,',
	 'category': u'Computer Science ',
	 'date': '2014-12-1',
	 'pdflink': u'http://arxiv.org/pdf/1412.0366',
	 'subjects': u'Networking and Internet Architecture (cs.NI)',
	 'title': u'\nNode Failure Time and Coverage Loss Time Analysis for Maximum Stability  Vs Minimum Distance Spanning Tree based Data Gathering in Mobile Sensor  Networks',
	 'urllink': u'http://arxiv.org/abs/1412.0366'}
2015-04-10 06:47:03+0000 [xxu46_10] INFO: Crawled 235 pages (at 1 pages/min), scraped 228 items (at 1 items/min)
2015-04-10 06:47:19+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1412.0364> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 06:47:19+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1412.0364>
	{'abstract': u'We present , an operator for interactively exploring a relational table to discover and summarize "interesting" groups of tuples. Each group of tuples is described by a . For instance, the rule tells us that there are a thousand tuples with value in the first column and in the second column (and any value in the third column). Smart drill-down presents an analyst with a list of rules that together describe interesting aspects of the table. The analyst can tailor the definition of interesting, and can interactively apply smart drill-down on an existing rule to explore that part of the table. We demonstrate that the underlying optimization problems are , and describe an algorithm for finding the approximately optimal list of rules to display when the user uses a smart drill-down, and a dynamic sampling scheme for efficiently interacting with large tables. Finally, we perform experiments on real datasets to demonstrate the usefulness of smart drill-down and study the performance of our algorithms.',
	 'authors': u'Manas Joglekar, Hector Garcia-Molina, Aditya Parameswaran,',
	 'category': u'Computer Science ',
	 'date': '2014-12-1',
	 'pdflink': u'http://arxiv.org/pdf/1412.0364',
	 'subjects': u'Databases (cs.DB)',
	 'title': u'\nSmart Drill Down',
	 'urllink': u'http://arxiv.org/abs/1412.0364'}
2015-04-10 06:48:03+0000 [xxu46_10] INFO: Crawled 236 pages (at 1 pages/min), scraped 229 items (at 1 items/min)
2015-04-10 06:48:32+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1412.0356> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 06:48:32+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1412.0356>
	{'abstract': u'We first prove a new separating hyperplane theorem characterizing when a pair of compact convex subsets of the Euclidean space intersect, and when they are disjoint. The theorem is distinct from classical separation theorems. It generalizes the proved in our earlier work for testing the membership of a distinguished point in the convex hull of a finite point set. Next by utilizing the theorem, we develop a substantially generalized and stronger version of the introduced in the previous work to perform any of the following three tasks: (1) To compute a pair , where either the Euclidean distance is to within a prescribed tolerance, or the orthogonal bisecting hyperplane of the line segment separates the two sets; (2) When and are disjoint, to compute so that approximates to within a prescribed tolerance; (3) When and are disjoint, to compute a pair of parallel supporting hyperplanes so that is to within a prescribed tolerance of the optimal margin. The worst-case complexity of each iteration is solving a linear objective over or . The resulting algorithm is a fully polynomial-time approximation scheme for such important special cases as when and are convex hulls of finite points sets, or the intersection of a finite number of halfspaces. The results find many theoretical and practical applications, such as in machine learning, statistics, linear, quadratic and convex programming. In a forthcoming work we show how the algorithms can provide a certain approximation to NP-complete problems.',
	 'authors': u'Bahman Kalantari,',
	 'category': u'Computer Science ',
	 'date': '2014-12-1',
	 'pdflink': u'http://arxiv.org/pdf/1412.0356',
	 'subjects': u'Computational Complexity (cs.CC)',
	 'title': u'\nAn Algorithmic Separating Hyperplane Theorem and Its Applications',
	 'urllink': u'http://arxiv.org/abs/1412.0356'}
2015-04-10 06:49:03+0000 [xxu46_10] INFO: Crawled 237 pages (at 1 pages/min), scraped 230 items (at 1 items/min)
2015-04-10 06:49:37+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1412.0349> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 06:49:37+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1412.0349>
	{'abstract': u'In this paper, we propose to use a wireless-powered friendly jammer to enable secure communication between a source node and destination node, in the presence of an eavesdropper. We consider a two-phase communication protocol with fixed-rate transmission. In the first phase, wireless power transfer is conducted from the source to the jammer. In the second phase, the source transmits the information-bearing signal under the protection of a jamming signal sent by the jammer using the harvested energy in the first phase. We analytically characterize the long-time behavior of the proposed protocol and derive a closed-form expression for the throughput. We further optimize the rate parameters for maximizing the throughput subject to a secrecy outage probability constraint. Our analytical results show that the throughput performance differs significantly between the single-antenna jammer case and the multi-antenna jammer case. For instance, as the source transmit power increases, the throughput quickly reaches an upper bound with single-antenna jammer, while the throughput grows unbounded with multi-antenna jammer. Our numerical results also validate the derived analytical results.',
	 'authors': u'Wanchun Liu, Xiangyun Zhou, Salman Durrani, Petar Popovski,',
	 'category': u'Computer Science ',
	 'date': '2014-12-1',
	 'pdflink': u'http://arxiv.org/pdf/1412.0349',
	 'subjects': u'Information Theory (cs.IT)',
	 'title': u'\nSecure Communication with a Wireless-Powered Friendly Jammer',
	 'urllink': u'http://arxiv.org/abs/1412.0349'}
2015-04-10 06:50:03+0000 [xxu46_10] INFO: Crawled 238 pages (at 1 pages/min), scraped 231 items (at 1 items/min)
2015-04-10 06:50:24+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1412.0348> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 06:50:24+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1412.0348>
	{'abstract': u'The edit distance (a.k.a. the Levenshtein distance) between two strings is defined as the minimum number of insertions, deletions or substitutions of symbols needed to transform one string into another. The problem of computing the edit distance between two strings is a classic computational task, with a well-known algorithm based on dynamic programming. Unfortunately, all known algorithms for this problem run in nearly quadratic time. In this paper we provide evidence that the near-quadratic running time bounds known for the problem of computing edit distance might be tight. Specifically, we show that, if the edit distance can be computed in time O(n^) for some constant delta&gt;0, then the satisfiability of conjunctive normal form formulas with N variables and M clauses can be solved in time M^ 2^ for a constant epsilon&gt;0. The latter result would violate the Strong Exponential Time Hypothesis, which postulates that such algorithms do not exist.',
	 'authors': u'Arturs Backurs, Piotr Indyk,',
	 'category': u'Computer Science ',
	 'date': '2014-12-1',
	 'pdflink': u'http://arxiv.org/pdf/1412.0348',
	 'subjects': u'Computational Complexity (cs.CC)',
	 'title': u'\nEdit Distance Cannot Be Computed in Strongly Subquadratic Time (unless  SETH is false)',
	 'urllink': u'http://arxiv.org/abs/1412.0348'}
2015-04-10 06:51:03+0000 [xxu46_10] INFO: Crawled 239 pages (at 1 pages/min), scraped 232 items (at 1 items/min)
2015-04-10 06:51:39+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1412.0340> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 06:51:39+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1412.0340>
	{'abstract': u"Baker's technique, which was created over three decades ago, is a powerful tool for designing polynomial time approximation schemes (PTAS) for NP-hard optimization problems on planar graphs and their generalizations. In this paper, we propose a unified framework to formulate the optimization problems where the local constraints of these problems are encoded by functions attached on the vertices. This framework has much stronger ability for modelling real world problems than classic combinatorial optimization problems. We prove that when the function attached on is a liberal function () for all , then there is a PTAS for computing the max-sum of on planar graphs. We also prove that computing the min-sum of does not admit PTAS even on planar graphs unless P = NP. But if the set of liberal functions satisfies the balance property, we have a PTAS for computing the min-sum. These results are further generalized to graphs with bounded local treewidth, H-minor-free graphs, -dimensional geometric graphs with bounded density and graphs with bounded number of crossings per edge. Our results lead to PTASs for MAX-CUT, MAX-DICUT, MAX--CUT on these graphs. We also prove that if the corresponding factor graph of a CNF formula can be transformed into these graphs through contractions, then computing the MAX-SAT of has a PTAS. Our technique generalizes Baker's technique and many existing methods for graph decomposition, which makes contributions to many significant computation problems in various fields such as wireless networks, distributed database systems, artificial intelligence, statistical physics and computer vision.",
	 'authors': u'Yi-Kai Wang,',
	 'category': u'Computer Science ',
	 'date': '2014-12-1',
	 'pdflink': u'http://arxiv.org/pdf/1412.0340',
	 'subjects': u'Data Structures and Algorithms (cs.DS)',
	 'title': u"\nBeyond Baker's Technique",
	 'urllink': u'http://arxiv.org/abs/1412.0340'}
2015-04-10 06:52:03+0000 [xxu46_10] INFO: Crawled 240 pages (at 1 pages/min), scraped 233 items (at 1 items/min)
2015-04-10 06:52:56+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1412.0327> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 06:52:56+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1412.0327>
	{'abstract': u"Recent outbreaks of Ebola and Dengue viruses have again elevated the significance of the capability to quickly predict disease spread in an emergent situation. However, existing approaches usually rely heavily on the time-consuming census processes, or the privacy-sensitive call logs, leading to their unresponsive nature when facing the abruptly changing dynamics in the event of an outbreak. In this paper we study the feasibility of using large-scale Twitter data as a proxy of human mobility to model and predict disease spread. We report that for Australia, Twitter users' distribution correlates well the census-based population distribution, and that the Twitter users' travel patterns appear to loosely follow the gravity law at multiple scales of geographic distances, i.e. national level, state level and metropolitan level. The radiation model is also evaluated on this dataset though it has shown inferior fitness as a result of Australia's sparse population and large landmass. The outcomes of the study form the cornerstones for future work towards a model-based, responsive prediction method from Twitter data for disease spread.",
	 'authors': u'Jiajun Liu, Kun Zhao, Saeed Khan, Mark Cameron, Raja Jurdak,',
	 'category': u'Computer Science ',
	 'date': '2014-12-1',
	 'pdflink': u'http://arxiv.org/pdf/1412.0327',
	 'subjects': u'Social and Information Networks (cs.SI)',
	 'title': u'\nMulti-scale Population and Mobility Estimation with Geo-tagged Tweets',
	 'urllink': u'http://arxiv.org/abs/1412.0327'}
2015-04-10 06:53:03+0000 [xxu46_10] INFO: Crawled 241 pages (at 1 pages/min), scraped 234 items (at 1 items/min)
2015-04-10 06:53:43+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1412.0325> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 06:53:43+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1412.0325>
	{'abstract': u'We are given a project-based university course, where our task is to assign students to suitable projects. In the Student/Project Allocation problem, or shortly SPA, each student submits a weighted list of their acceptable projects, and likewise, a set of assignable students is given to each project. Moreover, some projects are equipped with upper and lower quotas with regards to the number of students assigned to them. The quota requirement is strict: projects not reaching their lower quotas must be closed entirely. The challenge is to find a b-matching of maximum weight observing the lower quotas. In our present work we show that finding such a maximum weight matching is an NP-complete task even if all students mark at most two projects and all projects have upper and lower quota three. Moreover, we also prove that if no upper quota is larger than two, the problem immediately becomes polynomially tractable. For the general case, an approximation algorithm is presented that reaches the best possible ratio -- due to inapproximability bounds -- expressed in various terms.',
	 'authors': u'Ashwin Arulselvan, \xc1gnes Cseh, Jannik Matuschke,',
	 'category': u'Computer Science ',
	 'date': '2014-12-1',
	 'pdflink': u'http://arxiv.org/pdf/1412.0325',
	 'subjects': u'Discrete Mathematics (cs.DM)',
	 'title': u'\nThe Student/Project Allocation problem with group projects',
	 'urllink': u'http://arxiv.org/abs/1412.0325'}
2015-04-10 06:54:03+0000 [xxu46_10] INFO: Crawled 242 pages (at 1 pages/min), scraped 235 items (at 1 items/min)
2015-04-10 06:54:37+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1412.0321> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 06:54:37+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1412.0321>
	{'abstract': u'Long-term location tracking, where trajectory compression is commonly used, has gained high interest for many applications in transport, ecology, and wearable computing. However, state-of-the-art compression methods involve high space-time complexity or achieve unsatisfactory compression rate, leading to rapid exhaustion of memory, computation, storage and energy resources. We propose a novel online algorithm for error-bounded trajectory compression called the Bounded Quadrant System (BQS), which compresses trajectories with extremely small costs in space and time using convex-hulls. In this algorithm, we build a virtual coordinate system centered at a start point, and establish a rectangular bounding box as well as two bounding lines in each of its quadrants. In each quadrant, the points to be assessed are bounded by the convex-hull formed by the box and lines. Various compression error-bounds are therefore derived to quickly draw compression decisions without expensive error computations. In addition, we also propose a light version of the BQS version that achieves complexity in both time and space for processing each point to suit the most constrained computation environments. Furthermore, we briefly demonstrate how this algorithm can be naturally extended to the 3-D case. Using empirical GPS traces from flying foxes, cars and simulation, we demonstrate the effectiveness of our algorithm in significantly reducing the time and space complexity of trajectory compression, while greatly improving the compression rates of the state-of-the-art algorithms (up to 47%). We then show that with this algorithm, the operational time of the target resource-constrained hardware platform can be prolonged by up to 41%.',
	 'authors': u'Jiajun Liu, Kun Zhao, Philipp Sommer, Shuo Shang, Brano Kusy, Raja Jurdak,',
	 'category': u'Computer Science ',
	 'date': '2014-12-1',
	 'pdflink': u'http://arxiv.org/pdf/1412.0321',
	 'subjects': u'Data Structures and Algorithms (cs.DS)',
	 'title': u'\nBounded Quadrant System: Error-bounded Trajectory Compression on the Go',
	 'urllink': u'http://arxiv.org/abs/1412.0321'}
2015-04-10 06:55:03+0000 [xxu46_10] INFO: Crawled 243 pages (at 1 pages/min), scraped 236 items (at 1 items/min)
2015-04-10 06:55:52+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1412.0320> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 06:55:52+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1412.0320>
	{'abstract': u'emph (CP) refer to normal logic programs augmented with connective . In this paper we address the question of whether CP are emph with emph (PF). Our main result shows that the PARITY problem, which can be polynomially represented in PF but emph has exponential representations in CP. In other words, PARITY emph PF from CP. Simply speaking, this means that exponential size blowup is generally inevitable when translating a set of formulas in PF into an equivalent program in CP (without introducing new variables). Furthermore, since it has been shown by Lifschitz and Razborov that there is also a problem that separates CP from PF (assuming ), it follows that CP and PF are indeed succinctly incomparable. From the view of the theory of computation, the above result may also be considered as the separation of two emph, i.e., we identify a language in which is not in the set of languages computable by polynomial size CP programs.',
	 'authors': u'Yuping Shen, Xishun Zhao,',
	 'category': u'Computer Science ',
	 'date': '2014-12-1',
	 'pdflink': u'http://arxiv.org/pdf/1412.0320',
	 'subjects': u'Logic in Computer Science (cs.LO)',
	 'title': u'\nCanonical Logic Programs are Succinctly Incomparable with Propositional  Formulas',
	 'urllink': u'http://arxiv.org/abs/1412.0320'}
2015-04-10 06:56:03+0000 [xxu46_10] INFO: Crawled 244 pages (at 1 pages/min), scraped 237 items (at 1 items/min)
2015-04-10 06:56:29+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1412.0315> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 06:56:29+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1412.0315>
	{'abstract': u'Lifted probabilistic inference algorithms have been successfully applied to a large number of symmetric graphical models. Unfortunately, the majority of real-world graphical models is asymmetric. This is even the case for relational representations when evidence is given. Therefore, more recent work in the community moved to making the models symmetric and then applying existing lifted inference algorithms. However, this approach has two shortcomings. First, all existing over-symmetric approximations require a relational representation such as Markov logic networks. Second, the induced symmetries often change the distribution significantly, making the computed probabilities highly biased. We present a framework for probabilistic sampling-based inference that only uses the induced approximate symmetries to propose steps in a Metropolis-Hastings style Markov chain. The framework, therefore, leads to improved probability estimates while remaining unbiased. Experiments demonstrate that the approach outperforms existing MCMC algorithms.',
	 'authors': u'Guy Van den Broeck, Mathias Niepert,',
	 'category': u'Computer Science ',
	 'date': '2014-12-1',
	 'pdflink': u'http://arxiv.org/pdf/1412.0315',
	 'subjects': u'Artificial Intelligence (cs.AI)',
	 'title': u'\nLifted Probabilistic Inference for Asymmetric Graphical Models',
	 'urllink': u'http://arxiv.org/abs/1412.0315'}
2015-04-10 06:57:03+0000 [xxu46_10] INFO: Crawled 245 pages (at 1 pages/min), scraped 238 items (at 1 items/min)
2015-04-10 06:57:48+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1412.0307> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 06:57:48+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1412.0307>
	{'abstract': u'Most experimental studies initialize the population of evolutionary algorithms with random genotypes. In practice, however, optimizers are typically seeded with good candidate solutions either previously known or created according to some problem-specific method. This "seeding" has been studied extensively for single-objective problems. For multi-objective problems, however, very little literature is available on the approaches to seeding and their individual benefits and disadvantages. In this article, we are trying to narrow this gap via a comprehensive computational study on common real-valued test functions. We investigate the effect of two seeding techniques for five algorithms on 48 optimization problems with 2, 3, 4, 6, and 8 objectives. We observe that some functions (e.g., DTLZ4 and the LZ family) benefit significantly from seeding, while others (e.g., WFG) profit less. The advantage of seeding also depends on the examined algorithm.',
	 'authors': u'Tobias Friedrich, Markus Wagner,',
	 'category': u'Computer Science ',
	 'date': '2014-11-30',
	 'pdflink': u'http://arxiv.org/pdf/1412.0307',
	 'subjects': u'Neural and Evolutionary Computing (cs.NE)',
	 'title': u'\nSeeding the Initial Population of Multi-Objective Evolutionary  Algorithms: A Computational Study',
	 'urllink': u'http://arxiv.org/abs/1412.0307'}
2015-04-10 06:58:03+0000 [xxu46_10] INFO: Crawled 246 pages (at 1 pages/min), scraped 239 items (at 1 items/min)
2015-04-10 06:58:59+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1412.0305> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 06:58:59+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1412.0305>
	{'abstract': u'Lifted Reed-Solomon codes are a natural affine-invariant family of error-correcting codes which generalize Reed-Muller codes. They were known to have efficient local-testing and local-decoding algorithms (comparable to the known algorithms for Reed-Muller codes), but with significantly better rate. We give efficient algorithms for list-decoding and local list-decoding of lifted codes. Our algorithms are based on a new technical lemma, which says that codewords of lifted codes are low degree polynomials when viewed as univariate polynomials over a big field (even though they may be very high degree when viewed as multivariate polynomials over a small field).',
	 'authors': u'Alan Guo, Swastik Kopparty,',
	 'category': u'Computer Science ',
	 'date': '2014-11-30',
	 'pdflink': u'http://arxiv.org/pdf/1412.0305',
	 'subjects': u'Information Theory (cs.IT)',
	 'title': u'\nList-decoding algorithms for lifted codes',
	 'urllink': u'http://arxiv.org/abs/1412.0305'}
2015-04-10 06:59:03+0000 [xxu46_10] INFO: Crawled 247 pages (at 1 pages/min), scraped 240 items (at 1 items/min)
2015-04-10 06:59:38+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1412.0301> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 06:59:38+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1412.0301>
	{'abstract': u'In this paper we focus on the mobile sensor coverage problem formulated as a continuous locational optimization problem. Cort `es et al. first proposed a distributed version of the Lloyd descent algorithm with guaranteed convergence to a local optima. Since then researchers have studied a number of variations of the coverage problem. The quality of the final solution with the Lloyd descent depends on the initial sensor configuration. Inspired by the recent results on a related -means problem, in this paper we propose the weighted- sampling to choose the initial sensor configuration and show that it yields -competitive sensor coverage before even applying the Lloyd descent. Through extensive numerical simulations, we show that the initial coverage with the weighted- sampling is significantly lower than that with the uniform random initial sensor configuration. We also show that the average distance traveled by the sensors to reach the final configuration through the Lloyd descent is also significantly lower than that with the uniform random configuration. This also implies considerable savings in the energy spent by the sensors during motion and faster convergence.',
	 'authors': u'Ajay Deshpande,',
	 'category': u'Computer Science ',
	 'date': '2014-11-30',
	 'pdflink': u'http://arxiv.org/pdf/1412.0301',
	 'subjects': u'Systems and Control (cs.SY)',
	 'title': u'\nGuaranteed sensor coverage with the weighted-$D^2$ sampling',
	 'urllink': u'http://arxiv.org/abs/1412.0301'}
2015-04-10 07:00:03+0000 [xxu46_10] INFO: Crawled 248 pages (at 1 pages/min), scraped 241 items (at 1 items/min)
2015-04-10 07:00:44+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1412.0296> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 07:00:44+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1412.0296>
	{'abstract': u"Deep Convolutional Neural Networks (DCNNs) commonly use generic `max-pooling' (MP) layers to extract deformation-invariant features, but we argue in favor of a more refined treatment. First, we introduce epitomic convolution as a building block alternative to the common convolution-MP cascade of DCNNs; while having identical complexity to MP, Epitomic Convolution allows for parameter sharing across different filters, resulting in faster convergence and better generalization. Second, we introduce a Multiple Instance Learning approach to explicitly accommodate global translation and scaling when training a DCNN exclusively with class labels. For this we rely on a `patchwork' data structure that efficiently lays out all image scales and positions as candidates to a DCNN. Factoring global and local deformations allows a DCNN to `focus its resources' on the treatment of non-rigid deformations and yields a substantial classification accuracy improvement. Third, further pursuing this idea, we develop an efficient DCNN sliding window object detector that employs explicit search over position, scale, and aspect ratio. We provide competitive image classification and localization results on the ImageNet dataset and object detection results on the Pascal VOC 2007 benchmark.",
	 'authors': u'George Papandreou, Iasonas Kokkinos, Pierre-Andr\xe9 Savalle,',
	 'category': u'Computer Science ',
	 'date': '2014-11-30',
	 'pdflink': u'http://arxiv.org/pdf/1412.0296',
	 'subjects': u'Computer Vision and Pattern Recognition (cs.CV)',
	 'title': u'\nUntangling Local and Global Deformations in Deep Convolutional Networks  for Image Classification and Sliding Window Detection',
	 'urllink': u'http://arxiv.org/abs/1412.0296'}
2015-04-10 07:01:03+0000 [xxu46_10] INFO: Crawled 249 pages (at 1 pages/min), scraped 242 items (at 1 items/min)
2015-04-10 07:01:57+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1412.0271> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 07:01:57+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1412.0271>
	{'abstract': u'In the stable marriage and roommates problems, a set of agents is given, each of them having a strictly ordered preference list over some or all of the other agents. A matching is a set of disjoint pairs of mutually accepted agents. If any two agents mutually prefer each other to their partner, then they block the matching, otherwise, the matching is said to be stable. In this paper we investigate the complexity of finding a solution satisfying additional constraints on restricted pairs of agents. Restricted pairs can be either forced or forbidden. A stable solution must contain all of the forced pairs, while it must contain none of the forbidden pairs. Dias et al. gave a polynomial-time algorithm to decide whether such a solution exists in the presence of restricted edges. If the answer is no, one might look for a solution close to optimal. Since optimality in this context means that the matching is stable and satisfies all constraints on restricted pairs, there are two ways of relaxing the constraints by permitting a solution to: (1) be blocked by some pairs (as few as possible), or (2) violate some constraints on restricted pairs (again as few as possible). Our main theorems prove that for the (bipartite) stable marriage problem, case (1) leads to NP-hardness and inapproximability results, whilst case (2) can be solved in polynomial time. For the non-bipartite stable roommates instances, case (2) yields an NP-hard but 2-approximable problem. In the case of NP-hard problems, we also discuss polynomially solvable special cases, arising from restrictions on the lengths of the preference lists, or upper bounds on the numbers of restricted pairs.',
	 'authors': u'\xc1gnes Cseh, David F. Manlove,',
	 'category': u'Computer Science ',
	 'date': '2014-11-30',
	 'pdflink': u'http://arxiv.org/pdf/1412.0271',
	 'subjects': u'Discrete Mathematics (cs.DM)',
	 'title': u'\nStable marriage and roommates problems with restricted edges: complexity  and approximability',
	 'urllink': u'http://arxiv.org/abs/1412.0271'}
2015-04-10 07:02:03+0000 [xxu46_10] INFO: Crawled 250 pages (at 1 pages/min), scraped 243 items (at 1 items/min)
2015-04-10 07:02:51+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1412.0265> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 07:02:51+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1412.0265>
	{'abstract': u'In this paper, we develop an approach to exploiting kernel methods with manifold-valued data. In many computer vision problems, the data can be naturally represented as points on a Riemannian manifold. Due to the non-Euclidean geometry of Riemannian manifolds, usual Euclidean computer vision and machine learning algorithms yield inferior results on such data. In this paper, we define Gaussian radial basis function (RBF)-based positive definite kernels on manifolds that permit us to embed a given manifold with a corresponding metric in a high dimensional reproducing kernel Hilbert space. These kernels make it possible to utilize algorithms developed for linear spaces on nonlinear manifold-valued data. Since the Gaussian RBF defined with any given metric is not always positive definite, we present a unified framework for analyzing the positive definiteness of the Gaussian RBF on a generic metric space. We then use the proposed framework to identify positive definite kernels on two specific manifolds commonly encountered in computer vision: the Riemannian manifold of symmetric positive definite matrices and the Grassmann manifold, i.e., the Riemannian manifold of linear subspaces of a Euclidean space. We show that many popular algorithms designed for Euclidean spaces, such as support vector machines, discriminant analysis and principal component analysis can be generalized to Riemannian manifolds with the help of such positive definite Gaussian kernels.',
	 'authors': u'Sadeep Jayasumana, Richard Hartley, Mathieu Salzmann, Hongdong Li, Mehrtash Harandi,',
	 'category': u'Computer Science ',
	 'date': '2014-11-30',
	 'pdflink': u'http://arxiv.org/pdf/1412.0265',
	 'subjects': u'Computer Vision and Pattern Recognition (cs.CV)',
	 'title': u'\nKernel Methods on Riemannian Manifolds with Gaussian RBF Kernels',
	 'urllink': u'http://arxiv.org/abs/1412.0265'}
2015-04-10 07:03:03+0000 [xxu46_10] INFO: Crawled 251 pages (at 1 pages/min), scraped 244 items (at 1 items/min)
2015-04-10 07:03:55+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1412.0260> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 07:03:55+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1412.0260>
	{'abstract': u'Multi-tier cellular communication networks constitute a promising approach to expand the coverage of cellular networks and enable them to offer higher data rates. In this paper, an uplink two-tier communication network is studied, in which macro users, femto users and femto access points are geometrically located inside the coverage area of a macro base station according to Poisson point processes. Each femtocell is assumed to have a fixed backhaul constraint that puts a limit on the maximum number of femto and macro users it can service. Under this backhaul constraint, the network adopts a special open access policy, in which each macro user is either assigned to its closest femto access point or to the macro base station, depending on the ratio between its distances from those two. Under this model, upper and lower bounds on the outage probabilities experienced by users serviced by femto access points are derived as functions of the distance between the macro base station and the femto access point serving them. Similarly, upper and lower bounds on the outage probabilities of the users serviced by the macro base station are obtained. The bounds in both cases are confirmed via simulation results.',
	 'authors': u'Shirin Jalali, Zolfa Zeinalpour-Yazdi, H. Vincent Poor,',
	 'category': u'Computer Science ',
	 'date': '2014-11-30',
	 'pdflink': u'http://arxiv.org/pdf/1412.0260',
	 'subjects': u'Information Theory (cs.IT)',
	 'title': u'\nOutage Performance of Uplink Two-tier Networks Under Backhaul  Constraints',
	 'urllink': u'http://arxiv.org/abs/1412.0260'}
2015-04-10 07:04:03+0000 [xxu46_10] INFO: Crawled 252 pages (at 1 pages/min), scraped 245 items (at 1 items/min)
2015-04-10 07:04:56+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1412.0252> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 07:04:56+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1412.0252>
	{'abstract': u'The Internet of Things (IoT) could enable the development of cloud multiple-input multiple-output (MIMO) systems where internet-enabled devices can work as distributed transmission/reception entities. We expect that spatial multiplexing with distributed reception using cloud MIMO would be a key factor of future wireless communication systems. In this paper, we first review practical receivers for distributed reception of spatially multiplexed transmit data where the fusion center relies on quantized received signals conveyed from geographically separated receive nodes. Using the structures of these receivers, we propose practical channel estimation techniques for the block-fading scenario. The proposed channel estimation techniques rely on very simple operations at the received nodes while achieving near-optimal channel estimation performance as the training length becomes large.',
	 'authors': u'Junil Choi, David J. Love, D. Richard Brown III,',
	 'category': u'Computer Science ',
	 'date': '2014-11-30',
	 'pdflink': u'http://arxiv.org/pdf/1412.0252',
	 'subjects': u'Information Theory (cs.IT)',
	 'title': u'\nChannel Estimation Techniques for Quantized Distributed Reception in  MIMO Systems',
	 'urllink': u'http://arxiv.org/abs/1412.0252'}
2015-04-10 07:05:03+0000 [xxu46_10] INFO: Crawled 253 pages (at 1 pages/min), scraped 246 items (at 1 items/min)
2015-04-10 07:06:03+0000 [xxu46_10] INFO: Crawled 253 pages (at 0 pages/min), scraped 246 items (at 0 items/min)
2015-04-10 07:06:12+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1412.0251> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 07:06:12+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1412.0251>
	{'abstract': u'Blind deconvolution is the problem of recovering a sharp image and a blur kernel from a noisy blurry image. Recently, there has been a significant effort on understanding the basic mechanisms to solve blind deconvolution. While this effort resulted in the deployment of effective algorithms, the theoretical findings generated contrasting views on why these approaches worked. On the one hand, one could observe experimentally that alternating energy minimization algorithms converge to the desired solution. On the other hand, it has been shown that such alternating minimization algorithms should fail to converge and one should instead use a so-called Variational Bayes approach. To clarify this conundrum, recent work showed that a good image and blur prior is instead what makes a blind deconvolution algorithm work. Unfortunately, this analysis did not apply to algorithms based on total variation regularization. In this manuscript, we provide both analysis and experiments to get a clearer picture of blind deconvolution. Our analysis reveals the very reason why an algorithm based on total variation works. We also introduce an implementation of this algorithm and show that, in spite of its extreme simplicity, it is very robust and achieves a performance comparable to the state of the art.',
	 'authors': u'Daniele Perrone, Paolo Favaro,',
	 'category': u'Computer Science ',
	 'date': '2014-11-30',
	 'pdflink': u'http://arxiv.org/pdf/1412.0251',
	 'subjects': u'Computer Vision and Pattern Recognition (cs.CV)',
	 'title': u'\nA Clearer Picture of Blind Deconvolution',
	 'urllink': u'http://arxiv.org/abs/1412.0251'}
2015-04-10 07:07:03+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1412.0233> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 07:07:03+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1412.0233>
	{'abstract': u'We study the connection between the highly non-convex loss function of a simple model of the fully-connected feed-forward neural network and the Hamiltonian of the spherical spin-glass model under the assumptions of: i) variable independence, ii) redundancy in network parametrization, and iii) uniformity. These assumptions enable us to explain the complexity of the fully decoupled neural network through the prism of the results from random matrix theory. We show that for large-size decoupled networks the lowest critical values of the random loss function form a layered structure and they are located in a well-defined band lower-bounded by the global minimum. The number of local minima outside that band diminishes exponentially with the size of the network. We empirically verify that the mathematical model exhibits similar behavior as the computer simulations, despite the presence of high dependencies in real networks. We conjecture that both simulated annealing and SGD converge to the band of low critical points, and that all critical points found there are local minima of high quality measured by the test error. This emphasizes a major difference between large- and small-size networks where for the latter poor quality local minima have non-zero probability of being recovered. Finally, we prove that recovering the global minimum becomes harder as the network size increases and that it is in practice irrelevant as global minimum often leads to overfitting.',
	 'authors': u'Anna Choromanska, Mikael Henaff, Michael Mathieu, G\xe9rard Ben Arous, Yann LeCun,',
	 'category': u'Computer Science ',
	 'date': '2014-11-30',
	 'pdflink': u'http://arxiv.org/pdf/1412.0233',
	 'subjects': u'Learning (cs.LG)',
	 'title': u'\nThe Loss Surfaces of Multilayer Networks',
	 'urllink': u'http://arxiv.org/abs/1412.0233'}
2015-04-10 07:07:03+0000 [xxu46_10] INFO: Crawled 255 pages (at 2 pages/min), scraped 248 items (at 2 items/min)
2015-04-10 07:08:03+0000 [xxu46_10] INFO: Crawled 255 pages (at 0 pages/min), scraped 248 items (at 0 items/min)
2015-04-10 07:08:07+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1412.0223> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 07:08:07+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1412.0223>
	{'abstract': u'With the rapid development of mobile devices and the crowdsourcig platforms, the spatial crowdsourcing has attracted much attention from the database community, specifically, spatial crowdsourcing refers to sending a location-based request to workers according to their positions. In this paper, we consider an important spatial crowdsourcing problem, namely reliable diversity-based spatial crowdsourcing (RDB-SC), in which spatial tasks (such as taking videos/photos of a landmark or firework shows, and checking whether or not parking spaces are available) are time-constrained, and workers are moving towards some directions. Our RDB-SC problem is to assign workers to spatial tasks such that the completion reliability and the spatial/temporal diversities of spatial tasks are maximized. We prove that the RDB-SC problem is NP-hard and intractable. Thus, we propose three effective approximation approaches, including greedy, sampling, and divide-and-conquer algorithms. In order to improve the efficiency, we also design an effective cost-model-based index, which can dynamically maintain moving workers and spatial tasks with low cost, and efficiently facilitate the retrieval of RDB-SC answers. Through extensive experiments, we demonstrate the efficiency and effectiveness of our proposed approaches over both real and synthetic data sets.',
	 'authors': u'Peng Cheng, Xiang Lian, Zhao Chen, Lei Chen, Jinsong Han, Jizhong Zhao,',
	 'category': u'Computer Science ',
	 'date': '2014-11-30',
	 'pdflink': u'http://arxiv.org/pdf/1412.0223',
	 'subjects': u'Databases (cs.DB)',
	 'title': u'\nReliable Diversity-Based Spatial Crowdsourcing by Moving Workers',
	 'urllink': u'http://arxiv.org/abs/1412.0223'}
2015-04-10 07:09:03+0000 [xxu46_10] INFO: Crawled 256 pages (at 1 pages/min), scraped 249 items (at 1 items/min)
2015-04-10 07:09:18+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1412.0218> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 07:09:18+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1412.0218>
	{'abstract': u'Transformations of digital spaces preserving local and global topology play an important role in thinning, skeletonization and simplification of digital images. In the present paper, we introduce and study contractions of simple pair of points based on the notions of a digital contractible space and contractible transformations of digital spaces. We show that the contraction of a simple pair of points preserves local and global topology of a digital space. Relying on the obtained results, we study properties if digital manifolds. In particular, we show that a digital n-manifold can be transformed to its compressed form with the minimal number of points by sequential contractions of simple pairs. Key Words: Graph, digital space, contraction, splitting, simple pair, homotopy, thinning',
	 'authors': u'Alexander V. Evako,',
	 'category': u'Computer Science ',
	 'date': '2014-11-30',
	 'pdflink': u'http://arxiv.org/pdf/1412.0218',
	 'subjects': u'Discrete Mathematics (cs.DM)',
	 'title': u'\nSimple pairs of points in digital spaces. Topology-preserving  transformations of digital spaces by contracting simple pairs of points',
	 'urllink': u'http://arxiv.org/abs/1412.0218'}
2015-04-10 07:10:03+0000 [xxu46_10] INFO: Crawled 257 pages (at 1 pages/min), scraped 250 items (at 1 items/min)
2015-04-10 07:10:35+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1412.0197> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 07:10:35+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1412.0197>
	{'abstract': u'As far as Learning Management System is concerned, it offers an integrated platform for educational materials, distribution and management of learning as well as accessibility by a range of users including teachers, learners and content makers especially for distance learning. Usability evaluation is considered as one approach to assess the efficiency of e-Learning systems. It is used to evaluate how well technology and tools are working for users. There are some factors contributing as major reason why LMS is not used effectively and regularly. Learning Management Systems, as major part of e-Learning systems, can benefit from usability research to evaluate the LMS ease of use and satisfaction among its handlers. Many academic institutions worldwide prefer using their own customized Learning Management Systems; this is the case with Moodle, an open source Learning Management Systems platform designed and operated by most of the universities in Sri Lanka. This paper gives an overview of Learning Management Systems used in Sri Lankan universities, and evaluates its usability using some pre-defined usability standards. In addition it measures the effectiveness of Learning Management System by testing the Learning Management Systems. The findings and result of this study as well as the testing are discussed and presented.',
	 'authors': u'Selvarajah Thuseethan, Sivapalan Achchuthan, Sinnathamby Kuhanesan,',
	 'category': u'Computer Science ',
	 'date': '2014-11-30',
	 'pdflink': u'http://arxiv.org/pdf/1412.0197',
	 'subjects': u'Human-Computer Interaction (cs.HC)',
	 'title': u'\nUsability Evaluation of Learning Management Systems in Sri Lankan  Universities',
	 'urllink': u'http://arxiv.org/abs/1412.0197'}
2015-04-10 07:11:03+0000 [xxu46_10] INFO: Crawled 258 pages (at 1 pages/min), scraped 251 items (at 1 items/min)
2015-04-10 07:11:39+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1412.0193> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 07:11:39+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1412.0193>
	{'abstract': u"The new dual-pivot Quicksort by Vladimir Yaroslavskiy - used in Oracle's Java runtime library since version 7 - features intriguing asymmetries in its behavior. They were shown to cause a basic variant of this algorithm to use less comparisons than classic single-pivot Quicksort implementations. In this paper, we extend the analysis to the case where the two pivots are chosen as fixed order statistics of a random sample. Surprisingly, dual-pivot Quicksort then needs more comparisons than a corresponding version of classic Quicksort, so it is clear that counting comparisons is not sufficient to explain the running time advantages observed for Yaroslavskiy's algorithm in practice. Consequently, we take a more holistic approach in this paper and also give the precise leading term of the average number of swaps, the number of executed Java Bytecode instructions and the number of I/Os in the external-memory model and determine the optimal order statistics for each of these cost measures. It turns out that - unlike for classic Quicksort, where it is optimal to choose the pivot as median of the sample - the asymmetries in Yaroslavskiy's algorithm render pivots with a systematic skew more efficient than the symmetric choice. Moreover, we finally have a convincing explanation for the success of Yaroslavskiy's algorithm in practice: Compared with corresponding versions of classic single-pivot Quicksort, dual-pivot Quicksort needs significantly less I/Os, both with and without pivot sampling.",
	 'authors': u'Sebastian Wild, Markus E. Nebel, Conrado Mart\xednez,',
	 'category': u'Computer Science ',
	 'date': '2014-11-30',
	 'pdflink': u'http://arxiv.org/pdf/1412.0193',
	 'subjects': u'Data Structures and Algorithms (cs.DS)',
	 'title': u'\nAnalysis of Pivot Sampling in Dual-Pivot Quicksort',
	 'urllink': u'http://arxiv.org/abs/1412.0193'}
2015-04-10 07:12:03+0000 [xxu46_10] INFO: Crawled 259 pages (at 1 pages/min), scraped 252 items (at 1 items/min)
2015-04-10 07:12:25+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1412.0165> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 07:12:25+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1412.0165>
	{'abstract': u'D structure recovery from a collection of D images requires the estimation of the camera locations and orientations, i.e. the camera motion. For large, irregular collections of images, existing methods for the location estimation part, which can be formulated as the inverse problem of estimating locations in from noisy measurements of a subset of the pairwise directions , are sensitive to outliers in direction measurements. In this paper, we firstly provide a complete characterization of well-posed instances of the location estimation problem, by presenting its relation to the existing theory of parallel rigidity. For robust estimation of camera locations, we introduce a two-step approach, comprised of a pairwise direction estimation method robust to outliers in point correspondences between image pairs, and a convex program to maintain robustness to outlier directions. In the presence of partially corrupted measurements, we empirically demonstrate that our convex formulation can even recover the locations exactly. Lastly, we demonstrate the utility of our formulations through experiments on Internet photo collections.',
	 'authors': u'Onur Ozyesil, Amit Singer,',
	 'category': u'Computer Science ',
	 'date': '2014-11-29',
	 'pdflink': u'http://arxiv.org/pdf/1412.0165',
	 'subjects': u'Computer Vision and Pattern Recognition (cs.CV)',
	 'title': u'\nRobust Camera Location Estimation by Convex Programming',
	 'urllink': u'http://arxiv.org/abs/1412.0165'}
2015-04-10 07:13:03+0000 [xxu46_10] INFO: Crawled 260 pages (at 1 pages/min), scraped 253 items (at 1 items/min)
2015-04-10 07:13:24+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1412.0156> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 07:13:24+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1412.0156>
	{'abstract': u'We consider the least-squares regression problem and provide a detailed asymptotic analysis of the performance of averaged constant-step-size stochastic gradient descent (a.k.a. least-mean-squares). In the strongly-convex case, we provide an asymptotic expansion up to explicit exponentially decaying terms. Our analysis leads to new insights into stochastic approximation algorithms: (a) it gives a tighter bound on the allowed step-size; (b) the generalization error may be divided into a variance term which is decaying as O(1/n), independently of the step-size , and a bias term that decays as O(1/ 2 n 2); (c) when allowing non-uniform sampling, the choice of a good sampling density depends on whether the variance or bias terms dominate. In particular, when the variance term dominates, optimal sampling densities do not lead to much gain, while when the bias term dominates, we can choose larger step-sizes that leads to significant improvements.',
	 'authors': u'Alexandre D\xe9fossez, Francis Bach,',
	 'category': u'Computer Science ',
	 'date': '2014-11-29',
	 'pdflink': u'http://arxiv.org/pdf/1412.0156',
	 'subjects': u'Learning (cs.LG)',
	 'title': u'\nConstant Step Size Least-Mean-Square: Bias-Variance Trade-offs and  Optimal Sampling Distributions',
	 'urllink': u'http://arxiv.org/abs/1412.0156'}
2015-04-10 07:14:03+0000 [xxu46_10] INFO: Crawled 261 pages (at 1 pages/min), scraped 254 items (at 1 items/min)
2015-04-10 07:14:36+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1412.0152> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 07:14:36+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1412.0152>
	{'abstract': u"Web Service is one of the most important information sharing technologies on the web and one of the example of service oriented processing. To guarantee accurate execution of web services operations, they must be accountable with regulations of the social networks in which they sign up. This operations implement using controls called 'Commitment'. This paper studies commitments, then has an overview on existing researches, web service execution method using commitments and information sharing methods between web services based on commitments and social networks. A key challenge in this technique is consistency ensuring in execution time. The aim of this study is presenting an algorithm for consistency ensuring between commitments. An application designed for proving correctness of algorithm.",
	 'authors': u'Marzieh Adelnia, Mohammad Reza Khayyambashi,',
	 'category': u'Computer Science ',
	 'date': '2014-11-29',
	 'pdflink': u'http://arxiv.org/pdf/1412.0152',
	 'subjects': u'Social and Information Networks (cs.SI)',
	 'title': u'\nConsistency of Commitments in Social Web Services',
	 'urllink': u'http://arxiv.org/abs/1412.0152'}
2015-04-10 07:15:03+0000 [xxu46_10] INFO: Crawled 262 pages (at 1 pages/min), scraped 255 items (at 1 items/min)
2015-04-10 07:15:16+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1412.0143> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 07:15:16+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1412.0143>
	{'abstract': u'Using digital topology approach, we construct digital models of closed surfaces as the intersection graphs of LCL covers of the surfaces. It is proved that digital models of closed surfaces are digital 2-dimensional surfaces preserving the geometry and topology of their continuous counterparts. In the framework of the proposed models, we show that for any closed surface there exists a compressed model of this surface with the minimal number of points. Key words: Closed Surface; Digital space; Cover; Graph; Digital model; Medical imaging;',
	 'authors': u'Alexander V. Evako,',
	 'category': u'Computer Science ',
	 'date': '2014-11-29',
	 'pdflink': u'http://arxiv.org/pdf/1412.0143',
	 'subjects': u'Computational Geometry (cs.CG)',
	 'title': u'\nTopology preserving representations of compact 2D manifolds by digital  2-surfaces. Compressed digital models and digital weights of compact 2D  manifolds. Classification of closed surfaces by digital tools',
	 'urllink': u'http://arxiv.org/abs/1412.0143'}
2015-04-10 07:16:03+0000 [xxu46_10] INFO: Crawled 263 pages (at 1 pages/min), scraped 256 items (at 1 items/min)
2015-04-10 07:16:26+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1412.0134> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 07:16:26+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1412.0134>
	{'abstract': u'This paper presents the classification of digital n-manifolds based on the notion of complexity and homotopy equivalence. We introduce compressed n-manifolds and study their properties. We show that any n-manifold with p points is homotopy equivalent to a compressed n-manifold with m points, m&lt;p. We design an algorithm for the classification of digital n-manifolds of any dimension n.',
	 'authors': u'Alexander V. Evako,',
	 'category': u'Computer Science ',
	 'date': '2014-11-29',
	 'pdflink': u'http://arxiv.org/pdf/1412.0134',
	 'subjects': u'Discrete Mathematics (cs.DM)',
	 'title': u'\nClassification of digital n-manifolds',
	 'urllink': u'http://arxiv.org/abs/1412.0134'}
2015-04-10 07:17:03+0000 [xxu46_10] INFO: Crawled 264 pages (at 1 pages/min), scraped 257 items (at 1 items/min)
2015-04-10 07:17:17+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1412.0131> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 07:17:17+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1412.0131>
	{'abstract': u'Authors compare different ways of selecting change agents within network analysis paradigm and propose a new algorithm of doing so. All methods are evaluated against network coverage measure that calculates how many network members can be directly reached by selected nodes. Results from the analysis of organizational network show that compared to other methods the proposed algorithm provides better network coverage, at the same time selecting change agents that are well connected, influential opinion leaders.',
	 'authors': u'Blazej Zak, Anita Zbieg,',
	 'category': u'Computer Science ',
	 'date': '2014-11-29',
	 'pdflink': u'http://arxiv.org/pdf/1412.0131',
	 'subjects': u'Social and Information Networks (cs.SI)',
	 'title': u'\nHeuristic for Network Coverage Optimization Applied in Finding  Organizational Change Agents',
	 'urllink': u'http://arxiv.org/abs/1412.0131'}
2015-04-10 07:18:03+0000 [xxu46_10] INFO: Crawled 265 pages (at 1 pages/min), scraped 258 items (at 1 items/min)
2015-04-10 07:18:20+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1412.0129> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 07:18:20+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1412.0129>
	{'abstract': u'We consider a sequential inspection game where an inspector uses a limited number of inspections over a larger number of time periods to detect a violation (an illegal act) of an inspectee. Compared with earlier models, we allow varying rewards to the inspectee for successful violations. As one possible example, the most valuable reward may be the completion of a sequence of thefts of nuclear material needed to build a nuclear bomb. The inspectee can observe the inspector, but the inspector can only determine if a violation happens during a stage where he inspects, which terminates the game; otherwise the game continues. Under reasonable assumptions for the payoffs, the inspector\'s strategy is independent of the number of successful violations. This allows to apply a recursive description of the game, even though this normally assumes fully informed players after each stage. The resulting recursive equation in three variables for the equilibrium payoff of the game, which generalizes several other known equations of this kind, is solved explicitly in terms of sums of binomial coefficients. We also extend this approach to non-zero-sum games and, similar to Maschler (1966), "inspector leadership" where the inspector commits to (the same) randomized inspection schedule, but the inspectee acts legally (rather than mixes as in the simultaneous game) as long as inspections remain.',
	 'authors': u'Bernhard von Stengel,',
	 'category': u'Computer Science ',
	 'date': '2014-11-29',
	 'pdflink': u'http://arxiv.org/pdf/1412.0129',
	 'subjects': u'Computer Science and Game Theory (cs.GT)',
	 'title': u'\nRecursive Inspection Games',
	 'urllink': u'http://arxiv.org/abs/1412.0129'}
2015-04-10 07:19:03+0000 [xxu46_10] INFO: Crawled 266 pages (at 1 pages/min), scraped 259 items (at 1 items/min)
2015-04-10 07:19:38+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1412.0111> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 07:19:38+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1412.0111>
	{'abstract': u'This paper deals with color image quality assessment in the reduced-reference framework based on natural scenes statistics. In this context, we propose to model the statistics of the steerable pyramid coefficients by a Multivariate Generalized Gaussian distribution (MGGD). This model allows taking into account the high correlation between the components of the RGB color space. For each selected scale and orientation, we extract a parameter matrix from the three color components subbands. In order to quantify the visual degradation, we use a closed-form of Kullback-Leibler Divergence (KLD) between two MGGDs. Using "TID 2008" benchmark, the proposed measure has been compared with the most influential methods according to the FRTV1 VQEG framework. Results demonstrates its effectiveness for a great variety of distortion type. Among other benefits this measure uses only very little information about the original image.',
	 'authors': u'Mounir Omari, Abdelkaher Ait Abdelouahad, Mohammed El Hassouni, Hocine Cherifi,',
	 'category': u'Computer Science ',
	 'date': '2014-11-29',
	 'pdflink': u'http://arxiv.org/pdf/1412.0111',
	 'subjects': u'Computer Vision and Pattern Recognition (cs.CV)',
	 'title': u'\nColor image quality assessment measure using multivariate generalized  Gaussian distribution',
	 'urllink': u'http://arxiv.org/abs/1412.0111'}
2015-04-10 07:20:03+0000 [xxu46_10] INFO: Crawled 267 pages (at 1 pages/min), scraped 260 items (at 1 items/min)
2015-04-10 07:20:45+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1412.0103> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 07:20:45+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1412.0103>
	{'abstract': u'Network size is a fundamental statistic for a peer-to-peer system but is generally considered to contain too little information to be useful. However, most existing work only considers the metric by itself and does not explore what features could be extracted from this seem- ingly trivial metric. In this paper, we show that Fourier transform allows us to extract frequency features from such time series data, which can further be used to characterize user behaviors and detect system anoma- lies in a peer-to-peer system automatically without needing to resort to visual comparisons. By using the proposed algorithm, our system suc- cessfully discovers and clusters countries of similar user behavior and captures the anomalies like Sybil attacks and other real-world events with high accuracy. Our work in this paper highlights the usefulness of more advanced time series processing techniques in analyzing network measurements.',
	 'authors': u'Liang Wang, Jussi Kangasharju,',
	 'category': u'Computer Science ',
	 'date': '2014-11-29',
	 'pdflink': u'http://arxiv.org/pdf/1412.0103',
	 'subjects': u'Networking and Internet Architecture (cs.NI)',
	 'title': u'\nInference on the Network Evolution in BitTorrent Mainline DHT',
	 'urllink': u'http://arxiv.org/abs/1412.0103'}
2015-04-10 07:21:03+0000 [xxu46_10] INFO: Crawled 268 pages (at 1 pages/min), scraped 261 items (at 1 items/min)
2015-04-10 07:22:01+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1412.0100> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 07:22:01+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1412.0100>
	{'abstract': u'State-of-the-art visual recognition and detection systems increasingly rely on large amounts of training data and complex classifiers. Therefore it becomes increasingly expensive both to manually annotate datasets and to keep running times at levels acceptable for practical applications. In this paper, we propose two solutions to address these issues. First, we introduce a weakly supervised, segmentation-based approach to learn accurate detectors and image classifiers from weak supervisory signals that provide only approximate constraints on target localization. We illustrate our system on the problem of action detection in static images (Pascal VOC Actions 2012), using human visual search patterns as our training signal. Second, inspired from the saccade-and-fixate operating principle of the human visual system, we use reinforcement learning techniques to train efficient search models for detection. Our sequential method is weakly supervised and general (it does not require eye movements), finds optimal search strategies for any given detection confidence function and achieves performance similar to exhaustive sliding window search at a fraction of its computational cost.',
	 'authors': u'Stefan Mathe, Cristian Sminchisescu,',
	 'category': u'Computer Science ',
	 'date': '2014-11-29',
	 'pdflink': u'http://arxiv.org/pdf/1412.0100',
	 'subjects': u'Computer Vision and Pattern Recognition (cs.CV)',
	 'title': u'\nMultiple Instance Reinforcement Learning for Efficient Weakly-Supervised  Detection in Images',
	 'urllink': u'http://arxiv.org/abs/1412.0100'}
2015-04-10 07:22:03+0000 [xxu46_10] INFO: Crawled 269 pages (at 1 pages/min), scraped 262 items (at 1 items/min)
2015-04-10 07:23:03+0000 [xxu46_10] INFO: Crawled 269 pages (at 0 pages/min), scraped 262 items (at 0 items/min)
2015-04-10 07:23:09+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1412.0073> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 07:23:09+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1412.0073>
	{'abstract': u'Counting the number of independent sets for a bipartite graph (#BIS) plays a crucial role in the study of approximate counting. It has been conjectured that there is no fully polynomial-time (randomized) approximation scheme (FPTAS/FPRAS) for #BIS, and it was proved that the problem for instances with a maximum degree of is already as hard as the general problem. In this paper, we obtain a surprising tractability result for a family of #BIS instances. We design a very simple deterministic fully polynomial-time approximation scheme (FPTAS) for #BIS when the maximum degree for one side is no larger than . There is no restriction for the degrees on the other side, which do not even have to be bounded by a constant. Previously, FPTAS was only known for instances with a maximum degree of for both sides.',
	 'authors': u'Jingcheng Liu, Pinyan Lu,',
	 'category': u'Computer Science ',
	 'date': '2014-11-29',
	 'pdflink': u'http://arxiv.org/pdf/1412.0073',
	 'subjects': u'Data Structures and Algorithms (cs.DS)',
	 'title': u'\nFPTAS for #BIS with Degree Bounds on One Side',
	 'urllink': u'http://arxiv.org/abs/1412.0073'}
2015-04-10 07:24:03+0000 [xxu46_10] INFO: Crawled 270 pages (at 1 pages/min), scraped 263 items (at 1 items/min)
2015-04-10 07:24:13+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1412.0069> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 07:24:13+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1412.0069>
	{'abstract': u"Deep learning methods have achieved great success in pedestrian detection, owing to its ability to learn features from raw pixels. However, they mainly capture middle-level representations, such as pose of pedestrian, but confuse positive with hard negative samples, which have large ambiguity, e.g. the shape and appearance of `tree trunk' or `wire pole' are similar to pedestrian in certain viewpoint. This ambiguity can be distinguished by high-level representation. To this end, this work jointly optimizes pedestrian detection with semantic tasks, including pedestrian attributes (e.g. `carrying backpack') and scene attributes (e.g. `road', `tree', and `horizontal'). Rather than expensively annotating scene attributes, we transfer attributes information from existing scene segmentation datasets to the pedestrian dataset, by proposing a novel deep model to learn high-level features from multiple tasks and multiple data sources. Since distinct tasks have distinct convergence rates and data from different datasets have different distributions, a multi-task objective function is carefully designed to coordinate tasks and reduce discrepancies among datasets. The importance coefficients of tasks and network parameters in this objective function can be iteratively estimated. Extensive evaluations show that the proposed approach outperforms the state-of-the-art on the challenging Caltech and ETH datasets, where it reduces the miss rates of previous deep models by 17 and 5.5 percent, respectively.",
	 'authors': u'Yonglong Tian, Ping Luo, Xiaogang Wang, Xiaoou Tang,',
	 'category': u'Computer Science ',
	 'date': '2014-11-29',
	 'pdflink': u'http://arxiv.org/pdf/1412.0069',
	 'subjects': u'Computer Vision and Pattern Recognition (cs.CV)',
	 'title': u'\nPedestrian Detection aided by Deep Learning Semantic Tasks',
	 'urllink': u'http://arxiv.org/abs/1412.0069'}
2015-04-10 07:24:55+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1412.0065> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 07:24:55+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1412.0065>
	{'abstract': u'We focus on the task of everyday hand pose estimation from egocentric viewpoints. For this task, we show that depth sensors are particularly informative for extracting near-field interactions of the camera wearer with his/her environment. Despite the recent advances in full-body pose estimation using Kinect-like sensors, reliable monocular hand pose estimation in RGB-D images is still an unsolved problem. The problem is considerably exacerbated when analyzing hands performing daily activities from a first-person viewpoint, due to severe occlusions arising from object manipulations and a limited field-of-view. Our system addresses these difficulties by exploiting strong priors over viewpoint and pose in a discriminative tracking-by-detection framework. Our priors are operationalized through a photorealistic synthetic model of egocentric scenes, which is used to generate training data for learning depth-based pose classifiers. We evaluate our approach on an annotated dataset of real egocentric object manipulation scenes and compare to both commercial and academic approaches. Our method provides state-of-the-art performance for both hand detection and pose estimation in egocentric RGB-D images.',
	 'authors': u'Gregory Rogez, James S. Supancic III, Maryam Khademi, Jose Maria Martinez Montiel, Deva Ramanan,',
	 'category': u'Computer Science ',
	 'date': '2014-11-29',
	 'pdflink': u'http://arxiv.org/pdf/1412.0065',
	 'subjects': u'Computer Vision and Pattern Recognition (cs.CV)',
	 'title': u'\n3D Hand Pose Detection in Egocentric RGB-D Images',
	 'urllink': u'http://arxiv.org/abs/1412.0065'}
2015-04-10 07:25:03+0000 [xxu46_10] INFO: Crawled 272 pages (at 2 pages/min), scraped 265 items (at 2 items/min)
2015-04-10 07:25:35+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1412.0062> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 07:25:35+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1412.0062>
	{'abstract': u'A Bayesian framework for 3D human pose estimation from monocular images based on sparse representation (SR) is introduced. Our probabilistic approach aims at simultaneously learning two overcomplete dictionaries (one for the visual input space and the other for the pose space) with a shared sparse representation. Existing SR-based pose estimation approaches only offer a point estimation of the dictionary and the sparse codes. Therefore, they might be unreliable when the number of training examples is small. Our Bayesian framework estimates a posterior distribution for the sparse codes and the dictionaries from labeled training data. Hence, it is robust to overfitting on small-size training data. Experimental results on various human activities show that the proposed method is superior to the state of-the-art pose estimation algorithms.',
	 'authors': u'Behnam Babagholami-Mohamadabadi, Amin Jourabloo, Ali Zarghami, Shohreh Kasaei,',
	 'category': u'Computer Science ',
	 'date': '2014-11-29',
	 'pdflink': u'http://arxiv.org/pdf/1412.0062',
	 'subjects': u'Computer Vision and Pattern Recognition (cs.CV)',
	 'title': u'\nA Bayesian Framework for Sparse Representation-Based 3D Human Pose  Estimation',
	 'urllink': u'http://arxiv.org/abs/1412.0062'}
2015-04-10 07:26:03+0000 [xxu46_10] INFO: Crawled 273 pages (at 1 pages/min), scraped 266 items (at 1 items/min)
2015-04-10 07:26:52+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1412.0060> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 07:26:52+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1412.0060>
	{'abstract': u"We tackle the problem of estimating the 3D pose of an individual's upper limbs (arms+hands) from a chest mounted depth-camera. Importantly, we consider pose estimation during everyday interactions with objects. Past work shows that strong pose+viewpoint priors and depth-based features are crucial for robust performance. In egocentric views, hands and arms are observable within a well defined volume in front of the camera. We call this volume an egocentric workspace. A notable property is that hand appearance correlates with workspace location. To exploit this correlation, we classify arm+hand configurations in a global egocentric coordinate frame, rather than a local scanning window. This greatly simplify the architecture and improves performance. We propose an efficient pipeline which 1) generates synthetic workspace exemplars for training using a virtual chest-mounted camera whose intrinsic parameters match our physical camera, 2) computes perspective-aware depth features on this entire volume and 3) recognizes discrete arm+hand pose classes through a sparse multi-class SVM. Our method provides state-of-the-art hand pose recognition performance from egocentric RGB-D images in real-time.",
	 'authors': u'Gregory Rogez, James S. Supancic III, Deva Ramanan,',
	 'category': u'Computer Science ',
	 'date': '2014-11-29',
	 'pdflink': u'http://arxiv.org/pdf/1412.0060',
	 'subjects': u'Computer Vision and Pattern Recognition (cs.CV)',
	 'title': u'\nEgocentric Pose Recognition in Four Lines of Code',
	 'urllink': u'http://arxiv.org/abs/1412.0060'}
2015-04-10 07:27:03+0000 [xxu46_10] INFO: Crawled 274 pages (at 1 pages/min), scraped 267 items (at 1 items/min)
2015-04-10 07:27:47+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1412.0057> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 07:27:47+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1412.0057>
	{'abstract': u'This work investigates two physics-based models that simulate the non-linear partial differential algebraic equations describing an electric double layer supercapacitor. In one model the linear dependence between electrolyte concentration and conductivity is accounted for, while in the other model it is not. A spectral element method is used to discretise the model equations and it is found that the error convergence rate with respect to the number of elements is faster compared to a finite difference method. The increased accuracy of the spectral element approach means that, for a similar level of solution accuracy, the model simulation computing time is approximately 50% of that of the finite difference method. This suggests that the spectral element model could be used for control and state estimation purposes. For a typical supercapacitor charging profile, the numerical solutions from both models closely match experimental voltage and current data. However, when the electrolyte is dilute or where there is a long charging time, a noticeable difference between the numerical solutions of the two models is observed. Electrical impedance spectroscopy simulations show that the capacitance of the two models rapidly decreases when the frequency of the perturbation current exceeds an upper threshold.',
	 'authors': u'Ross Drummond, David A. Howey, Stephen R. Duncan,',
	 'category': u'Computer Science ',
	 'date': '2014-11-29',
	 'pdflink': u'http://arxiv.org/pdf/1412.0057',
	 'subjects': u'Systems and Control (cs.SY)',
	 'title': u'\nLow-Order Mathematical Modelling of Electric Double Layer  Supercapacitors Using Spectral Methods',
	 'urllink': u'http://arxiv.org/abs/1412.0057'}
2015-04-10 07:28:03+0000 [xxu46_10] INFO: Crawled 275 pages (at 1 pages/min), scraped 268 items (at 1 items/min)
2015-04-10 07:29:03+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1412.0056> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 07:29:03+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1412.0056>
	{'abstract': u'It is typically expected that if a mechanism is truthful, then the agents would, indeed, truthfully report their private information. But why would an agent believe that the mechanism is truthful? We wish to design truthful mechanisms, whose truthfulness can be verified efficiently (in the computational sense). Our approach involves three steps: (i) specifying the structure of mechanisms, (ii) constructing a verification algorithm, and (iii) measuring the quality of verifiably truthful mechanisms. We demonstrate this approach using a case study: approximate mechanism design without money for facility location.',
	 'authors': u'Simina Br\xe2nzei, Ariel D. Procaccia,',
	 'category': u'Computer Science ',
	 'date': '2014-11-29',
	 'pdflink': u'http://arxiv.org/pdf/1412.0056',
	 'subjects': u'Computer Science and Game Theory (cs.GT)',
	 'title': u'\nVerifiably Truthful Mechanisms',
	 'urllink': u'http://arxiv.org/abs/1412.0056'}
2015-04-10 07:29:03+0000 [xxu46_10] INFO: Crawled 276 pages (at 1 pages/min), scraped 269 items (at 1 items/min)
2015-04-10 07:30:03+0000 [xxu46_10] INFO: Crawled 276 pages (at 0 pages/min), scraped 269 items (at 0 items/min)
2015-04-10 07:30:15+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1412.0055> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 07:30:15+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1412.0055>
	{'abstract': u'To perform cooperative tasks in a decentralized manner, multi-robot systems are often required to communicate with each other. Therefore, maintaining the communication graph connectivity is a fundamental issue when roaming a territory with obstacles. However, when dealing with real-robot systems, several sources of data corruption can appear in the agent interaction. In this paper, the effects of failure and noise in the communication between agents are analyzed upon a connectivity maintenance control strategy. The results show that the connectivity strategy is resilient to the negative effects of such disturbances under realistic settings that consider a bandwidth limit for the control effort. This opens the perspective of applying the connectivity maintenance strategy in adaptive schemes that consider, for instance, autonomous adaptation to constraints other than connectivity itself, e.g. communication efficiency and energy harvesting.',
	 'authors': u'Vin\xedcius A. Battagello, Carlos H. C. Ribeiro,',
	 'category': u'Computer Science ',
	 'date': '2014-11-29',
	 'pdflink': u'http://arxiv.org/pdf/1412.0055',
	 'subjects': u'Multiagent Systems (cs.MA)',
	 'title': u'\nAnalysis of the Effects of Failure and Noise in the Distributed  Connectivity Maintenance of a Multi-robot System',
	 'urllink': u'http://arxiv.org/abs/1412.0055'}
2015-04-10 07:31:03+0000 [xxu46_10] INFO: Crawled 277 pages (at 1 pages/min), scraped 270 items (at 1 items/min)
2015-04-10 07:31:14+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1412.0041> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 07:31:14+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1412.0041>
	{'abstract': u'Information-centric networking extensively uses universal in-network caching. However, developing an efficient and fair collaborative caching algorithm for selfish caches is still an open question. In addition, the communication overhead induced by collaboration is especially poorly understood in a general network setting such as realistic ISP and Autonomous System networks. In this paper, we address these two problems by modeling the in-network caching problem as a Nash bargaining game. We show that the game is a convex optimization problem and further derive the corresponding distributed algorithm. We analytically investigate the collaboration overhead on general graph topologies, and theoretically show that collaboration has to be constrained within a small neighborhood due to its cost growing exponentially. Our proposed algorithm achieves at least 16% performance gain over its competitors on different network topologies in the evaluation, and guarantees provable convergence, Pareto efficiency and proportional fairness.',
	 'authors': u'Liang Wang, Jussi Kangasharju,',
	 'category': u'Computer Science ',
	 'date': '2014-11-28',
	 'pdflink': u'http://arxiv.org/pdf/1412.0041',
	 'subjects': u'Networking and Internet Architecture (cs.NI)',
	 'title': u'\nFair Collaborative In-Network Caching Game and Its Cost Analysis on  General Network Topologies',
	 'urllink': u'http://arxiv.org/abs/1412.0041'}
2015-04-10 07:32:03+0000 [xxu46_10] INFO: Crawled 278 pages (at 1 pages/min), scraped 271 items (at 1 items/min)
2015-04-10 07:32:29+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1412.0036> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 07:32:29+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1412.0036>
	{'abstract': u'The maximum volume -simplex problem asks to compute the -dimensional simplex of maximum volume inside the convex hull of a given set of points in . We give a deterministic approximation algorithm for this problem which achieves an approximation ratio of . The problem is known to be -hard to approximate within a factor of for some constant . Our algorithm also approximates the problem of finding the largest determinant principal submatrix of a rank positive semidefinite matrix, with approximation ratio . We achieve our approximation by rounding solutions to a generlization of the -optimal design problem, or, equivalently, the dual of an appropriate smallest enclosing ellipsoid probelm. Our arguments give a short and simple proof of a restricted invertibility principle for determinants.',
	 'authors': u'Aleksandar Nikolov,',
	 'category': u'Computer Science ',
	 'date': '2014-11-28',
	 'pdflink': u'http://arxiv.org/pdf/1412.0036',
	 'subjects': u'Computational Geometry (cs.CG)',
	 'title': u'\nRandomized Rounding for the Largest $j$-Simplex Problem',
	 'urllink': u'http://arxiv.org/abs/1412.0036'}
2015-04-10 07:33:03+0000 [xxu46_10] INFO: Crawled 279 pages (at 1 pages/min), scraped 272 items (at 1 items/min)
2015-04-10 07:33:29+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1412.0035> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 07:33:29+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1412.0035>
	{'abstract': u'Image representations, from SIFT and Bag of Visual Words to Convolutional Neural Networks (CNNs), are a crucial component of almost any image understanding system. Nevertheless, our understanding of them remains limited. In this paper we conduct a direct analysis of the visual information contained in representations by asking the following question: given an encoding of an image, to which extent is it possible to reconstruct the image itself? To answer this question we contribute a general framework to invert representations. We show that this method can invert representations such as HOG and SIFT more accurately than recent alternatives while being applicable to CNNs too. We then use this technique to study the inverse of recent state-of-the-art CNN image representations for the first time. Among our findings, we show that several layers in CNNs retain photographically accurate information about the image, with different degrees of geometric and photometric invariance.',
	 'authors': u'Aravindh Mahendran, Andrea Vedaldi,',
	 'category': u'Computer Science ',
	 'date': '2014-11-26',
	 'pdflink': u'http://arxiv.org/pdf/1412.0035',
	 'subjects': u'Computer Vision and Pattern Recognition (cs.CV)',
	 'title': u'\nUnderstanding Deep Image Representations by Inverting Them',
	 'urllink': u'http://arxiv.org/abs/1412.0035'}
2015-04-10 07:34:03+0000 [xxu46_10] INFO: Crawled 280 pages (at 1 pages/min), scraped 273 items (at 1 items/min)
2015-04-10 07:34:48+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1412.0034> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 07:34:48+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1412.0034>
	{'abstract': u'We investigate the length of a shortest preset distinguishing sequence (PDS) in the worst case for a -element subset of an -state Mealy automaton. It was mentioned by Sokolovskii that this problem is closely related to the problem of finding the maximal subsemigroup diameter for the full transformation semigroup of an -element set. We prove that as and, using approach of Sokolovskii, find the asymptotics of as and .',
	 'authors': u'Pavel Panteleev,',
	 'category': u'Computer Science ',
	 'date': '2014-11-28',
	 'pdflink': u'http://arxiv.org/pdf/1412.0034',
	 'subjects': u'Formal Languages and Automata Theory (cs.FL)',
	 'title': u'\nPreset Distinguishing Sequences and Diameter of Transformation  Semigroups',
	 'urllink': u'http://arxiv.org/abs/1412.0034'}
2015-04-10 07:35:03+0000 [xxu46_10] INFO: Crawled 281 pages (at 1 pages/min), scraped 274 items (at 1 items/min)
2015-04-10 07:36:03+0000 [xxu46_10] INFO: Crawled 281 pages (at 0 pages/min), scraped 274 items (at 0 items/min)
2015-04-10 07:36:06+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1412.0008> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 07:36:06+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1412.0008>
	{'abstract': u"We live and work in environments that are inundated with cameras embedded in devices such as phones, tablets, laptops, and monitors. Newer wearable devices like Google Glass, Narrative Clip, and Autographer offer the ability to quietly log our lives with cameras from a `first person' perspective. While capturing several meaningful and interesting moments, a significant number of images captured by these wearable cameras can contain computer screens. Given the potentially sensitive information that is visible on our displays, there is a need to guard computer screens from undesired photography. People need protection against photography of their screens, whether by other people's cameras or their own cameras. We present ScreenAvoider, a framework that controls the collection and disclosure of images with computer screens and their sensitive content. ScreenAvoider can detect images with computer screens with high accuracy and can even go so far as to discriminate amongst screen content. We also introduce a ScreenTag system that aids in the identification of screen content, flagging images with highly sensitive content such as messaging applications or email webpages. We evaluate our concept on realistic lifelogging datasets, showing that ScreenAvoider provides a practical and useful solution that can help users manage their privacy.",
	 'authors': u'Mohammed Korayem, Robert Templeman, Dennis Chen, David Crandall, Apu Kapadia,',
	 'category': u'Computer Science ',
	 'date': '2014-11-28',
	 'pdflink': u'http://arxiv.org/pdf/1412.0008',
	 'subjects': u'Cryptography and Security (cs.CR)',
	 'title': u'\nScreenAvoider: Protecting Computer Screens from Ubiquitous Cameras',
	 'urllink': u'http://arxiv.org/abs/1412.0008'}
2015-04-10 07:37:03+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1412.0007> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 07:37:03+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1412.0007>
	{'abstract': u'The paradigm shift in collagen research during the early 1970s marked by the discovery of the collagen precursor molecule procollagen was traced using co-citation analysis and title word frequency determination, confirming previous work performed by Henry Small.',
	 'authors': u'Johannes Stegmann,',
	 'category': u'Computer Science ',
	 'date': '2014-11-27',
	 'pdflink': u'http://arxiv.org/pdf/1412.0007',
	 'subjects': u'Digital Libraries (cs.DL)',
	 'title': u'\nParadigm shifts. Part I. Collagen. Confirming and complementing the work  of Henry Small',
	 'urllink': u'http://arxiv.org/abs/1412.0007'}
2015-04-10 07:37:03+0000 [xxu46_10] INFO: Crawled 283 pages (at 2 pages/min), scraped 276 items (at 2 items/min)
2015-04-10 07:38:03+0000 [xxu46_10] INFO: Crawled 283 pages (at 0 pages/min), scraped 276 items (at 0 items/min)
2015-04-10 07:38:17+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1412.0003> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 07:38:17+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1412.0003>
	{'abstract': u'Comparing two images in a view-invariant way has been a challenging problem in computer vision for a long time, as visual features are not stable under large view point changes. In this paper, given a single input image of an object, we synthesize new features for other views of the same object. To accomplish this, we introduce an aligned set of 3D models in the same class as the input object image. Each 3D model is represented by a set of views, and we study the correlation of image patches between different views, seeking what we call surrogates --- patches in one view whose feature content predicts well the features of a patch in another view. In particular, for each patch in the novel desired view, we seek surrogates from the observed view of the given image. For a given surrogate, we predict that surrogate using linear combination of the corresponding patches of the 3D model views, learn the coefficients, and then transfer these coefficients on a per patch basis to synthesize the features of the patch in the novel view. In this way we can create feature sets for all views of the latent object, providing us a multi-view representation of the object. View-invariant object comparisons are achieved simply by computing the distances between the features of corresponding views. We provide theoretical and empirical analysis of the feature synthesis process, and evaluate the proposed view-agnostic distance (VAD) in fine-grained image retrieval (100 object classes) and classification tasks. Experimental results show that our synthesized features do enable view-independent comparison between images and perform significantly better than traditional image features in this respect.',
	 'authors': u'Hao Su, Fan Wang, Li Yi, Leonidas Guibas,',
	 'category': u'Computer Science ',
	 'date': '2014-11-26',
	 'pdflink': u'http://arxiv.org/pdf/1412.0003',
	 'subjects': u'Computer Vision and Pattern Recognition (cs.CV)',
	 'title': u'\n3D-Assisted Image Feature Synthesis for Novel Views of an Object',
	 'urllink': u'http://arxiv.org/abs/1412.0003'}
2015-04-10 07:39:03+0000 [xxu46_10] INFO: Crawled 284 pages (at 1 pages/min), scraped 277 items (at 1 items/min)
2015-04-10 07:39:23+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1411.7924> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 07:39:23+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1411.7924>
	{'abstract': u'We review a method for click-through rate prediction based on the work of Menon et al. [11], which combines collaborative filtering and matrix factorization with a side-information model and fuses the outputs to proper probabilities in [0,1]. In addition we provide details, both for the modeling as well as the experimental part, that are not found elsewhere. We rigorously test the performance on several test data sets from consecutive days in a click-through rate prediction setup, in a manner which reflects a real-world pipeline. Our results confirm that performance can be increased using latent features, albeit the differences in the measures are small but significant.',
	 'authors': u'Bjarne \xd8rum Fruergaard,',
	 'category': u'Computer Science ',
	 'date': '2014-11-28',
	 'pdflink': u'http://arxiv.org/pdf/1411.7924',
	 'subjects': u'Machine Learning (stat.ML)',
	 'title': u'\nPredicting clicks in online display advertising with latent features and  side-information',
	 'urllink': u'http://arxiv.org/abs/1411.7924'}
2015-04-10 07:40:03+0000 [xxu46_10] INFO: Crawled 285 pages (at 1 pages/min), scraped 278 items (at 1 items/min)
2015-04-10 07:40:14+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1411.7920> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 07:40:14+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1411.7920>
	{'abstract': u'Within the Kolmogorov theory of probability, Bayes\' rule allows one to perform statistical inference by relating conditional probabilities to unconditional probabilities. As we show here, however, there is a continuous set of alternative inference rules that yield the same results, and that may have computational or practical advantages for certain problems. We formulate generalized axioms for probability theory, according to which the reverse conditional probability distribution P(B|A) is not specified by the forward conditional probability distribution P(A|B) and the marginals P(A) and P(B). Thus, in order to perform statistical inference, one must specify an additional "inference axiom," which relates P(B|A) to P(A|B), P(A), and P(B). We show that when Bayes\' rule is chosen as the inference axiom, the axioms are equivalent to the classical Kolmogorov axioms. We then derive consistency conditions on the inference axiom, and thereby characterize the set of all possible rules for inference. The set of "first-order" inference axioms, defined as the set of axioms in which P(B|A) depends on the first power of P(A|B), is found to be a 1-simplex, with Bayes\' rule at one of the extreme points. The other extreme point, the "inversion rule," is studied in depth.',
	 'authors': u'Samuel G. Rodriques,',
	 'category': u'Computer Science ',
	 'date': '2014-11-28',
	 'pdflink': u'http://arxiv.org/pdf/1411.7920',
	 'subjects': u'Probability (math.PR)',
	 'title': u"\nProbability Theory without Bayes' Rule",
	 'urllink': u'http://arxiv.org/abs/1411.7920'}
2015-04-10 07:41:03+0000 [xxu46_10] INFO: Crawled 286 pages (at 1 pages/min), scraped 279 items (at 1 items/min)
2015-04-10 07:41:13+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1411.7895> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 07:41:13+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1411.7895>
	{'abstract': u'Human mobility has been traditionally studied using surveys that deliver snapshots of population displacement patterns. The growing accessibility to ICT information obtained from portable digital media has recently opened the possibility of going beyond such fixed pictures, exploring human behavior at high spatio-temporal resolutions. Mobile phone records, geolocated tweets, check-ins from Foursquare or geotagged photos, have contributed to this purpose at different scales from cities to countries and in different areas of the world. Many of these previous works have lacked, however, details on the attributes of the individuals. In this work, we analyze credit-card transaction records as mobility proxies and assess the influence of sociodemographic characteristics on the way people move and spend their money in cities. In particular, we focus on Barcelona and Madrid, the two most populated cities of Spain, and by examining the geolocated credit-card transactions of individuals living in the two provinces, we find that consumption habits and mobility patterns vary according to gender, age and occupation. Differences in distance traveled and travel purpose are observed between younger and older people, but, curiously, either between males and females of similar age. While mobility displays some generic features, here we show that sociodemographic characteristics play a relevant role and must be taken into account for human mobility modelization.',
	 'authors': u'Maxime Lenormand, Thomas Louail, Oliva G. Cantu-Ros, Miguel Picornell, Ricardo Herranz, Juan Murillo Arias, Marc Barthelemy, Maxi San Miguel, Jose J. Ramasco,',
	 'category': u'Computer Science ',
	 'date': '2014-11-28',
	 'pdflink': u'http://arxiv.org/pdf/1411.7895',
	 'subjects': u'Physics and Society (physics.soc-ph)',
	 'title': u'\nInfluence of sociodemographic characteristics on human mobility',
	 'urllink': u'http://arxiv.org/abs/1411.7895'}
2015-04-10 07:42:03+0000 [xxu46_10] INFO: Crawled 287 pages (at 1 pages/min), scraped 280 items (at 1 items/min)
2015-04-10 07:42:18+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1411.7889> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 07:42:18+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1411.7889>
	{'abstract': u'Single particle 3D imaging with ultrashort X-ray laser pulses is based on collecting and combining the information content of 2D scattering patterns of an object at different orientations. Typical sample-delivery schemes leave little or no room for controlling the orientations. As such, the orientation associated with a given snapshot should be estimated after the experiment. Here we present an open-source code for the most rigorous technique having been reported in this context. Some practical issues along with proposed solutions are also discussed.',
	 'authors': u'Aliakbar Jafarpour,',
	 'category': u'Computer Science ',
	 'date': '2014-11-26',
	 'pdflink': u'http://arxiv.org/pdf/1411.7889',
	 'subjects': u'Optics (physics.optics)',
	 'title': u'\nOpen-source code for manifold-based 3D rotation recovery of X-ray  scattering patterns',
	 'urllink': u'http://arxiv.org/abs/1411.7889'}
2015-04-10 07:43:03+0000 [xxu46_10] INFO: Crawled 288 pages (at 1 pages/min), scraped 281 items (at 1 items/min)
2015-04-10 07:43:19+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1411.7864> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 07:43:19+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1411.7864>
	{'abstract': u'We discuss two views on extending existing methods for complex network modeling which we dub the communities first and the networks first view, respectively. Inspired by the networks first view that we attribute to White, Boorman, and Breiger (1976)[1], we formulate the multiple-networks stochastic blockmodel (MNSBM), which seeks to separate the observed network into subnetworks of different types and where the problem of inferring structure in each subnetwork becomes easier. We show how this model is specified in a generative Bayesian framework where parameters can be inferred efficiently using Gibbs sampling. The result is an effective multiple-membership model without the drawbacks of introducing complex definitions of "groups" and how they interact. We demonstrate results on the recovery of planted structure in synthetic networks and show very encouraging results on link prediction performances using multiple-networks models on a number of real-world network data sets.',
	 'authors': u'Bjarne \xd8rum Fruergaard, Tue Herlau,',
	 'category': u'Computer Science ',
	 'date': '2014-11-28',
	 'pdflink': u'http://arxiv.org/pdf/1411.7864',
	 'subjects': u'Machine Learning (stat.ML)',
	 'title': u'\nEfficient inference of overlapping communities in complex networks',
	 'urllink': u'http://arxiv.org/abs/1411.7864'}
2015-04-10 07:44:03+0000 [xxu46_10] INFO: Crawled 289 pages (at 1 pages/min), scraped 282 items (at 1 items/min)
2015-04-10 07:44:17+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1411.7817> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 07:44:17+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1411.7817>
	{'abstract': u'When solving data analysis problems it is important to integrate prior knowledge and/or structural invariances. This paper contributes by a novel framework for incorporating algebraic invariance structure into kernels. In particular, we show that algebraic properties such as sign symmetries in data, phase independence, scaling etc. can be included easily by essentially performing the kernel trick twice. We demonstrate the usefulness of our theory in simulations on selected applications such as sign-invariant spectral clustering and underdetermined ICA.',
	 'authors': u'Franz J. Kir\xe1ly, Andreas Ziehe, Klaus-Robert M\xfcller,',
	 'category': u'Computer Science ',
	 'date': '2014-11-28',
	 'pdflink': u'http://arxiv.org/pdf/1411.7817',
	 'subjects': u'Machine Learning (stat.ML)',
	 'title': u'\nLearning with Algebraic Invariances, and the Invariant Kernel Trick',
	 'urllink': u'http://arxiv.org/abs/1411.7817'}
2015-04-10 07:45:03+0000 [xxu46_10] INFO: Crawled 290 pages (at 1 pages/min), scraped 283 items (at 1 items/min)
2015-04-10 07:45:07+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1411.7801> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 07:45:07+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1411.7801>
	{'abstract': u"We examine the the convergence behavior of block GMRES and block FOM and characterize the phenomenon of stagnation in block GMRES. Stagnation is then related to the behavior of the block FOM method. Following from [Brown, SIAM J. Sci. Statist. Comput., '91], we generalize the block FOM method to generate well-defined approximations in the case that block FOM would normally break down, and these generalized solutions are used in our analysis. Two toy numerical examples are given to illustrate what we have proven, and we also apply both block methods to a small application problem to demonstrate the validity of the analysis in a non-pathological case.",
	 'authors': u'Kirk M. Soodhalter,',
	 'category': u'Computer Science ',
	 'date': '2014-11-28',
	 'pdflink': u'http://arxiv.org/pdf/1411.7801',
	 'subjects': u'Numerical Analysis (math.NA)',
	 'title': u'\nStagnation of block GMRES and its relationship to block FOM',
	 'urllink': u'http://arxiv.org/abs/1411.7801'}
2015-04-10 07:46:03+0000 [xxu46_10] INFO: Crawled 291 pages (at 1 pages/min), scraped 284 items (at 1 items/min)
2015-04-10 07:46:19+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1411.7783> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 07:46:19+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1411.7783>
	{'abstract': u'A network supporting deep unsupervised learning is presented. The network is an autoencoder with lateral shortcut connections from the encoder to decoder at each level of the hierarchy. The lateral shortcut connections allow the higher levels of the hierarchy to focus on abstract invariant features. While standard autoencoders are analogous to latent variable models with a single layer of stochastic variables, the proposed network is analogous to hierarchical latent variables models. Learning combines denoising autoencoder and denoising sources separation frameworks. Each layer of the network contributes to the cost function a term which measures the distance of the representations produced by the encoder and the decoder. Since training signals originate from all levels of the network, all layers can learn efficiently even in deep networks. The speedup offered by cost terms from higher levels of the hierarchy and the ability to learn invariant features are demonstrated in experiments.',
	 'authors': u'Harri Valpola,',
	 'category': u'Computer Science ',
	 'date': '2014-11-28',
	 'pdflink': u'http://arxiv.org/pdf/1411.7783',
	 'subjects': u'Machine Learning (stat.ML)',
	 'title': u'\nFrom neural PCA to deep unsupervised learning',
	 'urllink': u'http://arxiv.org/abs/1411.7783'}
2015-04-10 07:47:03+0000 [xxu46_10] INFO: Crawled 292 pages (at 1 pages/min), scraped 285 items (at 1 items/min)
2015-04-10 07:47:37+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1411.7719> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 07:47:37+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1411.7719>
	{'abstract': u'A self-organization of efficient and robust networks is important for a future design of communication or transportation systems, however both characteristics are incompatible in many real networks. Recently, it has been found that the robustness of onion-like structure with positive degree-degree correlations is optimal against intentional attacks. We show that, by biologically inspired copying, an onion-like network emerges in the incremental growth with functions of proxy access and reinforced connectivity on a space. The proposed network consists of the backbone of tree-like structure by copyings and the periphery by adding shortcut links between low degree nodes to enhance the connectivity. It has the fine properties of the statistically self-averaging unlike the conventional duplication-divergence model, exponential-like degree distribution without overloaded hubs, strong robustness against both malicious attacks and random failures, and the efficiency with short paths counted by the number of hops as mediators and by the Euclidean distances. The adaptivity to heal over and to recover the performance of networking is also discussed for a change of environment in such disasters or battlefields on a geographical map. These properties will be useful for a resilient and scalable infrastructure of network systems even in emergent situations or poor environments.',
	 'authors': u'Yukio Hayashi,',
	 'category': u'Computer Science ',
	 'date': '2014-11-27',
	 'pdflink': u'http://arxiv.org/pdf/1411.7719',
	 'subjects': u'Physics and Society (physics.soc-ph)',
	 'title': u'\nGrowing Self-organized Design of Efficient and Robust Complex Networks',
	 'urllink': u'http://arxiv.org/abs/1411.7719'}
2015-04-10 07:48:03+0000 [xxu46_10] INFO: Crawled 293 pages (at 1 pages/min), scraped 286 items (at 1 items/min)
2015-04-10 07:48:43+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1411.7718> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 07:48:43+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1411.7718>
	{'abstract': u'In this paper, we study a classification problem in which sample labels are randomly corrupted. In this scenario, there is an unobservable sample with noise-free labels. However, before being observed, the true labels are independently flipped with a probability , and the random label noise can be class-conditional. Here, we address two fundamental problems raised by this scenario. The first is how to best use the abundant surrogate loss functions designed for the traditional classification problem when there is label noise. We prove that any surrogate loss function can be used for classification with noisy labels by using importance reweighting, with consistency assurance that the label noise does not ultimately hinder the search for the optimal classifier of the noise-free sample. The other is the open problem of how to obtain the noise rate . We show that the rate is upper bounded by the conditional probability of the noisy sample. Consequently, the rate can be estimated, because the upper bound can be easily reached in classification problems. Experimental results on synthetic and real datasets confirm the efficiency of our methods.',
	 'authors': u'Tongliang Liu, Dacheng Tao,',
	 'category': u'Computer Science ',
	 'date': '2014-11-27',
	 'pdflink': u'http://arxiv.org/pdf/1411.7718',
	 'subjects': u'Machine Learning (stat.ML)',
	 'title': u'\nClassification with Noisy Labels by Importance Reweighting',
	 'urllink': u'http://arxiv.org/abs/1411.7718'}
2015-04-10 07:49:03+0000 [xxu46_10] INFO: Crawled 294 pages (at 1 pages/min), scraped 287 items (at 1 items/min)
2015-04-10 07:49:45+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1411.7716> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 07:49:45+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1411.7716>
	{'abstract': u'This paper studies the problem of sequential Gaussian binary hypothesis testing in a distributed multi-agent network. A sequential probability ratio test (SPRT) type algorithm in a distributed framework of the emph+ emph form is proposed, in which the agents update their decision statistics by simultaneously processing latest observations (innovations) sensed sequentially over time and information obtained from neighboring agents (consensus). For each pre-specified set of type I and type II error probabilities, local decision parameters are derived which ensure that the algorithm achieves the desired error performance and terminates in finite time almost surely (a.s.) at each network agent. Large deviation exponents for the tail probabilities of the agent stopping time distributions are obtained and it is shown that asymptotically (in the number of agents or in the high signal-to-noise-ratio regime) these exponents associated with the distributed algorithm approach that of the optimal centralized detector. The expected stopping time for the proposed algorithm at each network agent is evaluated and characterized in terms of network connectivity. Finally, simulation studies are presented which illustrate and verify the analytical findings.',
	 'authors': u'Anit Kumar Sahu, Soummya Kar,',
	 'category': u'Computer Science ',
	 'date': '2014-11-27',
	 'pdflink': u'http://arxiv.org/pdf/1411.7716',
	 'subjects': u'Optimization and Control (math.OC)',
	 'title': u'\nDistributed Sequential Detection for Gaussian Binary Hypothesis Testing',
	 'urllink': u'http://arxiv.org/abs/1411.7716'}
2015-04-10 07:50:03+0000 [xxu46_10] INFO: Crawled 295 pages (at 1 pages/min), scraped 288 items (at 1 items/min)
2015-04-10 07:51:03+0000 [xxu46_10] INFO: Crawled 295 pages (at 0 pages/min), scraped 288 items (at 0 items/min)
2015-04-10 07:51:03+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1411.7666> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 07:51:03+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1411.7666>
	{'abstract': u'First, I introduce quantum graph theory. I also discuss a known lower bound on the independence numbers and derive from it an upper bound on the chromatic numbers of quantum graphs. Then, I construct a family of quantum graphs that can be described as tropical, cyclical, and commutative. I also define a step logarithm function and express with it the bounds on quantum graph invariants in closed form. Finally, I obtain an upper bound on the independence numbers and a lower bound on the chromatic numbers of the constructed quantum graphs that are similar in form to the existing bounds. I also show that the constructed family contains graphs of any valence with arbitrarily high chromatic numbers and conclude by it that quantum graph colorings are quite different from classical graph colorings.',
	 'authors': u'Steven Lu,',
	 'category': u'Computer Science ',
	 'date': '2014-11-27',
	 'pdflink': u'http://arxiv.org/pdf/1411.7666',
	 'subjects': u'Quantum Physics (quant-ph)',
	 'title': u"\nNo Quantum Brooks' Theorem",
	 'urllink': u'http://arxiv.org/abs/1411.7666'}
2015-04-10 07:52:03+0000 [xxu46_10] INFO: Crawled 296 pages (at 1 pages/min), scraped 289 items (at 1 items/min)
2015-04-10 07:52:07+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1411.7632> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 07:52:07+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1411.7632>
	{'abstract': u'Various trade-off problems in the real world, such as communication bandwidth vs. data quality in data streaming, privacy vs. utility in real-time usage of sensitive data, and sensing cost vs. sensing quality in sensor scheduling can be discussed in the framework of the sequential rate-distortion (SRD) theory. In this paper, we consider multi-dimensional Gaussian SRD problems and their solution algorithms. For a given Gauss-Markov process , we first show that a Gaussian SRD problem is equivalent to the problem of designing the best linear sensing equation of the form of and building the Kalman filter on it. Introducing a novel variable elimination technique, we show that the optimal sensing matrix and the covariance matrix of the additive white Gaussian noise attaining the best trade-off can be very efficiently found by semidefinite programming (SDP).',
	 'authors': u'Takashi Tanaka, Kwang-Ki K. Kim, Pablo A. Parrilo, Sanjoy K. Mitter,',
	 'category': u'Computer Science ',
	 'date': '2014-11-27',
	 'pdflink': u'http://arxiv.org/pdf/1411.7632',
	 'subjects': u'Optimization and Control (math.OC)',
	 'title': u'\nSemidefinite Programming Approach to Gaussian Sequential Rate-Distortion  Trade-offs',
	 'urllink': u'http://arxiv.org/abs/1411.7632'}
2015-04-10 07:53:03+0000 [xxu46_10] INFO: Crawled 297 pages (at 1 pages/min), scraped 290 items (at 1 items/min)
2015-04-10 07:53:11+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1411.7613> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 07:53:11+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1411.7613>
	{'abstract': u'The assessment of fundamental properties for economic and financial systems, such as systemic risk, is systematically hindered by privacy issuesthat put severe limitations on the available information. Here we introduce a novel method to reconstruct partially-accessible networked systems of this kind. The method is based on the knowledge of the fitnesses, , intrinsic node-specific properties, and of the number of connections of only a limited subset of nodes. Such information is used to calibrate a directed configuration model which can generate ensembles of networks intended to represent the real system, so that the real network properties can be estimated within the generated ensemble in terms of mean values of the observables. Here we focus on estimating those properties that are commonly used to measure the network resilience to shock and crashes. Tests on both artificial and empirical networks shows that the method is remarkably robust with respect to the limitedness of the information available, thus representing a valuable tool for gaining insights on privacy-protected economic and financial systems.',
	 'authors': u'Giulio Cimini, Tiziano Squartini, Andrea Gabrielli, Diego Garlaschelli,',
	 'category': u'Computer Science ',
	 'date': '2014-11-27',
	 'pdflink': u'http://arxiv.org/pdf/1411.7613',
	 'subjects': u'Physics and Society (physics.soc-ph)',
	 'title': u'\nSystemic risk analysis in reconstructed economic and financial networks',
	 'urllink': u'http://arxiv.org/abs/1411.7613'}
2015-04-10 07:54:03+0000 [xxu46_10] INFO: Crawled 298 pages (at 1 pages/min), scraped 291 items (at 1 items/min)
2015-04-10 07:54:05+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1411.7610> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 07:54:05+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1411.7610>
	{'abstract': u'Leveraging advances in variational inference, we propose to enhance recurrent neural networks with latent variables, resulting in Stochastic Recurrent Networks (STORNs). The model i) can be trained with stochastic gradient methods, ii) allows structured and multi-modal conditionals at each time step, iii) features a reliable estimator of the marginal likelihood and iv) is a generalisation of deterministic recurrent neural networks. We evaluate the method on four polyphonic musical data sets and motion capture data.',
	 'authors': u'Justin Bayer, Christian Osendorfer,',
	 'category': u'Computer Science ',
	 'date': '2014-11-27',
	 'pdflink': u'http://arxiv.org/pdf/1411.7610',
	 'subjects': u'Machine Learning (stat.ML)',
	 'title': u'\nLearning Stochastic Recurrent Networks',
	 'urllink': u'http://arxiv.org/abs/1411.7610'}
2015-04-10 07:55:03+0000 [xxu46_10] INFO: Crawled 299 pages (at 1 pages/min), scraped 292 items (at 1 items/min)
2015-04-10 07:55:22+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1411.7516> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 07:55:22+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1411.7516>
	{'abstract': u'This work was intended to be an attempt to introduce the meta-language for working with multiple-conclusion inference rules that admit asserted propositions along with the rejected propositions. The presence of rejected propositions, and especially the presence of the rule of reverse substitution, requires certain change the definition of structurality.',
	 'authors': u'Alex Citkin,',
	 'category': u'Computer Science ',
	 'date': '2014-11-27',
	 'pdflink': u'http://arxiv.org/pdf/1411.7516',
	 'subjects': u'Logic (math.LO)',
	 'title': u'\nA Meta-Logic of Inference Rules: Syntax',
	 'urllink': u'http://arxiv.org/abs/1411.7516'}
2015-04-10 07:56:03+0000 [xxu46_10] INFO: Crawled 300 pages (at 1 pages/min), scraped 293 items (at 1 items/min)
2015-04-10 07:56:32+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1411.7494> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 07:56:32+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1411.7494>
	{'abstract': u'In this paper we present an evolutionary optimization approach to solve the risk parity portfolio selection problem. While there exist convex optimization approaches to solve this problem when long-only portfolios are considered, the optimization problem becomes non-trivial in the long-short case. To solve this problem, we propose a genetic algorithm as well as a local search heuristic. This algorithmic framework is able to compute solutions successfully. Numerical results using real-world data substantiate the practicability of the approach presented in this paper.',
	 'authors': u'Ronald Hochreiter,',
	 'category': u'Computer Science ',
	 'date': '2014-11-27',
	 'pdflink': u'http://arxiv.org/pdf/1411.7494',
	 'subjects': u'Portfolio Management (q-fin.PM)',
	 'title': u'\nAn Evolutionary Optimization Approach to Risk Parity Portfolio Selection',
	 'urllink': u'http://arxiv.org/abs/1411.7494'}
2015-04-10 07:57:03+0000 [xxu46_10] INFO: Crawled 301 pages (at 1 pages/min), scraped 294 items (at 1 items/min)
2015-04-10 07:57:33+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1411.7432> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 07:57:33+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1411.7432>
	{'abstract': u'We investigate the geometrical structure of probabilistic generative dimensionality reduction models using the tools of Riemannian geometry. We explicitly define a distribution over the natural metric given by the models. We provide the necessary algorithms to compute expected metric tensors where the distribution over mappings is given by a Gaussian process. We treat the corresponding latent variable model as a Riemannian manifold and we use the expectation of the metric under the Gaussian process prior to define interpolating paths and measure distance between latent points. We show how distances that respect the expected metric lead to more appropriate generation of new data.',
	 'authors': u'Alessandra Tosi, S\xf8ren Hauberg, Alfredo Vellido, Neil D. Lawrence,',
	 'category': u'Computer Science ',
	 'date': '2014-11-27',
	 'pdflink': u'http://arxiv.org/pdf/1411.7432',
	 'subjects': u'Machine Learning (stat.ML)',
	 'title': u'\nMetrics for Probabilistic Geometries',
	 'urllink': u'http://arxiv.org/abs/1411.7432'}
2015-04-10 07:58:03+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1411.7415> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 07:58:03+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1411.7415>
	{'abstract': u'We introduce a contagion-like model for competing opinions that includes dynamic resistance to alternative opinions. We show that this model can describe candidate vote distributions, spatial vote correlations, and slow opinion consensus with sensible parameter values. This may suggest that all these distinct behaviors, previously understood using distinct models, may be different aspects of a more unified model of human behavior introduced in this paper.',
	 'authors': u'K. Burghardt, W. Rand, M. Girvan,',
	 'category': u'Computer Science ',
	 'date': '2014-11-26',
	 'pdflink': u'http://arxiv.org/pdf/1411.7415',
	 'subjects': u'Physics and Society (physics.soc-ph)',
	 'title': u'\nCompeting Opinions and Stubborness: Connecting Models to Data',
	 'urllink': u'http://arxiv.org/abs/1411.7415'}
2015-04-10 07:58:03+0000 [xxu46_10] INFO: Crawled 303 pages (at 2 pages/min), scraped 296 items (at 2 items/min)
2015-04-10 07:59:03+0000 [xxu46_10] INFO: Crawled 303 pages (at 0 pages/min), scraped 296 items (at 0 items/min)
2015-04-10 07:59:20+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1411.7376> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 07:59:20+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1411.7376>
	{'abstract': u'Vertex coloring of a graph with -colors can be equivalently thought to be a graph homomorphism (edge preserving vertex mapping) of to the complete graph of order . So, in that sense, the chromatic number of will be the order of the smallest complete graph to which admits a homomorphism to. As every graph, which is not a complete graph, admits a homomorphism to a smaller complete graph, we can redefine the chromatic number of to be the order of the smallest graph to which admits a homomorphism to. Of course, such a smallest graph must be a complete graph as they are the only graphs with chromatic number equal to their order. The concept of vertex coloring can be generalize for other types of graphs. Naturally, the chromatic number is defined to be the order of the smallest graph (of the same type) to which a graph admits homomorphism to. The analogous notion of clique turns out to be the graphs with order equal to their (so defined) "chromatic number". These "cliques" turns out to be much more complicated than their undirected counterpart and are interesting objects of study. In this article, we mainly study different aspects of "cliques" for signed (graphs with positive or negative signs assigned to each edge) and switchable signed graphs (equivalence class of signed graph with respect to switching signs of edges incident to the same vertex).',
	 'authors': u'Julien Bensmail, Christopher Duffy, Sagnik Sen,',
	 'category': u'Computer Science ',
	 'date': '2014-11-26',
	 'pdflink': u'http://arxiv.org/pdf/1411.7376',
	 'subjects': u'Combinatorics (math.CO)',
	 'title': u'\nOn cliques of signed and switchable signed graphs',
	 'urllink': u'http://arxiv.org/abs/1411.7376'}
2015-04-10 08:00:03+0000 [xxu46_10] INFO: Crawled 304 pages (at 1 pages/min), scraped 297 items (at 1 items/min)
2015-04-10 08:00:24+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1411.7338> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 08:00:24+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1411.7338>
	{'abstract': u'We prove polynomial upper and lower bounds on the expected size of the maximum agreement subtree of two random binary phylogenetic trees under both the uniform distribution and Yule-Harding distribution. This positively answers a question posed in earlier work. Determining tight upper and lower bounds remains an open problem.',
	 'authors': u'Daniel Irving Bernstein, Lam Si Tung Ho, Colby Long, Mike Steel, Katherine St. John, Seth Sullivant,',
	 'category': u'Computer Science ',
	 'date': '2014-11-26',
	 'pdflink': u'http://arxiv.org/pdf/1411.7338',
	 'subjects': u'Populations and Evolution (q-bio.PE)',
	 'title': u'\nBounds on the Expected Size of the Maximum Agreement Subtree',
	 'urllink': u'http://arxiv.org/abs/1411.7338'}
2015-04-10 08:01:03+0000 [xxu46_10] INFO: Crawled 305 pages (at 1 pages/min), scraped 298 items (at 1 items/min)
2015-04-10 08:01:23+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1411.7280> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 08:01:23+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1411.7280>
	{'abstract': u'We study the query complexity of computing a function f:^n--&gt;R_+ in expectation. This requires the algorithm on input x to output a nonnegative random variable whose expectation equals f(x), using as few queries to the input x as possible. We exactly characterize both the randomized and the quantum query complexity by two polynomial degrees, the nonnegative literal degree and the sum-of-squares degree, respectively. We observe that the quantum complexity can be unboundedly smaller than the classical complexity for some functions, but can be at most polynomially smaller for functions with range . These query complexities relate to (and are motivated by) the extension complexity of polytopes. The linear extension complexity of a polytope is characterized by the randomized communication complexity of computing its slack matrix in expectation, and the semidefinite (psd) extension complexity is characterized by the analogous quantum model. Since query complexity can be used to upper bound communication complexity of related functions, we can derive some upper bounds on psd extension complexity by constructing efficient quantum query algorithms. As an example we give an exponentially-close entrywise approximation of the slack matrix of the perfect matching polytope with psd-rank only 2^. Finally, we show there is a precise sense in which randomized/quantum query complexity in expectation corresponds to the Sherali-Adams and Lasserre hierarchies, respectively.',
	 'authors': u'Jedrzej Kaniewski, Troy Lee, Ronald de Wolf,',
	 'category': u'Computer Science ',
	 'date': '2014-11-26',
	 'pdflink': u'http://arxiv.org/pdf/1411.7280',
	 'subjects': u'Quantum Physics (quant-ph)',
	 'title': u'\nQuery complexity in expectation',
	 'urllink': u'http://arxiv.org/abs/1411.7280'}
2015-04-10 08:02:03+0000 [xxu46_10] INFO: Crawled 306 pages (at 1 pages/min), scraped 299 items (at 1 items/min)
2015-04-10 08:02:29+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1411.7245> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 08:02:29+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1411.7245>
	{'abstract': u'The exact nonnegative matrix factorization (exact NMF) problem is the following: given an -by- nonnegative matrix and a factorization rank , find, if possible, an -by- nonnegative matrix and an -by- nonnegative matrix such that . In this paper, we propose two heuristics for exact NMF, one inspired from simulated annealing and the other from the greedy randomized adaptive search procedure. We show that these two heuristics are able to compute exact nonnegative factorizations for several classes of nonnegative matrices (namely, linear Euclidean distance matrices, slack matrices, unique-disjointness matrices, and randomly generated matrices) and as such demonstrate their superiority over standard multi-start strategies. We also consider a hybridization between these two heuristics that allows us to combine the advantages of both methods. Finally, we discuss the use of these heuristics to gain insight on the behavior of the nonnegative rank, i.e., the minimum factorization rank such that an exact NMF exists. In particular, we disprove a conjecture on the nonnegative rank of a Kronecker product, propose a new upper bound on the extension complexity of generic -gons and conjecture the exact value of (i) the extension complexity of regular -gons and (ii) the nonnegative rank of a submatrix of the slack matrix of the correlation polytope.',
	 'authors': u'Arnaud Vandaele, Nicolas Gillis, Fran\xe7ois Glineur, Daniel Tuyttens,',
	 'category': u'Computer Science ',
	 'date': '2014-11-26',
	 'pdflink': u'http://arxiv.org/pdf/1411.7245',
	 'subjects': u'Optimization and Control (math.OC)',
	 'title': u'\nHeuristics for Exact Nonnegative Matrix Factorization',
	 'urllink': u'http://arxiv.org/abs/1411.7245'}
2015-04-10 08:03:03+0000 [xxu46_10] INFO: Crawled 307 pages (at 1 pages/min), scraped 300 items (at 1 items/min)
2015-04-10 08:03:47+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1411.7231> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 08:03:47+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1411.7231>
	{'abstract': u'We establish a stochastic maximum principle (SMP) for control problems of partially observed diffusions of mean-field type with risk-sensitive performance functionals.',
	 'authors': u'Boualem Djehiche, Hamidou Tembine,',
	 'category': u'Computer Science ',
	 'date': '2014-11-26',
	 'pdflink': u'http://arxiv.org/pdf/1411.7231',
	 'subjects': u'Optimization and Control (math.OC)',
	 'title': u'\nRisk-Sensitive Mean-Field Type Control under Partial Observation',
	 'urllink': u'http://arxiv.org/abs/1411.7231'}
2015-04-10 08:04:03+0000 [xxu46_10] INFO: Crawled 308 pages (at 1 pages/min), scraped 301 items (at 1 items/min)
2015-04-10 08:05:03+0000 [xxu46_10] INFO: Crawled 308 pages (at 0 pages/min), scraped 301 items (at 0 items/min)
2015-04-10 08:05:04+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1411.7200> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 08:05:04+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1411.7200>
	{'abstract': u'We show two novel concentration inequalities for suprema of empirical processes when sampling without replacement, which both take the variance of the functions into account. While these inequalities may potentially have broad applications in learning theory in general, we exemplify their significance by studying the transductive setting of learning theory. For which we provide the first excess risk bounds based on the localized complexity of the hypothesis class, which can yield fast rates of convergence also in the transductive learning setting. We give a preliminary analysis of the localized complexities for the prominent case of kernel classes.',
	 'authors': u'Ilya Tolstikhin, Gilles Blanchard, Marius Kloft,',
	 'category': u'Computer Science ',
	 'date': '2014-11-26',
	 'pdflink': u'http://arxiv.org/pdf/1411.7200',
	 'subjects': u'Machine Learning (stat.ML)',
	 'title': u'\nLocalized Complexities for Transductive Learning',
	 'urllink': u'http://arxiv.org/abs/1411.7200'}
2015-04-10 08:06:03+0000 [xxu46_10] INFO: Crawled 309 pages (at 1 pages/min), scraped 302 items (at 1 items/min)
2015-04-10 08:06:08+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1411.7192> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 08:06:08+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1411.7192>
	{'abstract': u"The clique number of an undirected graph is the maximum order of a complete subgraph of and is a well-known lower bound for the chromatic number of . Every proper -coloring of may be viewed as a homomorphism (an edge-preserving vertex mapping) of to the complete graph of order . By considering homomorphisms of oriented graphs (digraphs without cycles of length at most 2), we get a natural notion of (oriented) colorings and oriented chromatic number of oriented graphs. An oriented clique is then an oriented graph whose number of vertices and oriented chromatic number coincide. However, the structure of oriented cliques is much less understood than in the undirected case. In this paper, we study the structure of outerplanar and planar oriented cliques. We first provide a list of 11 graphs and prove that an outerplanar graph can be oriented as an oriented clique if and only if it contains one of these graphs as a spanning subgraph. Klostermeyer and MacGillivray conjectured that the order of a planar oriented clique is at most 15, which was later proved by Sen [S. Sen. Maximum Order of a Planar Oclique Is 15. Proc. IWOCA'2012. 7643:130--142]. We show that any planar oriented clique on 15 vertices must contain a particular oriented graph as a spanning subgraph, thus reproving the above conjecture. We also provide tight upper bounds for the order of planar oriented cliques of girth for all .",
	 'authors': u'Ayan Nandy, Sagnik Sen, Eric Sopena,',
	 'category': u'Computer Science ',
	 'date': '2014-11-26',
	 'pdflink': u'http://arxiv.org/pdf/1411.7192',
	 'subjects': u'Combinatorics (math.CO)',
	 'title': u'\nOuterplanar and planar oriented cliques',
	 'urllink': u'http://arxiv.org/abs/1411.7192'}
2015-04-10 08:07:03+0000 [xxu46_10] INFO: Crawled 310 pages (at 1 pages/min), scraped 303 items (at 1 items/min)
2015-04-10 08:07:26+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1411.7159> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 08:07:26+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1411.7159>
	{'abstract': u'Extending results of Hershberger and Suri for the Euclidean plane, we show that ball hulls and ball intersections of sets of points in strictly convex normed planes can be constructed in time. In addition, we confirm that, like in the Euclidean subcase, the -center problem with constrained circles can be solved also for strictly convex normed planes in time. Some ideas for extending these results to more general types of normed planes are also presented.',
	 'authors': u'Pedro Mart\xedn, Horst Martini,',
	 'category': u'Computer Science ',
	 'date': '2014-11-26',
	 'pdflink': u'http://arxiv.org/pdf/1411.7159',
	 'subjects': u'Metric Geometry (math.MG)',
	 'title': u'\nAlgorithms for ball hulls and ball intersections in strictly convex  normed planes',
	 'urllink': u'http://arxiv.org/abs/1411.7159'}
2015-04-10 08:08:03+0000 [xxu46_10] INFO: Crawled 311 pages (at 1 pages/min), scraped 304 items (at 1 items/min)
2015-04-10 08:08:26+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1411.7101> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 08:08:26+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1411.7101>
	{'abstract': u'In this work, we study the single machine scheduling problem with uncertain release times and processing times of jobs. We adopt a robust scheduling approach, in which the measure of robustness to be minimized for a given sequence of jobs is the worst-case objective function value from the set of all possible realizations of release and processing times. The objective function value is the total flow time of all jobs. We discuss some important properties of robust schedules for zero and non-zero release times, and illustrate the added complexity in robust scheduling given non-zero release times. We propose heuristics based on variable neighborhood search and iterated local search to solve the problem and generate robust schedules. The algorithms are tested and their solution performance is compared with optimal solutions or lower bounds through numerical experiments based on synthetic data.',
	 'authors': u'Nitish Umang, Alan L. Erera, Michel Bierlaire,',
	 'category': u'Computer Science ',
	 'date': '2014-11-26',
	 'pdflink': u'http://arxiv.org/pdf/1411.7101',
	 'subjects': u'Optimization and Control (math.OC)',
	 'title': u'\nThe robust single machine scheduling problem with uncertain release and  processing times',
	 'urllink': u'http://arxiv.org/abs/1411.7101'}
2015-04-10 08:09:03+0000 [xxu46_10] INFO: Crawled 312 pages (at 1 pages/min), scraped 305 items (at 1 items/min)
2015-04-10 08:09:10+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1411.7091> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 08:09:10+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1411.7091>
	{'abstract': u'Online social networking services (SNSs) involve communication activities between large number of individuals over the public Internet and their crawled records are often regarded as proxies of real (i.e., offline) interaction structure. However, structure observed in these records might differ from real counterparts because individuals may behave differently online and non-human accounts may even participate. To understand the difference between online and real social networks, we investigate an empirical communication network between users on Twitter, which is perhaps one of the largest SNSs. We define a network of user pairs that send reciprocal messages. Based on the mixing pattern observed in this network, we argue that this network differs from conventional understandings in the sense that there is a small number of distinctive users that we call outsiders. Outsiders do not belong to any user groups but they are connected with different groups, while not being well connected with each other. We identify outsiders by maximizing the degree assortativity coefficient of the network via node removal, thereby confirming that local structural properties of outsiders identified are consistent with our hypothesis. Our findings suggest that the existence of outsiders should be considered when using Twitter communication networks for social network analysis.',
	 'authors': u'Taro Takaguchi, Takanori Maehara, Masashi Toyoda, Ken-ichi Kawarabayashi,',
	 'category': u'Computer Science ',
	 'date': '2014-11-26',
	 'pdflink': u'http://arxiv.org/pdf/1411.7091',
	 'subjects': u'Physics and Society (physics.soc-ph)',
	 'title': u'\nExistence of outsiders as a characteristic of online communication  networks',
	 'urllink': u'http://arxiv.org/abs/1411.7091'}
2015-04-10 08:10:03+0000 [xxu46_10] INFO: Crawled 313 pages (at 1 pages/min), scraped 306 items (at 1 items/min)
2015-04-10 08:10:13+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1411.7087> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 08:10:13+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1411.7087>
	{'abstract': u"This paper presents proof that the weakest theory of bounded arithmetic of Buss's hierarchy is capable of proving the consistency of a system based on a feasible arithmetic of Cook and Urquhart from which induction has been removed. This result apparently contradicts the result of Buss and Ignjatovi 'c, who stated that it is not possible to prove such a result. However, their proof actually shows that it is not possible to prove the consistency of the system, which is obtained by the addition of propositional logic and other axioms to a system such as ours. On the other hand, the system we considered is strictly equational, which is a property on which our proof relies. Our proof relies on the big-step semantics of the terms that appear in the theory of Cook and Urquhart. In our work, we first prove that, in the system under consideration, if an equation is proved and either its left or right hand side is computed, then there is a corresponding computation for its right or left hand side, respectively. By carefully computing the bound of the size of the computation, the proof inside a bounded arithmetic of this theorem is obtained, from which the consistency of the system is readily proven.",
	 'authors': u'Yoriyuki Yamagata,',
	 'category': u'Computer Science ',
	 'date': '2014-11-26',
	 'pdflink': u'http://arxiv.org/pdf/1411.7087',
	 'subjects': u'Logic (math.LO)',
	 'title': u'\nConsistency proof of a feasible arithmetic inside a bounded arithmetic',
	 'urllink': u'http://arxiv.org/abs/1411.7087'}
2015-04-10 08:11:03+0000 [xxu46_10] INFO: Crawled 314 pages (at 1 pages/min), scraped 307 items (at 1 items/min)
2015-04-10 08:11:14+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1411.7018> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 08:11:14+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1411.7018>
	{'abstract': u'The elliptic Monge-Amp `ere equation is a fully nonlinear partial differential equation which has been the focus of increasing attention from the scientific computing community [Tad12, FGN13]. Fast three dimensional solvers are needed, for example in medical image registration [HZTA04, HRT10, HPM+09, HM07], but are not yet available. We build fast solvers for smooth solutions in three dimensions, using a nonlinear full-approximation storage multigrid method. Starting from a second-order accurate centered finite difference approximation, we present a nonlinear Gauss-Seidel iterative method which has a mechanism for selecting the convex solution of the equation. The iterative method is used as an effective smoother, combined with the full-approximation storage multigrid method. Numerical experiments are provided to validate the accuracy of the finite difference scheme and illustrate the computational efficiency of the multigrid algorithm. The solution time is almost linear in the number of variables. Problems of size are solved in seconds and of size are solved in a couple of minutes on a recent model laptop.',
	 'authors': u'Jun Liu, Brittany D. Froese, Adam M. Oberman, Mingqing Xiao,',
	 'category': u'Computer Science ',
	 'date': '2014-11-25',
	 'pdflink': u'http://arxiv.org/pdf/1411.7018',
	 'subjects': u'Numerical Analysis (math.NA)',
	 'title': u'\nA multigrid solver for the three dimensional Monge-Amp\xe8re equation',
	 'urllink': u'http://arxiv.org/abs/1411.7018'}
2015-04-10 08:12:03+0000 [xxu46_10] INFO: Crawled 315 pages (at 1 pages/min), scraped 308 items (at 1 items/min)
2015-04-10 08:12:08+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1411.6997> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 08:12:08+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1411.6997>
	{'abstract': u'In this paper, we show that for every graph of maximum average degree bounded away from , any -coloring can be transformed into any other one within a polynomial number of vertex recolorings so that, at each step, the current coloring is proper. In particular, it implies that we can transform any -coloring of a planar graph into any other -coloring with a polynomial number of recolorings. These results give some evidence on a conjecture of Cereceda, van den Heuvel and Johnson which asserts that any coloring of a -degenerate graph can be transformed into any other one using a polynomial number of recolorings. We also show that any -coloring of a -degenerate graph can be transformed into any other one using a linear number of recolorings.',
	 'authors': u'Nicolas Bousquet, Guillem Perarnau,',
	 'category': u'Computer Science ',
	 'date': '2014-11-25',
	 'pdflink': u'http://arxiv.org/pdf/1411.6997',
	 'subjects': u'Combinatorics (math.CO)',
	 'title': u'\nFast Recoloring of Sparse Graphs',
	 'urllink': u'http://arxiv.org/abs/1411.6997'}
2015-04-10 08:13:03+0000 [xxu46_10] INFO: Crawled 316 pages (at 1 pages/min), scraped 309 items (at 1 items/min)
2015-04-10 08:13:09+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1411.6972> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 08:13:09+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1411.6972>
	{'abstract': u'We show that the Hankel determinants of a generalized Catalan sequence satisfy the equations of the elliptic sequence. As a consequence, the coordinates of the multiples of an arbitrary point on the elliptic curve are expressed by the Hankel determinants.',
	 'authors': u'Fumitaka Yura,',
	 'category': u'Computer Science ',
	 'date': '2014-11-25',
	 'pdflink': u'http://arxiv.org/pdf/1411.6972',
	 'subjects': u'Exactly Solvable and Integrable Systems (nlin.SI)',
	 'title': u'\nHankel Determinant Solution for Elliptic Sequence',
	 'urllink': u'http://arxiv.org/abs/1411.6972'}
2015-04-10 08:14:03+0000 [xxu46_10] INFO: Crawled 317 pages (at 1 pages/min), scraped 310 items (at 1 items/min)
2015-04-10 08:14:26+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1411.6892> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 08:14:26+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1411.6892>
	{'abstract': u'Time-frequency packing (TFP) transmission provides the highest achievable spectral efficiency with a constrained modulation format and detector complexity. In this work, the application of the TFP technique to fiber-optic systems is investigated and experimentally demonstrated. The main theoretical aspects, design guidelines, and implementation issues are discussed, focusing on those aspects which are peculiar to TFP systems. In particular, adaptive compensation of propagation impairments, matched filtering, and maximum a posteriori probability detection are obtained by a combination of a butterfly equalizer and four low-complexity parallel Bahl-Cocke-Jelinek-Raviv (BCJR) detectors. A novel algorithm that ensures adaptive equalization, channel estimation, and a proper distribution of tasks between the equalizer and BCJR detectors is proposed. A set of irregular low-density parity-check codes with different rates is designed to operate at low error rates and approach the spectral efficiency limit achievable by TFP at different signal-to-noise ratios. An experimental demonstration of the designed system is finally provided with five dual-polarization QPSK-modulated optical carriers, densely packed in a 100 GHz bandwidth, employing a recirculating loop to test the performance of the system at different transmission distances.',
	 'authors': u'Marco Secondini, Tommaso Foggi, Francesco Fresi, Gianluca Meloni, Fabio Cavaliere, Giulio Colavolpe, Enrico Forestieri, Luca Pot\xec, Roberto Sabella, Giancarlo Prati,',
	 'category': u'Computer Science ',
	 'date': '2014-11-25',
	 'pdflink': u'http://arxiv.org/pdf/1411.6892',
	 'subjects': u'Optics (physics.optics)',
	 'title': u'\nOptical Time-Frequency Packing: Principles, Design, Implementation, and  Experimental Demonstration',
	 'urllink': u'http://arxiv.org/abs/1411.6892'}
2015-04-10 08:15:03+0000 [xxu46_10] INFO: Crawled 318 pages (at 1 pages/min), scraped 311 items (at 1 items/min)
2015-04-10 08:15:37+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1411.6880> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 08:15:37+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1411.6880>
	{'abstract': u'Reconstructing a map of neuronal connectivity is a critical challenge in contemporary neuroscience. Recent advances in high-throughput electron microscopy have produced massive images of nanoscale brain tissue for the first time. This resolution allows for individual neurons and their synaptic connections to be directly observed. Manually tracing each neuronal process at this scale is unmanageable, and therefore researchers are developing automated image processing modules. Thus far, state-of-the-art algorithms focus only on the solution to a particular task (e.g., neuron segmentation or synapse identification). We have created the first fully automated images-to-graphs pipeline (i.e., a pipeline that begins with an imaged volume of neural tissue and produces a brain graph without any human interaction). To evaluate overall performance and select the best parameters and methods, we also develop a metric to assess the quality of the output graphs. We demonstrate our pipeline by selecting a set of algorithms and parameters, and search possible operating points to identify the best available brain graph. New algorithms can be easily integrated into our scalable pipeline, and used to improve results and facilitate analysis.',
	 'authors': u'William Gray Roncal, Dean M. Kleissas, Joshua T. Vogelstein, Priya Manavalan, Randal Burns, R. Jacob Vogelstein, Carey E. Priebe, Mark A. Chevillet, Gregory D. Hager,',
	 'category': u'Computer Science ',
	 'date': '2014-11-25',
	 'pdflink': u'http://arxiv.org/pdf/1411.6880',
	 'subjects': u'Quantitative Methods (q-bio.QM)',
	 'title': u'\nAn Automated Images-to-Graphs Pipeline for High Resolution Connectomics',
	 'urllink': u'http://arxiv.org/abs/1411.6880'}
2015-04-10 08:16:03+0000 [xxu46_10] INFO: Crawled 319 pages (at 1 pages/min), scraped 312 items (at 1 items/min)
2015-04-10 08:16:55+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1411.6871> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 08:16:55+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1411.6871>
	{'abstract': u'Many complex networks in natural and social phenomena have often been characterized by heavy-tailed degree distributions. However, due to rapidly growing size of network data and concerns on privacy issues about using these data, it becomes more difficult to analyze complete data sets. Thus, it is crucial to devise effective and efficient estimation methods for heavy tails of degree distributions in large-scale networks only using local information of a small fraction of sampled nodes. Here we propose a tail-scope method based on local observational bias of the friendship paradox. We show that the tail-scope method outperforms the uniform node sampling for estimating heavy tails of degree distributions, while the opposite tendency is observed in the range of small degrees. In order to take advantages of both sampling methods, we devise the hybrid method that successfully recovers the whole range of degree distributions. Our tail-scope method shows how structural heterogeneities of large-scale complex networks can be used to effectively reveal the network structure only with limited local information.',
	 'authors': u'Young-Ho Eom, Hang-Hyun Jo,',
	 'category': u'Computer Science ',
	 'date': '2014-11-25',
	 'pdflink': u'http://arxiv.org/pdf/1411.6871',
	 'subjects': u'Physics and Society (physics.soc-ph)',
	 'title': u'\nTail-scope: Using friends to estimate heavy tails of degree  distributions in large-scale complex networks',
	 'urllink': u'http://arxiv.org/abs/1411.6871'}
2015-04-10 08:17:03+0000 [xxu46_10] INFO: Crawled 320 pages (at 1 pages/min), scraped 313 items (at 1 items/min)
2015-04-10 08:18:03+0000 [xxu46_10] INFO: Crawled 320 pages (at 0 pages/min), scraped 313 items (at 0 items/min)
2015-04-10 08:18:03+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1411.6758> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 08:18:03+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1411.6758>
	{'abstract': u"The largest number factored on a quantum device reported until now was 143. That quantum computation, which used only 4 qubits at 300K, actually also factored much larger numbers such as 3599, 11663, and 56153, without the awareness of the authors of that work. Furthermore, unlike the implementations of Shor's algorithm performed thus far, these 4-qubit factorizations do not need to use prior knowledge of the answer. However, because they only use 4 qubits, these factorizations can also be performed trivially on classical computers. We discover a class of numbers for which the power of quantum information actually comes into play. We then demonstrate a 3-qubit factorization of 175, which would be the first quantum factorization of a triprime.",
	 'authors': u'Nikesh S. Dattani, Nathaniel Bryans,',
	 'category': u'Computer Science ',
	 'date': '2014-11-25',
	 'pdflink': u'http://arxiv.org/pdf/1411.6758',
	 'subjects': u'Quantum Physics (quant-ph)',
	 'title': u'\nQuantum factorization of 56153 with only 4 qubits',
	 'urllink': u'http://arxiv.org/abs/1411.6758'}
2015-04-10 08:19:03+0000 [xxu46_10] INFO: Crawled 321 pages (at 1 pages/min), scraped 314 items (at 1 items/min)
2015-04-10 08:19:12+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1411.6727> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 08:19:12+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1411.6727>
	{'abstract': u'The graph sharing game is played by two players, Alice and Bob, on a connected graph with non-negative weights assigned to the vertices. Starting with Alice, the players take the vertices of one by one, in each move keeping the set of all taken vertices connected, until the whole has been taken. Each player wants to maximize the total weight of the vertices they have gathered. It is proved that for any class of graphs with an odd number of vertices and with forbidden subdivision of a fixed graph, there is a constant such that Alice can guarantee herself at least a -fraction of the total weight of whenever . Known examples show that this is no longer true if any of the two conditions on the class (an odd number of vertices or a forbidden subdivision) is dropped. The main ingredient in the proof is a new structural result on weighted graphs with a forbidden subdivision.',
	 'authors': u'Adam G\u0105gol, Piotr Micek, Bartosz Walczak,',
	 'category': u'Computer Science ',
	 'date': '2014-11-25',
	 'pdflink': u'http://arxiv.org/pdf/1411.6727',
	 'subjects': u'Combinatorics (math.CO)',
	 'title': u'\nGraph sharing game and the structure of weighted graphs with a forbidden  subdivision',
	 'urllink': u'http://arxiv.org/abs/1411.6727'}
2015-04-10 08:20:03+0000 [xxu46_10] INFO: Crawled 322 pages (at 1 pages/min), scraped 315 items (at 1 items/min)
2015-04-10 08:20:27+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1411.6719> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 08:20:27+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1411.6719>
	{'abstract': u'We consider the problem of approximating optimal in the MMSE sense nonlinear filters in a discrete time setting, exploiting properties of appropriately stochastically convergent state process approximations. More specifically, we consider a class of nonlinear, partially observable stochastic systems, comprised by a (possibly nonstationary) hidden stochastic process (the state), observed through another conditionally Gaussian stochastic process (the observations). Under general assumptions, we show that given an approximating process which, for each time step, is stochastically convergent to the state process in some appropriate sense, an approximate filtering operator can be defined, which converges to the true optimal nonlinear filter of the state in a strong and well defined sense, i.e., compactly in time and uniformly in a measurable set of probability measure almost unity. The results presented in this paper can form a common basis for the analysis and characterization of a number of heuristic approaches for approximating a large class of optimal nonlinear filters, including approximate grid based techniques, known to perform well in a variety of applications.',
	 'authors': u'Dionysios S. Kalogerias, Athina P. Petropulu,',
	 'category': u'Computer Science ',
	 'date': '2014-11-25',
	 'pdflink': u'http://arxiv.org/pdf/1411.6719',
	 'subjects': u'Statistics Theory (math.ST)',
	 'title': u'\nAsymptotically Optimal Discrete Time Nonlinear Filters From  Stochastically Convergent State Process Approximations',
	 'urllink': u'http://arxiv.org/abs/1411.6719'}
2015-04-10 08:21:03+0000 [xxu46_10] INFO: Crawled 323 pages (at 1 pages/min), scraped 316 items (at 1 items/min)
2015-04-10 08:21:31+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1411.6663> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 08:21:31+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1411.6663>
	{'abstract': u'We study the class of 1-perfectly orientable (1-p.o.) graphs, that is, graphs having an orientation in which every out-neighborhood induces a tournament. 1-p.o. graphs form a common generalization of chordal graphs and circular arc graphs. Even though 1-p.o. graphs can be recognized in polynomial time, little is known about their structure. In this paper, we prove several structural results about 1-p.o. graphs and characterizations of 1-p.o. graphs in special graph classes. This includes: (i) a characterization of 1-p.o. graphs in terms of edge clique covers, (ii) identification of several graph transformations preserving the class of 1-p.o. graphs, (iii) a complete characterization of 1-p.o. cographs and of 1-p.o. complements of forests, and (iv) an infinite family of minimal forbidden induced minors for the class of 1-p.o. graphs.',
	 'authors': u'Tatiana Romina Hartinger, Martin Milani\u010d,',
	 'category': u'Computer Science ',
	 'date': '2014-11-24',
	 'pdflink': u'http://arxiv.org/pdf/1411.6663',
	 'subjects': u'Combinatorics (math.CO)',
	 'title': u'\nOn the structure of 1-perfectly orientable graphs',
	 'urllink': u'http://arxiv.org/abs/1411.6663'}
2015-04-10 08:22:03+0000 [xxu46_10] INFO: Crawled 324 pages (at 1 pages/min), scraped 317 items (at 1 items/min)
2015-04-10 08:22:43+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1411.6644> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 08:22:43+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1411.6644>
	{'abstract': u"We introduce the quasiminimal subshifts, subshifts having only finitely many subsystems. With -actions, their theory essentially reduces to the theory of minimal systems, but with -actions, the class is much larger. We show many examples of such subshifts, and in particular construct a universal system with only a single proper subsystem, refuting a conjecture of [Delvenne, K rrka, Blondel, '05].",
	 'authors': u'Ville Salo,',
	 'category': u'Computer Science ',
	 'date': '2014-11-24',
	 'pdflink': u'http://arxiv.org/pdf/1411.6644',
	 'subjects': u'Dynamical Systems (math.DS)',
	 'title': u'\nDecidability and Universality of Quasiminimal Subshifts',
	 'urllink': u'http://arxiv.org/abs/1411.6644'}
2015-04-10 08:23:03+0000 [xxu46_10] INFO: Crawled 325 pages (at 1 pages/min), scraped 318 items (at 1 items/min)
2015-04-10 08:23:42+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1411.6622> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 08:23:42+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1411.6622>
	{'abstract': u'This dissertation shows that careful injection of noise into sample data can substantially speed up Expectation-Maximization algorithms. Expectation-Maximization algorithms are a class of iterative algorithms for extracting maximum likelihood estimates from corrupted or incomplete data. The convergence speed-up is an example of a noise benefit or "stochastic resonance" in statistical signal processing. The dissertation presents derivations of sufficient conditions for such noise-benefits and demonstrates the speed-up in some ubiquitous signal-processing algorithms. These algorithms include parameter estimation for mixture models, the -means clustering algorithm, the Baum-Welch algorithm for training hidden Markov models, and backpropagation for training feedforward artificial neural networks. This dissertation also analyses the effects of data and model corruption on the more general Bayesian inference estimation framework. The main finding is a theorem guaranteeing that uniform approximators for Bayesian model functions produce uniform approximators for the posterior pdf via Bayes theorem. This result also applies to hierarchical and multidimensional Bayesian models.',
	 'authors': u'Osonde Adekorede Osoba,',
	 'category': u'Computer Science ',
	 'date': '2014-11-24',
	 'pdflink': u'http://arxiv.org/pdf/1411.6622',
	 'subjects': u'Machine Learning (stat.ML)',
	 'title': u'\nNoise Benefits in Expectation-Maximization Algorithms',
	 'urllink': u'http://arxiv.org/abs/1411.6622'}
2015-04-10 08:24:03+0000 [xxu46_10] INFO: Crawled 326 pages (at 1 pages/min), scraped 319 items (at 1 items/min)
2015-04-10 08:25:00+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1411.6590> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 08:25:00+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1411.6590>
	{'abstract': u'This paper establishes the consistency of a family of graph-cut-based algorithms for clustering of data clouds. We consider point clouds obtained as samples of a ground-truth measure. We investigate approaches to clustering based on minimizing objective functionals defined on proximity graphs of the given sample. Our focus is on functionals based on graph cuts like the Cheeger and ratio cuts. We show that minimizers of the these cuts converge as the sample size increases to a minimizer of a corresponding continuum cut (which partitions the ground truth measure). Moreover, we obtain sharp conditions on how the connectivity radius can be scaled with respect to the number of sample points for the consistency to hold. We provide results for two-way and for multiway cuts. Furthermore we provide numerical experiments that illustrate the results and explore the optimality of scaling in dimension two.',
	 'authors': u'Nicolas Garcia Trillos, Dejan Slepcev, James von Brecht, Thomas Laurent, Xavier Bresson,',
	 'category': u'Computer Science ',
	 'date': '2014-11-24',
	 'pdflink': u'http://arxiv.org/pdf/1411.6590',
	 'subjects': u'Machine Learning (stat.ML)',
	 'title': u'\nConsistency of Cheeger and Ratio Graph Cuts',
	 'urllink': u'http://arxiv.org/abs/1411.6590'}
2015-04-10 08:25:03+0000 [xxu46_10] INFO: Crawled 327 pages (at 1 pages/min), scraped 320 items (at 1 items/min)
2015-04-10 08:26:03+0000 [xxu46_10] INFO: Crawled 327 pages (at 0 pages/min), scraped 320 items (at 0 items/min)
2015-04-10 08:26:04+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1411.6520> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 08:26:04+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1411.6520>
	{'abstract': u'Solving logistic regression with L1-regularization in distributed settings is an important problem. This problem arises when training dataset is very large and cannot fit the memory of a single machine. We present d-GLMNET, a new algorithm solving logistic regression with L1-regularization in the distributed settings. We empirically show that it is superior over distributed online learning via truncated gradient.',
	 'authors': u'Ilya Trofimov, Alexander Genkin,',
	 'category': u'Computer Science ',
	 'date': '2014-11-24',
	 'pdflink': u'http://arxiv.org/pdf/1411.6520',
	 'subjects': u'Machine Learning (stat.ML)',
	 'title': u'\nDistributed Coordinate Descent for L1-regularized Logistic Regression',
	 'urllink': u'http://arxiv.org/abs/1411.6520'}
2015-04-10 08:27:03+0000 [xxu46_10] INFO: Crawled 328 pages (at 1 pages/min), scraped 321 items (at 1 items/min)
2015-04-10 08:27:10+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1411.6452> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 08:27:10+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1411.6452>
	{'abstract': u'Modal logics for reasoning about the power of coalitions capture the notion of effectivity functions associated with game forms. The main goal of coalition logics is to provide formal tools for modeling the dynamics of a game frame whose states may correspond to different game forms. The two classes of effectivity functions studied are the families of playable and truly playable effectivity functions, respectively. In this paper we generalize the concept of effectivity function beyond the yes/no truth scale. This enables us to describe the situations in which the goals of players are evaluated by utility functions on states. Then we introduce two modal extensions of L ukasiewicz -valued logic together with many-valued neighborhood semantics in order to encode the properties of many-valued effectivity functions associated with game forms. As our main results we prove completeness theorems for the two newly introduced modal logics.',
	 'authors': u'Tom\xe1\u0161 Kroupa, Bruno Teheux,',
	 'category': u'Computer Science ',
	 'date': '2014-11-24',
	 'pdflink': u'http://arxiv.org/pdf/1411.6452',
	 'subjects': u'Logic (math.LO)',
	 'title': u'\nModal extension of \u0141ukasiewicz logic for reasoning about coalitional  power',
	 'urllink': u'http://arxiv.org/abs/1411.6452'}
2015-04-10 08:28:03+0000 [xxu46_10] INFO: Crawled 329 pages (at 1 pages/min), scraped 322 items (at 1 items/min)
2015-04-10 08:28:28+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1411.6400> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 08:28:28+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1411.6400>
	{'abstract': u'Conventional mutual information (MI) based feature selection (FS) methods are unable to handle heterogeneous feature subset selection properly because of data format differences or estimation methods of MI between feature subset and class label. A way to solve this problem is feature transformation (FT). In this study, a novel unsupervised feature transformation (UFT) which can transform non-numerical features into numerical features is developed and tested. The UFT process is MI-based and independent of class label. MI-based FS algorithms, such as Parzen window feature selector (PWFS), minimum redundancy maximum relevance feature selection (mRMR), and normalized MI feature selection (NMIFS), can all adopt UFT for pre-processing of non-numerical features. Unlike traditional FT methods, the proposed UFT is unbiased while PWFS is utilized to its full advantage. Simulations and analyses of large-scale datasets showed that feature subset selected by the integrated method, UFT-PWFS, outperformed other FT-FS integrated methods in classification accuracy.',
	 'authors': u'Min Wei, Tommy W. S. Chow, Rosa H. M. Chan,',
	 'category': u'Computer Science ',
	 'date': '2014-11-24',
	 'pdflink': u'http://arxiv.org/e-print/1411.6400',
	 'subjects': u'Machine Learning (stat.ML)',
	 'title': u'\nMutual Information-Based Unsupervised Feature Transformation for  Heterogeneous Feature Subset Selection',
	 'urllink': u'http://arxiv.org/abs/1411.6400'}
2015-04-10 08:29:03+0000 [xxu46_10] INFO: Crawled 330 pages (at 1 pages/min), scraped 323 items (at 1 items/min)
2015-04-10 08:29:32+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1411.6346> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 08:29:32+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1411.6346>
	{'abstract': u"Suppose is a prime power and is a univariate polynomial with exactly monomial terms and degree . To establish a finite field analogue of Descartes' Rule, Bi, Cheng, and Rojas (2013) recently proved an upper bound of on the number of cosets of needed to cover the roots of in . Here, we give explicit with root structure approaching this bound: For a -th power we give an explicit -nomial vanishing on distinct cosets of . Over prime fields , computational data we provide suggests that it is harder to construct explicit sparse polynomials with many cosets of roots. Nevertheless, we find trinomials vanishing on distinct cosets in and, assuming the Generalized Riemann Hypothesis, distinct cosets in .",
	 'authors': u'Qi Cheng, Shuhong Gao, J. Maurice Rojas, Daqing Wan,',
	 'category': u'Computer Science ',
	 'date': '2014-11-24',
	 'pdflink': u'http://arxiv.org/pdf/1411.6346',
	 'subjects': u'Number Theory (math.NT)',
	 'title': u'\nSparse Univariate Polynomials with Many Roots Over Finite Fields',
	 'urllink': u'http://arxiv.org/abs/1411.6346'}
2015-04-10 08:30:03+0000 [xxu46_10] INFO: Crawled 331 pages (at 1 pages/min), scraped 324 items (at 1 items/min)
2015-04-10 08:30:35+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1411.6322> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 08:30:35+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1411.6322>
	{'abstract': u'Understanding the limits of predictability of biological evolution is a question of both practical and theoretical interest. A key inquiry along these lines is whether genetic diversity persists or not as a result of evolutionary pressures. We explore this question in the case of diploid species, i.e., organisms with two chromosomes per gene, which includes humans and the vast majority of all living organisms. Informally, genetic diversity is shown to be sustainable (even likely) in diploid species, but hard to predict even if we restrict ourselves to species with a single gene. This reveals a stark dichotomy when compared to the case of haploid species where diversity dies out, hinting at a strong evolutionary advantage that could help explain the near universal adoption of diploidy in living systems. Technically, our results revolve around bounding the likelihood of the existence of stable mixed (i.e., genetically diverse) equilibria given randomly chosen system parameters as well as characterizing the complexity of the corresponding decision problem. Critically, our results are structurally robust along several dimensions (e.g., choice of parameter distribution, choice of stability concept, restriction to typical subclasses of system instances). This is a strong indication that they are not an artifact of problem formulation, but instead reflect actual properties of biological systems.',
	 'authors': u'Ruta Mehta, Ioannis Panageas, Georgios Piliouras, Sadra Yazdanbod,',
	 'category': u'Computer Science ',
	 'date': '2014-11-24',
	 'pdflink': u'http://arxiv.org/pdf/1411.6322',
	 'subjects': u'Populations and Evolution (q-bio.PE)',
	 'title': u'\nThe Complexity of Genetic Diversity: Sex with Two Chromosomes is  Advantageous but Unpredictable',
	 'urllink': u'http://arxiv.org/abs/1411.6322'}
2015-04-10 08:31:03+0000 [xxu46_10] INFO: Crawled 332 pages (at 1 pages/min), scraped 325 items (at 1 items/min)
2015-04-10 08:31:40+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1411.6314> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 08:31:40+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1411.6314>
	{'abstract': u"Nonparametric two sample testing deals with the question of consistently deciding if two distributions are different, given samples from both, without making any parametric assumptions about the form of the distributions. The current literature is split into two kinds of tests - those which are consistent without any assumptions about how the distributions may differ ( textit alternatives), and those which are designed to specifically test easier alternatives, like a difference in means ( textit alternatives). The main contribution of this paper is to explicitly characterize the power of a popular nonparametric two sample test, designed for general alternatives, under a mean-shift alternative in the high-dimensional setting. Specifically, we explicitly derive the power of the linear-time Maximum Mean Discrepancy statistic using the Gaussian kernel, where the dimension and sample size can both tend to infinity at any rate, and the two distributions differ in their means. As a corollary, we find that if the signal-to-noise ratio is held constant, then the test's power goes to one if the number of samples increases faster than the dimension increases. This is the first explicit power derivation for a general nonparametric test in the high-dimensional setting, and also the first analysis of how tests designed for general alternatives perform when faced with easier ones.",
	 'authors': u'Aaditya Ramdas, Sashank J. Reddi, Barnabas Poczos, Aarti Singh, Larry Wasserman,',
	 'category': u'Computer Science ',
	 'date': '2014-11-23',
	 'pdflink': u'http://arxiv.org/pdf/1411.6314',
	 'subjects': u'Statistics Theory (math.ST)',
	 'title': u'\nOn the High-dimensional Power of Linear-time Kernel Two-Sample Testing  under Mean-difference Alternatives',
	 'urllink': u'http://arxiv.org/abs/1411.6314'}
2015-04-10 08:32:03+0000 [xxu46_10] INFO: Crawled 333 pages (at 1 pages/min), scraped 326 items (at 1 items/min)
2015-04-10 08:32:43+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1411.6285> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 08:32:43+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1411.6285>
	{'abstract': u'According to Cobanoglu et al and Murphy, it is now widely acknowledged that the single target paradigm (one protein or target, one disease, one drug) that has been the dominant premise in drug development in the recent past is untenable. More often than not, a drug-like compound (ligand) can be promiscuous - that is, it can interact with more than one target protein. In recent years, in in silico target prediction methods the promiscuity issue has been approached computationally in different ways. In this study we confine attention to the so-called ligand-based target prediction machine learning approaches, commonly referred to as target-fishing. With a few exceptions, the target-fishing approaches that are currently ubiquitous in cheminformatics literature can be essentially viewed as single-label multi-classification schemes; these approaches inherently bank on the single target paradigm assumption that a ligand can home in on one specific target. In order to address the ligand promiscuity issue, one might be able to cast target-fishing as a multi-label multi-class classification problem. For illustrative and comparison purposes, single-label and multi-label Naive Bayes classification models (denoted here by SMM and MMM, respectively) for target-fishing were implemented. The models were constructed and tested on 65,587 compounds and 308 targets retrieved from the ChEMBL17 database. SMM and MMM performed differently: for 16,344 test compounds, the MMM model returned recall and precision values of 0.8058 and 0.6622, respectively; the corresponding recall and precision values yielded by the SMM model were 0.7805 and 0.7596, respectively. However, at a significance level of 0.05 and one degree of freedom McNemar test performed on the target prediction results returned by SMM and MMM for the 16,344 test ligands gave a chi-squared value of 15.656, in favour of the MMM approach.',
	 'authors': u'Avid M. Afzal, Hamse Y. Mussa, Richard E. Turner, Andreas Bender, Robert C. Glen,',
	 'category': u'Computer Science ',
	 'date': '2014-11-23',
	 'pdflink': u'http://arxiv.org/pdf/1411.6285',
	 'subjects': u'Biomolecules (q-bio.BM)',
	 'title': u'\nTarget Fishing: A Single-Label or Multi-Label Problem?',
	 'urllink': u'http://arxiv.org/abs/1411.6285'}
2015-04-10 08:33:03+0000 [xxu46_10] INFO: Crawled 334 pages (at 1 pages/min), scraped 327 items (at 1 items/min)
2015-04-10 08:33:14+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1411.6257> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 08:33:14+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1411.6257>
	{'abstract': u'We consider dynamic versions of the mutual information of lifetime distributions, with focus on past lifetimes, residual lifetimes and mixed lifetimes evaluated at different instants. This allows to study multicomponent systems, by measuring the dependence in conditional lifetimes of two components having possibly different ages. We provide some bounds, and investigate the mutual information of residual lifetimes within the time-transformed exponential model (under both the assumptions of unbounded and truncated lifetimes). Moreover, with reference to the order statistics of a random sample, we evaluate explicitly the mutual information between the minimum and the maximum, conditional on inspection at different times, and show that it is distribution-free. Finally, we develop a copula-based approach aiming to express the dynamic mutual information for past and residual bivariate lifetimes in an alternative way.',
	 'authors': u'Jafar Ahmadi, Antonio Di Crescenzo, Maria Longobardi,',
	 'category': u'Computer Science ',
	 'date': '2014-11-23',
	 'pdflink': u'http://arxiv.org/pdf/1411.6257',
	 'subjects': u'Probability (math.PR)',
	 'title': u'\nOn dynamic mutual information for bivariate lifetimes',
	 'urllink': u'http://arxiv.org/abs/1411.6257'}
2015-04-10 08:34:03+0000 [xxu46_10] INFO: Crawled 335 pages (at 1 pages/min), scraped 328 items (at 1 items/min)
2015-04-10 08:34:16+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1411.6179> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 08:34:16+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1411.6179>
	{'abstract': u'With the aim to contribute to humanitarian response to disasters and violent events, scientists have proposed the development of analytical tools that could identify emergency events in real-time, using mobile phone data. The assumption is that dramatic and discrete changes in behavior, measured with mobile phone data, will indicate extreme events. In this study, we propose an efficient system for spatiotemporal detection of behavioral anomalies from mobile phone data and compare sites with behavioral anomalies to an extensive database of emergency and non-emergency events in Rwanda. Our methodology successfully captures anomalous behavioral patterns associated with a broad range of events, from religious and official holidays to earthquakes, floods, violence against civilians and protests. Our results suggest that human behavioral responses to extreme events are complex and multi-dimensional, including extreme increases and decreases in both calling and movement behaviors. We also find significant temporal and spatial variance in responses to extreme events. Our behavioral anomaly detection system and extensive discussion of results are a significant contribution to the long-term project of creating an effective real-time event detection system with mobile phone data and we discuss the implications of our findings for future research to this end. KEYWORDS: Big data, call detail record, emergency events, human mobility',
	 'authors': u'Adrian Dobra, Nathalie E. Williams, Nathan Eagle,',
	 'category': u'Computer Science ',
	 'date': '2014-11-22',
	 'pdflink': u'http://arxiv.org/pdf/1411.6179',
	 'subjects': u'Physics and Society (physics.soc-ph)',
	 'title': u'\nSpatiotemporal Detection of Unusual Human Population Behavior Using  Mobile Phone Data',
	 'urllink': u'http://arxiv.org/abs/1411.6179'}
2015-04-10 08:35:03+0000 [xxu46_10] INFO: Crawled 336 pages (at 1 pages/min), scraped 329 items (at 1 items/min)
2015-04-10 08:35:33+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1411.6160> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 08:35:33+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1411.6160>
	{'abstract': u'Sparsity is a key driver in modern statistical problems, from linear regression via the Lasso to matrix regression with nuclear norm penalties in matrix completion and beyond. In stark contrast to sparsity motivations for such problems, it is known in the field of robust optimization that a variety of vector regression problems, such as Lasso which appears as a loss function plus a regularization penalty, can arise by simply immunizing a nominal problem (with only a loss function) to uncertainty in the data. Such a robustification offers an explanation for why some linear regression methods perform well in the face of noise, even when these methods do not produce reliably sparse solutions. In this paper we deepen and extend the understanding of the connection between robustification and regularization in regression problems. Specifically, (a) in the context of linear regression, we characterize under which conditions on the model of uncertainty used and on the loss function penalties robustification and regularization are equivalent; (b) we show how to tractably robustify median regression problems; and (c) we extend the characterization of robustification and regularization to matrix regression problems (matrix completion and Principal Component Analysis).',
	 'authors': u'Dimitris Bertsimas, Martin S. Copenhaver,',
	 'category': u'Computer Science ',
	 'date': '2014-11-22',
	 'pdflink': u'http://arxiv.org/pdf/1411.6160',
	 'subjects': u'Statistics Theory (math.ST)',
	 'title': u'\nCharacterization of the equivalence of robustification and  regularization in linear, median, and matrix regression',
	 'urllink': u'http://arxiv.org/abs/1411.6160'}
2015-04-10 08:36:03+0000 [xxu46_10] INFO: Crawled 337 pages (at 1 pages/min), scraped 330 items (at 1 items/min)
2015-04-10 08:36:38+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1411.6149> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 08:36:38+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1411.6149>
	{'abstract': u"We consider the following detection problem: given a realization of a symmetric matrix of dimension , distinguish between the hypothesis that all upper triangular variables are i.i.d. Gaussians variables with mean 0 and variance and the hypothesis where is the sum of such matrix and an independent rank-one perturbation. This setup applies to the situation where under the alternative, there is a planted principal submatrix of size for which all upper triangular variables are i.i.d. Gaussians with mean and variance , whereas all other upper triangular elements of not in are i.i.d. Gaussians variables with mean 0 and variance . We refer to this as the `Gaussian hidden clique problem.' When (), it is possible to solve this detection problem with probability by computing the spectrum of and considering the largest eigenvalue of . We prove that this condition is tight in the following sense: when no algorithm that examines only the eigenvalues of can detect the existence of a hidden Gaussian clique, with error probability vanishing as . We prove this result as an immediate consequence of a more general result on rank-one perturbations of -dimensional Gaussian tensors. In this context we establish a lower bound on the critical signal-to-noise ratio below which a rank-one signal cannot be detected.",
	 'authors': u'Andrea Montanari, Daniel Reichman, Ofer Zeitouni,',
	 'category': u'Computer Science ',
	 'date': '2014-11-22',
	 'pdflink': u'http://arxiv.org/pdf/1411.6149',
	 'subjects': u'Statistics Theory (math.ST)',
	 'title': u'\nOn the limitation of spectral methods: From the Gaussian hidden clique  problem to rank one perturbations of Gaussian tensors',
	 'urllink': u'http://arxiv.org/abs/1411.6149'}
2015-04-10 08:37:03+0000 [xxu46_10] INFO: Crawled 338 pages (at 1 pages/min), scraped 331 items (at 1 items/min)
2015-04-10 08:37:31+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1411.6057> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 08:37:31+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1411.6057>
	{'abstract': u'A class of networks are those with both positive and negative links.In this manuscript, we studied the interplay between positive and negative ties on mesoscopic level of these networks, i.e., their community structure.A community is considered as a tightly interconnected group of actors; therefore, it does not borrow any assumption from balance theory and merely uses the well-known assumption in the community detection literature.We found that if one detects the communities based on only positive relations (by ignoring the negative ones), the majority of negative relations are already placed between the communities.In other words, negative ties do not have a major role in community formation of signed networks.Moreover, regarding the internal negative ties, we proved that most unbalanced communities are maximally balanced, and hence they cannot be partitioned into k nonempty sub-clusters with higher balancedness (k &gt;= 2).Furthermore, we showed that although the mediator triad + + - (hostile-mediator-hostile) is underrepresented, it constitutes a considerable portion of triadic relations among communities.Hence, mediator triads should not be ignored by community detection and clustering algorithms.As a result, if one uses a clustering algorithm that operates merely based on social balance, mesoscopic structure of signed networks significantly remains hidden.',
	 'authors': u'Pouya Esmailian, Seyed Ebrahim Abtahi, Mahdi Jalili,',
	 'category': u'Computer Science ',
	 'date': '2014-11-22',
	 'pdflink': u'http://arxiv.org/pdf/1411.6057',
	 'subjects': u'Physics and Society (physics.soc-ph)',
	 'title': u'\nMesoscopic analysis of online social networks - The role of negative  ties',
	 'urllink': u'http://arxiv.org/abs/1411.6057'}
2015-04-10 08:38:03+0000 [xxu46_10] INFO: Crawled 339 pages (at 1 pages/min), scraped 332 items (at 1 items/min)
2015-04-10 08:38:31+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1411.5977> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 08:38:31+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1411.5977>
	{'abstract': u'Human computation or crowdsourcing involves joint inference of the ground-truth-answers and the worker-abilities by optimizing an objective function, for instance, by maximizing the data likelihood based on an assumed underlying model. A variety of methods have been proposed in the literature to address this inference problem. As far as we know, none of the objective functions in existing methods is convex. In machine learning and applied statistics, a convex function such as the objective function of support vector machines (SVMs) is generally preferred, since it can leverage the high-performance algorithms and rigorous guarantees established in the extensive literature on convex optimization. One may thus wonder if there exists a meaningful convex objective function for the inference problem in human computation. In this paper, we investigate this convexity issue for human computation. We take an axiomatic approach by formulating a set of axioms that impose two mild and natural assumptions on the objective function for the inference. Under these axioms, we show that it is unfortunately impossible to ensure convexity of the inference problem. On the other hand, we show that interestingly, in the absence of a requirement to model "spammers", one can construct reasonable objective functions for crowdsourcing that guarantee convex inference.',
	 'authors': u'Nihar B. Shah, Dengyong Zhou,',
	 'category': u'Computer Science ',
	 'date': '2014-11-21',
	 'pdflink': u'http://arxiv.org/pdf/1411.5977',
	 'subjects': u'Machine Learning (stat.ML)',
	 'title': u'\nOn the Impossibility of Convex Inference in Human Computation',
	 'urllink': u'http://arxiv.org/abs/1411.5977'}
2015-04-10 08:39:03+0000 [xxu46_10] INFO: Crawled 340 pages (at 1 pages/min), scraped 333 items (at 1 items/min)
2015-04-10 08:39:30+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1411.5945> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 08:39:30+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1411.5945>
	{'abstract': u"Analyzing a large data set of publications drawn from the most competitive journals in the natural and social sciences we show that research careers exhibit the broad distributions of individual achievement characteristic of systems in which cumulative advantage plays a key role. While most researchers are personally aware of the competition implicit in the publication process, little is known about the levels of inequality at the level of individual researchers. We analyzed both productivity and impact measures for a large set of researchers publishing in high-impact journals. For each researcher cohort we calculated Gini inequality coefficients, with average Gini values around 0.48 for total publications and 0.73 for total citations. For perspective, these observed values are well in excess of the inequality levels observed for personal income in developing countries. Investigating possible sources of this inequality, we identify two potential mechanisms that act at the level of the individual that may play defining roles in the emergence of the broad productivity and impact distributions found in science. First, we show that the average time interval between a researcher's successive publications in top journals decreases with each subsequent publication. Second, after controlling for the time dependent features of citation distributions, we compare the citation impact of subsequent publications within a researcher's publication record. We find that as researchers continue to publish in top journals, there is more likely to be a decreasing trend in the relative citation impact with each subsequent publication. This pattern highlights the difficulty of repeatedly publishing high-impact research and the intriguing possibility that confirmation bias plays a role in the evaluation of scientific careers.",
	 'authors': u'Alexander M. Petersen, Orion Penner,',
	 'category': u'Computer Science ',
	 'date': '2014-11-21',
	 'pdflink': u'http://arxiv.org/pdf/1411.5945',
	 'subjects': u'Physics and Society (physics.soc-ph)',
	 'title': u'\nInequality and cumulative advantage in science careers: a case study of  high-impact journals',
	 'urllink': u'http://arxiv.org/abs/1411.5945'}
2015-04-10 08:40:03+0000 [xxu46_10] INFO: Crawled 341 pages (at 1 pages/min), scraped 334 items (at 1 items/min)
2015-04-10 08:40:20+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1411.5943> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 08:40:20+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1411.5943>
	{'abstract': u'Gaia is an ESA cornerstone mission, which was successfully launched December 2013 and commenced operations in July 2014. Within the Gaia Data Processing and Analysis consortium, Coordination Unit 7 (CU7) is responsible for the variability analysis of over a billion celestial sources and nearly 4 billion associated time series (photometric, spectrophotometric, and spectroscopic), encoding information in over 800 billion observations during the 5 years of the mission, resulting in a petabyte scale analytical problem. In this article, we briefly describe the solutions we developed to address the challenges of time series variability analysis: from the structure for a distributed data-oriented scientific collaboration to architectural choices and specific components used. Our approach is based on Open Source components with a distributed, partitioned database as the core to handle incrementally: ingestion, distributed processing, analysis, results and export in a constrained time window.',
	 'authors': u'Krzysztof Nienartowicz, Diego Ord\xf3\xf1ez Blanco, Leanne Guy, Berry Holl, Isabelle Lecoeur-Ta\xefbi, Nami Mowlavi, Lorenzo Rimoldini, Idoia Ruiz, Maria S\xfcveges, Laurent Eyer,',
	 'category': u'Computer Science ',
	 'date': '2014-11-21',
	 'pdflink': u'http://arxiv.org/pdf/1411.5943',
	 'subjects': u'Instrumentation and Methods for Astrophysics (astro-ph.IM)',
	 'title': u'\nTime series data mining for the Gaia variability analysis',
	 'urllink': u'http://arxiv.org/abs/1411.5943'}
2015-04-10 08:41:03+0000 [xxu46_10] INFO: Crawled 342 pages (at 1 pages/min), scraped 335 items (at 1 items/min)
2015-04-10 08:41:37+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1411.5925> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 08:41:37+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1411.5925>
	{'abstract': u'We consider finite horizon reach-avoid problems for discrete time stochastic systems with additive Gaussian mixture noise. Our goal is to approximate the optimal value function of such problems on dimensions higher than what can be handled via state-space gridding techniques. We achieve this by relaxing the recursive equations of the finite horizon reach-avoid optimal control problem into inequalities, projecting the optimal value function to a finite dimensional basis and sampling the associated infinite set of constraints. We focus on a specific value function parametrization using Gaussian radial basis functions that enables the analytical computation of the one-step reach-avoid reward in the case of hyper-rectangular safe and target sets, achieving significant computational benefits compared to state-space gridding. We analyze the performance of the overall method numerically by approximating simple reach-avoid control problems and comparing the results to benchmark controllers based on well-studied methods. The full power of the method is demonstrated on a nonlinear control problem inspired from constrained path planning for autonomous race cars.',
	 'authors': u'Nikolaos Kariotoglou, Maryam Kamgarpour, Tyler Summers, John Lygeros,',
	 'category': u'Computer Science ',
	 'date': '2014-11-21',
	 'pdflink': u'http://arxiv.org/pdf/1411.5925',
	 'subjects': u'Optimization and Control (math.OC)',
	 'title': u'\nA numerical approach to stochastic reach-avoid problems for Markov  Decision Processes',
	 'urllink': u'http://arxiv.org/abs/1411.5925'}
2015-04-10 08:42:03+0000 [xxu46_10] INFO: Crawled 343 pages (at 1 pages/min), scraped 336 items (at 1 items/min)
2015-04-10 08:42:53+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1411.5890> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 08:42:53+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1411.5890>
	{'abstract': u'A crucial problem in genome assembly is the discovery and correction of misassembly errors in draft genomes. We develop a method that will enhance the quality of draft genomes by identifying and removing misassembly errors using paired short read sequence data and optical mapping data. We apply our method to various assemblies of the loblolly pine and Francisella tularensis genomes. Our results demonstrate that we detect more than 54% of extensively misassembled contigs and more than 60% of locally misassembed contigs in an assembly of Francisella tularensis, and between 31% and 100% of extensively misassembled contigs and between 57% and 73% of locally misassembed contigs in the assemblies of loblolly pine. MISSEQUEL can be downloaded at this http URL',
	 'authors': u'Martin D. Muggli, Simon J. Puglisi, Roy Ronen, Christina Boucher,',
	 'category': u'Computer Science ',
	 'date': '2014-11-20',
	 'pdflink': u'http://arxiv.org/pdf/1411.5890',
	 'subjects': u'Genomics (q-bio.GN)',
	 'title': u'\nMisassembly Detection using Paired-End Sequence Reads and Optical  Mapping Data',
	 'urllink': u'http://arxiv.org/abs/1411.5890'}
2015-04-10 08:43:03+0000 [xxu46_10] INFO: Crawled 344 pages (at 1 pages/min), scraped 337 items (at 1 items/min)
2015-04-10 08:43:31+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1411.5873> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 08:43:31+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1411.5873>
	{'abstract': u'We study the problem of minimizing the average of a large number of smooth convex functions penalized with a strongly convex regularizer. We propose and analyze a novel primal-dual method (Quartz) which at every iteration samples and updates a random subset of the dual variables, chosen according to an arbitrary distribution. In contrast to typical analysis, we directly bound the decrease of the primal-dual error (in expectation), without the need to first analyze the dual error. Depending on the choice of the sampling, we obtain efficient serial, parallel and distributed variants of the method. In the serial case, our bounds match the best known bounds for SDCA (both with uniform and importance sampling). With standard mini-batching, our bounds predict initial data-independent speedup as well as additional data-driven speedup which depends on spectral and sparsity properties of the data. We calculate theoretical speedup factors and find that they are excellent predictors of actual speedup in practice. Moreover, we illustrate that it is possible to design an efficient mini-batch importance sampling. The distributed variant of Quartz is the first distributed SDCA-like method with an analysis for non-separable data.',
	 'authors': u'Zheng Qu, Peter Richt\xe1rik, Tong Zhang,',
	 'category': u'Computer Science ',
	 'date': '2014-11-21',
	 'pdflink': u'http://arxiv.org/pdf/1411.5873',
	 'subjects': u'Optimization and Control (math.OC)',
	 'title': u'\nRandomized Dual Coordinate Ascent with Arbitrary Sampling',
	 'urllink': u'http://arxiv.org/abs/1411.5873'}
2015-04-10 08:44:03+0000 [xxu46_10] INFO: Crawled 345 pages (at 1 pages/min), scraped 338 items (at 1 items/min)
2015-04-10 08:44:39+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1411.5729> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 08:44:39+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1411.5729>
	{'abstract': u'We achieve essentially the largest possible separation between quantum and classical query complexities. We do so using a property-testing problem called Forrelation, where one needs to decide whether one Boolean function is highly correlated with the Fourier transform of a second function. This problem can be solved using 1 quantum query, yet we show that any randomized algorithm needs ~sqrt(N)/log(N) queries (improving an ~N^ lower bound of Aaronson). Conversely, we show that this 1 versus ~sqrt(N) separation is optimal: indeed, any t-query quantum algorithm whatsoever can be simulated by an O(N^)-query randomized algorithm. Thus, resolving an open question of Buhrman et al. from 2002, there is no partial Boolean function whose quantum query complexity is constant and whose randomized query complexity is linear. We conjecture that a natural generalization of Forrelation achieves the optimal t versus ~N^ separation for all t. As a bonus, we show that this generalization is BQP-complete. This yields what\'s arguably the simplest BQP-complete problem yet known, and gives a second sense in which Forrelation "captures the maximum power of quantum computation."',
	 'authors': u'Scott Aaronson, Andris Ambainis,',
	 'category': u'Computer Science ',
	 'date': '2014-11-21',
	 'pdflink': u'http://arxiv.org/pdf/1411.5729',
	 'subjects': u'Quantum Physics (quant-ph)',
	 'title': u'\nForrelation: A Problem that Optimally Separates Quantum from Classical  Computing',
	 'urllink': u'http://arxiv.org/abs/1411.5729'}
2015-04-10 08:45:03+0000 [xxu46_10] INFO: Crawled 346 pages (at 1 pages/min), scraped 339 items (at 1 items/min)
2015-04-10 08:45:54+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1411.5720> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 08:45:54+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1411.5720>
	{'abstract': u'We analyze directed, unweighted graphs obtained from by connecting vertex to iff . Examples of such graphs include -nearest neighbor graphs, where varies from point to point, and, arguably, many real world graphs such as co-purchasing graphs. We ask whether we can recover the underlying Euclidean metric and the associated density given only the directed graph and . We show that consistent recovery is possible up to isometric scaling when the vertex degree is at least . Our estimator is based on a careful characterization of a random walk over the directed graph and the associated continuum limit. As an algorithm, it resembles the PageRank centrality metric. We demonstrate empirically that the estimator performs well on simulated examples as well as on real-world co-purchasing graphs even with a small number of points and degree scaling as low as .',
	 'authors': u'Tatsunori B. Hashimoto, Yi Sun, Tommi S. Jaakkola,',
	 'category': u'Computer Science ',
	 'date': '2014-11-20',
	 'pdflink': u'http://arxiv.org/pdf/1411.5720',
	 'subjects': u'Machine Learning (stat.ML)',
	 'title': u'\nMetric recovery from directed unweighted graphs',
	 'urllink': u'http://arxiv.org/abs/1411.5720'}
2015-04-10 08:46:03+0000 [xxu46_10] INFO: Crawled 347 pages (at 1 pages/min), scraped 340 items (at 1 items/min)
2015-04-10 08:46:43+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1411.5713> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 08:46:43+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1411.5713>
	{'abstract': u'We study the problem of detecting the presence of an underlying high-dimensional geometric structure in a random graph. Under the null hypothesis, the observed graph is a realization of an Erds-Rnyi random graph . Under the alternative, the graph is generated from the model, where each vertex corresponds to a latent independent random vector uniformly distributed on the sphere , and two vertices are connected if the corresponding latent vectors are close enough. In the dense regime (i.e., is a constant), we propose a near-optimal and computationally efficient testing procedure based on a new quantity which we call signed triangles. The proof of the detection lower bound is based on a new bound on the total variation distance between a Wishart matrix and an appropriately normalized GOE matrix. In the sparse regime, we make a conjecture for the optimal detection boundary. We conclude the paper with some preliminary steps on the problem of estimating the dimension in .',
	 'authors': u'S\xe9bastien Bubeck, Jian Ding, Ronen Eldan, Mikl\xf3s R\xe1cz,',
	 'category': u'Computer Science ',
	 'date': '2014-11-20',
	 'pdflink': u'http://arxiv.org/pdf/1411.5713',
	 'subjects': u'Statistics Theory (math.ST)',
	 'title': u'\nTesting for high-dimensional geometry in random graphs',
	 'urllink': u'http://arxiv.org/abs/1411.5713'}
2015-04-10 08:47:03+0000 [xxu46_10] INFO: Crawled 348 pages (at 1 pages/min), scraped 341 items (at 1 items/min)
2015-04-10 08:47:43+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1411.5668> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 08:47:43+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1411.5668>
	{'abstract': u'We consider the following interpolation problem. Suppose one is given a finite set , a function , and possibly the gradients of at the points of as well. We want to interpolate the given information with a function with the minimum possible value of . We present practical, efficient algorithms for constructing an such that is minimal, or for less computational effort, within a small dimensionless constant of being minimal.',
	 'authors': u'Ariel Herbert-Voss, Matthew J. Hirn, Frederick McCollum,',
	 'category': u'Computer Science ',
	 'date': '2014-11-20',
	 'pdflink': u'http://arxiv.org/pdf/1411.5668',
	 'subjects': u'Numerical Analysis (math.NA)',
	 'title': u'\nComputing minimal interpolants in $C^{1,1}(\\mathbb{R}^d)$',
	 'urllink': u'http://arxiv.org/abs/1411.5668'}
2015-04-10 08:48:03+0000 [xxu46_10] INFO: Crawled 349 pages (at 1 pages/min), scraped 342 items (at 1 items/min)
2015-04-10 08:48:57+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1411.5620> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 08:48:57+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1411.5620>
	{'abstract': u'A new nonparametric approach for system identification has been recently proposed where the impulse response is modeled as the realization of a zero-mean Gaussian process whose covariance (kernel) has to be estimated from data. In this scheme, quality of the estimates crucially depends on the parametrization of the covariance of the Gaussian process. A family of kernels that have been shown to be particularly effective in the system identification framework is the family of Diagonal/Correlated (DC) kernels. Maximum entropy properties of a related family of kernels, the Tuned/Correlated (TC) kernels, have been recently pointed out in the literature. In this paper we show that maximum entropy properties indeed extends to the whole family of DC kernels. The maximum entropy interpretation can be exploited in conjunction with results on matrix completion problems in the graphical models literature to shed light on the structure of the DC kernel. In particular, we prove that DC kernels admit a closed-form inverse, determinant and factorization. Maximum likelihood properties of the DC kernel are also highlighted. These results can be exploited both to improve the stability and to reduce the computational complexity associated with the computation of DC estimators, as detailed in the paper.',
	 'authors': u'Francesca Paola Carli, Tianshi Chen, Lennart Ljung,',
	 'category': u'Computer Science ',
	 'date': '2014-11-20',
	 'pdflink': u'http://arxiv.org/pdf/1411.5620',
	 'subjects': u'Optimization and Control (math.OC)',
	 'title': u'\nMaximum Entropy Kernels for System Identification',
	 'urllink': u'http://arxiv.org/abs/1411.5620'}
2015-04-10 08:49:03+0000 [xxu46_10] INFO: Crawled 350 pages (at 1 pages/min), scraped 343 items (at 1 items/min)
2015-04-10 08:50:03+0000 [xxu46_10] INFO: Crawled 350 pages (at 0 pages/min), scraped 343 items (at 0 items/min)
2015-04-10 08:50:15+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1411.5553> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 08:50:15+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1411.5553>
	{'abstract': u'Interactions in time-varying complex systems are often very heterogeneous at the topological level (who interacts with whom) and at the temporal level (when interactions occur and how often). While it is known that temporal heterogeneities often have strong effects on dynamical processes, e.g. the burstiness of contact sequences is associated with slower spreading dynamics, the picture is far from complete. In this paper, we show that temporal heterogeneities result in temporal sparsity at the time scale of average inter-event times, and that temporal sparsity determines the amount of slowdown of Susceptible-Infectious (SI) spreading dynamics on temporal networks. This result is based on the analysis of several empirical temporal network data sets. An approximate solution for a simple network model confirms the association between temporal sparsity and slowdown of SI spreading dynamics. Since deterministic SI spreading always follows the fastest temporal paths, our results generalize -- paths are slower to traverse because of temporal sparsity, and therefore all dynamical processes are slower as well.',
	 'authors': u'Juan Ignacio Perotti, Hang-Hyun Jo, Petter Holme, Jari Saram\xe4ki,',
	 'category': u'Computer Science ',
	 'date': '2014-11-20',
	 'pdflink': u'http://arxiv.org/pdf/1411.5553',
	 'subjects': u'Physics and Society (physics.soc-ph)',
	 'title': u'\nTemporal network sparsity and the slowing down of spreading',
	 'urllink': u'http://arxiv.org/abs/1411.5553'}
2015-04-10 08:51:03+0000 [xxu46_10] INFO: Crawled 351 pages (at 1 pages/min), scraped 344 items (at 1 items/min)
2015-04-10 08:51:22+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1411.5392> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 08:51:22+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1411.5392>
	{'abstract': u"Using agent-directed simulations, we investigate fluctuations in the collective emotional states on a chat network where agents interchange messages with a fixed number of moderators and emotional Bot. To design a realistic chat system, the interaction rules and some statistical parameters, as well as the agent's attributes, are inferred from the empirical chat channel texttt. In the simulations, the Bot's emotion is fixed; the moderators tune the level of its activity by passing a fraction of messages to the Bot. At , the collective emotional state matching the Bot's emotion polarity gradually arises; the average growth rate of the dominant emotional charge serves as an order parameter. Due to self-organizing effects, the collective dynamics is more explosive when positive emotions arise by positive Bot than the onset of negative emotions in the presence of negative Bot at the same . Furthermore, when the emotions matching the Bot's emotion polarity are spread over the system, the underlying fractal processes exhibit higher persistence and stronger clustering of events than the processes spreading of emotion polarity opposite to the Bot's emotion. On the other hand, the relaxation dynamics is controlled by the external noise; the related nonextensive parameter, estimated from the statistics of returns, is virtually independent of the Bot's activity level and emotion contents.",
	 'authors': u'M. \u0160uvakov, B. Tadi\u0107,',
	 'category': u'Computer Science ',
	 'date': '2014-11-18',
	 'pdflink': u'http://arxiv.org/pdf/1411.5392',
	 'subjects': u'Physics and Society (physics.soc-ph)',
	 'title': u'\nCollective emotion dynamics in chats with agents, moderators and Bots',
	 'urllink': u'http://arxiv.org/abs/1411.5392'}
2015-04-10 08:52:03+0000 [xxu46_10] INFO: Crawled 352 pages (at 1 pages/min), scraped 345 items (at 1 items/min)
2015-04-10 08:52:37+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1411.5383> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 08:52:37+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1411.5383>
	{'abstract': u'Johnson-Lindenstrauss (JL) matrices implemented by sparse random synaptic connections are thought to be a prime candidate for how convergent pathways in the brain compress information. However, to date, there is no complete mathematical support for such implementations given the constraints of real neural tissue. The fact that neurons are either excitatory or inhibitory implies that every so implementable JL matrix must be sign-consistent (i.e., all entries in a single column must be either all non-negative or all non-positive), and the fact that any given neuron connects to a relatively small subset of other neurons implies that the JL matrix had better be sparse. We construct sparse JL matrices that are sign-consistent, and prove that our construction is essentially optimal. Our work answers a mathematical question that was triggered by earlier work and is necessary to justify the existence of JL compression in the brain, and emphasizes that inhibition is crucial if neurons are to perform efficient, correlation-preserving compression.',
	 'authors': u'Zeyuan Allen-Zhu, Rati Gelashvili, Silvio Micali, Nir Shavit,',
	 'category': u'Computer Science ',
	 'date': '2014-11-19',
	 'pdflink': u'http://arxiv.org/pdf/1411.5383',
	 'subjects': u'Neurons and Cognition (q-bio.NC)',
	 'title': u'\nJohnson-Lindenstrauss Compression with Neuroscience-Based Constraints',
	 'urllink': u'http://arxiv.org/abs/1411.5383'}
2015-04-10 08:53:03+0000 [xxu46_10] INFO: Crawled 353 pages (at 1 pages/min), scraped 346 items (at 1 items/min)
2015-04-10 08:53:55+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1411.5371> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 08:53:55+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1411.5371>
	{'abstract': u'The need to estimate smooth probability distributions (a.k.a. probability densities) from finite sampled data is ubiquitous in science. Many approaches to this problem have been described, but none is yet regarded as providing a definitive solution. Maximum entropy estimation and Bayesian field theory are two such approaches. Both have origins in statistical physics, but the relationship between them has remained unclear. Here I unify these two methods by showing that every maximum entropy density estimate can be recovered in the infinite smoothness limit of an appropriate Bayesian field theory. I also show that Bayesian field theory estimation can be performed without imposing any boundary conditions on candidate densities, and that the infinite smoothness limit of these theories recovers the most common types of maximum entropy estimates. Bayesian field theory is thus seen to provide a natural test of the validity of the maximum entropy null hypothesis. Bayesian field theory also returns a lower entropy density estimate when the maximum entropy hypothesis is falsified. The computations necessary for this approach can be performed rapidly for one-dimensional data, and software for doing this is provided. Based on these results, I argue that Bayesian field theory is poised to provide a definitive solution to the density estimation problem in one dimension.',
	 'authors': u'Justin B. Kinney,',
	 'category': u'Computer Science ',
	 'date': '2014-11-19',
	 'pdflink': u'http://arxiv.org/pdf/1411.5371',
	 'subjects': u'Data Analysis, Statistics and Probability (physics.data-an)',
	 'title': u'\nUnification of field theory and maximum entropy methods for learning  probability densities',
	 'urllink': u'http://arxiv.org/abs/1411.5371'}
2015-04-10 08:54:03+0000 [xxu46_10] INFO: Crawled 354 pages (at 1 pages/min), scraped 347 items (at 1 items/min)
2015-04-10 08:54:48+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1411.5350> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 08:54:48+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1411.5350>
	{'abstract': u'Inferring the coupling structure of complex systems from time series data in general by means of statistical and information-theoretic techniques is a challenging problem in applied science. The reliability of statistical inferences requires the construction of suitable information-theoretic measures that take into account both direct and indirect influences, manifest in the form of information flows, between the components within the system. In this work, we present an application of the optimal causation entropy (oCSE) principle to identify the coupling structure of a synthetic biological system, the repressilator. Specifically, when the system reaches an equilibrium state, we use a stochastic perturbation approach to extract time series data that approximate a linear stochastic process. Then, we present and jointly apply the aggregative discovery and progressive removal algorithms based on the oCSE principle to infer the coupling structure of the system from the measured data. Finally, we show that the success rate of our coupling inferences not only improves with the amount of available data, but it also increases with a higher frequency of sampling and is especially immune to false positives.',
	 'authors': u'Jie Sun, Carlo Cafaro, Erik M. Bollt,',
	 'category': u'Computer Science ',
	 'date': '2014-11-19',
	 'pdflink': u'http://arxiv.org/pdf/1411.5350',
	 'subjects': u'Data Analysis, Statistics and Probability (physics.data-an)',
	 'title': u'\nIdentifying Coupling Structure in Complex Systems through the Optimal  Causation Entropy Principle',
	 'urllink': u'http://arxiv.org/abs/1411.5350'}
2015-04-10 08:55:03+0000 [xxu46_10] INFO: Crawled 355 pages (at 1 pages/min), scraped 348 items (at 1 items/min)
2015-04-10 08:56:02+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1411.5340> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 08:56:02+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1411.5340>
	{'abstract': u"How do we know that a kitchen is a kitchen by looking? Relatively little is known about how we conceptualize and categorize different visual environments. Traditional models of visual perception posit that scene categorization is achieved through the recognition of a scene's objects, yet these models cannot account for the mounting evidence that human observers are relatively insensitive to the local details in an image. Psychologists have long theorized that the affordances, or actionable possibilities of a stimulus are pivotal to its perception. To what extent are scene categories created from similar affordances? Using a large-scale experiment using hundreds of scene categories, we show that the activities afforded by a visual scene provide a fundamental categorization principle. Affordance-based similarity explained the majority of the structure in the human scene categorization patterns, outperforming alternative similarities based on objects or visual features. We all models were combined, affordances provided the majority of the predictive power in the combined model, and nearly half of the total explained variance is captured only by affordances. These results challenge many existing models of high-level visual perception, and provide immediately testable hypotheses for the functional organization of the human perceptual system.",
	 'authors': u'Michelle R. Greene, Christopher Baldassano, Andre Esteva, Diane M. Beck, Li Fei-Fei,',
	 'category': u'Computer Science ',
	 'date': '2014-11-19',
	 'pdflink': u'http://arxiv.org/pdf/1411.5340',
	 'subjects': u'Neurons and Cognition (q-bio.NC)',
	 'title': u'\nAffordances Provide a Fundamental Categorization Principle for Visual  Scenes',
	 'urllink': u'http://arxiv.org/abs/1411.5340'}
2015-04-10 08:56:03+0000 [xxu46_10] INFO: Crawled 356 pages (at 1 pages/min), scraped 349 items (at 1 items/min)
2015-04-10 08:57:03+0000 [xxu46_10] INFO: Crawled 356 pages (at 0 pages/min), scraped 349 items (at 0 items/min)
2015-04-10 08:57:14+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1411.5275> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 08:57:14+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1411.5275>
	{'abstract': u'We consider the problem of computing identifying codes of graphs and its fractional relaxation. The ratio between the size of optimal integer and fractional solutions is between 1 and 2 ln(|V|)+1 where V is the set of vertices of the graph. We focus on vertex-transitive graphs for which we can compute the exact fractional solution. There are known examples of vertex-transitive graphs that reach both bounds. We exhibit infinite families of vertex-transitive graphs with integer and fractional identifying codes of order |V|^a with a in . These families are generalized quadrangles (strongly regular graphs based on finite geometries). They also provide examples for metric dimension of graphs.',
	 'authors': u'Sylvain Gravier, Aline Parreau, Sara Rottey, Leo Storme, Elise Vandomme,',
	 'category': u'Computer Science ',
	 'date': '2014-11-19',
	 'pdflink': u'http://arxiv.org/pdf/1411.5275',
	 'subjects': u'Combinatorics (math.CO)',
	 'title': u'\nIdentifying codes in vertex-transitive graphs',
	 'urllink': u'http://arxiv.org/abs/1411.5275'}
2015-04-10 08:58:03+0000 [xxu46_10] INFO: Crawled 357 pages (at 1 pages/min), scraped 350 items (at 1 items/min)
2015-04-10 08:58:22+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1411.5260> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 08:58:22+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1411.5260>
	{'abstract': u"Binary classification is a common statistical learning problem in which a model is estimated on a set of covariates for some outcome indicating the membership of one of two classes. In the literature, there exists a distinction between hard and soft classification. In soft classification, the conditional class probability is modeled as a function of the covariates. In contrast, hard classification methods only target the optimal prediction boundary. While hard and soft classification methods have been studied extensively, not much work has been done to compare the actual tasks of hard and soft classification. In this paper we propose a spectrum of statistical learning problems which span the hard and soft classification tasks based on fitting multiple decision rules to the data. By doing so, we reveal a novel collection of learning tasks of increasing complexity. We study the problems using the framework of large-margin classifiers and a class of piecewise linear convex surrogates, for which we derive statistical properties and a corresponding sub-gradient descent algorithm. We conclude by applying our approach to simulation settings and a magnetic resonance imaging (MRI) dataset from the Alzheimer's Disease Neuroimaging Initiative (ADNI) study.",
	 'authors': u'Patrick K. Kimes, D. Neil Hayes, J. S. Marron, Yufeng Liu,',
	 'category': u'Computer Science ',
	 'date': '2014-11-19',
	 'pdflink': u'http://arxiv.org/pdf/1411.5260',
	 'subjects': u'Machine Learning (stat.ML)',
	 'title': u'\nLarge-Margin Classification with Multiple Decision Rules',
	 'urllink': u'http://arxiv.org/abs/1411.5260'}
2015-04-10 08:59:03+0000 [xxu46_10] INFO: Crawled 358 pages (at 1 pages/min), scraped 351 items (at 1 items/min)
2015-04-10 08:59:43+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1411.5254> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 08:59:43+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1411.5254>
	{'abstract': u'Encryption schemes often derive their power from the properties of the underlying algebra on the symbols used. Inspired by group theoretic tools, we use the centralizer of the group of unitary operations to present a private-key quantum homomorphic encryption scheme that hides arbitrary quantum computations. A particular instance of our encoding hides information at least proportional to m/log(m) bits when m bits are encrypted. This highlights the potential of our protocol to hide a non-trivial amount of information, and is suggestive of a large class of encodings that might yield better security.',
	 'authors': u'Si-Hui Tan, Joshua A. Kettlewell, Yingkai Ouyang, Lin Chen, Joseph F. Fitzsimons,',
	 'category': u'Computer Science ',
	 'date': '2014-11-19',
	 'pdflink': u'http://arxiv.org/pdf/1411.5254',
	 'subjects': u'Quantum Physics (quant-ph)',
	 'title': u'\nA quantum approach to fully homomorphic encryption',
	 'urllink': u'http://arxiv.org/abs/1411.5254'}
2015-04-10 09:00:03+0000 [xxu46_10] INFO: Crawled 359 pages (at 1 pages/min), scraped 352 items (at 1 items/min)
2015-04-10 09:00:43+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1411.5235> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 09:00:43+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1411.5235>
	{'abstract': u'For a prescribed porosity, the coupled magma/mantle flow equations can be formulated as a two field system of equations with velocity and pressure unknowns. Previous work has shown that while optimal preconditioners for the two field formulation can be constructed, the construction of preconditioners that are uniform with respect to model parameters is difficult. This limits the applicability of two field preconditioners in certain regimes of practical interest. We address this issue by reformulating the governing equations as a three field problem, which removes a term that was problematic in the two field formulation in favour of an additional equation for a pressure-like field. For the three-field problem, we develop and analyse new preconditioners and we show numerically that the new three-field preconditioners are optimal in terms of problem size and less sensitive to model parameters compared to the two-field preconditioner. This extends the applicability of optimal preconditioners for coupled mantle/magma dynamics into parameter regimes of important physical interest.',
	 'authors': u'Sander Rhebergen, Garth N. Wells, Andrew J. Wathen, Richard F. Katz,',
	 'category': u'Computer Science ',
	 'date': '2014-11-19',
	 'pdflink': u'http://arxiv.org/pdf/1411.5235',
	 'subjects': u'Numerical Analysis (math.NA)',
	 'title': u'\nOptimal three-field block-preconditioners for models of coupled  magma/mantle dynamics',
	 'urllink': u'http://arxiv.org/abs/1411.5235'}
2015-04-10 09:01:03+0000 [xxu46_10] INFO: Crawled 360 pages (at 1 pages/min), scraped 353 items (at 1 items/min)
2015-04-10 09:01:59+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1411.5125> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 09:01:59+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1411.5125>
	{'abstract': u'When facing a task of balancing a dynamic system near an unstable equilibrium, humans often adopt intermittent control strategy: instead of continuously controlling the system, they repeatedly switch the control on and off. Paradigmatic example of such a task is stick balancing. Despite the simplicity of the task itself, the complexity of human intermittent control dynamics in stick balancing still puzzles researchers in motor control. Here we attempt to model one of the key mechanisms of human intermittent control, control activation, using as an example the task of overdamped stick balancing. In so doing, we focus on the concept of noise-driven activation, a more general alternative to the conventional threshold-driven activation. We describe control activation as a random walk in an energy potential, which changes in response to the state of the controlled system. By way of numerical simulations, we show that the developed model captures the core properties of human control activation observed previously in the experiments on overdamped stick balancing. Our results demonstrate that the double-well potential model provides tractable mathematical description of human control activation at least in the considered task, and suggest that the adopted approach can potentially aid in understanding human intermittent control in more complex processes.',
	 'authors': u'Arkady Zgonnikov, Ihor Lubashevsky,',
	 'category': u'Computer Science ',
	 'date': '2014-11-19',
	 'pdflink': u'http://arxiv.org/pdf/1411.5125',
	 'subjects': u'Biological Physics (physics.bio-ph)',
	 'title': u'\nBistable dynamics of control activation in human intermittent control',
	 'urllink': u'http://arxiv.org/abs/1411.5125'}
2015-04-10 09:02:03+0000 [xxu46_10] INFO: Crawled 361 pages (at 1 pages/min), scraped 354 items (at 1 items/min)
2015-04-10 09:03:03+0000 [xxu46_10] INFO: Crawled 361 pages (at 0 pages/min), scraped 354 items (at 0 items/min)
2015-04-10 09:03:13+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1411.5121> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 09:03:13+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1411.5121>
	{'abstract': u"In this note we announce the availability of an electronic compendium of extreme functions for Gomory--Johnson's infinite group problem. These functions serve as the strongest cut-generating functions for integer linear optimization problems. We also close several gaps in the literature.",
	 'authors': u'Matthias K\xf6ppe, Yuan Zhou,',
	 'category': u'Computer Science ',
	 'date': '2014-11-19',
	 'pdflink': u'http://arxiv.org/pdf/1411.5121',
	 'subjects': u'Optimization and Control (math.OC)',
	 'title': u'\nAn Electronic Compendium of Extreme Functions for the Gomory--Johnson  Infinite Group Problem',
	 'urllink': u'http://arxiv.org/abs/1411.5121'}
2015-04-10 09:04:03+0000 [xxu46_10] INFO: Crawled 362 pages (at 1 pages/min), scraped 355 items (at 1 items/min)
2015-04-10 09:04:13+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1411.5010> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 09:04:13+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1411.5010>
	{'abstract': u'We augment the nonnegative matrix factorization method for audio source separation with cues about directionality of sound propagation. This improves separation quality greatly and removes the need for training data, but doubles the computation.',
	 'authors': u'Noah D. Stein,',
	 'category': u'Computer Science ',
	 'date': '2014-11-18',
	 'pdflink': u'http://arxiv.org/pdf/1411.5010',
	 'subjects': u'Machine Learning (stat.ML)',
	 'title': u'\nNonnegative Tensor Factorization for Directional Blind Audio Source  Separation',
	 'urllink': u'http://arxiv.org/abs/1411.5010'}
2015-04-10 09:05:03+0000 [xxu46_10] INFO: Crawled 363 pages (at 1 pages/min), scraped 356 items (at 1 items/min)
2015-04-10 09:05:08+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1411.4906> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 09:05:08+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1411.4906>
	{'abstract': u"We consider higher-dimensional generalizations of the normalized Laplacian and the adjacency matrix of graphs and study their eigenvalues for the Linial-Meshulam model of random -dimensional simplicial complexes on vertices. We show that for , the eigenvalues of these matrices are a.a.s. concentrated around two values. The main tool, which goes back to the work of Garland, are arguments that relate the eigenvalues of these matrices to those of graphs that arise as links of -dimensional faces. Garland's result concerns the Laplacian; we develop an analogous result for the adjacency matrix. The same arguments apply to other models of random complexes which allow for dependencies between the choices of -dimensional simplices. In the second part of the paper, we apply this to the question of possible higher-dimensional analogues of the discrete Cheeger inequality, which in the classical case of graphs relates the eigenvalues of a graph and its edge expansion. It is very natural to ask whether this generalizes to higher dimensions and, in particular, whether the higher-dimensional Laplacian spectra capture the notion of coboundary expansion - a generalization of edge expansion that arose in recent work of Linial and Meshulam and of Gromov. We show that this most straightforward version of a higher-dimensional discrete Cheeger inequality fails, in quite a strong way: For every and , there is a -dimensional complex on vertices that has strong spectral expansion properties (all nontrivial eigenvalues of the normalised -dimensional Laplacian lie in the interval ) but whose coboundary expansion is bounded from above by and so tends to zero as ; moreover, can be taken to have vanishing integer homology in dimension less than .",
	 'authors': u'Anna Gundert, Uli Wagner,',
	 'category': u'Computer Science ',
	 'date': '2014-11-18',
	 'pdflink': u'http://arxiv.org/pdf/1411.4906',
	 'subjects': u'Combinatorics (math.CO)',
	 'title': u'\nOn Eigenvalues of Random Complexes',
	 'urllink': u'http://arxiv.org/abs/1411.4906'}
2015-04-10 09:06:03+0000 [xxu46_10] INFO: Crawled 364 pages (at 1 pages/min), scraped 357 items (at 1 items/min)
2015-04-10 09:06:12+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1411.4884> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 09:06:12+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1411.4884>
	{'abstract': u'We consider a network topology design problem in which an initial undirected graph underlying the network is given and the objective is to select a set of edges to add to the graph to optimize the coherence of the resulting network. We show that network coherence is a submodular function of the network topology. As a consequence, a simple greedy algorithm is guaranteed to produce near optimal edge set selections. We also show that fast rank one updates of the Laplacian pseudoinverse using generalizations of the Sherman-Morrison formula and an accelerated variant of the greedy algorithm can speed up the algorithm by several orders of magnitude in practice. These allow our algorithms to scale to network sizes far beyond those that can be handled by convex relaxation heuristics.',
	 'authors': u'Tyler Summers, Iman Shames, John Lygeros, Florian D\xf6rfler,',
	 'category': u'Computer Science ',
	 'date': '2014-11-18',
	 'pdflink': u'http://arxiv.org/pdf/1411.4884',
	 'subjects': u'Optimization and Control (math.OC)',
	 'title': u'\nTopology Design for Optimal Network Coherence',
	 'urllink': u'http://arxiv.org/abs/1411.4884'}
2015-04-10 09:06:55+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1411.4867> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 09:06:55+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1411.4867>
	{'abstract': u'Respondent-driven sampling (RDS) is frequently used when sampling hard-to-reach and/or stigmatized communities. RDS utilizes a peer-driven recruitment mechanism where sampled individuals pass on participation coupons to at most of their acquaintances in the community ( being a common choice), who then in turn pass on to their acquaintances if they choose to participate, and so on. This process of distributing coupons is shown to behave like a new Reed-Frost type network epidemic model, in which becoming infected corresponds to receiving a coupon. The difference from existing network epidemic models is that an infected individual can not infect (i.e. sample) all of its contacts, but only at most of them. We calculate , the probability of a major "outbreak", and the relative size of a major outbreak in the limit of infinite population size and evaluate their adequacy in finite populations. We study the effect of varying and compare RDS to the corresponding usual epidemic models, i.e. the case of . Our results suggest that the number of coupons has a large effect on RDS recruitment. Additionally, we use our findings to explain previous empirical observations.',
	 'authors': u'Jens Malmros, Fredrik Liljeros, Tom Britton,',
	 'category': u'Computer Science ',
	 'date': '2014-11-11',
	 'pdflink': u'http://arxiv.org/pdf/1411.4867',
	 'subjects': u'Physics and Society (physics.soc-ph)',
	 'title': u'\nRespondent-driven sampling and an unusual epidemic',
	 'urllink': u'http://arxiv.org/abs/1411.4867'}
2015-04-10 09:07:03+0000 [xxu46_10] INFO: Crawled 366 pages (at 2 pages/min), scraped 359 items (at 2 items/min)
2015-04-10 09:07:53+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1411.4840> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 09:07:53+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1411.4840>
	{'abstract': u'Recent discoveries of human activity reveal the existence of long-term correlation and its relation with the fat-tailed distribution of inter-event times, which imply that there exists the fractality of human activity. However, works further analyzing the category of fractality and its origin still lack. Herein, both the DFA and MFDFA are applied in the analysis of time series of online reviewing activity from Movielens and Netflix. Results show the long-term correlation at individual and whole community level, while the extent of correlation at individual level is restricted to activity level. Such long-term correlation also reveals the fractality of online reviewing activity. In our further investigation of this fractality, we emph demonstrate it is multifractality, which results from the dual effect of broad probability density function and long-term correlation of time series of online reviewing activity. This result is also verified by three synthesized series. Therefore, we conclude that the combining impact of both broad probability density function and long-term correlation is the origin of multifractality behaving in human online activity.',
	 'authors': u'Yuhao Qin, Zhidan Zhao, Shimin Cai, Liang Gao,',
	 'category': u'Computer Science ',
	 'date': '2014-11-18',
	 'pdflink': u'http://arxiv.org/pdf/1411.4840',
	 'subjects': u'Physics and Society (physics.soc-ph)',
	 'title': u'\nDual-induced multifractality of human online activity',
	 'urllink': u'http://arxiv.org/abs/1411.4840'}
2015-04-10 09:08:03+0000 [xxu46_10] INFO: Crawled 367 pages (at 1 pages/min), scraped 360 items (at 1 items/min)
2015-04-10 09:09:03+0000 [xxu46_10] INFO: Crawled 367 pages (at 0 pages/min), scraped 360 items (at 0 items/min)
2015-04-10 09:09:05+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1411.4814> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 09:09:05+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1411.4814>
	{'abstract': u'We study the optimal control problem of minimizing the convergence time in the discrete Hegselmann--Krause model of opinion dynamics. The underlying model is extended with a set of strategic agents that can freely place their opinion at every time step. Indeed, if suitably coordinated, the strategic agents can significantly lower the convergence time of an instance of the Hegselmann--Krause model. We give several lower and upper worst-case bounds for the convergence time of a Hegselmann--Krause system with a given number of strategic agents, while still leaving some gaps for future research.',
	 'authors': u'Sascha Kurz,',
	 'category': u'Computer Science ',
	 'date': '2014-11-18',
	 'pdflink': u'http://arxiv.org/pdf/1411.4814',
	 'subjects': u'Dynamical Systems (math.DS)',
	 'title': u'\nOptimal control of the convergence time in the Hegselmann--Krause  dynamics',
	 'urllink': u'http://arxiv.org/abs/1411.4814'}
2015-04-10 09:10:03+0000 [xxu46_10] INFO: Crawled 368 pages (at 1 pages/min), scraped 361 items (at 1 items/min)
2015-04-10 09:10:11+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1411.4763> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 09:10:11+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1411.4763>
	{'abstract': u"In this paper, we tackle for the first time the problem of maximum likelihood (ML) estimation of the signal-to-noise ratio (SNR) parameter over time-varying single-input multiple-output (SIMO) channels. Both the data-aided (DA) and the non-data-aided (NDA) schemes are investigated. Unlike classical techniques where the channel is assumed to be slowly time-varying and, therefore, considered as constant over the entire observation period, we address the more challenging problem of instantaneous (i.e., short-term or local) SNR estimation over fast time-varying channels. The channel variations are tracked locally using a polynomial-in-time expansion. First, we derive in closed-form expressions the DA ML estimator and its bias. The latter is subsequently subtracted in order to obtain a new unbiased DA estimator whose variance and the corresponding Cram 'er-Rao lower bound (CRLB) are also derived in closed form. Due to the extreme nonlinearity of the log-likelihood function (LLF) in the NDA case, we resort to the expectation-maximization (EM) technique to iteratively obtain the exact NDA ML SNR estimates within very few iterations. Most remarkably, the new EM-based NDA estimator is applicable to any linearly-modulated signal and provides sufficiently accurate soft estimates (i.e., soft detection) for each of the unknown transmitted symbols. Therefore, hard detection can be easily embedded in the iteration loop in order to improve its performance at low to moderate SNR levels. We show by extensive computer simulations that the new estimators are able to accurately estimate the instantaneous per-antenna SNRs as they coincide with the DA CRLB over a wide range of practical SNRs.",
	 'authors': u'Faouzi Bellili, Rabii Meftehi, Sofiene Affes, Alex Stephenne,',
	 'category': u'Computer Science ',
	 'date': '2014-11-18',
	 'pdflink': u'http://arxiv.org/pdf/1411.4763',
	 'subjects': u'Applications (stat.AP)',
	 'title': u'\nMaximum Likelihood SNR Estimation of Linearly-Modulated Signals over  Time-Varying Flat-Fading SIMO Channels',
	 'urllink': u'http://arxiv.org/abs/1411.4763'}
2015-04-10 09:11:03+0000 [xxu46_10] INFO: Crawled 369 pages (at 1 pages/min), scraped 362 items (at 1 items/min)
2015-04-10 09:11:21+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1411.4702> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 09:11:21+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1411.4702>
	{'abstract': u'A wide range of evidence points toward the existence of a common algorithm underlying the processing of information throughout the cerebral cortex. Several hypothesized features of this cortical algorithm are reviewed, including sparse distributed representation, Bayesian inference, hierarchical organization composed of alternating template matching and pooling layers, temporal slowness and predictive coding. Hierarchical Temporal Memory (HTM) is a family of learning algorithms and corresponding theories of cortical function that embodies these principles. HTM has previously been applied mainly to perceptual tasks typical of posterior cortex. In order to evaluate HTM as a candidate model of cortical function, it is necessary also to investigate its compatibility with the requirements of frontal cortical function. To this end, a variety of models of frontal cortical function are reviewed and integrated, to arrive at the hypothesis that frontal functions including attention, working memory and action selection depend largely upon the same basic algorithms as do posterior functions, with the notable additions of a mechanism for the active maintenance of representations and of multiple cortico-striato-thalamo-cortical loops that allow communication between regions of frontal cortex to be gated in an adaptive manner. Computational models of this system are reviewed. Finally, there is a discussion of how HTM can contribute to the understanding of frontal cortical function, and of what the requirements of frontal cortical function mean for the future development of HTM.',
	 'authors': u'Michael R. Ferrier,',
	 'category': u'Computer Science ',
	 'date': '2014-11-18',
	 'pdflink': u'http://arxiv.org/pdf/1411.4702',
	 'subjects': u'Neurons and Cognition (q-bio.NC)',
	 'title': u'\nToward a Universal Cortical Algorithm: Examining Hierarchical Temporal  Memory in Light of Frontal Cortical Function',
	 'urllink': u'http://arxiv.org/abs/1411.4702'}
2015-04-10 09:12:03+0000 [xxu46_10] INFO: Crawled 370 pages (at 1 pages/min), scraped 363 items (at 1 items/min)
2015-04-10 09:12:17+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1411.4686> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 09:12:17+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1411.4686>
	{'abstract': u"We present a simple and flexible method to prove consistency of semidefinite optimization problems on random graphs. The method is based on Grothendieck's inequality. Unlike the previous uses of this inequality that lead to constant relative accuracy, we achieve arbitrary relative accuracy by leveraging randomness. We illustrate the method with the problem of community detection in sparse networks. Despite much progress in the recent years, almost no rigorous results have been known for totally sparse networks -- those with bounded average degrees. We demonstrate that even in this regime, various natural semidefinite programs can be used to recover the community structure up to an arbitrarily small fraction of misclassified vertices. The method is general; it can be applied to a variety of stochastic models of networks and semidefinite programs.",
	 'authors': u'Olivier Gu\xe9don, Roman Vershynin,',
	 'category': u'Computer Science ',
	 'date': '2014-11-17',
	 'pdflink': u'http://arxiv.org/pdf/1411.4686',
	 'subjects': u'Statistics Theory (math.ST)',
	 'title': u"\nCommunity detection in sparse networks via Grothendieck's inequality",
	 'urllink': u'http://arxiv.org/abs/1411.4686'}
2015-04-10 09:13:03+0000 [xxu46_10] INFO: Crawled 371 pages (at 1 pages/min), scraped 364 items (at 1 items/min)
2015-04-10 09:13:21+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1411.4521> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 09:13:21+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1411.4521>
	{'abstract': u'Semi-supervised learning is an important and active topic of research in pattern recognition. For classification using linear discriminant analysis specifically, several semi-supervised variants have been proposed. Using any one of these methods is not guaranteed to outperform the supervised classifier which does not take the additional unlabeled data into account. In this work we compare traditional Expectation Maximization type approaches for semi-supervised linear discriminant analysis with approaches based on intrinsic constraints and propose a new principled approach for semi-supervised linear discriminant analysis, using so-called implicit constraints. We explore the relationships between these methods and consider the question if and in what sense we can expect improvement in performance over the supervised procedure. The constraint based approaches are more robust to misspecification of the model, and may outperform alternatives that make more assumptions on the data, in terms of the log-likelihood of unseen objects.',
	 'authors': u'Jesse H. Krijthe, Marco Loog,',
	 'category': u'Computer Science ',
	 'date': '2014-11-17',
	 'pdflink': u'http://arxiv.org/pdf/1411.4521',
	 'subjects': u'Machine Learning (stat.ML)',
	 'title': u'\nImplicitly Constrained Semi-Supervised Linear Discriminant Analysis',
	 'urllink': u'http://arxiv.org/abs/1411.4521'}
2015-04-10 09:14:03+0000 [xxu46_10] INFO: Crawled 372 pages (at 1 pages/min), scraped 365 items (at 1 items/min)
2015-04-10 09:14:34+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1411.4510> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 09:14:34+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1411.4510>
	{'abstract': u'The expressive power of a Gaussian process (GP) model comes at a cost of poor scalability in the data size. To improve its scalability, this paper presents a low-rank-cum-Markov approximation (LMA) of the GP model that is novel in leveraging the dual computational advantages stemming from complementing a low-rank approximate representation of the full-rank GP based on a support set of inputs with a Markov approximation of the resulting residual process; the latter approximation is guaranteed to be closest in the Kullback-Leibler distance criterion subject to some constraint and is considerably more refined than that of existing sparse GP models utilizing low-rank representations due to its more relaxed conditional independence assumption (especially with larger data). As a result, our LMA method can trade off between the size of the support set and the order of the Markov property to (a) incur lower computational cost than such sparse GP models while achieving predictive performance comparable to them and (b) accurately represent features/patterns of any scale. Interestingly, varying the Markov order produces a spectrum of LMAs with PIC approximation and full-rank GP at the two extremes. An advantage of our LMA method is that it is amenable to parallelization on multiple machines/cores, thereby gaining greater scalability. Empirical evaluation on three real-world datasets in clusters of up to 32 computing nodes shows that our centralized and parallel LMA methods are significantly more time-efficient and scalable than state-of-the-art sparse and full-rank GP regression methods while achieving comparable predictive performances.',
	 'authors': u'Kian Hsiang Low, Jiangbo Yu, Jie Chen, Patrick Jaillet,',
	 'category': u'Computer Science ',
	 'date': '2014-11-17',
	 'pdflink': u'http://arxiv.org/pdf/1411.4510',
	 'subjects': u'Machine Learning (stat.ML)',
	 'title': u'\nParallel Gaussian Process Regression for Big Data: Low-Rank  Representation Meets Markov Approximation',
	 'urllink': u'http://arxiv.org/abs/1411.4510'}
2015-04-10 09:15:03+0000 [xxu46_10] INFO: Crawled 373 pages (at 1 pages/min), scraped 366 items (at 1 items/min)
2015-04-10 09:15:51+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1411.4439> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 09:15:51+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1411.4439>
	{'abstract': u'Lattice Quantum Chromodynamics simulations typically spend most of the runtime in inversions of the Fermion Matrix. This part is therefore frequently optimized for various HPC architectures. Here we compare the performance of the Intel Xeon Phi to current Kepler-based NVIDIA Tesla GPUs running a conjugate gradient solver. By exposing more parallelism to the accelerator through inverting multiple vectors at the same time, we obtain a performance greater than 300 GFlop/s on both architectures. This more than doubles the performance of the inversions. We also give a short overview of the Knights Corner architecture, discuss some details of the implementation and the effort required to obtain the achieved performance.',
	 'authors': u'O. Kaczmarek, C. Schmidt, P. Steinbrecher, M. Wagner,',
	 'category': u'Computer Science ',
	 'date': '2014-11-17',
	 'pdflink': u'http://arxiv.org/pdf/1411.4439',
	 'subjects': u'Computational Physics (physics.comp-ph)',
	 'title': u'\nConjugate gradient solvers on Intel Xeon Phi and NVIDIA GPUs',
	 'urllink': u'http://arxiv.org/abs/1411.4439'}
2015-04-10 09:16:03+0000 [xxu46_10] INFO: Crawled 374 pages (at 1 pages/min), scraped 367 items (at 1 items/min)
2015-04-10 09:16:40+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1411.4328> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 09:16:40+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1411.4328>
	{'abstract': u"Background: Confirmation bias is the tendency to acquire or evaluate new information in a way that is consistent with one's preexisting beliefs. It is omnipresent in psychology, economics, and even scientific practices. Prior theoretical research of this phenomenon has mainly focused on its economic implications possibly missing its potential connections with broader notions of cognitive science. Methodology/Principal Findings: We formulate a (non-Bayesian) model for revising subjective probabilistic opinion of a confirmationally-biased agent in the light of a persuasive opinion. The revision rule ensures that the agent does not react to persuasion that is either far from his current opinion or coincides with it. We demonstrate that the model accounts for the basic phenomenology of the social judgment theory, and allows to study various phenomena such as cognitive dissonance and boomerang effect. The model also displays the order of presentation effect|when consecutively exposed to two opinions, the preference is given to the last opinion (recency) or the first opinion (primacy)|and relates recency to confirmation bias. Finally, we study the model in the case of repeated persuasion and analyze its convergence properties. Conclusions: The standard Bayesian approach to probabilistic opinion revision is inadequate for describing the observed phenomenology of persuasion process. The simple non-Bayesian model proposed here does agree with this phenomenology and is capable of reproducing a spectrum of effects observed in psychology: primacy-recency phenomenon, boomerang effect and cognitive dissonance. We point out several limitations of the model that should motivate its future development.",
	 'authors': u'A.E. Allahverdyan, Aram Galstyan,',
	 'category': u'Computer Science ',
	 'date': '2014-11-16',
	 'pdflink': u'http://arxiv.org/pdf/1411.4328',
	 'subjects': u'Physics and Society (physics.soc-ph)',
	 'title': u'\nOpinion Dynamics with Confirmation Bias',
	 'urllink': u'http://arxiv.org/abs/1411.4328'}
2015-04-10 09:17:03+0000 [xxu46_10] INFO: Crawled 375 pages (at 1 pages/min), scraped 368 items (at 1 items/min)
2015-04-10 09:17:29+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1411.4324> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 09:17:29+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1411.4324>
	{'abstract': u'Higher-order singular value decomposition (HOSVD) is an efficient way for data reduction and also eliciting intrinsic structure of multi-dimensional array data. It has been used in many applications, and some of them involve incomplete data. To obtain HOSVD of the data with missing values, one can first impute the missing entries through a certain tensor completion method and then perform HOSVD to the reconstructed data. However, the two-step procedure can be inefficient and does not make reliable decomposition. In this paper, we formulate an incomplete HOSVD problem and combine the two steps into solving a single optimization problem, which simultaneously achieves imputation of missing values and also tensor decomposition. We also present two algorithms for solving the problem based on block coordinate update. Global convergence of both algorithms is shown under mild assumptions. The convergence of the second algorithm implies that of the popular higher-order orthogonality iteration (HOOI) method, and thus we, for the first time, give global convergence of HOOI. In addition, we compare the proposed methods to state-of-the-art ones for solving incomplete HOSVD and also low-rank tensor completion problems and demonstrate the superior performance of our methods over other compared ones. Furthermore, we apply them to face recognition and MRI image reconstruction to show their practical performance.',
	 'authors': u'Yangyang Xu,',
	 'category': u'Computer Science ',
	 'date': '2014-11-16',
	 'pdflink': u'http://arxiv.org/pdf/1411.4324',
	 'subjects': u'Numerical Analysis (math.NA)',
	 'title': u'\nOn Higher-order Singular Value Decomposition from Incomplete Data',
	 'urllink': u'http://arxiv.org/abs/1411.4324'}
2015-04-10 09:18:03+0000 [xxu46_10] INFO: Crawled 376 pages (at 1 pages/min), scraped 369 items (at 1 items/min)
2015-04-10 09:18:38+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1411.4286> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 09:18:38+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1411.4286>
	{'abstract': u'We consider classification tasks in the regime of scarce labeled training data in high dimensional feature space, where specific expert knowledge is also available. We propose a new hybrid optimization algorithm that solves the elastic-net support vector machine (SVM) through an alternating direction method of multipliers in the first phase, followed by an interior-point method for the classical SVM in the second phase. Both SVM formulations are adapted to knowledge incorporation. Our proposed algorithm addresses the challenges of automatic feature selection, high optimization accuracy, and algorithmic flexibility for taking advantage of prior knowledge. We demonstrate the effectiveness and efficiency of our algorithm and compare it with existing methods on a collection of synthetic and real-world data.',
	 'authors': u'Zhiwei Qin, Xiaocheng Tang, Ioannis Akrotirianakis, Amit Chakraborty,',
	 'category': u'Computer Science ',
	 'date': '2014-11-16',
	 'pdflink': u'http://arxiv.org/pdf/1411.4286',
	 'subjects': u'Machine Learning (stat.ML)',
	 'title': u'\nHIPAD - A Hybrid Interior-Point Alternating Direction algorithm for  knowledge-based SVM and feature selection',
	 'urllink': u'http://arxiv.org/abs/1411.4286'}
2015-04-10 09:19:03+0000 [xxu46_10] INFO: Crawled 377 pages (at 1 pages/min), scraped 370 items (at 1 items/min)
2015-04-10 09:19:51+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1411.4226> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 09:19:51+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1411.4226>
	{'abstract': u"The largest eigenvalue of a Wishart matrix, known as Roy's largest root (RLR), plays an important role in a variety of applications. Most works to date derived approximations to its distribution under various asymptotic regimes, such as degrees of freedom, dimension, or both tending to infinity. However, several applications involve finite and relative small parameters, for which the above approximations may be inaccurate. Recently, via a small noise perturbation approach with fixed dimension and degrees of freedom, Johnstone and Nadler derived simple yet accurate stochastic approximations to the distribution of Roy's largest root in the real valued case, under a rank-one alternative. In this paper, we extend their results to the complex valued case. Furthermore, we analyze the behavior of the leading eigenvector by developing new stochastic approximations. Specifically, we derive simple stochastic approximations to the distribution of the largest eigenvalue under five common complex single-matrix and double-matrix scenarios. We then apply these results to investigate several problems in signal detection and communications. In particular, we analyze the performance of RLR detector in cognitive radio spectrum sensing and constant modulus signal detection in the high signal-to-noise ratio (SNR) regime. Moreover, we address the problem of determining the optimal transmit-receive antenna configuration (here optimality is in the sense of outage minimization) for rank-one multiple-input multiple-output Rician Fading channels at high SNR.",
	 'authors': u'Prathapasinghe Dharmawansa, Boaz Nadler, Ofer Shwartz,',
	 'category': u'Computer Science ',
	 'date': '2014-11-16',
	 'pdflink': u'http://arxiv.org/pdf/1411.4226',
	 'subjects': u'Statistics Theory (math.ST)',
	 'title': u"\nRoy's largest root under rank-one alternatives:The complex valued case  and applications",
	 'urllink': u'http://arxiv.org/abs/1411.4226'}
2015-04-10 09:20:03+0000 [xxu46_10] INFO: Crawled 378 pages (at 1 pages/min), scraped 371 items (at 1 items/min)
2015-04-10 09:20:47+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1411.4186> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 09:20:47+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1411.4186>
	{'abstract': u'We describe a protocol for the average consensus problem on any fixed undirected graph whose convergence time scales linearly in the total number nodes . More precisely, we provide a protocol which results in each node having a value within an of the initial average after iterations. The protocol is completely distributed, with the exception of requiring each node to know an upper bound on the total number of nodes which is correct within a constant multiplicative factor. We next discuss applications of this protocol to problems in multi-agent control connected to the consensus problem. In particular, we describe protocols for formation maintenance and leader-following with convergence times which also scale linearly with the number of nodes. Most importantly, we develop a distributed protocol for minimizing an average of (possibly nondifferentiable) convex functions , in the setting where only node in an undirected, connected graph knows the function . Under the same assumption about all nodes knowing , and additionally assuming that the subgradients of each have norms upper bounded by some constant known to the nodes, we show that after iterations our protocol has error which is .',
	 'authors': u'Alex Olshevsky,',
	 'category': u'Computer Science ',
	 'date': '2014-11-15',
	 'pdflink': u'http://arxiv.org/pdf/1411.4186',
	 'subjects': u'Optimization and Control (math.OC)',
	 'title': u'\nLinear Time Average Consensus on Fixed Graphs and Implications for  Decentralized Optimization and Multi-Agent Control',
	 'urllink': u'http://arxiv.org/abs/1411.4186'}
2015-04-10 09:21:03+0000 [xxu46_10] INFO: Crawled 379 pages (at 1 pages/min), scraped 372 items (at 1 items/min)
2015-04-10 09:22:03+0000 [xxu46_10] INFO: Crawled 379 pages (at 0 pages/min), scraped 372 items (at 0 items/min)
2015-04-10 09:22:05+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1411.4179> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 09:22:05+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1411.4179>
	{'abstract': u'Strongly correlated electron systems such as the rare-earth nickelates (RNiO3, R = rare-earth element) can exhibit synapse-like continuous long term potentiation and depression when gated with ionic liquids; exploiting the extreme sensitivity of coupled charge, spin, orbital, and lattice degrees of freedom to stoichiometry. We present experimental real-time, device-level classical conditioning and unlearning using nickelate-based synaptic devices in an electronic circuit compatible with both excitatory and inhibitory neurons. We establish a physical model for the device behavior based on electric-field driven coupled ionic-electronic diffusion that can be utilized for design of more complex systems. We use the model to simulate a variety of associate and non-associative learning mechanisms, as well as a feedforward recurrent network for storing memory. Our circuit intuitively parallels biological neural architectures, and it can be readily generalized to other forms of cellular learning and extinction. The simulation of neural function with electronic device analogues may provide insight into biological processes such as decision making, learning and adaptation, while facilitating advanced parallel information processing in hardware.',
	 'authors': u'Sieu D. Ha, Jian Shi, Yasmine Meroz, L. Mahadevan, Shriram Ramanathan,',
	 'category': u'Computer Science ',
	 'date': '2014-11-15',
	 'pdflink': u'http://arxiv.org/pdf/1411.4179',
	 'subjects': u'Strongly Correlated Electrons (cond-mat.str-el)',
	 'title': u'\nNeuromimetic Circuits with Synaptic Devices based on Strongly Correlated  Electron Systems',
	 'urllink': u'http://arxiv.org/abs/1411.4179'}
2015-04-10 09:23:03+0000 [xxu46_10] INFO: Crawled 380 pages (at 1 pages/min), scraped 373 items (at 1 items/min)
2015-04-10 09:23:16+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1411.4108> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 09:23:16+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1411.4108>
	{'abstract': u'High-throughput RNA sequencing (RNA-seq) has emerged as a revolutionary and powerful technology for expression profiling. Most proposed methods for detecting differentially expressed (DE) genes from RNA-seq are based on statistics that compare normalized read counts between conditions. However, there are few methods considering the expression measurement uncertainty into DE detection. Moreover, most methods are only capable of detecting DE genes, and few methods are available for detecting DE isoforms. In this paper, a Bayesian framework (BDSeq) is proposed to detect DE genes and isoforms with consideration of expression measurement uncertainty. This expression measurement uncertainty provides useful information which can help to improve the performance of DE detection. Three real RAN-seq data sets are used to evaluate the performance of BDSeq and results show that the inclusion of expression measurement uncertainty improves accuracy in detection of DE genes and isoforms. Finally, we develop a GamSeq-BDSeq RNA-seq analysis pipeline to facilitate users, which is freely available at the website this http URL',
	 'authors': u'Li Zhang, Xuejun Liu, Songcan Chen,',
	 'category': u'Computer Science ',
	 'date': '2014-11-15',
	 'pdflink': u'http://arxiv.org/pdf/1411.4108',
	 'subjects': u'Genomics (q-bio.GN)',
	 'title': u'\nDetecting Differential Expression from RNA-seq Data with Expression  Measurement Uncertainty',
	 'urllink': u'http://arxiv.org/abs/1411.4108'}
2015-04-10 09:23:50+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1411.4105> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 09:23:50+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1411.4105>
	{'abstract': u'Many resource allocation problems can be formulated as an optimization problem whose constraints contain sensitive information about participating users. This paper concerns solving this kind of optimization problem in a distributed manner while protecting the privacy of user information. Without privacy considerations, existing distributed algorithms normally consist in a central entity computing and broadcasting certain public coordination signals to participating users. However, the coordination signals often depend on user information, so that an adversary who has access to the coordination signals can potentially decode information on individual users and put user privacy at risk. We present a distributed optimization algorithm that preserves differential privacy, which is a strong notion that guarantees user privacy regardless of any auxiliary information an adversary may have. The algorithm achieves privacy by perturbing the public signals with additive noise, whose magnitude is determined by the sensitivity of the projection operation onto user-specified constraints. By viewing the differentially private algorithm as an implementation of stochastic gradient descent, we are able to derive a bound for the suboptimality of the algorithm. We illustrate the implementation of our algorithm via a case study of electric vehicle charging. Specifically, we derive the sensitivity and present numerical simulations for the algorithm. Through numerical simulations, we are able to investigate various aspects of the algorithm when being used in practice, including the choice of step size, number of iterations, and the trade-off between privacy level and suboptimality.',
	 'authors': u'Shuo Han, Ufuk Topcu, George J. Pappas,',
	 'category': u'Computer Science ',
	 'date': '2014-11-15',
	 'pdflink': u'http://arxiv.org/pdf/1411.4105',
	 'subjects': u'Optimization and Control (math.OC)',
	 'title': u'\nDifferentially Private Distributed Constrained Optimization',
	 'urllink': u'http://arxiv.org/abs/1411.4105'}
2015-04-10 09:24:03+0000 [xxu46_10] INFO: Crawled 382 pages (at 2 pages/min), scraped 375 items (at 2 items/min)
2015-04-10 09:24:34+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1411.4101> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 09:24:34+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1411.4101>
	{'abstract': u'Scene parsing is an important and challenging prob- lem in computer vision. It requires labeling each pixel in an image with the category it belongs to. Tradition- ally, it has been approached with hand-engineered features from color information in images. Recently convolutional neural networks (CNNs), which automatically learn hierar- chies of features, have achieved record performance on the task. These approaches typically include a post-processing technique, such as superpixels, to produce the final label- ing. In this paper, we propose a novel network architecture that combines deep deconvolutional neural networks with CNNs. Our experiments show that deconvolutional neu- ral networks are capable of learning higher order image structure beyond edge primitives in comparison to CNNs. The new network architecture is employed for multi-patch training, introduced as part of this work. Multi-patch train- ing makes it possible to effectively learn spatial priors from scenes. The proposed approach yields state-of-the-art per- formance on four scene parsing datasets, namely Stanford Background, SIFT Flow, CamVid, and KITTI. In addition, our system has the added advantage of having a training system that can be completely automated end-to-end with- out requiring any post-processing.',
	 'authors': u'Rahul Mohan,',
	 'category': u'Computer Science ',
	 'date': '2014-11-15',
	 'pdflink': u'http://arxiv.org/pdf/1411.4101',
	 'subjects': u'Machine Learning (stat.ML)',
	 'title': u'\nDeep Deconvolutional Networks for Scene Parsing',
	 'urllink': u'http://arxiv.org/abs/1411.4101'}
2015-04-10 09:25:03+0000 [xxu46_10] INFO: Crawled 383 pages (at 1 pages/min), scraped 376 items (at 1 items/min)
2015-04-10 09:25:47+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1411.4086> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 09:25:47+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1411.4086>
	{'abstract': u'Crowdsourcing has become an effective and popular tool for human-powered computation to label large datasets. Since the workers can be unreliable, it is common in crowdsourcing to assign multiple workers to one task, and to aggregate the labels in order to obtain results of high quality. In this paper, we provide finite-sample exponential bounds on the error rate (in probability and in expectation) of general aggregation rules under the Dawid-Skene crowdsourcing model. The bounds are derived for multi-class labeling, and can be used to analyze many aggregation methods, including majority voting, weighted majority voting and the oracle Maximum A Posteriori (MAP) rule. We show that the oracle MAP rule approximately optimizes our upper bound on the mean error rate of weighted majority voting in certain setting. We propose an iterative weighted majority voting (IWMV) method that optimizes the error rate bound and approximates the oracle MAP rule. Its one step version has a provable theoretical guarantee on the error rate. The IWMV method is intuitive and computationally simple. Experimental results on simulated and real data show that IWMV performs at least on par with the state-of-the-art methods, and it has a much lower computational cost (around one hundred times faster) than the state-of-the-art methods.',
	 'authors': u'Hongwei Li, Bin Yu,',
	 'category': u'Computer Science ',
	 'date': '2014-11-15',
	 'pdflink': u'http://arxiv.org/pdf/1411.4086',
	 'subjects': u'Machine Learning (stat.ML)',
	 'title': u'\nError Rate Bounds and Iterative Weighted Majority Voting for  Crowdsourcing',
	 'urllink': u'http://arxiv.org/abs/1411.4086'}
2015-04-10 09:26:03+0000 [xxu46_10] INFO: Crawled 384 pages (at 1 pages/min), scraped 377 items (at 1 items/min)
2015-04-10 09:26:46+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1411.4074> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 09:26:46+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1411.4074>
	{'abstract': u'Consider a central problem in randomized approximation schemes that use a Monte Carlo approach. Given a sequence of independent, identically distributed random variables with mean and standard deviation at most , where is a known constant, and , create an estimate for such that . This technique has been used for building randomized approximation schemes for the volume of a convex body, the permanent of a nonnegative matrix, the number of linear extensions of a poset, the partition function of the Ising model and many other problems. Existing methods use (to the leading order) samples. This is the best possible number up to the constant factor, and it is an open question as to what is the best constant possible. This work gives an easy to apply estimate that only uses samples in the leading order.',
	 'authors': u'Mark Huber,',
	 'category': u'Computer Science ',
	 'date': '2014-11-14',
	 'pdflink': u'http://arxiv.org/pdf/1411.4074',
	 'subjects': u'Statistics Theory (math.ST)',
	 'title': u'\nImproving Monte Carlo randomized approximation schemes',
	 'urllink': u'http://arxiv.org/abs/1411.4074'}
2015-04-10 09:27:03+0000 [xxu46_10] INFO: Crawled 385 pages (at 1 pages/min), scraped 378 items (at 1 items/min)
2015-04-10 09:27:53+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1411.4070> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 09:27:53+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1411.4070>
	{'abstract': u'Research on probabilistic models of networks now spans a wide variety of fields, including physics, sociology, biology, statistics, and machine learning. These efforts have produced a diverse ecology of models and methods. Despite this diversity, many of these models share a common underlying structure: pairwise interactions (edges) are generated with probability conditional on latent vertex attributes. Differences between models generally stem from different philosophical choices about how to learn from data or different empirically-motivated goals. The highly interdisciplinary nature of work on these generative models, however, has inhibited the development of a unified view of their similarities and differences. For instance, novel theoretical models and optimization techniques developed in machine learning are largely unknown within the social and biological sciences, which have instead emphasized model interpretability. Here, we describe a unified view of generative models for networks that draws together many of these disparate threads and highlights the fundamental similarities and differences that span these fields. We then describe a number of opportunities and challenges for future work that are revealed by this view.',
	 'authors': u'Abigail Z. Jacobs, Aaron Clauset,',
	 'category': u'Computer Science ',
	 'date': '2014-11-14',
	 'pdflink': u'http://arxiv.org/pdf/1411.4070',
	 'subjects': u'Machine Learning (stat.ML)',
	 'title': u'\nA unified view of generative models for networks: models, methods,  opportunities, and challenges',
	 'urllink': u'http://arxiv.org/abs/1411.4070'}
2015-04-10 09:28:03+0000 [xxu46_10] INFO: Crawled 386 pages (at 1 pages/min), scraped 379 items (at 1 items/min)
2015-04-10 09:29:03+0000 [xxu46_10] INFO: Crawled 386 pages (at 0 pages/min), scraped 379 items (at 0 items/min)
2015-04-10 09:29:09+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1411.4068> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 09:29:09+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1411.4068>
	{'abstract': u'Labeling data for classification requires significant human effort. To reduce labeling cost, instead of labeling every instance, a group of instances (bag) is labeled by a single bag label. Computer algorithms are then used to infer the label for each instance in a bag, a process referred to as instance annotation. This task is challenging due to the ambiguity regarding the instance labels. We propose a discriminative probabilistic model for the instance annotation problem and introduce an expectation maximization framework for inference, based on the maximum likelihood approach. For many probabilistic approaches, brute-force computation of the instance label posterior probability given its bag label is exponential in the number of instances in the bag. Our key contribution is a dynamic programming method for computing the posterior that is linear in the number of instances. We evaluate our methods using both benchmark and real world data sets, in the domain of bird song, image annotation, and activity recognition. In many cases, the proposed framework outperforms, sometimes significantly, the current state-of-the-art MIML learning methods, both in instance label prediction and bag label prediction.',
	 'authors': u'Anh T. Pham, Raviv Raich, Xiaoli Z. Fern,',
	 'category': u'Computer Science ',
	 'date': '2014-11-14',
	 'pdflink': u'http://arxiv.org/pdf/1411.4068',
	 'subjects': u'Machine Learning (stat.ML)',
	 'title': u'\nDynamic Programming for Instance Annotation in Multi-instance  Multi-label Learning',
	 'urllink': u'http://arxiv.org/abs/1411.4068'}
2015-04-10 09:29:46+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1411.4059> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 09:29:46+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1411.4059>
	{'abstract': u"As today's nanotechnology focus becomes primarily oriented toward production and manipulation of materials at the subatomic level, allowing the performance and complexity of interconnects where the device density accepts more than hundreds devices on a single chip, the manipulation of semiconductor nanostructures at the subatomic level sets its prime tasks on preserving and adequate transmission of information encoded in specified (quantum) states. The presented study employs the quantum communication protocol based on the hypergraph network model where the numerical solutions of equations of motion of quantum particles are associated to vertices (assembled with device chip), which follow specific controllable paths in the phase space. We address these findings towards ultimate quest for prediction and selective control of quantum particle trajectories. In addition, presented protocols could represent valuable tool for reducing background noise and uncertainty in low-dimensional and operationally meaningful, scalable complex systems.",
	 'authors': u'Vesna Berec,',
	 'category': u'Computer Science ',
	 'date': '2014-11-14',
	 'pdflink': u'http://arxiv.org/pdf/1411.4059',
	 'subjects': u'Quantum Physics (quant-ph)',
	 'title': u'\nPhase space dynamics and control of the quantum particles associated to  hypergraph states',
	 'urllink': u'http://arxiv.org/abs/1411.4059'}
2015-04-10 09:30:03+0000 [xxu46_10] INFO: Crawled 388 pages (at 2 pages/min), scraped 381 items (at 2 items/min)
2015-04-10 09:31:01+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1411.3990> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 09:31:01+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1411.3990>
	{'abstract': u'The modeling of cascading failure in power systems is difficult because of the many different mechanisms involved; no single model captures all of these mechanisms. Understanding the relative importance of these different mechanisms is an important step in choosing which mechanisms need to be modeled for particular types of cascading failure analysis. This work presents a dynamic simulation model of both power networks and protection systems, which can simulate a wider variety of cascading outage mechanisms, relative to existing quasi-steady state (QSS) models. The model allows one to test the impact of different load models and protections on cascading outage sizes. This paper describes each module of the developed dynamic model and demonstrates how different mechanisms interact. In order to test the model we simulated a batch of randomly selected contingencies for several different static load configurations, and found that the distribution of blackout sizes and event lengths from the proposed dynamic simulator correlates well with historical trends. The results also show that load models have significant impacts on the cascading risks. This dynamic model was also compared against a QSS model based on the dc power flow approximations; we find that the two models largely agree, but produce substantially different results for later stages of cascading.',
	 'authors': u'Jiajia Song, Eduardo Cotilla-Sanchez, Goodarz Ghanavati, Paul D. H. Hines,',
	 'category': u'Computer Science ',
	 'date': '2014-11-14',
	 'pdflink': u'http://arxiv.org/pdf/1411.3990',
	 'subjects': u'Physics and Society (physics.soc-ph)',
	 'title': u'\nDynamic Modeling of Cascading Failure in Power Systems',
	 'urllink': u'http://arxiv.org/abs/1411.3990'}
2015-04-10 09:31:03+0000 [xxu46_10] INFO: Crawled 389 pages (at 1 pages/min), scraped 382 items (at 1 items/min)
2015-04-10 09:32:03+0000 [xxu46_10] INFO: Crawled 389 pages (at 0 pages/min), scraped 382 items (at 0 items/min)
2015-04-10 09:32:10+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1411.3902> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 09:32:10+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1411.3902>
	{'abstract': u'We improve by an exponential factor the lower bound of K\x7forner and Muzi for the cardinality of the largest family of Hamilton paths in a complete graph of n vertices in which the union of any two paths has degree 4. The improvement is through an explicit construction while the previous bound was obtained by a greedy algorithm. We solve a similar problem for permutations up to an exponential factor.',
	 'authors': u'Janos Korner, Angelo Monti,',
	 'category': u'Computer Science ',
	 'date': '2014-11-14',
	 'pdflink': u'http://arxiv.org/pdf/1411.3902',
	 'subjects': u'Combinatorics (math.CO)',
	 'title': u'\nFamilies of locally separated Hamilton paths',
	 'urllink': u'http://arxiv.org/abs/1411.3902'}
2015-04-10 09:33:03+0000 [xxu46_10] INFO: Crawled 390 pages (at 1 pages/min), scraped 383 items (at 1 items/min)
2015-04-10 09:33:16+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1411.3834> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 09:33:16+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1411.3834>
	{'abstract': u"We introduce a high-performance virtual machine (VM) written in a numerically fast language like Fortran or C to evaluate very large expressions. We discuss the general concept of how to perform computations in terms of a VM and present specifically a VM that is able to compute tree-level cross sections for any number of external legs, given the corresponding byte code from the optimal matrix element generator, O'Mega. Furthermore, this approach allows to formulate the parallel computation of a single phase space point in a simple and obvious way. We analyze hereby the scaling behaviour with multiple threads as well as the benefits and drawbacks that are introduced with this method. Our implementation of a VM can run faster than the corresponding native, compiled code for certain processes and compilers, especially for very high multiplicities, and has in general runtimes in the same order of magnitude. By avoiding the tedious compile and link steps, which may fail for source code files of gigabyte sizes, new processes or complex higher order corrections that are currently out of reach could be evaluated with a VM given enough computing power.",
	 'authors': u'Bijan Chokoufe Nejad, Thorsten Ohl, J\xfcrgen Reuter,',
	 'category': u'Computer Science ',
	 'date': '2014-11-14',
	 'pdflink': u'http://arxiv.org/pdf/1411.3834',
	 'subjects': u'Computational Physics (physics.comp-ph)',
	 'title': u'\nSimple, Parallel, High-Performance Virtual Machines for Extreme  Computations',
	 'urllink': u'http://arxiv.org/abs/1411.3834'}
2015-04-10 09:34:03+0000 [xxu46_10] INFO: Crawled 391 pages (at 1 pages/min), scraped 384 items (at 1 items/min)
2015-04-10 09:34:04+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1411.3827> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 09:34:04+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1411.3827>
	{'abstract': u'We define the free autonomous category generated by a monoidal category and study some of its properties. From a linguistic perspective, this expands the range of possible models of meaning within the distributional compositional framework, by allowing nonlinearities in maps. From a categorical point of view, this provides a factorization of the construction in [Preller and Lambek, 2007] of the free autonomous category generated by a category.',
	 'authors': u'Antonin Delpeuch,',
	 'category': u'Computer Science ',
	 'date': '2014-11-14',
	 'pdflink': u'http://arxiv.org/pdf/1411.3827',
	 'subjects': u'Category Theory (math.CT)',
	 'title': u'\nAutonomization of Monoidal Categories',
	 'urllink': u'http://arxiv.org/abs/1411.3827'}
2015-04-10 09:34:52+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1411.3796> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 09:34:52+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1411.3796>
	{'abstract': u'This paper considers a model for cascades on random networks in which the cascade propagation at any node depends on the load at the failed neighbor, the degree of the neighbor as well as the load at that node. Each node in the network bears an initial load that is below the capacity of the node. The trigger for the cascade emanates at a single node or a small fraction of the nodes from some external shock. Upon failure, the load at the failed node gets divided randomly and added to the existing load at those neighboring nodes that have not yet failed. Subsequently, a neighboring node fails if its accumulated load exceeds its capacity. The failed node then plays no further part in the process. The cascade process stops as soon as the accumulated load at all nodes that have not yet failed is below their respective capacities. The model is shown to operate in two regimes, one in which the cascade terminates with only a finite number of node failures. In the other regime there is a positive probability that the cascade continues indefinitely. Bounds are obtained on the critical parameter where the phase transition occurs.',
	 'authors': u'Srikanth K. Iyer, Rahul Vaze, Dheeraj Narasimha,',
	 'category': u'Computer Science ',
	 'date': '2014-11-14',
	 'pdflink': u'http://arxiv.org/pdf/1411.3796',
	 'subjects': u'Physics and Society (physics.soc-ph)',
	 'title': u'\nAutoregressive Cascades on Random Networks',
	 'urllink': u'http://arxiv.org/abs/1411.3796'}
2015-04-10 09:35:03+0000 [xxu46_10] INFO: Crawled 393 pages (at 2 pages/min), scraped 386 items (at 2 items/min)
2015-04-10 09:35:55+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1411.3787> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 09:35:55+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1411.3787>
	{'abstract': u'Minwise hashing (Minhash) is a widely popular indexing scheme in practice. Minhash is designed for estimating set resemblance and is known to be suboptimal in many applications where the desired measure is set overlap (i.e., inner product between binary vectors) or set containment. Minhash has inherent bias towards smaller sets, which adversely affects its performance in applications where such a penalization is not desirable. In this paper, we propose asymmetric minwise hashing (MH-ALSH), to provide a solution to this problem. The new scheme utilizes asymmetric transformations to cancel the bias of traditional minhash towards smaller sets, making the final "collision probability" monotonic in the inner product. Our theoretical comparisons show that for the task of retrieving with binary inner products asymmetric minhash is provably better than traditional minhash and other recently proposed hashing algorithms for general inner products. Thus, we obtain an algorithmic improvement over existing approaches in the literature. Experimental evaluations on four publicly available high-dimensional datasets validate our claims and the proposed scheme outperforms, often significantly, other hashing algorithms on the task of near neighbor retrieval with set containment. Our proposal is simple and easy to implement in practice.',
	 'authors': u'Anshumali Shrivastava, Ping Li,',
	 'category': u'Computer Science ',
	 'date': '2014-11-14',
	 'pdflink': u'http://arxiv.org/pdf/1411.3787',
	 'subjects': u'Machine Learning (stat.ML)',
	 'title': u'\nAsymmetric Minwise Hashing',
	 'urllink': u'http://arxiv.org/abs/1411.3787'}
2015-04-10 09:36:03+0000 [xxu46_10] INFO: Crawled 394 pages (at 1 pages/min), scraped 387 items (at 1 items/min)
2015-04-10 09:36:56+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1411.3784> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 09:36:56+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1411.3784>
	{'abstract': u'We show that deep narrow Boltzmann machines are universal approximators of probability distributions on the activities of their visible units, provided they have sufficiently many hidden layers, each containing the same number of units as the visible layer. We show that, within certain parameter domains, deep Boltzmann machines can be studied as feedforward networks. We provide upper and lower bounds on the sufficient depth and width of universal approximators. These results settle various intuitions regarding undirected networks and, in particular, they show that deep narrow Boltzmann machines are at least as compact universal approximators as narrow sigmoid belief networks and restricted Boltzmann machines, with respect to the currently available bounds for those models.',
	 'authors': u'Guido Montufar,',
	 'category': u'Computer Science ',
	 'date': '2014-11-14',
	 'pdflink': u'http://arxiv.org/pdf/1411.3784',
	 'subjects': u'Machine Learning (stat.ML)',
	 'title': u'\nDeep Narrow Boltzmann Machines are Universal Approximators',
	 'urllink': u'http://arxiv.org/abs/1411.3784'}
2015-04-10 09:37:03+0000 [xxu46_10] INFO: Crawled 395 pages (at 1 pages/min), scraped 388 items (at 1 items/min)
2015-04-10 09:37:32+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1411.3757> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 09:37:32+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1411.3757>
	{'abstract': u'We consider the point process of signal strengths from transmitters in a wireless network observed from a fixed position under models with general signal path loss and random propagation effects. We show via coupling arguments that under general conditions this point process of signal strengths can be well-approximated by an inhomogeneous Poisson or a Cox point processes on the positive real line. We also provide some bounds on the total variation distance between the laws of these point processes and both Poisson and Cox point processes. Under appropriate conditions, these results support the use of a spatial Poisson point process for the underlying positioning of transmitters in models of wireless networks, even if in reality the positioning does not appear Poisson. We apply the results to a number of models with popular choices for positioning of transmitters, path loss functions, and distributions of propagation effects.',
	 'authors': u'Holger Paul Keeler, Nathan Ross, Aihua Xia,',
	 'category': u'Computer Science ',
	 'date': '2014-11-13',
	 'pdflink': u'http://arxiv.org/pdf/1411.3757',
	 'subjects': u'Probability (math.PR)',
	 'title': u'\nWhen do wireless network signals appear Poisson?',
	 'urllink': u'http://arxiv.org/abs/1411.3757'}
2015-04-10 09:38:03+0000 [xxu46_10] INFO: Crawled 396 pages (at 1 pages/min), scraped 389 items (at 1 items/min)
2015-04-10 09:38:36+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1411.3708> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 09:38:36+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1411.3708>
	{'abstract': u'In the collective-risk social dilemma, players lose their personal endowments if contributions to the common pool are too small. This fact alone, however, does not always deter selfish individuals from defecting. The temptations to free-ride on the prosocial efforts of others are strong because we are hardwired to maximize our own fitness regardless of the consequences this might have for the public good. Here we show that the addition of risky assets to the personal endowments, both of which are lost if the collective target is not reached, can contribute to solving the collective-risk social dilemma. In infinite well-mixed populations risky assets introduce new stable and unstable mixed steady states, whereby the stable mixed steady state converges to full cooperation as either the risk of collective failure or the amount of risky assets increases. Similarly, in finite well-mixed populations the introduction of risky assets enforces configurations where cooperative behavior thrives. In structured populations cooperation is promoted as well, but the distribution of assets amongst the groups is crucial. Surprisingly, we find that the completely rational allocation of assets only to the most successful groups is not optimal, and this regardless of whether the risk of collective failure is high or low. Instead, in low-risk situations bounded rational allocation of assets works best, while in high-risk situations the simplest uniform distribution of assets among all the groups is optimal. These results indicate that prosocial behavior depends sensitively on the potential losses individuals are likely to endure if they fail to cooperate.',
	 'authors': u'Xiaojie Chen, Yanling Zhang, Ting-Zhu Huang, Matjaz Perc,',
	 'category': u'Computer Science ',
	 'date': '2014-11-13',
	 'pdflink': u'http://arxiv.org/pdf/1411.3708',
	 'subjects': u'Physics and Society (physics.soc-ph)',
	 'title': u'\nSolving the collective-risk social dilemma with risky assets in  well-mixed and structured populations',
	 'urllink': u'http://arxiv.org/abs/1411.3708'}
2015-04-10 09:39:03+0000 [xxu46_10] INFO: Crawled 397 pages (at 1 pages/min), scraped 390 items (at 1 items/min)
2015-04-10 09:39:32+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1411.3662> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 09:39:32+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1411.3662>
	{'abstract': u'Massive Open Online Courses (MOOCs) bring together a global crowd of thousands of learners for several weeks or months. In theory, the openness and scale of MOOCs can promote iterative dialogue that facilitates group cognition and knowledge construction. Using data from two successive instances of a popular business strategy MOOC, we filter observed communication patterns to arrive at the "significant" interaction networks between learners and use complex network analysis to explore the vulnerability and information diffusion potential of the discussion forums. We find that different discussion topics and pedagogical practices promote varying levels of 1) "significant" peer-to-peer engagement, 2) participant inclusiveness in dialogue, and ultimately, 3) modularity, which impacts information diffusion to prevent a truly "global" exchange of knowledge and learning. These results indicate the structural limitations of large-scale crowd-based learning and highlight the different ways that learners in MOOCs leverage, and learn within, social contexts. We conclude by exploring how these insights may inspire new developments in online education.',
	 'authors': u'Nabeel Gillani, Taha Yasseri, Rebecca Eynon, Isis Hjorth,',
	 'category': u'Computer Science ',
	 'date': '2014-11-13',
	 'pdflink': u'http://arxiv.org/pdf/1411.3662',
	 'subjects': u'Physics and Society (physics.soc-ph)',
	 'title': u'\nStructural limitations of learning in a crowd: communication  vulnerability and information diffusion in MOOCs',
	 'urllink': u'http://arxiv.org/abs/1411.3662'}
2015-04-10 09:40:03+0000 [xxu46_10] INFO: Crawled 398 pages (at 1 pages/min), scraped 391 items (at 1 items/min)
2015-04-10 09:40:31+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1411.3617> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 09:40:31+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1411.3617>
	{'abstract': u'In the original Gilbert model of random geometric graphs, nodes are placed according to a Poisson process, and links formed between those within a fixed range. Motivated by wireless network applications "soft" or "probabilistic" connection models have recently been introduced, involving a "connection function" H(r) that gives the probability that two nodes at distance r directly connect. In many applications, not only in wireless networks, it is desirable that the graph is fully connected, that is every node is connected to every other node in a multihop fashion. Here, the full connection probability of a dense network in a convex polygonal or polyhedral domain is expressed in terms of contributions from boundary components, for a very general class of connection functions. It turns out that only a few quantities such as moments of the connection function appear. Good agreement is found with connection functions used in previous studies and with numerical simulations.',
	 'authors': u'Carl P. Dettmann, Orestis Georgiou,',
	 'category': u'Computer Science ',
	 'date': '2014-11-13',
	 'pdflink': u'http://arxiv.org/pdf/1411.3617',
	 'subjects': u'Statistical Mechanics (cond-mat.stat-mech)',
	 'title': u'\nConnectivity of networks with general connection functions',
	 'urllink': u'http://arxiv.org/abs/1411.3617'}
2015-04-10 09:41:03+0000 [xxu46_10] INFO: Crawled 399 pages (at 1 pages/min), scraped 392 items (at 1 items/min)
2015-04-10 09:41:16+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1411.3601> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 09:41:16+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1411.3601>
	{'abstract': u'An constant--dimension subspace code, , is a collection of --dimensional projective subspaces of such that every --dimensional projective subspace of is contained in at most a member of . Constant--dimension subspace codes gained recently lot of interest due to the work by Koetter and Kschischang, where they presented an application of such codes for error-correction in random network coding. Here a constant--dimension subspace code is constructed, for every . The size of our codes is considerably larger than all known constructions so far, whenever . When a further improvement is provided by constructing an constant--dimension subspace code, with .',
	 'authors': u'Antonio Cossidente, Francesco Pavese,',
	 'category': u'Computer Science ',
	 'date': '2014-11-13',
	 'pdflink': u'http://arxiv.org/pdf/1411.3601',
	 'subjects': u'Combinatorics (math.CO)',
	 'title': u'\nSubspace codes in PG(2n-1,q)',
	 'urllink': u'http://arxiv.org/abs/1411.3601'}
2015-04-10 09:42:03+0000 [xxu46_10] INFO: Crawled 400 pages (at 1 pages/min), scraped 393 items (at 1 items/min)
2015-04-10 09:42:11+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1411.3530> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 09:42:11+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1411.3530>
	{'abstract': u'We introduce a family of multi-way Cheeger-type constants on a signed graph such that if and only if has balanced connected components. These constants are switching invariant and bring together in a unified viewpoint a number of important graph-theoretical concepts, including the classical Cheeger constant, the non-bipartiteness parameter of Desai and Rao, the bipartiteness ratio of Trevisan, the dual Cheeger constant of Bauer and Jost on unsigned graphs, and the frustration index (originally called the line index of balance by Harary) on signed graphs. We further unify the (higher-order or improved) Cheeger and dual Cheeger inequalities for unsigned graphs as well as the underlying algorithmic proof techniques by establishing their corresponding versions on signed graphs. In particular, we develop a spectral clustering method for finding almost-balanced subgraphs, each defining a sparse cut. The proper metric for such a clustering is the metric on a real projective space. We also prove estimates of the extremal eigenvalues of signed Laplace matrix in terms of number of signed triangles (-cycles).',
	 'authors': u'Fatihcan M. Atay, Shiping Liu,',
	 'category': u'Computer Science ',
	 'date': '2014-11-13',
	 'pdflink': u'http://arxiv.org/pdf/1411.3530',
	 'subjects': u'Combinatorics (math.CO)',
	 'title': u'\nCheeger constants, structural balance, and spectral clustering analysis  for signed graphs',
	 'urllink': u'http://arxiv.org/abs/1411.3530'}
2015-04-10 09:43:03+0000 [xxu46_10] INFO: Crawled 401 pages (at 1 pages/min), scraped 394 items (at 1 items/min)
2015-04-10 09:43:04+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1411.3489> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 09:43:04+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1411.3489>
	{'abstract': u"This paper provides a proof of concept for an EEG-based evolution of a visual image which is on a user's mind. Our approach is based on the Rapid Serial Visual Presentation (RSVP) of polygon primitives and Brain-Computer Interface (BCI) technology. The presentation of polygons that contribute to build a target image (because they match the shape and/or color of the target) trigger attention-related EEG patterns. Accordingly, these target primitives can be determined using BCI classification of Event-Related Potentials (ERPs). They are then accumulated in the display until a satisfactory reconstruction is reached. Each selection step had an average classification accuracy of about . An in-depth investigation suggests that most of the misclassification were not misinterpretations of the BCI concerning the users' intent, but rather stemmed from the fact that the users tried to select different polygons than the experimenter wanted them to select. Open problems and alternatives to develop a practical BCI- based image reconstruction application are discussed.",
	 'authors': u'Lu\xeds F Seoane, Stephan Gabler, Benjamin Blankertz,',
	 'category': u'Computer Science ',
	 'date': '2014-11-13',
	 'pdflink': u'http://arxiv.org/pdf/1411.3489',
	 'subjects': u'Neurons and Cognition (q-bio.NC)',
	 'title': u'\nImages from the Mind: BCI image evolution based on RSVP of polygon  primitives',
	 'urllink': u'http://arxiv.org/abs/1411.3489'}
2015-04-10 09:44:03+0000 [xxu46_10] INFO: Crawled 402 pages (at 1 pages/min), scraped 395 items (at 1 items/min)
2015-04-10 09:44:13+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1411.3444> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 09:44:13+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1411.3444>
	{'abstract': u"We propose a model that generates a new class of networks exhibiting power-law degree distribution with a spectrum of exponents depending on the number of links () with which incoming nodes join the existing network. Unlike the Barab 'si-Albert (BA) model, each new node first picks an existing node at random, and connects not with this but with of its neighbors also picked at random. Counterintuitively enough, such a mediation-driven attachment rule results not only in preferential but super-preferential attachment, albeit in disguise. We show that for small , the dynamics of our model is governed by winners take all phenomenon, and for higher it is governed by winners take some. Besides, we show that the mean of the inverse harmonic mean of degrees of the neighborhood of all existing nodes is a measure that can well qualify how straight the degree distribution is.",
	 'authors': u'Kamrul Hassan, Liana Islam,',
	 'category': u'Computer Science ',
	 'date': '2014-11-13',
	 'pdflink': u'http://arxiv.org/pdf/1411.3444',
	 'subjects': u'Physics and Society (physics.soc-ph)',
	 'title': u'\nGrowing Scale-free Networks by a Mediation-Driven Attachment Rule',
	 'urllink': u'http://arxiv.org/abs/1411.3444'}
2015-04-10 09:45:03+0000 [xxu46_10] INFO: Crawled 403 pages (at 1 pages/min), scraped 396 items (at 1 items/min)
2015-04-10 09:45:06+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1411.3436> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 09:45:06+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1411.3436>
	{'abstract': u'We describe and analyze a new boosting algorithm for deep learning called SelfieBoost. Unlike other boosting algorithms, like AdaBoost, which construct ensembles of classifiers, SelfieBoost boosts the accuracy of a single network. We prove a convergence rate for SelfieBoost under some "SGD success" assumption which seems to hold in practice.',
	 'authors': u'Shai Shalev-Shwartz,',
	 'category': u'Computer Science ',
	 'date': '2014-11-13',
	 'pdflink': u'http://arxiv.org/pdf/1411.3436',
	 'subjects': u'Machine Learning (stat.ML)',
	 'title': u'\nSelfieBoost: A Boosting Algorithm for Deep Learning',
	 'urllink': u'http://arxiv.org/abs/1411.3436'}
2015-04-10 09:46:03+0000 [xxu46_10] INFO: Crawled 404 pages (at 1 pages/min), scraped 397 items (at 1 items/min)
2015-04-10 09:46:23+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1411.3413> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 09:46:23+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1411.3413>
	{'abstract': u'We propose a nonparametric Bayesian probabilistic latent variable model for multi-view anomaly detection, which is the task of finding instances that have inconsistent views. With the proposed model, all views of a non-anomalous instance are assumed to be generated from a single latent vector. On the other hand, an anomalous instance is assumed to have multiple latent vectors, and its different views are generated from different latent vectors. By inferring the number of latent vectors used for each instance with Dirichlet process priors, we obtain multi-view anomaly scores. The proposed model can be seen as a robust extension of probabilistic canonical correlation analysis for noisy multi-view data. We present Bayesian inference procedures for the proposed model based on a stochastic EM algorithm. The effectiveness of the proposed model is demonstrated in terms of performance when detecting multi-view anomalies and imputing missing values in multi-view data with anomalies.',
	 'authors': u'Tomoharu Iwata, Makoto Yamada,',
	 'category': u'Computer Science ',
	 'date': '2014-11-13',
	 'pdflink': u'http://arxiv.org/pdf/1411.3413',
	 'subjects': u'Machine Learning (stat.ML)',
	 'title': u'\nMulti-view Anomaly Detection via Probabilistic Latent Variable Models',
	 'urllink': u'http://arxiv.org/abs/1411.3413'}
2015-04-10 09:47:03+0000 [xxu46_10] INFO: Crawled 405 pages (at 1 pages/min), scraped 398 items (at 1 items/min)
2015-04-10 09:47:35+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1411.3409> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 09:47:35+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1411.3409>
	{'abstract': u'We present RandomizedCCA, a randomized algorithm for computing canonical analysis, suitable for large datasets stored either out of core or on a distributed file system. Accurate results can be obtained in as few as two data passes, which is relevant for distributed processing frameworks in which iteration is expensive (e.g., Hadoop). The strategy also provides an excellent initializer for standard iterative solutions.',
	 'authors': u'Paul Mineiro, Nikos Karampatziakis,',
	 'category': u'Computer Science ',
	 'date': '2014-11-13',
	 'pdflink': u'http://arxiv.org/pdf/1411.3409',
	 'subjects': u'Machine Learning (stat.ML)',
	 'title': u'\nA Randomized Algorithm for CCA',
	 'urllink': u'http://arxiv.org/abs/1411.3409'}
2015-04-10 09:48:03+0000 [xxu46_10] INFO: Crawled 406 pages (at 1 pages/min), scraped 399 items (at 1 items/min)
2015-04-10 09:48:19+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1411.3359> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 09:48:19+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1411.3359>
	{'abstract': u'Let be a family of contractive similitudes on satisfying the open set condition. Let be a probability vector with for all . We study the asymptotic geometric mean errors , in the quantization for the in-homogeneous self-similar measure associated with the condensation system . We focus on the following two independent cases: (I) is a self-similar measure on associated with ; (II) is a self-similar measure associated with another family of contractive similitudes on satisfying the open set condition and satisfies a version of in-homogeneous open set condition. We show that, in both cases, the quantization dimension of of order zero exists and agrees with that of , which is independent of the probability vector . We determine the convergence order of ; namely, for , there exists a constant , such that [ D^n^ leq e_( mu) leq D n^, n geq 1. ]',
	 'authors': u'Sanguo Zhu, Youming Zhou, Yongjian Sheng,',
	 'category': u'Computer Science ',
	 'date': '2014-10-23',
	 'pdflink': u'http://arxiv.org/pdf/1411.3359',
	 'subjects': u'Dynamical Systems (math.DS)',
	 'title': u'\nAsymptotics of the geometric mean error for in-homogeneous self-similar  measures',
	 'urllink': u'http://arxiv.org/abs/1411.3359'}
2015-04-10 09:49:03+0000 [xxu46_10] INFO: Crawled 407 pages (at 1 pages/min), scraped 400 items (at 1 items/min)
2015-04-10 09:49:38+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1411.3334> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 09:49:38+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1411.3334>
	{'abstract': u'We describe a general method for turning quantum circuits into sparse quantum subsystem codes. Using this prescription, we can map an arbitrary stabilizer code into a new subsystem code with the same distance and number of encoded qubits but where all the generators have constant weight, at the cost of adding some ancilla qubits. With an additional overhead of ancilla qubits, the new code can also be made spatially local. Applying our construction to certain concatenated stabilizer codes yields families of subsystem codes with constant-weight generators and with minimum distance , where . For spatially local codes in dimensions we nearly saturate a bound due to Bravyi and Terhal and achieve . Previously the best code distance achievable with constant-weight generators in any dimension, due to Freedman, Meyer and Luo, was for a stabilizer code.',
	 'authors': u'Dave Bacon, Steven T. Flammia, Aram W. Harrow, Jonathan Shi,',
	 'category': u'Computer Science ',
	 'date': '2014-11-12',
	 'pdflink': u'http://arxiv.org/pdf/1411.3334',
	 'subjects': u'Quantum Physics (quant-ph)',
	 'title': u'\nSparse Quantum Codes from Quantum Circuits',
	 'urllink': u'http://arxiv.org/abs/1411.3334'}
2015-04-10 09:50:03+0000 [xxu46_10] INFO: Crawled 408 pages (at 1 pages/min), scraped 401 items (at 1 items/min)
2015-04-10 09:50:13+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1411.3317> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 09:50:13+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1411.3317>
	{'abstract': u'We investigate algorithms to find the first vertex in large trees generated by either the uniform attachment or preferential attachment model. We require the algorithm to output a set of vertices, such that, with probability at least , the first vertex is in this set. We show that for any , there exist such algorithms with independent of the size of the input tree. Moreover, we provide almost tight bounds for the best value of as a function of . In the uniform attachment case we show that the optimal is subpolynomial in , and that it has to be at least superpolylogarithmic. On the other hand, the preferential attachment case is exponentially harder, as we prove that the best is polynomial in . We conclude the paper with several open problems.',
	 'authors': u'S\xe9bastien Bubeck, Luc Devroye, G\xe1bor Lugosi,',
	 'category': u'Computer Science ',
	 'date': '2014-11-12',
	 'pdflink': u'http://arxiv.org/pdf/1411.3317',
	 'subjects': u'Probability (math.PR)',
	 'title': u'\nFinding Adam in random growing trees',
	 'urllink': u'http://arxiv.org/abs/1411.3317'}
2015-04-10 09:51:03+0000 [xxu46_10] INFO: Crawled 409 pages (at 1 pages/min), scraped 402 items (at 1 items/min)
2015-04-10 09:51:20+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1411.3140> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 09:51:20+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1411.3140>
	{'abstract': u'Recent wide-spread adoption of electronic and pervasive technologies has enabled the study of human behavior at an unprecedented level, uncovering universal patterns underlying human activity, mobility, and inter-personal communication. In the present work, we investigate whether deviations from these universal patterns may reveal information about the socio-economical status of geographical regions. We quantify the extent to which deviations in diurnal rhythm, mobility patterns, and communication styles across regions relate to their unemployment incidence. For this we examine a country-scale publicly articulated social media dataset, where we quantify individual behavioral features from over 145 million geo-located messages distributed among more than 340 different Spanish economic regions, inferred by computing communities of cohesive mobility fluxes. We find that regions exhibiting more diverse mobility fluxes, earlier diurnal rhythms, and more correct grammatical styles display lower unemployment rates. As a result, we provide a simple model able to produce accurate, easily interpretable reconstruction of regional unemployment incidence from their social-media digital fingerprints alone. Our results show that cost-effective economical indicators can be built based on publicly-available social media datasets.',
	 'authors': u'Alejandro Llorente, Manuel Garcia-Herranz, Manuel Cebrian, Esteban Moro,',
	 'category': u'Computer Science ',
	 'date': '2014-11-12',
	 'pdflink': u'http://arxiv.org/pdf/1411.3140',
	 'subjects': u'Physics and Society (physics.soc-ph)',
	 'title': u'\nSocial media fingerprints of unemployment',
	 'urllink': u'http://arxiv.org/abs/1411.3140'}
2015-04-10 09:52:03+0000 [xxu46_10] INFO: Crawled 410 pages (at 1 pages/min), scraped 403 items (at 1 items/min)
2015-04-10 09:52:07+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1411.3047> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 09:52:07+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1411.3047>
	{'abstract': u'An edge colouring of a graph is called acyclic if it is proper and every cycle contains at least three colours. We show that for every , there exists a such that if has girth at least then admits an acyclic edge colouring with at most colours.',
	 'authors': u'Xing Shi Cai, Guillem Perarnau, Bruce Reed, Adam Bene Watts,',
	 'category': u'Computer Science ',
	 'date': '2014-11-12',
	 'pdflink': u'http://arxiv.org/pdf/1411.3047',
	 'subjects': u'Combinatorics (math.CO)',
	 'title': u'\nAcyclic edge colourings of graphs with large girth',
	 'urllink': u'http://arxiv.org/abs/1411.3047'}
2015-04-10 09:53:03+0000 [xxu46_10] INFO: Crawled 411 pages (at 1 pages/min), scraped 404 items (at 1 items/min)
2015-04-10 09:53:20+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1411.3035> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 09:53:20+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1411.3035>
	{'abstract': u'We propose a notion of state distinguishability that does not refer to probabilities, but rather to the ability of a set of states to serve as programs for a desired set of gates. Using this notion, we reconstruct the structural features of the task of state discrimination, such as the equivalence with cloning and the impossibility to extract information from two non-distinguishable pure states without causing a disturbance. All these features express intrinsic links among operational tasks, which are valid independently of the particular theory under consideration.',
	 'authors': u'Giulio Chiribella,',
	 'category': u'Computer Science ',
	 'date': '2014-11-12',
	 'pdflink': u'http://arxiv.org/pdf/1411.3035',
	 'subjects': u'Quantum Physics (quant-ph)',
	 'title': u'\nDistinguishability and copiability of programs in general process  theories',
	 'urllink': u'http://arxiv.org/abs/1411.3035'}
2015-04-10 09:54:03+0000 [xxu46_10] INFO: Crawled 412 pages (at 1 pages/min), scraped 405 items (at 1 items/min)
2015-04-10 09:54:21+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1411.2940> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 09:54:21+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1411.2940>
	{'abstract': u'We have extended the domain-specific language UFL, the Unified Form Language, to support symbolic product operations on finite elements. This allows representation of scalar- and vector-valued finite element spaces on quadrilateral, triangular prism and hexahedral cells. In particular, we concentrate on spaces that correspond to scalar and vector identifications of tensor product finite element differential forms. This includes fully continuous, curl-conforming, div-conforming, and discontinuous spaces in both two and three dimensions. We have made corresponding extensions to FIAT, the FInite element Automatic Tabulator, to enable numerical tabulation of these spaces. This facilitates the automatic generation of low-level code that carries out local assembly operations, within the wider context of solving finite element problems posed over such function spaces. We have done this work within the code-generation pipeline of the software package Firedrake; we make use of the full Firedrake package in this paper to present numerical examples.',
	 'authors': u'Andrew T. T. McRae, Gheorge-Teodor Bercea, Lawrence Mitchell, David A. Ham, Colin J. Cotter,',
	 'category': u'Computer Science ',
	 'date': '2014-11-11',
	 'pdflink': u'http://arxiv.org/pdf/1411.2940',
	 'subjects': u'Numerical Analysis (math.NA)',
	 'title': u'\nAutomated generation and symbolic manipulation of tensor product finite  elements',
	 'urllink': u'http://arxiv.org/abs/1411.2940'}
2015-04-10 09:55:03+0000 [xxu46_10] INFO: Crawled 413 pages (at 1 pages/min), scraped 406 items (at 1 items/min)
2015-04-10 09:55:21+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1411.2846> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 09:55:21+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1411.2846>
	{'abstract': u'Based on the computation of a superset of the implicit support, implicitization of a parametrically given hyper-surface is reduced to computing the nullspace of a numeric matrix. Our approach exploits the sparseness of the given parametric equations and of the implicit polynomial. In this work, we study how this interpolation matrix can be used to reduce some key geometric predicates on the hyper-surface to simple numerical operations on the matrix, namely membership and sidedness for given query points. We illustrate our results with examples based on our Maple implementation.',
	 'authors': u'Ioannis Emiris, Tatjana Kalinka, Christos Konaxis,',
	 'category': u'Computer Science ',
	 'date': '2014-11-11',
	 'pdflink': u'http://arxiv.org/pdf/1411.2846',
	 'subjects': u'Algebraic Geometry (math.AG)',
	 'title': u'\nSparse implicitization by interpolation: Geometric computations using  matrix representations',
	 'urllink': u'http://arxiv.org/abs/1411.2846'}
2015-04-10 09:56:03+0000 [xxu46_10] INFO: Crawled 414 pages (at 1 pages/min), scraped 407 items (at 1 items/min)
2015-04-10 09:56:28+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1411.2721> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 09:56:28+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1411.2721>
	{'abstract': u'Community detection is of considerable importance for analyzing the structure and function of complex networks. Many real-world networks may possess community structures at multiple scales, and recently, various multi-resolution methods were proposed to identify the community structures at different scales. In this paper, we present a type of multi-resolution methods by using the generalized self-loop rescaling strategy. The self-loop rescaling strategy provides one uniform ansatz for the design of multi-resolution community detection methods. Many quality functions for community detection can be unified in the framework of the self-loop rescaling. The resulting multi-resolution quality functions can be optimized directly using the existing modularity-optimization algorithms. Several derived multi-resolution methods are applied to the analysis of community structures in several synthetic and real-world networks. The results show that these methods can find the pre-defined substructures in synthetic networks and real splits observed in real-world networks. Finally, we give a discussion on the methods themselves and their relationship. We hope that the study in the paper can be helpful for the understanding of the multi-resolution methods and provide useful insight into designing new community detection methods.',
	 'authors': u'Ju Xiang, Yan-Ni Tang, Yuan-Yuan Gao, Yan Zhang, Ke Deng, Xiao-Ke Xu, Ke Hu,',
	 'category': u'Computer Science ',
	 'date': '2014-11-11',
	 'pdflink': u'http://arxiv.org/pdf/1411.2721',
	 'subjects': u'Physics and Society (physics.soc-ph)',
	 'title': u'\nMulti-resolution community detection based on generalized self-loop  rescaling strategy',
	 'urllink': u'http://arxiv.org/abs/1411.2721'}
2015-04-10 09:57:03+0000 [xxu46_10] INFO: Crawled 415 pages (at 1 pages/min), scraped 408 items (at 1 items/min)
2015-04-10 09:57:30+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1411.2720> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 09:57:30+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1411.2720>
	{'abstract': u'The present work studies quantum and classical correlations in three qubits and four qubits general Bell states, produced by operating with braid operators on the computational basis of states. The analogies between the general three qubits and four qubits Bell states and that of two qubits Bell states are discussed. The general Bell states are shown to be maximal entangled, i.e., with quantum correlations which are lost by tracing these states over one qubit, remaining only with classical correlations, as shown by HS decomposition.',
	 'authors': u'Yacob Ben-Aryeh,',
	 'category': u'Computer Science ',
	 'date': '2014-11-11',
	 'pdflink': u'http://arxiv.org/pdf/1411.2720',
	 'subjects': u'Quantum Physics (quant-ph)',
	 'title': u'\nQuantum and classical correlations in Bell three and four qubits,  related to Hilbert-Schmidt decomposition',
	 'urllink': u'http://arxiv.org/abs/1411.2720'}
2015-04-10 09:58:03+0000 [xxu46_10] INFO: Crawled 416 pages (at 1 pages/min), scraped 409 items (at 1 items/min)
2015-04-10 09:58:48+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1411.2674> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 09:58:48+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1411.2674>
	{'abstract': u'We present the Bayesian Echo Chamber, a new Bayesian generative model for social interaction data. By modeling the evolution of people\'s language usage over time, this model discovers latent influence relationships between them. Unlike previous work on inferring influence, which has primarily focused on simple temporal dynamics evidenced via turn-taking behavior, our model captures more nuanced influence relationships, evidenced via linguistic accommodation patterns in interaction content. The model, which is based on a discrete analog of the multivariate Hawkes process, permits a fully Bayesian inference algorithm. We validate our model\'s ability to discover latent influence patterns using transcripts of arguments heard by the US Supreme Court and the movie "12 Angry Men." We showcase our model\'s capabilities by using it to infer latent influence patterns from Federal Open Market Committee meeting transcripts, demonstrating state-of-the-art performance at uncovering social dynamics in group discussions.',
	 'authors': u'Fangjian Guo, Charles Blundell, Hanna Wallach, Katherine Heller,',
	 'category': u'Computer Science ',
	 'date': '2014-11-11',
	 'pdflink': u'http://arxiv.org/pdf/1411.2674',
	 'subjects': u'Machine Learning (stat.ML)',
	 'title': u'\nThe Bayesian Echo Chamber: Modeling Social Influence via Linguistic  Accommodation',
	 'urllink': u'http://arxiv.org/abs/1411.2674'}
2015-04-10 09:59:03+0000 [xxu46_10] INFO: Crawled 417 pages (at 1 pages/min), scraped 410 items (at 1 items/min)
2015-04-10 09:59:56+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1411.2668> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 09:59:56+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1411.2668>
	{'abstract': u'Pyrosequencing is emerging as one of the important next-generation sequencing technologies. We derive the statistical distributions of this technique in terms of nucleotide probabilities of the target sequences. We give exact distributions both for fixed number of flow cycles and for fixed sequence length. Explicit formulas are derived for the mean and variance of these distributions. In both cases, the distributions can be approximated accurately by normal distributions with the same mean and variance. The statistical distributions will be useful for instrument and software development for pyrosequencing platforms.',
	 'authors': u'Yong Kong,',
	 'category': u'Computer Science ',
	 'date': '2014-11-6',
	 'pdflink': u'http://arxiv.org/pdf/1411.2668',
	 'subjects': u'Genomics (q-bio.GN)',
	 'title': u'\nStatistical distributions of pyrosequencing',
	 'urllink': u'http://arxiv.org/abs/1411.2668'}
2015-04-10 10:00:03+0000 [xxu46_10] INFO: Crawled 418 pages (at 1 pages/min), scraped 411 items (at 1 items/min)
2015-04-10 10:00:47+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1411.2636> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 10:00:47+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1411.2636>
	{'abstract': u'Given empirical evidence for the dependence of an outcome variable on an exposure variable, we can typically only provide bounds for the "probability of causation" in the case of an individual who has developed the outcome after being exposed. We show how these bounds can be adapted or improved if further information becomes available. In addition to reviewing existing work on this topic, we provide a new analysis for the case where a mediating variable can be observed. In particular we show how the probability of causation can be bounded when there is no direct effect and no confounding. Keywords: Causal inference, Mediation Analysis, Probability of Causation',
	 'authors': u'A. P. Dawid, R. Murtas, M. Musio,',
	 'category': u'Computer Science ',
	 'date': '2014-11-10',
	 'pdflink': u'http://arxiv.org/pdf/1411.2636',
	 'subjects': u'Statistics Theory (math.ST)',
	 'title': u'\nBounding the Probability of Causation in Mediation Analysis',
	 'urllink': u'http://arxiv.org/abs/1411.2636'}
2015-04-10 10:01:03+0000 [xxu46_10] INFO: Crawled 419 pages (at 1 pages/min), scraped 412 items (at 1 items/min)
2015-04-10 10:02:01+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1411.2581> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 10:02:01+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1411.2581>
	{'abstract': u'We describe textit (DEFs), a class of latent variable models that are inspired by the hidden structures used in deep neural networks. DEFs capture a hierarchy of dependencies between latent variables, and are easily generalized to many settings through exponential families. We perform inference using recent "black box" variational inference techniques. We then evaluate various DEFs on text and combine multiple DEFs into a model for pairwise recommendation data. In an extensive study, we show that going beyond one layer improves predictions for DEFs. We demonstrate that DEFs find interesting exploratory structure in large data sets, and give better predictive performance than state-of-the-art models.',
	 'authors': u'Rajesh Ranganath, Linpeng Tang, Laurent Charlin, David M. Blei,',
	 'category': u'Computer Science ',
	 'date': '2014-11-10',
	 'pdflink': u'http://arxiv.org/pdf/1411.2581',
	 'subjects': u'Machine Learning (stat.ML)',
	 'title': u'\nDeep Exponential Families',
	 'urllink': u'http://arxiv.org/abs/1411.2581'}
2015-04-10 10:02:03+0000 [xxu46_10] INFO: Crawled 420 pages (at 1 pages/min), scraped 413 items (at 1 items/min)
2015-04-10 10:03:03+0000 [xxu46_10] INFO: Crawled 420 pages (at 0 pages/min), scraped 413 items (at 0 items/min)
2015-04-10 10:03:21+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1411.2549> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 10:03:21+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1411.2549>
	{'abstract': u'Pyrosequencing is one of the important next-generation sequencing technologies. We derive the distribution of the number of positive signals in pyrograms of this sequencing technology as a function of flow cycle numbers and nucleotide probabilities of the target sequences. As for the distribution of sequence length, we also derive the distribution of positive signals for the fixed flow cycle model. Explicit formulas are derived for the mean and variance of the distributions. A simple result for the mean of the distribution is that the mean number of positive signals in a pyrogram is approximately twice the number of flow cycles, regardless of nucleotide probabilities. The statistical distributions will be useful for instrument and software development for pyrosequencing and other related platforms.',
	 'authors': u'Yong Kong,',
	 'category': u'Computer Science ',
	 'date': '2014-11-6',
	 'pdflink': u'http://arxiv.org/pdf/1411.2549',
	 'subjects': u'Genomics (q-bio.GN)',
	 'title': u'\nDistributions of positive signals in pyrosequencing',
	 'urllink': u'http://arxiv.org/abs/1411.2549'}
2015-04-10 10:04:03+0000 [xxu46_10] INFO: Crawled 421 pages (at 1 pages/min), scraped 414 items (at 1 items/min)
2015-04-10 10:04:39+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1411.2548> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 10:04:39+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1411.2548>
	{'abstract': u'Sequencing by synthesis is the underlying technology for many next-generation DNA sequencing platforms. We developed a new model, the fixed flow cycle model, to derive the distributions of sequence length for a given number of flow cycles under the general conditions where the nucleotide incorporation is probabilistic and may be incomplete, as in some single-molecule sequencing technologies. Unlike the previous model, the new model yields the probability distribution for the sequence length. Explicit closed form formulas are derived for the mean and variance of the distribution.',
	 'authors': u'Yong Kong,',
	 'category': u'Computer Science ',
	 'date': '2014-11-6',
	 'pdflink': u'http://arxiv.org/pdf/1411.2548',
	 'subjects': u'Genomics (q-bio.GN)',
	 'title': u'\nLength distribution of sequencing by synthesis: fixed flow cycle model',
	 'urllink': u'http://arxiv.org/abs/1411.2548'}
2015-04-10 10:05:03+0000 [xxu46_10] INFO: Crawled 422 pages (at 1 pages/min), scraped 415 items (at 1 items/min)
2015-04-10 10:05:46+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1411.2547> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 10:05:46+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1411.2547>
	{'abstract': u'Sequencing by synthesis is used in many next-generation DNA sequencing technologies. Some of the technologies, especially those exploring the principle of single-molecule sequencing, allow incomplete nucleotide incorporation in each cycle. We derive statistical distributions for sequencing by synthesis by taking into account the possibility that nucleotide incorporation may not be complete in each flow cycle. The statistical distributions are expressed in terms of nucleotide probabilities of the target sequences and the nucleotide incorporation probabilities for each nucleotide. We give exact distributions both for fixed number of flow cycles and for fixed sequence length. Explicit formulas are derived for the mean and variance of these distributions. The results are generalizations of our previous work for pyrosequencing. Incomplete nucleotide incorporation leads to significant change in the mean and variance of the distributions, but still they can be approximated by normal distributions with the same mean and variance. The results are also generalized to handle sequence context dependent incorporation. The statistical distributions will be useful for instrument and software development for sequencing by synthesis platforms.',
	 'authors': u'Yong Kong,',
	 'category': u'Computer Science ',
	 'date': '2014-11-6',
	 'pdflink': u'http://arxiv.org/pdf/1411.2547',
	 'subjects': u'Genomics (q-bio.GN)',
	 'title': u'\nStatistical distributions of sequencing by synthesis with probabilistic  nucleotide incorporation',
	 'urllink': u'http://arxiv.org/abs/1411.2547'}
2015-04-10 10:06:03+0000 [xxu46_10] INFO: Crawled 423 pages (at 1 pages/min), scraped 416 items (at 1 items/min)
2015-04-10 10:06:37+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1411.2484> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 10:06:37+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1411.2484>
	{'abstract': u'Parallel supercomputer-based Monte Carlo applications depend on pseudorandom number generators that produce independent pseudorandom streams across many separate processes. We propose a new scalable class of parallel pseudorandom number generators based on Pohlig--Hellman exponentiation ciphers. The method generates uniformly distributed floating point pseudorandom streams by encrypting simple sequences of integer textit into textit by exponentiation modulo prime numbers. The advantages of the method are: the method is trivially parallelizable by parameterization with each pseudorandom number generator derived from an independent prime modulus, the method is fully scalable on massively parallel computing clusters due to the large number of primes available for each implementation, the seeding and initialization of the independent streams is simple, the method requires only a few integer multiply--mod operations per pseudorandom number, the state of each instance is defined by only a few integer values, the period of each instance is different, and the method passes a battery of intrastream and interstream correlation tests using up to pseudorandom numbers per test. The 32-bit implementation we propose has millions of possible instances, all with periods greater than . A 64-bit implementation depends on 128-bit arithmetic, but would have more than possible instances and periods greater than .',
	 'authors': u'Paul D. Beale,',
	 'category': u'Computer Science ',
	 'date': '2014-11-10',
	 'pdflink': u'http://arxiv.org/pdf/1411.2484',
	 'subjects': u'Computational Physics (physics.comp-ph)',
	 'title': u'\nA new class of scalable parallel pseudorandom number generators based on  Pohlig-Hellman exponentiation ciphers',
	 'urllink': u'http://arxiv.org/abs/1411.2484'}
2015-04-10 10:07:03+0000 [xxu46_10] INFO: Crawled 424 pages (at 1 pages/min), scraped 417 items (at 1 items/min)
2015-04-10 10:07:47+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1411.2427> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 10:07:47+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1411.2427>
	{'abstract': u'We address the general problem of how best to attack and destroy a network by node removal, given limited or no prior information about the edges. We consider a family of strategies in which nodes are randomly chosen, but not removed. Instead, a random acquaintance (i.e., a first neighbour) of the chosen node is removed from the network. By assigning an informal cost to the information about the network structure, we show using cost-benefit analysis that acquaintance removal is the optimal strategy to destroy networks efficiently.',
	 'authors': u'T. M. Vieira, G. M. Viswanathan, L. R. da Silva,',
	 'category': u'Computer Science ',
	 'date': '2014-11-10',
	 'pdflink': u'http://arxiv.org/pdf/1411.2427',
	 'subjects': u'Physics and Society (physics.soc-ph)',
	 'title': u'\nHow to efficiently destroy a network with limited information',
	 'urllink': u'http://arxiv.org/abs/1411.2427'}
2015-04-10 10:08:03+0000 [xxu46_10] INFO: Crawled 425 pages (at 1 pages/min), scraped 418 items (at 1 items/min)
2015-04-10 10:08:48+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1411.2419> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 10:08:48+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1411.2419>
	{'abstract': u"We study rational numbers with purely periodic R 'enyi -expansions. For bases satisfying with dividing , we give a necessary and sufficient condition for , i.e., that all rational numbers with have a purely periodic -expansion. A simple algorithm for determining the value of for all quadratic Pisot numbers is described.",
	 'authors': u'Tom\xe1\u0161 Hejda, Wolfgang Steiner,',
	 'category': u'Computer Science ',
	 'date': '2014-11-10',
	 'pdflink': u'http://arxiv.org/pdf/1411.2419',
	 'subjects': u'Dynamical Systems (math.DS)',
	 'title': u'\nBeta-expansions of rational numbers in quadratic Pisot bases',
	 'urllink': u'http://arxiv.org/abs/1411.2419'}
2015-04-10 10:09:03+0000 [xxu46_10] INFO: Crawled 426 pages (at 1 pages/min), scraped 419 items (at 1 items/min)
2015-04-10 10:09:43+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1411.2384> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 10:09:43+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1411.2384>
	{'abstract': u'The effectiveness of fiscal policy for emissions reductions in private passenger road transport depends on its ability to incentivise consumers to make choices oriented towards lower emissions vehicles. However, car purchase choices are known to be strongly socially determined, and this sector is highly diverse due to significant socio-economic differences between consumer groups. Here, we present a comprehensive dataset and analysis of the structure of the 2012 private passenger vehicle fleet-years in six major economies across the World (UK, USA, China, India, Japan and Brazil) in terms of price, engine size and emissions distributions. We argue that aggregate choices and corresponding elasticities of substitution under changes of fiscal policy can be estimated, with uncertainty, using this data. This enables to evaluate the likely effectiveness of potential fiscal and technological change policies on fleet-year emissions reductions. We provide tools to do so based on the distributive structure of prices and emissions in segments of a diverse market, both for conventional as well as unconventional engine technologies. We find that markets differ significantly between nations, and that correlations between engine sizes, emissions and prices exist strongly in some markets and not strongly in others. We furthermore find that markets for unconventional engine technologies have patchy coverages of varying levels. These findings are interpreted in terms of policy strategy.',
	 'authors': u'J.-F. Mercure, A. Lam,',
	 'category': u'Computer Science ',
	 'date': '2014-11-10',
	 'pdflink': u'http://arxiv.org/pdf/1411.2384',
	 'subjects': u'Physics and Society (physics.soc-ph)',
	 'title': u'\nThe effectiveness of policy on consumer choices for private road  passenger transport emissions reductions in six major economies',
	 'urllink': u'http://arxiv.org/abs/1411.2384'}
2015-04-10 10:10:03+0000 [xxu46_10] INFO: Crawled 427 pages (at 1 pages/min), scraped 420 items (at 1 items/min)
2015-04-10 10:11:01+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1411.2377> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 10:11:01+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1411.2377>
	{'abstract': u'We evaluate the performance of the Krylov subspace method by using highly efficient multiple precision sparse matrix-vector multiplication (SpMV). BNCpack is our multiple precision numerical computation library based on MPFR/GMP, which is one of the most efficient arbitrary precision floating-point arithmetic libraries. However, it does not include functions that can manipulate multiple precision sparse matrices. Therefore, by using benchmark tests, we show that SpMV implemented in these functions can be more efficient. Finally, we also show that product-type Krylov subspace methods such as BiCG and GPBiCG in which we have embedded SpMV, can efficiently solve large-scale linear systems of equations provided in the UF sparse matrix collections in a memory-restricted computing environment.',
	 'authors': u'Tomonori Kouya,',
	 'category': u'Computer Science ',
	 'date': '2014-11-10',
	 'pdflink': u'http://arxiv.org/pdf/1411.2377',
	 'subjects': u'Numerical Analysis (math.NA)',
	 'title': u'\nA Highly Efficient Implementation of Multiple Precision Sparse  Matrix-Vector Multiplication and Its Application to Product-type Krylov  Subspace Methods',
	 'urllink': u'http://arxiv.org/abs/1411.2377'}
2015-04-10 10:11:03+0000 [xxu46_10] INFO: Crawled 428 pages (at 1 pages/min), scraped 421 items (at 1 items/min)
2015-04-10 10:11:56+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1411.2337> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 10:11:56+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1411.2337>
	{'abstract': u'Multi-task learning (MTL) improves prediction performance in different contexts by learning models jointly on multiple different, but related tasks. Network data, which are a priori data with a rich relational structure, provide an important context for applying MTL. In particular, the explicit relational structure implies that network data is not i.i.d. data. Network data also often comes with significant metadata (i.e., attributes) associated with each entity (node). Moreover, due to the diversity and variation in network data (e.g., multi-relational links or multi-category entities), various tasks can be performed and often a rich correlation exists between them. Learning algorithms should exploit all of these additional sources of information for better performance. In this work we take a metric-learning point of view for the MTL problem in the network context. Our approach builds on structure preserving metric learning (SPML). In particular SPML learns a Mahalanobis distance metric for node attributes using network structure as supervision, so that the learned distance function encodes the structure and can be used to predict link patterns from attributes. SPML is described for single-task learning on single network. Herein, we propose a multi-task version of SPML, abbreviated as MT-SPML, which is able to learn across multiple related tasks on multiple networks via shared intermediate parametrization. MT-SPML learns a specific metric for each task and a common metric for all tasks. The task correlation is carried through the common metric and the individual metrics encode task specific information. When combined together, they are structure-preserving with respect to individual tasks. MT-SPML works on general networks, thus is suitable for a wide variety of problems. In experiments, we challenge MT-SPML on two real-word problems, where MT-SPML achieves significant improvement.',
	 'authors': u'Chen Fang, Daniel N. Rockmore,',
	 'category': u'Computer Science ',
	 'date': '2014-11-10',
	 'pdflink': u'http://arxiv.org/pdf/1411.2337',
	 'subjects': u'Machine Learning (stat.ML)',
	 'title': u'\nMulti-Task Metric Learning on Network Data',
	 'urllink': u'http://arxiv.org/abs/1411.2337'}
2015-04-10 10:12:03+0000 [xxu46_10] INFO: Crawled 429 pages (at 1 pages/min), scraped 422 items (at 1 items/min)
2015-04-10 10:13:03+0000 [xxu46_10] INFO: Crawled 429 pages (at 0 pages/min), scraped 422 items (at 0 items/min)
2015-04-10 10:13:05+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1411.2331> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 10:13:05+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1411.2331>
	{'abstract': u'We propose a feature selection method that finds non-redundant features from a large and high-dimensional data in nonlinear way. Specifically, we propose a nonlinear extension of the non-negative least-angle regression (LARS) called NLARS, where the similarity between input and output is measured through the normalized version of the Hilbert-Schmidt Independence Criterion (HSIC). An advantage of NLARS is that it can easily incorporate with map-reduce frameworks such as Hadoop and Spark. Thus, with the help of distributed computing, a set of features can be efficiently selected from a large and high-dimensional data. Moreover, NLARS is a convex method and can find a global optimum solution. The effectiveness of the proposed method is first demonstrated through feature selection experiments for classification and regression with small and high-dimensional datasets. Finally, we evaluate our proposed method over a large and high-dimensional biology dataset.',
	 'authors': u'Makoto Yamada, Avishek Saha, Hua Ouyang, Dawei Yin, Yi Chang,',
	 'category': u'Computer Science ',
	 'date': '2014-11-10',
	 'pdflink': u'http://arxiv.org/pdf/1411.2331',
	 'subjects': u'Machine Learning (stat.ML)',
	 'title': u'\nN$^3$LARS: Minimum Redundancy Maximum Relevance Feature Selection for  Large and High-dimensional Data',
	 'urllink': u'http://arxiv.org/abs/1411.2331'}
2015-04-10 10:14:03+0000 [xxu46_10] INFO: Crawled 430 pages (at 1 pages/min), scraped 423 items (at 1 items/min)
2015-04-10 10:14:13+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1411.2315> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 10:14:13+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1411.2315>
	{'abstract': u'We study the problem of constructing multi-source extractors in the quantum setting, which extract almost uniform random bits against quantum side information collected from several initially independent classical random sources. This is a natural generalization of seeded randomness extraction against quantum side information and classical independent source extraction. With new challenges such as potential entanglement in the side information, it is not a prior clear under what conditions do quantum multi-source extractors exist; the only previous work is [KK12], where the classical inner-product two-source extractors of [CG88] and [DEOR04] are shown to be quantum secure in the restricted Independent Adversary (IA) Model and entangled Bounded Storage (BS) Model. In this paper we propose a new model called General Entangled (GE) Adversary Model, which allows arbitrary entanglement in the side information and subsumes both the IA model and the BS model. We proceed to show how to construct GE-secure quantum multi-source extractors. To that end, we propose another model called One-sided Adversary (OA) Model, which is weaker than all the above models. Somewhat surprisingly, we establish equivalence between strong OA-security and strong GE-security. As a result, all classical multi-source extractors can either directly work, or be modified to work in the GE model at the cost of one extra random source. Thus, our constructions essentially match the best known constructions of classical multi-source extractors. We also apply our techniques to two important problems in cryptography and distributed computing --- privacy amplification and network extractor. We show that as long as the sources have certain amounts of conditional min-entropy in our GE model (even with entangled quantum side information), we can design very efficient privacy amplification protocols and network extractors.',
	 'authors': u'Kai-Min Chung, Xin Li, Xiaodi Wu,',
	 'category': u'Computer Science ',
	 'date': '2014-11-10',
	 'pdflink': u'http://arxiv.org/pdf/1411.2315',
	 'subjects': u'Quantum Physics (quant-ph)',
	 'title': u'\nMulti-Source Randomness Extractors Against Quantum Side Information, and  their Applications',
	 'urllink': u'http://arxiv.org/abs/1411.2315'}
2015-04-10 10:15:03+0000 [xxu46_10] INFO: Crawled 431 pages (at 1 pages/min), scraped 424 items (at 1 items/min)
2015-04-10 10:15:07+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1411.2223> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 10:15:07+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1411.2223>
	{'abstract': u'Kepler GTX Titan Black and Kepler Tesla K40 are still the best GPUs for high performance computing, although Maxwell GPUs such as GTX 980 are available in the market. Hence, we measure the performance of our lattice QCD codes using the Kepler GPUs. We also upgrade our code to use the latest CPS (Columbia Physics System) library along with the most recent QUDA (QCD CUDA) library for lattice QCD. These new libraries improve the performance of our conjugate gradient (CG) inverter so that it runs twice faster than before. We also investigate the performance of Xeon Phi 7120P coprocessor. It has similar computing power with the Kepler GPUs in principle. However, its performance for our CG code is significantly inferior to that of the GTX Titan Black GPUs at present.',
	 'authors': u'Yong-Chull Jang, Hwancheol Jeong, Jangho Kim, Weonjong Lee, Jeonghwan Pak, Yuree Chung,',
	 'category': u'Computer Science ',
	 'date': '2014-11-9',
	 'pdflink': u'http://arxiv.org/pdf/1411.2223',
	 'subjects': u'High Energy Physics - Lattice (hep-lat)',
	 'title': u'\nCode Optimization on Kepler GPUs and Xeon Phi',
	 'urllink': u'http://arxiv.org/abs/1411.2223'}
2015-04-10 10:16:03+0000 [xxu46_10] INFO: Crawled 432 pages (at 1 pages/min), scraped 425 items (at 1 items/min)
2015-04-10 10:16:27+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1411.2158> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 10:16:27+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1411.2158>
	{'abstract': u'Biological and social systems consist of myriad interacting units. The interactions can be intuitively represented in the form of a graph or network. Measurements of these graphs can reveal the underlying structure of these interactions, which provides insight into the systems that generated the graphs. Moreover, in applications such as neuroconnectomics, social networks, and genomics, graph data is accompanied by contextualizing measures on each node. We leverage these node covariates to help uncover latent communities in a graph, using a modification of spectral clustering. Statistical guarantees are provided under a joint mixture model that we call the Node Contextualized Stochastic Blockmodel, including a bound on the mis-clustering rate. For most simulated conditions, covariate assisted spectral clustering yields superior results relative to both regularized spectral clustering without node covariates and an adaptation of canonical correlation analysis. We apply covariate assisted spectral clustering to large brain graphs derived from diffusion MRI data, using the node locations or neurological region membership as covariates. In both cases, covariate assisted spectral clustering yields clusters that are easier to interpret neurologically.',
	 'authors': u'Norbert Binkiewicz, Joshua T. Vogelstein, Karl Rohe,',
	 'category': u'Computer Science ',
	 'date': '2014-11-8',
	 'pdflink': u'http://arxiv.org/pdf/1411.2158',
	 'subjects': u'Machine Learning (stat.ML)',
	 'title': u'\nCovariate Assisted Spectral Clustering',
	 'urllink': u'http://arxiv.org/abs/1411.2158'}
2015-04-10 10:17:03+0000 [xxu46_10] INFO: Crawled 433 pages (at 1 pages/min), scraped 426 items (at 1 items/min)
2015-04-10 10:17:39+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1411.2111> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 10:17:39+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1411.2111>
	{'abstract': u'The revolution of hydraulic fracturing has dramatically increased the supply and lowered the cost of natural gas in the United States driving an expansion of natural gas-fired generation capacity in many electrical grids. Unrelated to the natural gas expansion, lower capital costs and renewable portfolio standards are driving an expansion of intermittent renewable generation capacity such as wind and photovoltaic generation. These two changes may potentially combine to create new threats to the reliability of these interdependent energy infrastructures. Natural gas-fired generators are often used to balance the fluctuating output of wind generation. However, the time-varying output of these generators results in time-varying natural gas burn rates that impact the pressure in interstate transmission pipelines. Fluctuating pressure impacts the reliability of natural gas deliveries to those same generators and the safety of pipeline operations. We adopt a partial differential equation model of natural gas pipelines and use this model to explore the effect of intermittent wind generation on the fluctuations of pressure in natural gas pipelines. The mean square pressure fluctuations are found to grow linearly in time with points of maximum deviation occurring at the locations of flow reversals.',
	 'authors': u'Michael Chertkov, Vladimir Lebedev, Scott Backhaus,',
	 'category': u'Computer Science ',
	 'date': '2014-11-8',
	 'pdflink': u'http://arxiv.org/pdf/1411.2111',
	 'subjects': u'Physics and Society (physics.soc-ph)',
	 'title': u'\nCascading of Fluctuations in Interdependent Energy Infrastructures:  Gas-Grid Coupling',
	 'urllink': u'http://arxiv.org/abs/1411.2111'}
2015-04-10 10:18:03+0000 [xxu46_10] INFO: Crawled 434 pages (at 1 pages/min), scraped 427 items (at 1 items/min)
2015-04-10 10:18:27+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1411.2066> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 10:18:27+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1411.2066>
	{'abstract': u'We focus on the distribution regression problem: regressing to vector-valued outputs from probability measures. Many important machine learning and statistical tasks fit into this framework, including multi-instance learning or point estimation problems without analytical solution such as hyperparameter or entropy estimation. Despite the large number of available heuristics in the literature, the inherent two-stage sampled nature of the problem family makes the theoretical analysis quite challenging: in practice only samples from sampled distributions are observable, and the estimates have to rely on similarities computed between sets of points. To the best of our knowledge, the only existing technique with consistency guarantees for distribution regression requires kernel density estimation as an intermediate step (which often performs poorly in practice), and the domain of the distributions to be compact Euclidean. In this paper, we study a simple, analytically computable, ridge regression based alternative to distribution regression: we embed the distributions to a reproducing kernel Hilbert space, and learn the regressor from the embeddings to the outputs. Our main contribution is to show that this scheme is consistent in the two-stage sampled setup under mild conditions (on separable topological domains enriched with kernels). Specifically, we answer a 15-year-old open question: we establish the consistency of the classical set kernel [Haussler, 1999; Gaertner et. al, 2002] in regression, and cover more recent kernels on distributions, including those due to [Christmann and Steinwart, 2010].',
	 'authors': u'Zoltan Szabo, Bharath Sriperumbudur, Barnabas Poczos, Arthur Gretton,',
	 'category': u'Computer Science ',
	 'date': '2014-11-8',
	 'pdflink': u'http://arxiv.org/pdf/1411.2066',
	 'subjects': u'Statistics Theory (math.ST)',
	 'title': u'\nLearning Theory for Distribution Regression',
	 'urllink': u'http://arxiv.org/abs/1411.2066'}
2015-04-10 10:19:03+0000 [xxu46_10] INFO: Crawled 435 pages (at 1 pages/min), scraped 428 items (at 1 items/min)
2015-04-10 10:19:23+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1411.2031> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 10:19:23+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1411.2031>
	{'abstract': u'The Astrophysics Source Code Library (ASCL; ascl.net) is a free online registry of codes used in astronomy research; it currently contains over 900 codes and is indexed by ADS. The ASCL has recently moved a new infrastructure into production. The new site provides a true database for the code entries and integrates the WordPress news and information pages and the discussion forum into one site. Previous capabilities are retained and permalinks to ascl.net continue to work. This improvement offers more functionality and flexibility than the previous site, is easier to maintain, and offers new possibilities for collaboration. This presentation covers these recent changes to the ASCL.',
	 'authors': u'Robert J. Hanisch, Alice Allen, G. Bruce Berriman, Kimberly DuPrie, Jessica Mink, Robert J. Nemiroff, Judy Schmidt, Lior Shamir, Keith Shortridge, Mark Taylor, Peter J. Teuben, John Wallin,',
	 'category': u'Computer Science ',
	 'date': '2014-11-7',
	 'pdflink': u'http://arxiv.org/pdf/1411.2031',
	 'subjects': u'Instrumentation and Methods for Astrophysics (astro-ph.IM)',
	 'title': u'\nAstrophysics Source Code Library Enhancements',
	 'urllink': u'http://arxiv.org/abs/1411.2031'}
2015-04-10 10:20:03+0000 [xxu46_10] INFO: Crawled 436 pages (at 1 pages/min), scraped 429 items (at 1 items/min)
2015-04-10 10:20:37+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1411.1973> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 10:20:37+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1411.1973>
	{'abstract': u'Lagrangian duality in mixed integer optimization is a useful framework for problems decomposition and for producing tight lower bounds to the optimal objective, but in contrast to the convex counterpart, it is generally unable to produce optimal solutions directly. In fact, solutions recovered from the dual may be not only suboptimal, but even infeasible. In this paper we concentrate on large scale mixed--integer programs with a specific structure that is of practical interest, as it appears in a variety of application domains such as power systems or supply chain management. We propose a solution method for these structures, in which the primal problem is modified in a certain way, guaranteeing that the solutions produced by the corresponding dual are feasible for the original unmodified primal problem. The modification is simple to implement and the method is amenable to distributed computations. We also demonstrate that the quality of the solutions recovered using our procedure improves as the problem size increases, making it particularly useful for large scale instances for which commercial solvers are inadequate. We illustrate the efficacy of our method with extensive experimentations on a problem stemming from power systems.',
	 'authors': u'Robin Vujanic, Peyman Mohajerin Esfahani, Paul Goulart, Sebastien Mariethoz, Manfred Morari,',
	 'category': u'Computer Science ',
	 'date': '2014-11-7',
	 'pdflink': u'http://arxiv.org/pdf/1411.1973',
	 'subjects': u'Optimization and Control (math.OC)',
	 'title': u'\nA Decomposition Method for Large Scale MILPs, with Performance  Guarantees and a Power System Application',
	 'urllink': u'http://arxiv.org/abs/1411.1973'}
2015-04-10 10:21:03+0000 [xxu46_10] INFO: Crawled 437 pages (at 1 pages/min), scraped 430 items (at 1 items/min)
2015-04-10 10:21:54+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1411.1924> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 10:21:54+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1411.1924>
	{'abstract': u"We show that the behaviour of Bitcoin has interesting similarities to stock and precious metal markets, such as gold and silver. We report that whilst Litecoin, the second largest cryptocurrency, closely follows Bitcoin's behaviour, it does not show all the reported properties of Bitcoin. Agreements between apparently disparate complexity measures have been found, and it is shown that statistical, information-theoretic, algorithmic and fractal measures have different but interesting capabilities of clustering families of markets by type. The report is particularly interesting because of the range and novel use of some measures of complexity to characterize price behaviour, because of the IRS designation of Bitcoin as an investment property and not a currency, and the announcement of the Canadian government's own electronic currency MintChip.",
	 'authors': u'Daniel Wilson-Nunn, Hector Zenil,',
	 'category': u'Computer Science ',
	 'date': '2014-11-7',
	 'pdflink': u'http://arxiv.org/pdf/1411.1924',
	 'subjects': u'Statistical Finance (q-fin.ST)',
	 'title': u'\nOn the Complexity and Behaviour of Cryptocurrencies Compared to Other  Markets',
	 'urllink': u'http://arxiv.org/abs/1411.1924'}
2015-04-10 10:22:03+0000 [xxu46_10] INFO: Crawled 438 pages (at 1 pages/min), scraped 431 items (at 1 items/min)
2015-04-10 10:22:59+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1411.1810> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 10:22:59+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1411.1810>
	{'abstract': u'Stochastic variational inference (SVI) maps posterior inference in latent variable models to non-convex stochastic optimization. While they enable approximate posterior inference for many otherwise intractable models, variational inference methods suffer from local optima. We introduce deterministic annealing for SVI to overcome this issue. We introduce a temperature parameter that deterministically deforms the objective, and then reduce this parameter over the course of the optimization. Initially it encourages high entropy variational distributions, which we find eases convergence to better optima. We test our method with Latent Dirichlet Allocation on three large corpora. Compared to SVI, we show improved predictive likelihoods on held-out data.',
	 'authors': u'Farhan Abrol, Stephan Mandt, Rajesh Ranganath, David Blei,',
	 'category': u'Computer Science ',
	 'date': '2014-11-7',
	 'pdflink': u'http://arxiv.org/pdf/1411.1810',
	 'subjects': u'Machine Learning (stat.ML)',
	 'title': u'\nDeterministic Annealing for Stochastic Variational Inference',
	 'urllink': u'http://arxiv.org/abs/1411.1810'}
2015-04-10 10:23:03+0000 [xxu46_10] INFO: Crawled 439 pages (at 1 pages/min), scraped 432 items (at 1 items/min)
2015-04-10 10:24:03+0000 [xxu46_10] INFO: Crawled 439 pages (at 0 pages/min), scraped 432 items (at 0 items/min)
2015-04-10 10:24:09+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1411.1804> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 10:24:09+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1411.1804>
	{'abstract': u'Beta process is the standard nonparametric Bayesian prior for latent factor model. In this paper, we derive a structured mean-field variational inference algorithm for a beta process non-negative matrix factorization (NMF) model with Poisson likelihood. Unlike the linear Gaussian model, which is well-studied in the nonparametric Bayesian literature, NMF model with beta process prior does not enjoy the conjugacy. We leverage the recently developed stochastic structured mean-field variational inference to relax the conjugacy constraint and restore the dependencies among the latent variables in the approximating variational distribution. Preliminary results on both synthetic and real examples demonstrate that the proposed inference algorithm can reasonably recover the hidden structure of the data.',
	 'authors': u'Dawen Liang, Matthew D. Hoffman,',
	 'category': u'Computer Science ',
	 'date': '2014-11-7',
	 'pdflink': u'http://arxiv.org/pdf/1411.1804',
	 'subjects': u'Machine Learning (stat.ML)',
	 'title': u'\nBeta Process Non-negative Matrix Factorization with Stochastic  Structured Mean-Field Variational Inference',
	 'urllink': u'http://arxiv.org/abs/1411.1804'}
2015-04-10 10:25:03+0000 [xxu46_10] INFO: Crawled 440 pages (at 1 pages/min), scraped 433 items (at 1 items/min)
2015-04-10 10:25:13+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1411.1743> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 10:25:13+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1411.1743>
	{'abstract': u'Containing the spreading of crime in urban societies remains a major challenge. Empirical evidence suggests that, left unchecked, crimes may be recurrent and proliferate. On the other hand, eradicating a culture of crime may be difficult, especially under extreme social circumstances that impair the creation of a shared sense of social responsibility. Although our understanding of the mechanisms that drive the emergence and diffusion of crime is still incomplete, recent research highlights applied mathematics and methods of statistical physics as valuable theoretical resources that may help us better understand criminal activity. We review different approaches aimed at modeling and improving our understanding of crime, focusing on the nucleation of crime hotspots using partial differential equations, self-exciting point process and agent-based modeling, adversarial evolutionary games, and the network science behind the formation of gangs and large-scale organized crime. We emphasize that statistical physics of crime can relevantly inform the design of successful crime prevention strategies, as well as improve the accuracy of expectations about how different policing interventions should impact malicious human activity deviating from social norms. We also outline possible directions for future research, related to the effects of social and coevolving networks and to the hierarchical growth of criminal structures due to self-organization.',
	 'authors': u"Maria R. D'Orsogna, Matjaz Perc,",
	 'category': u'Computer Science ',
	 'date': '2014-11-6',
	 'pdflink': u'http://arxiv.org/pdf/1411.1743',
	 'subjects': u'Physics and Society (physics.soc-ph)',
	 'title': u'\nStatistical physics of crime: A review',
	 'urllink': u'http://arxiv.org/abs/1411.1743'}
2015-04-10 10:26:03+0000 [xxu46_10] INFO: Crawled 441 pages (at 1 pages/min), scraped 434 items (at 1 items/min)
2015-04-10 10:26:18+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1411.1723> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 10:26:18+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1411.1723>
	{'abstract': u"When opinions, behaviors or ideas diffuse within a population, some are invariably stickier than others. The stickier the opinion, behavior or idea, the greater is an individual's inertia to replace it with an alternative. Here we study the effect of stickiness of opinions in a two-opinion model, where individuals change their opinion only after a certain number of consecutive encounters with the alternative opinion. Assuming that one opinion has a fixed stickiness, we investigate how the critical size of the competing opinion required to tip over the entire population varies as a function of the competing opinion's stickiness. We analyze this scenario for the case of a complete-graph topology through simulations, and through a semi-analytical approach which yields an upper bound for the critical minority size. We present analogous simulation results for the case of the Erd Hs-R 'enyi random network. Finally, we investigate the coarsening properties of sticky opinion spreading on two-dimensional lattices, and show that the presence of stickiness gives rise to an effective surface tension that causes the coarsening behavior to become curvature-driven.",
	 'authors': u'C. Doyle, S. Sreenivasan, B. K. Szymanski, G. Korniss,',
	 'category': u'Computer Science ',
	 'date': '2014-11-6',
	 'pdflink': u'http://arxiv.org/pdf/1411.1723',
	 'subjects': u'Physics and Society (physics.soc-ph)',
	 'title': u'\nSocial consensus and tipping points with opinion inertia',
	 'urllink': u'http://arxiv.org/abs/1411.1723'}
2015-04-10 10:27:03+0000 [xxu46_10] INFO: Crawled 442 pages (at 1 pages/min), scraped 435 items (at 1 items/min)
2015-04-10 10:27:31+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1411.1652> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 10:27:31+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1411.1652>
	{'abstract': u"A new bound (Theorem ref) for the duration of the chip-firing game with chips on a -vertex graph is obtained, by a careful analysis of the pseudo-inverse of the discrete Laplacian matrix of the graph. This new bound is expressed in terms of the entries of the pseudo-inverse. It is shown (Section 5) to be always better than the classic bound due to Bjrner, Lov 'sz and Shor. In some cases the improvement is dramatic. For instance: for strongly regular graphs the classic and the new bounds reduce to and , respectively. For dense regular graphs - - the classic and the new bounds reduce to and , respectively. This is a snapshot of a work in progress, so further results in this vein are in the works.",
	 'authors': u'Felix Goldberg,',
	 'category': u'Computer Science ',
	 'date': '2014-11-6',
	 'pdflink': u'http://arxiv.org/pdf/1411.1652',
	 'subjects': u'Combinatorics (math.CO)',
	 'title': u'\nChip-firing may be much faster than you think',
	 'urllink': u'http://arxiv.org/abs/1411.1652'}
2015-04-10 10:28:03+0000 [xxu46_10] INFO: Crawled 443 pages (at 1 pages/min), scraped 436 items (at 1 items/min)
2015-04-10 10:28:43+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1411.1611> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 10:28:43+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1411.1611>
	{'abstract': u"We first review recent versions of Barbalat's Lemma by comparing their proofs and by discussing a concrete example. Then we present a proof which allows for a quantitative interpretation.",
	 'authors': u'B\xe1lint Farkas, Sven-Ake Wegner,',
	 'category': u'Computer Science ',
	 'date': '2014-11-6',
	 'pdflink': u'http://arxiv.org/pdf/1411.1611',
	 'subjects': u'Optimization and Control (math.OC)',
	 'title': u"\nVariations on Barbalat's Lemma",
	 'urllink': u'http://arxiv.org/abs/1411.1611'}
2015-04-10 10:29:03+0000 [xxu46_10] INFO: Crawled 444 pages (at 1 pages/min), scraped 437 items (at 1 items/min)
2015-04-10 10:30:02+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1411.1582> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 10:30:02+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1411.1582>
	{'abstract': u'In the context of multiplayer games, the parallel repetition problem can be phrased as follows: given a game with optimal winning probability and its repeated version (in which games are played together, in parallel), can the players use strategies that are substantially better than ones in which each game is played independently? This question is relevant in physics for the study of correlations and plays an important role in computer science in the context of complexity and cryptography. In this work the case of multiplayer non-signalling games is considered, i.e., the only restriction on the players is that they are not allowed to communicate during the game. For complete-support games (games where all possible combinations of questions have non-zero probability to be asked) with any number of players we prove a threshold theorem stating that the probability that non-signalling players win more than a fraction of the games is exponentially small in , for every . For games with incomplete support we derive a similar statement, for a slightly modified form of repetition. The result is proved using a new technique, based on a recent de Finetti theorem, which allows us to avoid central technical difficulties that arise in standard proofs of parallel repetition theorems.',
	 'authors': u'Rotem Arnon-Friedman, Renato Renner, Thomas Vidick,',
	 'category': u'Computer Science ',
	 'date': '2014-11-6',
	 'pdflink': u'http://arxiv.org/pdf/1411.1582',
	 'subjects': u'Quantum Physics (quant-ph)',
	 'title': u'\nNon-signalling parallel repetition using de Finetti reductions',
	 'urllink': u'http://arxiv.org/abs/1411.1582'}
2015-04-10 10:30:03+0000 [xxu46_10] INFO: Crawled 445 pages (at 1 pages/min), scraped 438 items (at 1 items/min)
2015-04-10 10:31:03+0000 [xxu46_10] INFO: Crawled 445 pages (at 0 pages/min), scraped 438 items (at 0 items/min)
2015-04-10 10:31:09+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1411.1537> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 10:31:09+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1411.1537>
	{'abstract': u"Determinantal point processes (DPPs) offer a powerful approach to modeling diversity in many applications where the goal is to select a diverse subset. We study the problem of learning the parameters (the kernel matrix) of a DPP from labeled training data. We make two contributions. First, we show how to reparameterize a DPP's kernel matrix with multiple kernel functions, thus enhancing modeling flexibility. Second, we propose a novel parameter estimation technique based on the principle of large margin separation. In contrast to the state-of-the-art method of maximum likelihood estimation, our large-margin loss function explicitly models errors in selecting the target subsets, and it can be customized to trade off different types of errors (precision vs. recall). Extensive empirical studies validate our contributions, including applications on challenging document and video summarization, where flexibility in modeling the kernel matrix and balancing different errors is indispensable.",
	 'authors': u'Boqing Gong, Wei-lun Chao, Kristen Grauman, Fei Sha,',
	 'category': u'Computer Science ',
	 'date': '2014-11-6',
	 'pdflink': u'http://arxiv.org/pdf/1411.1537',
	 'subjects': u'Machine Learning (stat.ML)',
	 'title': u'\nLarge-Margin Determinantal Point Processes',
	 'urllink': u'http://arxiv.org/abs/1411.1537'}
2015-04-10 10:32:03+0000 [xxu46_10] INFO: Crawled 446 pages (at 1 pages/min), scraped 439 items (at 1 items/min)
2015-04-10 10:32:27+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1411.1432> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 10:32:27+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1411.1432>
	{'abstract': u'In this study, we consider the simulation of subsurface flow and solute transport processes in the stationary limit. In the convection-dominant case, the numerical solution of the transport problem may exhibit non-physical diffusion and under- and overshoots. For an interior penalty discontinuous Galerkin (DG) discretization, we present a -adaptive refinement strategy and, alternatively, a new efficient approach for reducing numerical under- and overshoots using a diffusive -projection. Furthermore, we illustrate an efficient way of solving the linear system arising from the DG discretization. In -D and -D examples, we compare the DG-based methods to the streamline diffusion approach with respect to computing time and their ability to resolve steep fronts.',
	 'authors': u'A.Q.T. Ngo, P. Bastian, O. Ippisch,',
	 'category': u'Computer Science ',
	 'date': '2014-11-5',
	 'pdflink': u'http://arxiv.org/pdf/1411.1432',
	 'subjects': u'Numerical Analysis (math.NA)',
	 'title': u'\nNumerical solution of steady-state groundwater flow and solute transport  problems: Discontinuous Galerkin based methods compared to the Streamline  Diffusion approach',
	 'urllink': u'http://arxiv.org/abs/1411.1432'}
2015-04-10 10:33:03+0000 [xxu46_10] INFO: Crawled 447 pages (at 1 pages/min), scraped 440 items (at 1 items/min)
2015-04-10 10:33:17+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1411.1397> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 10:33:17+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1411.1397>
	{'abstract': u'We present a strong parallel repetition theorem for the entangled value of multi-player, one-round free games (games where the inputs come from a product distribution). Our result is the first parallel repetition theorem for entangled games involving more than two players. Furthermore, our theorem applies to games where the players are allowed to output (possibly entangled) quantum states as answers. More specifically, let be a -player free game, with entangled value . We show that the entangled value of the -fold repetition of , , is at most . In the traditional setting of players, our parallel repetition theorem is optimal in terms of its dependence on and . For an arbitrary number of players, our result is nearly optimal: for all , we exhibit a -player free game and such that . Hence, exponent of the repeated game value cannot be improved beyond . Our parallel repetition theorem improves on the prior results of [Jain, et al. 2014] and [Chailloux, Scarpa 2014] in a number of ways: (1) our theorem applies to a larger class of games (arbitrary number of players, quantum outputs); (2) we demonstrate that strong parallel repetition holds for the entangled value of free games: i.e., the base of the repeated game value is , rather than ; and (3) there is no dependence of the repeated game value on the input and output alphabets of . In contrast, it is known that the repeated game value of classical free games must depend on the output size. Thus our results demonstrate a seperation between the behavior of entangled games and classical games.',
	 'authors': u'Kai-Min Chung, Xiaodi Wu, Henry Yuen,',
	 'category': u'Computer Science ',
	 'date': '2014-11-5',
	 'pdflink': u'http://arxiv.org/e-print/1411.1397',
	 'subjects': u'Quantum Physics (quant-ph)',
	 'title': u'\nStrong parallel repetition for free entangled games, with any number of  players',
	 'urllink': u'http://arxiv.org/abs/1411.1397'}
2015-04-10 10:34:03+0000 [xxu46_10] INFO: Crawled 448 pages (at 1 pages/min), scraped 441 items (at 1 items/min)
2015-04-10 10:34:35+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1411.1356> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 10:34:35+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1411.1356>
	{'abstract': u'It had been believed in the conventional practice that the risk of a bank going bankrupt is lessened in a straightforward manner by transferring the risk of loan defaults. But the failure of American International Group in 2008 posed a more complex aspect of financial contagion. This study presents an extension of the asset network systemic risk model (ANWSER) to investigate whether credit default swaps mitigate or intensify the severity of financial contagion. A protection buyer bank transfers the risk of every possible debtor bank default to protection seller banks. The empirical distribution of the number of bank bankruptcies is obtained with the extended model. Systemic capital buffer ratio is calculated from the distribution. The ratio quantifies the effective loss absorbency capability of the entire financial system to force back financial contagion. The key finding is that the leverage ratio is a good estimate of a systemic capital buffer ratio as the backstop of a financial system. The risk transfer from small and medium banks to big banks in an interbank network does not mitigate the severity of financial contagion.',
	 'authors': u'Yoshiharu Maeno, Kenji Nishiguchi, Satoshi Morinaga, Hirokazu Matsushima,',
	 'category': u'Computer Science ',
	 'date': '2014-9-25',
	 'pdflink': u'http://arxiv.org/pdf/1411.1356',
	 'subjects': u'Risk Management (q-fin.RM)',
	 'title': u'\nImpact of credit default swaps on financial contagion',
	 'urllink': u'http://arxiv.org/abs/1411.1356'}
2015-04-10 10:35:03+0000 [xxu46_10] INFO: Crawled 449 pages (at 1 pages/min), scraped 442 items (at 1 items/min)
2015-04-10 10:35:52+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1411.1323> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 10:35:52+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1411.1323>
	{'abstract': u'We study feedback control of a system of coupled nonlinear stochastic oscillators in a force field. We first consider the problem of asymptotically driving the system to a desired steady state corresponding to lower thermal noise. Among the feedback controls achieving the desired asymptotic transfer, we find that the most efficient one from an energy point of view is characterized by time-reversibility. We also extend the theory of the Schroedinger bridges to this model thereby steering the system in finite time and with minimum effort to a target steady-state distribution. The system can then be maintained in this state through the optimal steady-state feedback control. The solution, in the finite-horizon case, involves a space-time harmonic function and plays the role of an artificial, time-varying potential in which the desired evolution occurs. This framework appears extremely general and flexible and can be viewed as a considerable generalization of existing active control strategies such as macromolecular cooling. In the case of a quadratic potential, the results assume a form particularly attractive from the algorithmic viewpoint as the optimal control can be computed via deterministic matricial differential equations. An example involving inertial particles illustrates both transient and steady state optimal feedback control.',
	 'authors': u'Yongxin Chen, Tryphon Georgiou, Michele Pavon,',
	 'category': u'Computer Science ',
	 'date': '2014-11-5',
	 'pdflink': u'http://arxiv.org/pdf/1411.1323',
	 'subjects': u'Mathematical Physics (math-ph)',
	 'title': u'\nFast cooling for a system of stochastic oscillators',
	 'urllink': u'http://arxiv.org/abs/1411.1323'}
2015-04-10 10:36:03+0000 [xxu46_10] INFO: Crawled 450 pages (at 1 pages/min), scraped 443 items (at 1 items/min)
2015-04-10 10:36:55+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1411.1303> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 10:36:55+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1411.1303>
	{'abstract': u'We show that the maximum number of convex polygons in a triangulation of points in the plane is . This improves an earlier bound of established by van Kreveld, L "offler, and Pach (2012) and almost matches the current best lower bound of due to the same authors. Given a planar straight-line graph with vertices, we show how to compute efficiently the number of convex polygons in .',
	 'authors': u'Adrian Dumitrescu, Csaba D. T\xf3th,',
	 'category': u'Computer Science ',
	 'date': '2014-11-4',
	 'pdflink': u'http://arxiv.org/pdf/1411.1303',
	 'subjects': u'Metric Geometry (math.MG)',
	 'title': u'\nConvex polygons in geometric triangulations',
	 'urllink': u'http://arxiv.org/abs/1411.1303'}
2015-04-10 10:37:03+0000 [xxu46_10] INFO: Crawled 451 pages (at 1 pages/min), scraped 444 items (at 1 items/min)
2015-04-10 10:38:03+0000 [xxu46_10] INFO: Crawled 451 pages (at 0 pages/min), scraped 444 items (at 0 items/min)
2015-04-10 10:38:11+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1411.1293> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 10:38:11+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1411.1293>
	{'abstract': u"Concurrently coupled numerical simulations using heterogeneous solvers are powerful tools for modeling multiscale phenomena. However, major modifications to existing codes are often required to enable such simulations, posing significant difficulties in practice. In this paper we present a C++ library, i.e. the Multiscale Universal Interface (MUI), which is capable of facilitating the coupling effort for a wide range of multiscale simulations. The library adopts a header-only form with minimal external dependency and hence can be easily dropped into existing codes. A data sampler concept is introduced, combined with a hybrid dynamic/static typing mechanism, to create an easily customizable framework for solver-independent data interpretation. The library integrates MPI MPMD support and an asynchronous communication protocol to handle inter-solver information exchange irrespective of the solvers' own MPI awareness. Template metaprogramming is heavily employed to simultaneously improve runtime performance and code flexibility. We validated the library by solving three different multiscale problems, which also serve to demonstrate the flexibility of the framework in handling heterogeneous models and solvers. In the first example, a Couette flow was simulated using two concurrently coupled Smoothed Particle Hydrodynamics (SPH) simulations of different spatial resolutions. In the second example, we coupled the deterministic SPH method with the stochastic Dissipative Particle Dynamics (DPD) method to study the effect of surface grafting on the hydrodynamics properties on the surface. In the third example, we consider conjugate heat transfer between a solid domain and a fluid domain by coupling the particle-based energy-conserving DPD (eDPD) method with the Finite Element Method (FEM).",
	 'authors': u'Yu-Hang Tang, Shuhei Kudo, Xin Bian, Zhen Li, George E. Karniadakis,',
	 'category': u'Computer Science ',
	 'date': '2014-11-5',
	 'pdflink': u'http://arxiv.org/pdf/1411.1293',
	 'subjects': u'Computational Physics (physics.comp-ph)',
	 'title': u'\nMultiscale Universal Interface: A Concurrent Framework for Coupling  Heterogeneous Solvers',
	 'urllink': u'http://arxiv.org/abs/1411.1293'}
2015-04-10 10:38:51+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1411.1282> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 10:38:51+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1411.1282>
	{'abstract': u'In the last few years, the study of multi-layer complex networks has received significant attention. In this work, we provide new measures to analyse dependencies between directed links in different layers of multiplex networks. We show that this requires more than a straightforward extension of the corresponding multiplexity measures that have been developed for undirected multiplexes. In particular, one should take into account the effects of reciprocity, i.e. the tendency of pairs of vertices to establish mutual connections. It is well known that reciprocity is a crucial property of many directed single-layer networks, affecting several dynamical processes taking place on such systems. Here we extend this quantity to directed multiplexes and introduce the notion of multireciprocity, defined as the tendency of links in one layer to be reciprocated by links in a different layer. We introduce multireciprocity measures valid for both binary and weighted networks and then validate these novel quantities on the World Trade Multiplex (WTM), representing the import-export bilateral relations between world countries in different commodities. We show that several pairs of layers exhibit strong multiplexity, an effect which can however be largely encoded into the degree or strength sequences of the various layers. Moreover, we find that most of the pairs of commodities are characterised by positive multireciprocities, even though such values are significantly lower than the usual reciprocity measured on the aggregated network. These results confirm that a multiplex approach to the phenomenon of reciprocity conveys much more information than the analysis of the aggregated network does.',
	 'authors': u'Valerio Gemmetto, Tiziano Squartini, Francesco Picciolo, Franco Ruzzenenti, Diego Garlaschelli,',
	 'category': u'Computer Science ',
	 'date': '2014-11-5',
	 'pdflink': u'http://arxiv.org/pdf/1411.1282',
	 'subjects': u'Physics and Society (physics.soc-ph)',
	 'title': u'\nMultiplexity and multireciprocity in directed multiplexes',
	 'urllink': u'http://arxiv.org/abs/1411.1282'}
2015-04-10 10:39:03+0000 [xxu46_10] INFO: Crawled 453 pages (at 2 pages/min), scraped 446 items (at 2 items/min)
2015-04-10 10:39:57+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1411.1243> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 10:39:57+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1411.1243>
	{'abstract': u"Twitter has been proven to be a notable source for predictive modelling on various domains such as the stock market, the dissemination of diseases or sports outcomes. However, such a study has not been conducted in football (soccer) so far. The purpose of this research was to study whether data mined from Twitter can be used for this purpose. We built a set of predictive models for the outcome of football games of the English Premier League for a 3 month period based on tweets and we studied whether these models can overcome predictive models which use only historical data and simple football statistics. Moreover, combined models are constructed using both Twitter and historical data. The final results indicate that data mined from Twitter can indeed be a useful source for predicting games in the Premier League. The final Twitter-based model performs significantly better than chance when measured by Cohen's kappa and is comparable to the model that uses simple statistics and historical data. Combining both models raises the performance higher than it was achieved by each individual model. Thereby, this study provides evidence that Twitter derived features can indeed provide useful information for the prediction of football (soccer) outcomes.",
	 'authors': u'Stylianos Kampakis, Andreas Adamides,',
	 'category': u'Computer Science ',
	 'date': '2014-11-5',
	 'pdflink': u'http://arxiv.org/pdf/1411.1243',
	 'subjects': u'Machine Learning (stat.ML)',
	 'title': u'\nUsing Twitter to predict football outcomes',
	 'urllink': u'http://arxiv.org/abs/1411.1243'}
2015-04-10 10:40:03+0000 [xxu46_10] INFO: Crawled 454 pages (at 1 pages/min), scraped 447 items (at 1 items/min)
2015-04-10 10:40:44+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1411.1152> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 10:40:44+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1411.1152>
	{'abstract': u"We introduce an equilibrium framework that relaxes the standard assumption that people have a correctly-specified view of their environment. Players repeatedly play a simultaneous-move game where they potentially face both strategic and payoff uncertainty. Each player has a potentially misspecified view of the environment and uses Bayes' rule to update her views based on the (possibly partial) feedback obtained at the end of each period. We show that steady-state behavior of this multi-player decision and learning problem is captured by a generalized notion of equilibrium: a strategy profile such that each player optimizes given certain beliefs and where these beliefs put probability one on those subjective distributions over consequences that are closest---in terms of relative entropy---to the correct, equilibrium distribution. Standard solution concepts such as Nash equilibrium and self-confirming equilibrium constitute special cases where players learn with correctly-specified models. The framework provides a systematic approach to modeling players with misspecified views and also unifies a specific bounded rationality literature where mistakes are driven by misspecifications.",
	 'authors': u'Ignacio Esponda, Demian Pouzo,',
	 'category': u'Computer Science ',
	 'date': '2014-11-5',
	 'pdflink': u'http://arxiv.org/pdf/1411.1152',
	 'subjects': u'Economics (q-fin.EC)',
	 'title': u'\nAn Equilibrium Framework for Players with Misspecified Models',
	 'urllink': u'http://arxiv.org/abs/1411.1152'}
2015-04-10 10:41:03+0000 [xxu46_10] INFO: Crawled 455 pages (at 1 pages/min), scraped 448 items (at 1 items/min)
2015-04-10 10:41:52+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1411.1098> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 10:41:52+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1411.1098>
	{'abstract': u"In complex systems, the network of interactions we observe between system's components is the aggregate of the interactions that occur through different mechanisms or layers. Recent studies reveal that the existence of multiple interaction layers can have a dramatic impact in the dynamical processes occurring on these systems. However, these studies assume that the interactions between systems components in each one of the layers are known, while typically for real-world systems we do not have that information. Here, we address the issue of uncovering the different interaction layers from aggregate data by introducing multilayer stochastic block models (SBMs), a generalization of single-layer SBMs that considers different mechanisms of layer aggregation. First, we find the complete probabilistic solution to the problem of finding the optimal multilayer SBM for a given aggregate observed network. Because this solution is computationally intractable, we propose an approximation that enables us to verify that multilayer SBMs are more predictive of network structure in real-world complex systems.",
	 'authors': u'Toni Valles-Catala, Francesco A. Massucci, Roger Guimera, Marta Sales-Pardo,',
	 'category': u'Computer Science ',
	 'date': '2014-11-4',
	 'pdflink': u'http://arxiv.org/pdf/1411.1098',
	 'subjects': u'Physics and Society (physics.soc-ph)',
	 'title': u'\nMultilayer stochastic block models reveal the multilayer structure of  complex networks',
	 'urllink': u'http://arxiv.org/abs/1411.1098'}
2015-04-10 10:42:03+0000 [xxu46_10] INFO: Crawled 456 pages (at 1 pages/min), scraped 449 items (at 1 items/min)
2015-04-10 10:42:53+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1411.1088> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 10:42:53+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1411.1088>
	{'abstract': u'A determinantal point process (DPP) is a probabilistic model of set diversity compactly parameterized by a positive semi-definite kernel matrix. To fit a DPP to a given task, we would like to learn the entries of its kernel matrix by maximizing the log-likelihood of the available data. However, log-likelihood is non-convex in the entries of the kernel matrix, and this learning problem is conjectured to be NP-hard. Thus, previous work has instead focused on more restricted convex learning settings: learning only a single weight for each row of the kernel matrix, or learning weights for a linear combination of DPPs with fixed kernel matrices. In this work we propose a novel algorithm for learning the full kernel matrix. By changing the kernel parameterization from matrix entries to eigenvalues and eigenvectors, and then lower-bounding the likelihood in the manner of expectation-maximization algorithms, we obtain an effective optimization procedure. We test our method on a real-world product recommendation task, and achieve relative gains of up to 16.5% in test log-likelihood compared to the naive approach of maximizing likelihood by projected gradient ascent on the entries of the kernel matrix.',
	 'authors': u'Jennifer Gillenwater, Alex Kulesza, Emily Fox, Ben Taskar,',
	 'category': u'Computer Science ',
	 'date': '2014-11-4',
	 'pdflink': u'http://arxiv.org/pdf/1411.1088',
	 'subjects': u'Machine Learning (stat.ML)',
	 'title': u'\nExpectation-Maximization for Learning Determinantal Point Processes',
	 'urllink': u'http://arxiv.org/abs/1411.1088'}
2015-04-10 10:43:03+0000 [xxu46_10] INFO: Crawled 457 pages (at 1 pages/min), scraped 450 items (at 1 items/min)
2015-04-10 10:44:03+0000 [xxu46_10] INFO: Crawled 457 pages (at 0 pages/min), scraped 450 items (at 0 items/min)
2015-04-10 10:44:07+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1411.1000> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 10:44:07+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1411.1000>
	{'abstract': u"We show new upper and lower bounds for the effective differential Nullstellensatz for differential fields of characteristic zero with several commuting derivations. Seidenberg was the first to address this problem in 1956, without giving a complete solution. The first explicit bounds appeared in 2009 in a paper by Golubitsky, Kondratieva, Szanto, and Ovchinnikov, with the upper bound expressed in terms of the Ackermann function. D'Alfonso, Jeronimo, and Solern 'o, using novel ideas, obtained in 2014 a new bound if restricted to the case of one derivation and constant coefficients. To obtain the bound in the present paper without this restriction, we extend this approach and use the new methods of Freitag and Le 'on S 'anchez and of Pierce from 2014, which represent a model-theoretic approach to differential algebraic geometry.",
	 'authors': u'Richard Gustavson, Marina Kondratieva, Alexey Ovchinnikov,',
	 'category': u'Computer Science ',
	 'date': '2014-11-4',
	 'pdflink': u'http://arxiv.org/pdf/1411.1000',
	 'subjects': u'Commutative Algebra (math.AC)',
	 'title': u'\nNew effective differential Nullstellensatz',
	 'urllink': u'http://arxiv.org/abs/1411.1000'}
2015-04-10 10:44:54+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1411.0972> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 10:44:54+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1411.0972>
	{'abstract': u'This article reviews recent advances in convex optimization algorithms for Big Data, which aim to reduce the computational, storage, and communications bottlenecks. We provide an overview of this emerging field, describe contemporary approximation techniques like first-order methods and randomization for scalability, and survey the important role of parallel and distributed computation. The new Big Data algorithms are based on surprisingly simple principles and attain staggering accelerations even on classical problems.',
	 'authors': u'Volkan Cevher, Stephen Becker, Mark Schmidt,',
	 'category': u'Computer Science ',
	 'date': '2014-11-4',
	 'pdflink': u'http://arxiv.org/pdf/1411.0972',
	 'subjects': u'Optimization and Control (math.OC)',
	 'title': u'\nConvex Optimization for Big Data',
	 'urllink': u'http://arxiv.org/abs/1411.0972'}
2015-04-10 10:45:03+0000 [xxu46_10] INFO: Crawled 459 pages (at 2 pages/min), scraped 452 items (at 2 items/min)
2015-04-10 10:45:52+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1411.0882> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 10:45:52+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1411.0882>
	{'abstract': u'Currents represent generalized surfaces studied in geometric measure theory. They range from relatively tame integral currents that represent oriented manifolds with integer multiplicities and finite volume as well as boundary, to arbitrary elements of the dual space of differential forms. The flat norm provides a natural distance in the space of currents, and works by decomposing a -dimensional current into - and (the boundary of) -dimensional pieces. A natural question about currents is the following. If the input is an integral current, can its flat norm decomposition be integral as well? The answer is not known in general, except in the case of -currents that are boundaries of -currents in . Following correspondences between the flat norm and total variation (TV) of functionals, the answer is affirmative in this case. On the other hand, for the discretization of the flat norm on a finite simplicial complex, the analogous statement remains true even when the inputs are not boundaries. This result is implied by the boundary matrix of the simplicial complex being totally unimodular -- a result distinct from the TV approach. We develop an analysis framework that extends the result in the simplicial setting to that for -currents in , provided a suitable triangulation result holds. Following results of Shewchuk on triangulating planar straight line graphs while bounding both the size and location of small angles, our framework shows that the discrete result implies the continuous result for the case of -currents in .',
	 'authors': u'Sharif Ibrahim, Bala Krishnamoorthy, Kevin R. Vixie,',
	 'category': u'Computer Science ',
	 'date': '2014-11-4',
	 'pdflink': u'http://arxiv.org/pdf/1411.0882',
	 'subjects': u'Differential Geometry (math.DG)',
	 'title': u'\nFlat Norm Decomposition of Integral Currents',
	 'urllink': u'http://arxiv.org/abs/1411.0882'}
2015-04-10 10:46:03+0000 [xxu46_10] INFO: Crawled 460 pages (at 1 pages/min), scraped 453 items (at 1 items/min)
2015-04-10 10:46:53+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1411.0849> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 10:46:53+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1411.0849>
	{'abstract': u'We consider a Hidden Markov Model (HMM) where the integrated continuous-time Markov chain can be observed at discrete time points perturbed by a Brownian motion. The aim is to derive a filter for the underlying continuous-time Markov chain. The recursion formula for the discrete-time filter is easy to derive, however involves densities which are very hard to obtain. In this paper we derive exact formulas for the necessary densities in the case the state space of the HMM consists of two elements only. This is done by relating the underlying integrated continuous-time Markov chain to the so-called asymmetric telegraph process and by using recent results on this process. In case the state space consists of more than two elements we present three different ways to approximate the densities for the filter. The first approach is based on the continuous filter problem. The second approach is to derive a PDE for the densities and solve it numerically and the third approach is a crude discrete time approximation of the Markov chain. All three approaches are compared in a numerical study.',
	 'authors': u'Nicole B\xe4uerle, Igor Gilitschenski, Uwe D. Hanebeck,',
	 'category': u'Computer Science ',
	 'date': '2014-11-4',
	 'pdflink': u'http://arxiv.org/pdf/1411.0849',
	 'subjects': u'Probability (math.PR)',
	 'title': u'\nExact and Approximate Hidden Markov Chain Filters Based on Discrete  Observations',
	 'urllink': u'http://arxiv.org/abs/1411.0849'}
2015-04-10 10:47:03+0000 [xxu46_10] INFO: Crawled 461 pages (at 1 pages/min), scraped 454 items (at 1 items/min)
2015-04-10 10:48:00+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1411.0784> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 10:48:00+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1411.0784>
	{'abstract': u'In previous works, hexagonal cellular automata (CA) have been studied as a variation of the famous Game of Life CA, mainly for spiral phenomena simulations; where the most interesting constructions are related to the Belousov-Zhabotinsky reaction. In this paper, we analyse a special kind of hexagonal CA, . Such automaton shows a non-trivial complex behaviour related to discrete models of reaction-diffusion chemical media, dominated by spiral guns which easily emerge from random initial conditions. The computing capabilities of this automaton are shown by means of logic gates. These are defined by collisions between mobile localizations. Also, an extended classification of complex self-localisation patterns is presented, including some self-organised patterns.',
	 'authors': u'Rogelio Basurto, Paulina A. Le\xf3n, Genaro J. Mart\xednez, Juan C. Seck-Tuoh-Mora,',
	 'category': u'Computer Science ',
	 'date': '2014-11-4',
	 'pdflink': u'http://arxiv.org/pdf/1411.0784',
	 'subjects': u'Cellular Automata and Lattice Gases (nlin.CG)',
	 'title': u'\nLogic gates and complex dynamics in a hexagonal cellular automaton: the  Spiral rule',
	 'urllink': u'http://arxiv.org/abs/1411.0784'}
2015-04-10 10:48:03+0000 [xxu46_10] INFO: Crawled 462 pages (at 1 pages/min), scraped 455 items (at 1 items/min)
2015-04-10 10:49:02+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1411.0729> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 10:49:02+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1411.0729>
	{'abstract': u"In this paper we consider the problem of generating arbitrary three-party correlations from a combination of public and secret correlations. Two parties -- called Alice and Bob -- share perfectly correlated bits that are secret from a collaborating third party, Charlie. At the same time, all three parties have access to a separate source of correlated bits, and their goal is to convert these two resources into multiple copies of some given tripartite distribution . We obtain a single-letter characterization of the trade-off between public and private bits that are needed to achieve this task. The rate of private bits is shown to generalize Wyner's classic notion of common information held between a pair of random variables. The problem we consider is also closely related to the task of secrecy formation in which is generated using public communication and local randomness but with Charlie functioning as an adversary instead of a collaborator. We describe in detail the differences between the collaborative and adversarial scenarios.",
	 'authors': u'Eric Chitambar, Min-Hsiu Hsieh, Andreas Winter,',
	 'category': u'Computer Science ',
	 'date': '2014-11-3',
	 'pdflink': u'http://arxiv.org/pdf/1411.0729',
	 'subjects': u'Quantum Physics (quant-ph)',
	 'title': u'\nThe Private and Public Correlation Cost of Three Random Variables with  Collaboration',
	 'urllink': u'http://arxiv.org/abs/1411.0729'}
2015-04-10 10:49:03+0000 [xxu46_10] INFO: Crawled 463 pages (at 1 pages/min), scraped 456 items (at 1 items/min)
2015-04-10 10:50:03+0000 [xxu46_10] INFO: Crawled 463 pages (at 0 pages/min), scraped 456 items (at 0 items/min)
2015-04-10 10:50:19+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1411.0722> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 10:50:19+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1411.0722>
	{'abstract': u'Describing the dynamics of a city is a crucial step to both understanding the human activity in urban environments and to planning and designing cities accordingly. Here we describe the collective dynamics of New York City and surrounding areas as seen through the lens of Twitter usage. In particular, we observe and quantify the patterns that emerge naturally from the hourly activities in different areas of New York City, and discuss how they can be used to understand the urban areas. Using a dataset that includes more than 6 million geolocated Twitter messages we construct a movie of the geographic density of tweets. We observe the diurnal "heartbeat" of the NYC area. The largest scale dynamics are the waking and sleeping cycle and commuting from residential communities to office areas in Manhattan. Hourly dynamics reflect the interplay of commuting, work and leisure, including whether people are preoccupied with other activities or actively using Twitter. Differences between weekday and weekend dynamics point to changes in when people wake and sleep, and engage in social activities. We show that by measuring the average distances to the heart of the city one can quantify the weekly differences and the shift in behavior during weekends. We also identify locations and times of high Twitter activity that occur because of specific activities. These include early morning high levels of traffic as people arrive and wait at air transportation hubs, and on Sunday at the Meadowlands Sports Complex and Statue of Liberty. We analyze the role of particular individuals where they have large impacts on overall Twitter activity. Our analysis points to the opportunity to develop insight into both geographic social dynamics and attention through social media analysis.',
	 'authors': u'Urbano Fran\xe7a, Hiroki Sayama, Colin McSwiggen, Roozbeh Daneshvar, Yaneer Bar-Yam,',
	 'category': u'Computer Science ',
	 'date': '2014-11-3',
	 'pdflink': u'http://arxiv.org/pdf/1411.0722',
	 'subjects': u'Physics and Society (physics.soc-ph)',
	 'title': u'\nVisualizing the "Heartbeat" of a City with Tweets',
	 'urllink': u'http://arxiv.org/abs/1411.0722'}
2015-04-10 10:51:03+0000 [xxu46_10] INFO: Crawled 464 pages (at 1 pages/min), scraped 457 items (at 1 items/min)
2015-04-10 10:51:31+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1411.0715> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 10:51:31+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1411.0715>
	{'abstract': u'In studying network growth, the conventional approach is to devise a growth mechanism, quantify the evolution of a statistic or distribution (such as the degree distribution), and then solve the equations in the steady state (the infinite-size limit). Consequently, empirical studies also seek to verify the steady-state prediction in real data. The caveat concomitant with confining the analysis to this time regime is that no real system has infinite size; most real growing networks are far from the steady state. This underlines the importance of finite-size analysis. In this paper, we consider the shifted-linear preferential attachment as an illustrative example of arbitrary-time network growth analysis. We obtain the degree distribution for arbitrary initial conditions at arbitrary times. We corroborate our theoretical predictions with Monte Carlo simulations.',
	 'authors': u'Babak Fotouhi,',
	 'category': u'Computer Science ',
	 'date': '2014-11-3',
	 'pdflink': u'http://arxiv.org/pdf/1411.0715',
	 'subjects': u'Physics and Society (physics.soc-ph)',
	 'title': u'\nBeyond the Steady-State: Analytical Study of Network Growth at Arbitrary  Times, for Arbitrary Initial Conditions',
	 'urllink': u'http://arxiv.org/abs/1411.0715'}
2015-04-10 10:52:03+0000 [xxu46_10] INFO: Crawled 465 pages (at 1 pages/min), scraped 458 items (at 1 items/min)
2015-04-10 10:52:46+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1411.0650> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 10:52:46+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1411.0650>
	{'abstract': u'We establish the satisfiability threshold for random k-SAT for all k &gt;= k_0. That is, there exists a limiting density alpha_s(k) such that a random k-SAT formula of clause density alpha is with high probability satisfiable for alpha &lt; alpha_s(k), and unsatisfiable for alpha &gt; alpha_s(k). The satisfiability threshold alpha_s(k) is given explicitly by the one-step replica symmetry breaking prediction from statistical physics. We believe that our methods may apply to a range of random CSPs in the 1RSB universality class.',
	 'authors': u'Jian Ding, Allan Sly, Nike Sun,',
	 'category': u'Computer Science ',
	 'date': '2014-11-3',
	 'pdflink': u'http://arxiv.org/pdf/1411.0650',
	 'subjects': u'Probability (math.PR)',
	 'title': u'\nProof of the satisfiability conjecture for large k',
	 'urllink': u'http://arxiv.org/abs/1411.0650'}
2015-04-10 10:53:03+0000 [xxu46_10] INFO: Crawled 466 pages (at 1 pages/min), scraped 459 items (at 1 items/min)
2015-04-10 10:54:03+0000 [xxu46_10] INFO: Crawled 466 pages (at 0 pages/min), scraped 459 items (at 0 items/min)
2015-04-10 10:54:05+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1411.0630> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 10:54:05+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1411.0630>
	{'abstract': u'We consider active maximum a posteriori (MAP) inference problem for Hidden Markov Models (HMM), where, given an initial MAP estimate of the hidden sequence, we select to label certain states in the sequence to improve the estimation accuracy of the remaining states. We develop an analytical approach to this problem for the case of binary symmetric HMMs, and obtain a closed form solution that relates the expected error reduction to model parameters under the specified active inference scheme. We then use this solution to determine most optimal active inference scheme in terms of error reduction, and examine the relation of those schemes to heuristic principles of uncertainty reduction and solution unicity.',
	 'authors': u'Armen E. Allahverdyan, Aram Galstyan,',
	 'category': u'Computer Science ',
	 'date': '2014-11-3',
	 'pdflink': u'http://arxiv.org/pdf/1411.0630',
	 'subjects': u'Machine Learning (stat.ML)',
	 'title': u'\nActive Inference for Binary Symmetric Hidden Markov Models',
	 'urllink': u'http://arxiv.org/abs/1411.0630'}
2015-04-10 10:55:03+0000 [xxu46_10] INFO: Crawled 467 pages (at 1 pages/min), scraped 460 items (at 1 items/min)
2015-04-10 10:55:23+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1411.0591> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 10:55:23+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1411.0591>
	{'abstract': u'Identifying small subsets of features that are relevant for prediction and/or classification tasks is a central problem in machine learning and statistics. The feature selection task is especially important, and computationally difficult, for modern datasets where the number of features can be comparable to, or even exceed, the number of samples. Here, we show that feature selection with Bayesian inference takes a universal form and reduces to calculating the magnetizations of an Ising model, under some mild conditions. Our results exploit the observation that the evidence takes a universal form for strongly-regularizing priors --- priors that have a large effect on the posterior probability even in the infinite data limit. We derive explicit expressions for feature selection for generalized linear models, a large class of statistical techniques that include linear and logistic regression. We illustrate the power of our approach by analyzing feature selection in a logistic regression-based classifier trained to distinguish between the letters B and D in the notMNIST dataset.',
	 'authors': u'Charles K. Fisher, Pankaj Mehta,',
	 'category': u'Computer Science ',
	 'date': '2014-11-3',
	 'pdflink': u'http://arxiv.org/pdf/1411.0591',
	 'subjects': u'Statistical Mechanics (cond-mat.stat-mech)',
	 'title': u'\nBayesian feature selection with strongly-regularizing priors maps to the  Ising Model',
	 'urllink': u'http://arxiv.org/abs/1411.0591'}
2015-04-10 10:56:03+0000 [xxu46_10] INFO: Crawled 468 pages (at 1 pages/min), scraped 461 items (at 1 items/min)
2015-04-10 10:56:32+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1411.0583> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 10:56:32+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1411.0583>
	{'abstract': u'This article provides a short overview of the theory of First Order Automatic Differentiation (AD) for readers unfamiliar with this topic. In particular, we explain why different characterisations of Forward AD, like the vector-matrix based approach, the idea of lifting functions to the algebra of dual numbers, the method of Taylor series expansion on dual numbers and the application of the push-forward operator, all reduce to the same actual chain of computations (and are, hence, equivalent). We further give a short summary of Reverse AD and again point out the underlying computational steps.',
	 'authors': u'Philipp H. W. Hoffmann,',
	 'category': u'Computer Science ',
	 'date': '2014-11-3',
	 'pdflink': u'http://arxiv.org/pdf/1411.0583',
	 'subjects': u'Numerical Analysis (math.NA)',
	 'title': u'\nThe Principles of First Order Automatic Differentiation',
	 'urllink': u'http://arxiv.org/abs/1411.0583'}
2015-04-10 10:57:03+0000 [xxu46_10] INFO: Crawled 469 pages (at 1 pages/min), scraped 462 items (at 1 items/min)
2015-04-10 10:57:39+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1411.0473> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 10:57:39+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1411.0473>
	{'abstract': u'Understanding the collective reaction to individual actions is key to effectively spread information in social media. In this work we define efficiency on Twitter, as the ratio between the emergent spreading process and the activity employed by the user. We characterize this property by means of a quantitative analysis of the structural and dynamical patterns emergent from human interactions, and show it to be universal across several Twitter conversations. We found that some influential users efficiently cause remarkable collective reactions by each message sent, while the majority of users must employ extremely larger efforts to reach similar effects. Next we propose a model that reproduces the retweet cascades occurring on Twitter to explain the emergent distribution of the user efficiency. The model shows that the dynamical patterns of the conversations are strongly conditioned by the topology of the underlying network. We conclude that the appearance of a small fraction of extremely efficient users results from the heterogeneity of the followers network and independently of the individual user behavior.',
	 'authors': u'A.J Morales, J. Borondo, J.C. Losada, R.M. Benito,',
	 'category': u'Computer Science ',
	 'date': '2014-11-3',
	 'pdflink': u'http://arxiv.org/pdf/1411.0473',
	 'subjects': u'Physics and Society (physics.soc-ph)',
	 'title': u'\nEfficiency of Human Activity on Information Spreading on Twitter',
	 'urllink': u'http://arxiv.org/abs/1411.0473'}
2015-04-10 10:58:03+0000 [xxu46_10] INFO: Crawled 470 pages (at 1 pages/min), scraped 463 items (at 1 items/min)
2015-04-10 10:58:48+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1411.0416> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 10:58:48+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1411.0416>
	{'abstract': u'The availability of geocoded health data and the inherent temporal structure of communicable diseases have led to an increased interest in statistical models and software for spatio-temporal data with epidemic features. The open source R package surveillance can handle various levels of aggregation at which infective events have been recorded: individual-level time-stamped geo-referenced data (case reports) in either continuous space or discrete space, as well as counts aggregated by period and region. For each of these data types, the surveillance package implements tools for visualization, likelihoood inference and simulation from recently developed statistical regression frameworks capturing endemic and epidemic dynamics. Altogether, this paper is a guide to the spatio-temporal modeling of epidemic phenomena, exemplified by analyses of public health surveillance data on measles and invasive meningococcal disease.',
	 'authors': u'Sebastian Meyer, Leonhard Held, Michael H\xf6hle,',
	 'category': u'Computer Science ',
	 'date': '2014-11-3',
	 'pdflink': u'http://arxiv.org/pdf/1411.0416',
	 'subjects': u'Computation (stat.CO)',
	 'title': u'\nSpatio-Temporal Analysis of Epidemic Phenomena Using the R Package  surveillance',
	 'urllink': u'http://arxiv.org/abs/1411.0416'}
2015-04-10 10:59:03+0000 [xxu46_10] INFO: Crawled 471 pages (at 1 pages/min), scraped 464 items (at 1 items/min)
2015-04-10 11:00:03+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1411.0402> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 11:00:03+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1411.0402>
	{'abstract': u'We study on-line colorings of certain graphs given as intersection graphs of objects "between two lines", i.e., there is a pair of horizontal lines such that each object of the representation is a connected set contained in the strip between the lines and touches both. Some of the graph classes admitting such a representation are permutation graphs (segments), interval graphs (axis-aligned rectangles), trapezoid graphs (trapezoids) and cocomparability graphs (simple curves). We present an on-line algorithm coloring graphs given by convex sets between two lines that uses colors on graphs with maximum clique size . In contrast intersection graphs of segments attached to a single line may force any on-line coloring algorithm to use an arbitrary number of colors even when . The relation makes the complement of intersection graphs of objects between two lines into a poset. As an aside we discuss the relation of the class of posets obtained from convex sets between two lines with some other classes of posets: all -dimensional posets and all posets of height are in but there is a -dimensional poset of height that does not belong to . We also show that the on-line coloring problem for curves between two lines is as hard as the on-line chain partition problem for arbitrary posets.',
	 'authors': u'Stefan Felsner, Piotr Micek, Torsten Ueckerdt,',
	 'category': u'Computer Science ',
	 'date': '2014-11-3',
	 'pdflink': u'http://arxiv.org/pdf/1411.0402',
	 'subjects': u'Combinatorics (math.CO)',
	 'title': u'\nOn-line coloring between two lines',
	 'urllink': u'http://arxiv.org/abs/1411.0402'}
2015-04-10 11:00:03+0000 [xxu46_10] INFO: Crawled 472 pages (at 1 pages/min), scraped 465 items (at 1 items/min)
2015-04-10 11:01:01+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1411.0349> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 11:01:01+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1411.0349>
	{'abstract': u'In this short note we give an example of a four-person finite positional game with perfect information that has no positions of chance and no Nash equilibria in pure stationary strategies. The corresponding directed graph has only one directed cycle and only five terminal positions. It remains open: (i) if the number of the players can be reduced from to , (ii) if the number of the terminals can be reduced from to , and most important, (iii) whether it is possible to get a similar example in which the outcome corresponding to all (possibly, more than one) directed cycles is worse than every terminal for each player. Yet, it is known that (j) cannot be reduced to , (jj) cannot be reduced to , and (jjj) there can be no similar example in which each player makes a decision in a unique position. Keywords: stochastic, positional, chess-like, transition-free games with perfect information and without moves of chance; Nash equilibrium, directed cycles (dicycles), terminal position.',
	 'authors': u'Vladimir Gurvich, Vladimir Oudalov,',
	 'category': u'Computer Science ',
	 'date': '2014-11-3',
	 'pdflink': u'http://arxiv.org/pdf/1411.0349',
	 'subjects': u'Combinatorics (math.CO)',
	 'title': u'\nA four-person chess-like game without Nash equilibria in pure stationary  strategies',
	 'urllink': u'http://arxiv.org/abs/1411.0349'}
2015-04-10 11:01:03+0000 [xxu46_10] INFO: Crawled 473 pages (at 1 pages/min), scraped 466 items (at 1 items/min)
2015-04-10 11:02:03+0000 [xxu46_10] INFO: Crawled 473 pages (at 0 pages/min), scraped 466 items (at 0 items/min)
2015-04-10 11:02:16+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1411.0347> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 11:02:16+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1411.0347>
	{'abstract': u'We study randomized sketching methods for approximately solving least-squares problem with a general convex constraint. The quality of a least-squares approximation can be assessed in different ways: either in terms of the value of the quadratic objective function (cost approximation), or in terms of some distance measure between the approximate minimizer and the true minimizer (solution approximation). Focusing on the latter criterion, our first main result provides a general lower bound on any randomized method that sketches both the data matrix and vector in a least-squares problem; as a surprising consequence, the most widely used least-squares sketch is sub-optimal for solution approximation. We then present a new method known as the iterative Hessian sketch, and show that it can be used to obtain approximations to the original least-squares problem using a projection dimension proportional to the statistical complexity of the least-squares minimizer, and a logarithmic number of iterations. We illustrate our general theory with simulations for both unconstrained and constrained versions of least-squares, including -regularization and nuclear norm constraints. We also numerically demonstrate the practicality of our approach in a real face expression classification experiment.',
	 'authors': u'Mert Pilanci, Martin J. Wainwright,',
	 'category': u'Computer Science ',
	 'date': '2014-11-3',
	 'pdflink': u'http://arxiv.org/pdf/1411.0347',
	 'subjects': u'Optimization and Control (math.OC)',
	 'title': u'\nIterative Hessian sketch: Fast and accurate solution approximation for  constrained least-squares',
	 'urllink': u'http://arxiv.org/abs/1411.0347'}
2015-04-10 11:03:03+0000 [xxu46_10] INFO: Crawled 474 pages (at 1 pages/min), scraped 467 items (at 1 items/min)
2015-04-10 11:03:14+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1411.0344> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 11:03:14+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1411.0344>
	{'abstract': u'The multiplex network growth literature has been confined to homogeneous growth hitherto, where the number of links that each new incoming node establishes is the same across layers. This paper focuses on heterogeneous growth. We first analyze the case of two preferentially growing layers and find a closed-form expression for the inter-layer degree distribution, and demonstrate that non-trivial inter-layer degree correlations emerge in the steady state. Then we focus on the case of uniform growth. Surprisingly, inter-layer correlations arise in the random case, too. Also, we observe that the expression for the average layer-2 degree of nodes whose layer-1 degree is k, is identical for the uniform and preferential schemes. Throughout, theoretical predictions are corroborated using Monte Carlo simulations.',
	 'authors': u'Babak Fotouhi, Naghmeh Momeni,',
	 'category': u'Computer Science ',
	 'date': '2014-11-3',
	 'pdflink': u'http://arxiv.org/pdf/1411.0344',
	 'subjects': u'Physics and Society (physics.soc-ph)',
	 'title': u'\nNon-trivial Inter-layer Degree Correlations in Heterogeneously Growing  Multiplex Networks',
	 'urllink': u'http://arxiv.org/abs/1411.0344'}
2015-04-10 11:04:03+0000 [xxu46_10] INFO: Crawled 475 pages (at 1 pages/min), scraped 468 items (at 1 items/min)
2015-04-10 11:04:06+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1411.0306> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 11:04:06+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1411.0306>
	{'abstract': u'One approach to improving the running time of kernel-based machine learning methods is to build a small sketch of the input and use it in lieu of the full kernel matrix in the machine learning task of interest. Here, we describe a version of this approach that comes with running time guarantees as well as improved guarantees on its statistical performance. By extending the notion of emph to the setting of kernel ridge regression, our main statistical result is to identify an importance sampling distribution that reduces the size of the sketch (i.e., the required number of columns to be sampled) to the emph of the problem. This quantity is often much smaller than previous bounds that depend on the emph. Our main algorithmic result is to present a fast algorithm to compute approximations to these scores. This algorithm runs in time that is linear in the number of samples---more precisely, the running time is , where the parameter depends only on the trace of the kernel matrix and the regularization parameter---and it can be applied to the matrix of feature vectors, without having to form the full kernel matrix. This is obtained via a variant of length-squared sampling that we adapt to the kernel setting in a way that is of independent interest. Lastly, we provide empirical results illustrating our theory, and we discuss how this new notion of the statistical leverage of a data point captures in a fine way the difficulty of the original statistical learning problem.',
	 'authors': u'Ahmed El Alaoui, Michael W. Mahoney,',
	 'category': u'Computer Science ',
	 'date': '2014-11-2',
	 'pdflink': u'http://arxiv.org/pdf/1411.0306',
	 'subjects': u'Machine Learning (stat.ML)',
	 'title': u'\nFast Randomized Kernel Methods With Statistical Guarantees',
	 'urllink': u'http://arxiv.org/abs/1411.0306'}
2015-04-10 11:04:57+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1411.0292> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 11:04:57+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1411.0292>
	{'abstract': u'Predictive inference uses a model to analyze a dataset and make predictions about new observations. When a model does not match the data, predictive accuracy suffers. To mitigate this effect, we develop the profile predictive, a predictive density that incorporates the population distribution of data into Bayesian inference. This leads to a practical method for reducing the effect of model mismatch. We extend this method into variational inference and propose a stochastic optimization algorithm, called bumping variational inference. We demonstrate improved predictive accuracy over classical variational inference in two models: a Bayesian mixture model of image histograms and a latent Dirichlet allocation topic model of a text corpus.',
	 'authors': u'Alp Kucukelbir, David M. Blei,',
	 'category': u'Computer Science ',
	 'date': '2014-11-2',
	 'pdflink': u'http://arxiv.org/pdf/1411.0292',
	 'subjects': u'Machine Learning (stat.ML)',
	 'title': u'\nProfile Predictive Inference',
	 'urllink': u'http://arxiv.org/abs/1411.0292'}
2015-04-10 11:05:03+0000 [xxu46_10] INFO: Crawled 477 pages (at 2 pages/min), scraped 470 items (at 2 items/min)
2015-04-10 11:06:03+0000 [xxu46_10] INFO: Crawled 477 pages (at 0 pages/min), scraped 470 items (at 0 items/min)
2015-04-10 11:06:14+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1411.0290> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 11:06:14+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1411.0290>
	{'abstract': u'A proper edge-coloring of a graph with colors is called an emph if all colors are used, and the edges incident to each vertex are colored by consecutive colors modulo , where is the degree of a vertex in . A graph is emph if it has an interval cyclic -coloring for some positive integer . The set of all interval cyclically colorable graphs is denoted by . For a graph , the least and the greatest values of for which it has an interval cyclic -coloring are denoted by and , respectively. In this paper we investigate some properties of interval cyclic colorings. In particular, we prove that if is a triangle-free graph with at least two vertices and , then . We also obtain bounds on and for various classes of graphs. Finally, we give some methods for constructing of interval cyclically non-colorable graphs.',
	 'authors': u'Petros A. Petrosyan, Sargis T. Mkhitaryan,',
	 'category': u'Computer Science ',
	 'date': '2014-11-2',
	 'pdflink': u'http://arxiv.org/pdf/1411.0290',
	 'subjects': u'Combinatorics (math.CO)',
	 'title': u'\nInterval cyclic edge-colorings of graphs',
	 'urllink': u'http://arxiv.org/abs/1411.0290'}
2015-04-10 11:07:03+0000 [xxu46_10] INFO: Crawled 478 pages (at 1 pages/min), scraped 471 items (at 1 items/min)
2015-04-10 11:07:30+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1411.0282> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 11:07:30+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1411.0282>
	{'abstract': u'This paper examines a general class of noisy matrix completion tasks where the goal is to estimate a matrix from observations obtained at a subset of its entries, each of which is subject to random noise or corruption. Our specific focus is on settings where the matrix to be estimated is well-approximated by a product of two (a priori unknown) matrices, one of which is sparse. Such structural models - referred to here as "sparse factor models" - have been widely used, for example, in subspace clustering applications, as well as in contemporary sparse modeling and dictionary learning tasks. Our main theoretical contributions are estimation error bounds for sparsity-regularized maximum likelihood estimators for problems of this form, which are applicable to a number of different observation noise or corruption models. Several specific implications are examined, including scenarios where observations are corrupted by additive Gaussian noise or additive heavier-tailed (Laplace) noise, Poisson-distributed observations, and highly-quantized (e.g., one-bit) observations. We also propose a simple algorithmic approach based on the alternating direction method of multipliers for these tasks, and provide experimental evidence to support our error analyses.',
	 'authors': u'Akshay Soni, Swayambhoo Jain, Jarvis Haupt, Stefano Gonella,',
	 'category': u'Computer Science ',
	 'date': '2014-11-2',
	 'pdflink': u'http://arxiv.org/pdf/1411.0282',
	 'subjects': u'Machine Learning (stat.ML)',
	 'title': u'\nNoisy Matrix Completion under Sparse Factor Models',
	 'urllink': u'http://arxiv.org/abs/1411.0282'}
2015-04-10 11:08:03+0000 [xxu46_10] INFO: Crawled 479 pages (at 1 pages/min), scraped 472 items (at 1 items/min)
2015-04-10 11:08:46+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1411.0247> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 11:08:46+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1411.0247>
	{'abstract': u"The brain processes information through many layers of neurons. This deep architecture is representationally powerful, but it complicates learning by making it hard to identify the responsible neurons when a mistake is made. In machine learning, the backpropagation algorithm assigns blame to a neuron by computing exactly how it contributed to an error. To do this, it multiplies error signals by matrices consisting of all the synaptic weights on the neuron's axon and farther downstream. This operation requires a precisely choreographed transport of synaptic weight information, which is thought to be impossible in the brain. Here we present a surprisingly simple algorithm for deep learning, which assigns blame by multiplying error signals by random synaptic weights. We show that a network can learn to extract useful information from signals sent through these random feedback connections. In essence, the network learns to learn. We demonstrate that this new mechanism performs as quickly and accurately as backpropagation on a variety of problems and describe the principles which underlie its function. Our demonstration provides a plausible basis for how a neuron can be adapted using error signals generated at distal locations in the brain, and thus dispels long-held assumptions about the algorithmic constraints on learning in neural circuits.",
	 'authors': u'Timothy P. Lillicrap, Daniel Cownden, Douglas B. Tweed, Colin J. Akerman,',
	 'category': u'Computer Science ',
	 'date': '2014-11-2',
	 'pdflink': u'http://arxiv.org/pdf/1411.0247',
	 'subjects': u'Neurons and Cognition (q-bio.NC)',
	 'title': u'\nRandom feedback weights support learning in deep neural networks',
	 'urllink': u'http://arxiv.org/abs/1411.0247'}
2015-04-10 11:09:03+0000 [xxu46_10] INFO: Crawled 480 pages (at 1 pages/min), scraped 473 items (at 1 items/min)
2015-04-10 11:09:37+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1411.0183> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 11:09:38+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1411.0183>
	{'abstract': u'A sensor network is considered where at each sensor a sequence of random variables is observed. At each time step, a processed version of the observations is transmitted from the sensors to a common node called the fusion center. At some unknown point in time the distribution of observations at an unknown subset of the sensor nodes changes. The objective is to detect the outlying sequences as quickly as possible, subject to constraints on the false alarm rate, the cost of observations taken at each sensor, and the cost of communication between the sensors and the fusion center. Minimax formulations are proposed for the above problem and algorithms are proposed that are shown to be asymptotically optimal for the proposed formulations, as the false alarm rate goes to zero. It is also shown, via numerical studies, that the proposed algorithms perform significantly better than those based on fractional sampling, in which the classical algorithms from the literature are used and the constraint on the cost of observations is met by using the outcome of a sequence of biased coin tosses, independent of the observation process.',
	 'authors': u'Taposh Banerjee, Venugopal V. Veeravalli,',
	 'category': u'Computer Science ',
	 'date': '2014-11-1',
	 'pdflink': u'http://arxiv.org/pdf/1411.0183',
	 'subjects': u'Statistics Theory (math.ST)',
	 'title': u'\nData-Efficient Quickest Outlying Sequence Detection in Sensor Networks',
	 'urllink': u'http://arxiv.org/abs/1411.0183'}
2015-04-10 11:10:03+0000 [xxu46_10] INFO: Crawled 481 pages (at 1 pages/min), scraped 474 items (at 1 items/min)
2015-04-10 11:10:44+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1411.0158> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 11:10:44+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1411.0158>
	{'abstract': u'We introduce a new tool, called the orbit automaton, that describes the action of an automaton group on the subtrees corresponding to the orbits of on levels of the tree. The connection between and the groups generated by the orbit automata is used to find elements of infinite order in certain automaton groups for which other methods failed to work.',
	 'authors': u'Ines Klimann, Matthieu Picantin, Dmytro Savchuk,',
	 'category': u'Computer Science ',
	 'date': '2014-11-1',
	 'pdflink': u'http://arxiv.org/pdf/1411.0158',
	 'subjects': u'Group Theory (math.GR)',
	 'title': u'\nOrbit automata as a new tool to attack the order problem in automaton  groups',
	 'urllink': u'http://arxiv.org/abs/1411.0158'}
2015-04-10 11:11:03+0000 [xxu46_10] INFO: Crawled 482 pages (at 1 pages/min), scraped 475 items (at 1 items/min)
2015-04-10 11:11:57+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1411.0024> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 11:11:57+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1411.0024>
	{'abstract': u'Many learning tasks, such as cross-validation, parameter search, or leave-one-out analysis, involve multiple instances of similar problems, each instance sharing a large part of learning data with the others. We introduce a robust framework for solving multiple square-root LASSO problems, based on a sketch of the learning data that uses low-rank approximations. Our approach allows a dramatic reduction in computational effort, in effect reducing the number of observations from (the number of observations to start with) to (the number of singular values retained in the low-rank model), while not sacrificing---sometimes even improving---the statistical performance. Theoretical analysis, as well as numerical experiments on both synthetic and real data, illustrate the efficiency of the method in large scale applications.',
	 'authors': u'Vu Pham, Laurent El Ghaoui, Arturo Fernandez,',
	 'category': u'Computer Science ',
	 'date': '2014-10-30',
	 'pdflink': u'http://arxiv.org/pdf/1411.0024',
	 'subjects': u'Optimization and Control (math.OC)',
	 'title': u'\nRobust sketching for multiple square-root LASSO problems',
	 'urllink': u'http://arxiv.org/abs/1411.0024'}
2015-04-10 11:12:03+0000 [xxu46_10] INFO: Crawled 483 pages (at 1 pages/min), scraped 476 items (at 1 items/min)
2015-04-10 11:12:45+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1411.8003> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 11:12:45+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1411.8003>
	{'abstract': u'Matrix factorization is a popular approach for large-scale matrix completion and constitutes a basic component of many solutions for Netflix Prize competition. In this approach, the unknown low-rank matrix is expressed as the product of two much smaller matrices so that the low-rank property is automatically fulfilled. The resulting optimization problem, even with huge size, can be solved (to stationary points) very efficiently through standard optimization algorithms such as alternating minimization and stochastic gradient descent (SGD). However, due to the non-convexity caused by the factorization model, there is a limited theoretical understanding of whether these algorithms will generate a good solution. In this paper, we establish a theoretical guarantee for the factorization based formulation to correctly recover the underlying low-rank matrix. In particular, we show that under similar conditions to those in previous works, many standard optimization algorithms converge to the global optima of the factorization based formulation, thus recovering the true low-rank matrix. To the best of our knowledge, our result is the first one that provides recovery guarantee for many standard algorithms such as gradient descent, SGD and block coordinate gradient descent. Our result also applies to alternating minimization, and a notable difference from previous studies on alternating minimization is that we do not need the resampling scheme (i.e. using independent samples in each iteration).',
	 'authors': u'Ruoyu Sun, Zhi-Quan Luo,',
	 'category': u'Computer Science ',
	 'date': '2014-11-28',
	 'pdflink': u'http://arxiv.org/pdf/1411.8003',
	 'subjects': u'Learning (cs.LG)',
	 'title': u'\nGuaranteed Matrix Completion via Non-convex Factorization',
	 'urllink': u'http://arxiv.org/abs/1411.8003'}
2015-04-10 11:13:03+0000 [xxu46_10] INFO: Crawled 484 pages (at 1 pages/min), scraped 477 items (at 1 items/min)
2015-04-10 11:14:03+0000 [xxu46_10] INFO: Crawled 484 pages (at 0 pages/min), scraped 477 items (at 0 items/min)
2015-04-10 11:14:04+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1411.7974> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 11:14:04+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1411.7974>
	{'abstract': u'We propose a novel online learning method for minimizing regret in large extensive-form games. The approach learns a function approximator online to estimate the regret for choosing a particular action. A no-regret algorithm uses these estimates in place of the true regrets to define a sequence of policies. We prove the approach sound by providing a bound relating the quality of the function approximation and regret of the algorithm. A corollary being that the method is guaranteed to converge to a Nash equilibrium in self-play so long as the regrets are ultimately realizable by the function approximator. Our technique can be understood as a principled generalization of existing work on abstraction in large games; in our work, both the abstraction as well as the equilibrium are learned during self-play. We demonstrate empirically the method achieves higher quality strategies than state-of-the-art abstraction techniques given the same resources.',
	 'authors': u'Kevin Waugh, Dustin Morrill, J. Andrew Bagnell, Michael Bowling,',
	 'category': u'Computer Science ',
	 'date': '2014-11-28',
	 'pdflink': u'http://arxiv.org/pdf/1411.7974',
	 'subjects': u'Artificial Intelligence (cs.AI)',
	 'title': u'\nSolving Games with Functional Regret Estimation',
	 'urllink': u'http://arxiv.org/abs/1411.7974'}
2015-04-10 11:15:03+0000 [xxu46_10] INFO: Crawled 485 pages (at 1 pages/min), scraped 478 items (at 1 items/min)
2015-04-10 11:15:16+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1411.7973> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 11:15:16+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1411.7973>
	{'abstract': u'Many factors can affect the predictability of public bus services such as traffic, weather and local events. Other aspects, such as day of week or hour of day, may influence bus travel times as well, either directly or in conjunction with other variables. However, the exact nature of such relationships between travel times and predictor variables is, in most situations, not known. In this paper we develop a framework that allows for flexible modeling of bus travel times through the use of Additive Models. In particular, we model travel times as a sum of linear as well as nonlinear terms that are modeled as smooth functions of predictor variables. The proposed class of models provides a principled statistical framework that is highly flexible in terms of model building. The experimental results demonstrate uniformly superior performance of our best model as compared to previous prediction methods when applied to a very large GPS data set obtained from buses operating in the city of Rio de Janeiro.',
	 'authors': u'Matthias Kormaksson, Luciano Barbosa, Marcos R. Vieira, Bianca Zadrozny,',
	 'category': u'Computer Science ',
	 'date': '2014-11-28',
	 'pdflink': u'http://arxiv.org/pdf/1411.7973',
	 'subjects': u'Learning (cs.LG)',
	 'title': u'\nBus Travel Time Predictions Using Additive Models',
	 'urllink': u'http://arxiv.org/abs/1411.7973'}
2015-04-10 11:16:03+0000 [xxu46_10] INFO: Crawled 486 pages (at 1 pages/min), scraped 479 items (at 1 items/min)
2015-04-10 11:16:34+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1411.7964> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 11:16:34+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1411.7964>
	{'abstract': u'"Frontalization" is the process of synthesizing frontal facing views of faces appearing in single unconstrained photos. Recent reports have suggested that this process may substantially boost the performance of face recognition systems. This, by transforming the challenging problem of recognizing faces viewed from unconstrained viewpoints to the easier problem of recognizing faces in constrained, forward facing poses. Previous frontalization methods did this by attempting to approximate 3D facial shapes for each query image. We observe that 3D face shape estimation from unconstrained photos may be a harder problem than frontalization and can potentially introduce facial misalignments. Instead, we explore the simpler approach of using a single, unmodified, 3D surface as an approximation to the shape of all input faces. We show that this leads to a straightforward, efficient and easy to implement method for frontalization. More importantly, it produces aesthetic new frontal views and is surprisingly effective when used for face recognition and gender estimation.',
	 'authors': u'Tal Hassner, Shai Harel, Eran Paz, Roee Enbar,',
	 'category': u'Computer Science ',
	 'date': '2014-11-28',
	 'pdflink': u'http://arxiv.org/pdf/1411.7964',
	 'subjects': u'Computer Vision and Pattern Recognition (cs.CV)',
	 'title': u'\nEffective Face Frontalization in Unconstrained Images',
	 'urllink': u'http://arxiv.org/abs/1411.7964'}
2015-04-10 11:17:03+0000 [xxu46_10] INFO: Crawled 487 pages (at 1 pages/min), scraped 480 items (at 1 items/min)
2015-04-10 11:17:28+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1411.7960> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 11:17:28+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1411.7960>
	{'abstract': u"This paper presents the first systematic investigation of the potential performance gains for crowdsourcing systems, deriving from available information at the requester about individual worker earnestness (reputation). In particular, we first formalize the optimal task assignment problem when workers' reputation estimates are available, as the maximization of a monotone (submodular) function subject to Matroid constraints. Then, being the optimal problem NP-hard, we propose a simple but efficient greedy heuristic task allocation algorithm. We also propose a simple ``maximum a-posteriori`` decision rule. Finally, we test and compare different solutions, showing that system performance can greatly benefit from information about workers' reputation. Our main findings are that: i) even largely inaccurate estimates of workers' reputation can be effectively exploited in the task assignment to greatly improve system performance; ii) the performance of the maximum a-posteriori decision rule quickly degrades as worker reputation estimates become inaccurate; iii) when workers' reputation estimates are significantly inaccurate, the best performance can be obtained by combining our proposed task assignment algorithm with the LRA decision rule introduced in the literature.",
	 'authors': u'Alberto Tarable, Alessandro Nordio, Emilio Leonardi, Marco Ajmone Marsan,',
	 'category': u'Computer Science ',
	 'date': '2014-11-26',
	 'pdflink': u'http://arxiv.org/pdf/1411.7960',
	 'subjects': u'Human-Computer Interaction (cs.HC)',
	 'title': u'\nThe Importance of Being Earnest in Crowdsourcing Systems',
	 'urllink': u'http://arxiv.org/abs/1411.7960'}
2015-04-10 11:18:03+0000 [xxu46_10] INFO: Crawled 488 pages (at 1 pages/min), scraped 481 items (at 1 items/min)
2015-04-10 11:18:23+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1411.7942> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 11:18:23+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1411.7942>
	{'abstract': u'The functional approach to compositional distributional semantics considers transitive verbs to be linear maps that transform the distributional vectors representing nouns into a vector representing a sentence. We conduct an initial investigation that uses a matrix consisting of the parameters of a logistic regression classifier trained on a plausibility task as a transitive verb function. We compare our method to a commonly used corpus-based method for constructing a verb matrix and find that the plausibility training may be more effective for disambiguation tasks.',
	 'authors': u'Tamara Polajnar, Laura Rimell, Stephen Clark,',
	 'category': u'Computer Science ',
	 'date': '2014-11-28',
	 'pdflink': u'http://arxiv.org/pdf/1411.7942',
	 'subjects': u'Computation and Language (cs.CL)',
	 'title': u'\nUsing Sentence Plausibility to Learn the Semantics of Transitive Verbs',
	 'urllink': u'http://arxiv.org/abs/1411.7942'}
2015-04-10 11:19:03+0000 [xxu46_10] INFO: Crawled 489 pages (at 1 pages/min), scraped 482 items (at 1 items/min)
2015-04-10 11:19:42+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1411.7935> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 11:19:42+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1411.7935>
	{'abstract': u'Multiple people tracking is a key problem for many applications such as surveillance, animation or car navigation, and a key input for tasks such as activity recognition. In crowded environments occlusions and false detections are common, and although there have been substantial advances in recent years, tracking is still a challenging task. Tracking is typically divided into two steps: detection, i.e., locating the pedestrians in the image, and data association, i.e., linking detections across frames to form complete trajectories. For the data association task, approaches typically aim at developing new, more complex formulations, which in turn put the focus on the optimization techniques required to solve them. However, they still utilize very basic information such as distance between detections. In this thesis, I focus on the data association task and argue that there is contextual information that has not been fully exploited yet in the tracking community, mainly social context and spatial context coming from different views.',
	 'authors': u'Laura Leal-Taix\xe9,',
	 'category': u'Computer Science ',
	 'date': '2014-11-24',
	 'pdflink': u'http://arxiv.org/pdf/1411.7935',
	 'subjects': u'Computer Vision and Pattern Recognition (cs.CV)',
	 'title': u'\nMultiple object tracking with context awareness',
	 'urllink': u'http://arxiv.org/abs/1411.7935'}
2015-04-10 11:20:03+0000 [xxu46_10] INFO: Crawled 490 pages (at 1 pages/min), scraped 483 items (at 1 items/min)
2015-04-10 11:20:15+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1411.7925> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 11:20:15+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1411.7925>
	{'abstract': u"Arkan's polar coding technique is based on the idea of synthesizing channels from the instances of the physical channel by a simple linear encoding transformation. Each synthesized channel corresponds to a particular input to the encoder. For large , the synthesized channels become either essentially noiseless or almost perfectly noisy, but in total carry as much information as the original channels. Capacity can therefore be achieved by transmitting messages over the essentially noiseless synthesized channels. Unfortunately, the set of inputs corresponding to reliable synthesized channels is poorly understood, in particular how the set depends on the underlying physical channel. In this work, we present two analytic conditions sufficient to determine if the reliable inputs corresponding to different discrete memoryless channels are aligned or not, i.e. if one set is contained in the other. Understanding the alignment of the polarized sets is important as it is directly related to universality properties of the induced polar codes, which are essential in particular for network coding problems. We demonstrate the performance of our conditions on a few examples for wiretap and broadcast channels. Finally we show that these conditions imply that the simple quantum polar coding scheme of Renes et al. [Phys. Rev. Lett. 109, 050504 (2012)] requires entanglement assistance for general channels, but also show such assistance to be unnecessary in many cases of interest.",
	 'authors': u'Joseph M. Renes, David Sutter, S. Hamed Hassani,',
	 'category': u'Computer Science ',
	 'date': '2014-11-28',
	 'pdflink': u'http://arxiv.org/pdf/1411.7925',
	 'subjects': u'Information Theory (cs.IT)',
	 'title': u'\nAlignment of Polarized Sets',
	 'urllink': u'http://arxiv.org/abs/1411.7925'}
2015-04-10 11:21:03+0000 [xxu46_10] INFO: Crawled 491 pages (at 1 pages/min), scraped 484 items (at 1 items/min)
2015-04-10 11:21:09+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1411.7923> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 11:21:09+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1411.7923>
	{'abstract': u'Pushing by big data and deep convolutional neural network (CNN), the performance of face recognition is becoming comparable to human. Using private large scale training datasets, several groups achieve very high performance on LFW, i.e., 97% to 99%. While there are many open source implementations of CNN, none of large scale face dataset is publicly available. The current situation in the field of face recognition is that data is more important than algorithm. To solve this problem, this paper proposes a semi-automatical way to collect face images from Internet and builds a large scale dataset containing about 10,000 subjects and 500,000 images, called CASIAWebFace. Based on the database, we use a 11-layer CNN to learn discriminative representation and obtain state-of-theart accuracy on LFW and YTF. The publication of CASIAWebFace will attract more research groups entering this field and accelerate the development of face recognition in the wild.',
	 'authors': u'Dong Yi, Zhen Lei, Shengcai Liao, Stan Z. Li,',
	 'category': u'Computer Science ',
	 'date': '2014-11-28',
	 'pdflink': u'http://arxiv.org/pdf/1411.7923',
	 'subjects': u'Computer Vision and Pattern Recognition (cs.CV)',
	 'title': u'\nLearning Face Representation from Scratch',
	 'urllink': u'http://arxiv.org/abs/1411.7923'}
2015-04-10 11:22:03+0000 [xxu46_10] INFO: Crawled 492 pages (at 1 pages/min), scraped 485 items (at 1 items/min)
2015-04-10 11:22:14+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1411.7911> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 11:22:14+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1411.7911>
	{'abstract': u'We propose a novel approach to synthesizing images that are effective for training object detectors. Starting from a small set of real images, our algorithm estimates the rendering parameters required to synthesize similar images given a coarse 3D model of the target object. These parameters can then be reused to generate an unlimited number of training images of the object of interest in arbitrary 3D poses, which can then be used to increase classification performances. A key insight of our approach is that the synthetically generated images should be similar to real images, not in terms of image quality, but rather in terms of features used during the detector training. We show in the context of drone, plane, and car detection that using such synthetically generated images yields significantly better performances than simply perturbing real images or even synthesizing images in such way that they look very realistic, as is often done when only limited amounts of training data are available.',
	 'authors': u'Artem Rozantsev, Vincent Lepetit, Pascal Fua,',
	 'category': u'Computer Science ',
	 'date': '2014-11-28',
	 'pdflink': u'http://arxiv.org/pdf/1411.7911',
	 'subjects': u'Computer Vision and Pattern Recognition (cs.CV)',
	 'title': u'\nOn Rendering Synthetic Images for Training an Object Detector',
	 'urllink': u'http://arxiv.org/abs/1411.7911'}
2015-04-10 11:23:03+0000 [xxu46_10] INFO: Crawled 493 pages (at 1 pages/min), scraped 486 items (at 1 items/min)
2015-04-10 11:23:14+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1411.7910> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 11:23:14+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1411.7910>
	{'abstract': u'In-memory (transactional) data stores are recognized as a first-class data management technology for cloud platforms, thanks to their ability to match the elasticity requirements imposed by the pay-as-you-go cost model. On the other hand, defining the well-suited amount of cache servers to be deployed, and the degree of in-memory replication of slices of data, in order to optimize reliability/availability and performance tradeoffs, is far from being a trivial task. Yet, it is an essential aspect of the provisioning process of cloud platforms, given that it has an impact on how well cloud resources are actually exploited. To cope with the issue of determining optimized configurations of cloud in-memory data stores, in this article we present a flexible simulation framework offering skeleton simulation models that can be easily specialized in order to capture the dynamics of diverse data grid systems, such as those related to the specific protocol used to provide data consistency and/or transactional guarantees. Besides its flexibility, another peculiar aspect of the framework lies in that it integrates simulation and machine-learning (black-box) techniques, the latter being essentially used to capture the dynamics of the data-exchange layer (e.g. the message passing layer) across the cache servers. This is a relevant aspect when considering that the actual data-transport/networking infrastructure on top of which the data grid is deployed might be unknown, hence being not feasible to be modeled via white-box (namely purely simulative) approaches. We also provide an extended experimental study aimed at validating instances of simulation models supported by our framework against execution dynamics of real data grid systems deployed on top of either private or public cloud infrastructures.',
	 'authors': u'Pierangelo Di Sanzo, Francesco Quaglia, Bruno Ciciani, Alessandro Pellegrini, Diego Didona, Paolo Romano, Roberto Palmieri, Sebastiano Peluso,',
	 'category': u'Computer Science ',
	 'date': '2014-11-28',
	 'pdflink': u'http://arxiv.org/pdf/1411.7910',
	 'subjects': u'Performance (cs.PF)',
	 'title': u'\nA Flexible Framework for Accurate Simulation of Cloud In-Memory Data  Stores',
	 'urllink': u'http://arxiv.org/abs/1411.7910'}
2015-04-10 11:24:03+0000 [xxu46_10] INFO: Crawled 494 pages (at 1 pages/min), scraped 487 items (at 1 items/min)
2015-04-10 11:24:17+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1411.7883> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 11:24:17+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1411.7883>
	{'abstract': u'We propose an unsupervised approach for discovering characteristic motion patterns in videos of highly articulated objects performing natural, unscripted behaviors, such as tigers in the wild. We discover consistent patterns in a bottom-up manner by analyzing the relative displacements of large numbers of ordered trajectory pairs through time, such that each trajectory is attached to a different moving part on the object. The pairs of trajectories descriptor relies entirely on motion and is more discriminative than state-of-the-art features that employ single trajectories. Our method generates temporal video intervals, each automatically trimmed to one instance of the discovered behavior, and clusters them by type (e.g., running, turning head, drinking water). We present experiments on two datasets: dogs from the YouTube objects and a new dataset of National Geographic tiger videos. Results confirm that our proposed descriptor outperforms existing appearance- and trajectory-based descriptors (e.g., HOG and IDTF) on both datasets and enables us to segment unconstrained animal video into intervals containing single behaviors.',
	 'authors': u'Luca Del Pero, Susanna Ricco, Rahul Sukthankar, Vittorio Ferrari,',
	 'category': u'Computer Science ',
	 'date': '2014-11-28',
	 'pdflink': u'http://arxiv.org/pdf/1411.7883',
	 'subjects': u'Computer Vision and Pattern Recognition (cs.CV)',
	 'title': u'\nArticulated motion discovery using pairs of trajectories',
	 'urllink': u'http://arxiv.org/abs/1411.7883'}
2015-04-10 11:25:03+0000 [xxu46_10] INFO: Crawled 495 pages (at 1 pages/min), scraped 488 items (at 1 items/min)
2015-04-10 11:25:28+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1411.7855> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 11:25:28+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1411.7855>
	{'abstract': u'V-variable fractals, where is a positive integer, are intuitively fractals with at most different "forms" or "shapes" at all levels of magnification. In this paper we describe how V-variable fractals can be used for the purpose of image compression.',
	 'authors': u'Franklin Mendivil, \xd6rjan Stenflo,',
	 'category': u'Computer Science ',
	 'date': '2014-11-28',
	 'pdflink': u'http://arxiv.org/pdf/1411.7855',
	 'subjects': u'Computer Vision and Pattern Recognition (cs.CV)',
	 'title': u'\nV-variable image compression',
	 'urllink': u'http://arxiv.org/abs/1411.7855'}
2015-04-10 11:26:03+0000 [xxu46_10] INFO: Crawled 496 pages (at 1 pages/min), scraped 489 items (at 1 items/min)
2015-04-10 11:26:36+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1411.7839> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 11:26:36+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1411.7839>
	{'abstract': u'We provide a formal model of tracing JIT compilation of programs using abstract interpretation. Hot path detection corresponds to an abstraction of the trace semantics of the program. The optimization phase corresponds to a transform of the original program that preserves its trace semantics up to an observation modeled by some abstraction. We provide a generic framework to express dynamic optimizations and prove them correct. We instantiate it to prove the correctness of dynamic type specialization. We show that our framework is more general than the model of tracing compilation introduced by Guo and Palsberg [2011] based on operational bisimulations.',
	 'authors': u'Stefano Dissegna, Francesco Logozzo, Francesco Ranzato,',
	 'category': u'Computer Science ',
	 'date': '2014-11-28',
	 'pdflink': u'http://arxiv.org/pdf/1411.7839',
	 'subjects': u'Programming Languages (cs.PL)',
	 'title': u'\nAn Abstract Interpretation-based Model of Tracing Just-In-Time  Compilation',
	 'urllink': u'http://arxiv.org/abs/1411.7839'}
2015-04-10 11:27:03+0000 [xxu46_10] INFO: Crawled 497 pages (at 1 pages/min), scraped 490 items (at 1 items/min)
2015-04-10 11:27:49+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1411.7838> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 11:27:49+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1411.7838>
	{'abstract': u'The NP-hard EFFECTORS problem on directed graphs is motivated by applications in network mining, particularly concerning the analysis of (random) information-propagation processes. In the corresponding model the arcs carry probabilities and there is a probabilistic diffusion process activating nodes by neighboring activated nodes with probabilities as specified by the arcs. The point is to explain a given network activation state best possible using a minimum number of "effector nodes"; these are selected before the activation process starts. We complement and extend previous work from the data mining community by a more thorough computational complexity analysis of EFFECTORS, identifying both tractable and intractable cases. To this end, we also exploit a parameterization measuring the "degree of randomness" (the number of "really" probabilistic arcs) which might prove useful for analyzing other probabilistic network diffusion problems.',
	 'authors': u'Laurent Bulteau, Stefan Fafianie, Vincent Froese, Rolf Niedermeier, Nimrod Talmon,',
	 'category': u'Computer Science ',
	 'date': '2014-11-28',
	 'pdflink': u'http://arxiv.org/pdf/1411.7838',
	 'subjects': u'Data Structures and Algorithms (cs.DS)',
	 'title': u'\nThe Complexity of Finding Effectors',
	 'urllink': u'http://arxiv.org/abs/1411.7838'}
2015-04-10 11:28:03+0000 [xxu46_10] INFO: Crawled 498 pages (at 1 pages/min), scraped 491 items (at 1 items/min)
2015-04-10 11:28:46+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1411.7825> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 11:28:46+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1411.7825>
	{'abstract': u'We prove that the model checking and the satisfiability problem of both Dynamic Logic of Propositional Assignments DL-PA and Coalition Logic of Propositional Control and Delegation DCL-PC are in PSPACE. We explain why the proof of EXPTIME-hardness of the model checking problem of DL-PA presented in (Balbiani, Herzig, Troquard, 2013) is false. We also explain why the proof of membership in PSPACE of the model checking problem of DCL-PC given in (van der Hoek, Walther, Wooldridge, 2010) is wrong.',
	 'authors': u'Philippe Balbiani, Andreas Herzig, Fran\xe7ois Schwarzentruber, Nicolas Troquard,',
	 'category': u'Computer Science ',
	 'date': '2014-11-28',
	 'pdflink': u'http://arxiv.org/pdf/1411.7825',
	 'subjects': u'Logic in Computer Science (cs.LO)',
	 'title': u'\nDL-PA and DCL-PC: model checking and satisfiability problem are indeed  in PSPACE',
	 'urllink': u'http://arxiv.org/abs/1411.7825'}
2015-04-10 11:29:03+0000 [xxu46_10] INFO: Crawled 499 pages (at 1 pages/min), scraped 492 items (at 1 items/min)
2015-04-10 11:29:48+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1411.7820> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 11:29:48+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1411.7820>
	{'abstract': u'We present a method for coarse-grained cross-lingual alignment of comparable texts: segments consisting of contiguous paragraphs that discuss the same theme (e.g. history, economy) are aligned based on induced multilingual topics. The method combines three ideas: a two-level LDA model that filters out words that do not convey themes, an HMM that models the ordering of themes in the collection of documents, and language-independent concept annotations to serve as a cross-language bridge and to strengthen the connection between paragraphs in the same segment through concept relations. The method is evaluated on English and French data previously used for monolingual alignment. The results show state-of-the-art performance in both monolingual and cross-lingual settings.',
	 'authors': u'Vivi Nastase, Angela Fahrni,',
	 'category': u'Computer Science ',
	 'date': '2014-11-28',
	 'pdflink': u'http://arxiv.org/pdf/1411.7820',
	 'subjects': u'Computation and Language (cs.CL)',
	 'title': u'\nCoarse-grained Cross-lingual Alignment of Comparable Texts with Topic  Models and Encyclopedic Knowledge',
	 'urllink': u'http://arxiv.org/abs/1411.7820'}
2015-04-10 11:30:03+0000 [xxu46_10] INFO: Crawled 500 pages (at 1 pages/min), scraped 493 items (at 1 items/min)
2015-04-10 11:31:03+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1411.7819> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 11:31:03+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1411.7819>
	{'abstract': u'Teramoto et al. defined a new measure of uniformity of point distribution called the emph that measures the uniformity of a finite point set sampled from , a bounded subset of . We attempt to generalize the definition of this measure over all metric spaces. While they look at online algorithms minimizing the measure at every instance, wherein the final size of the sampled set may not be known a priori, we look at instances in which the final size is known and we wish to minimize the final gap ratio. We solve optimization related questions about selecting uniform point samples from metric spaces; the uniformity is measured using gap ratio. We give lower bounds for specific as well as general instances, prove hardness results on specific metric spaces, and a general approximation algorithm framework giving different approximation ratios for different metric spaces.',
	 'authors': u'Arijit Bishnu, Sameer Desai, Arijit Ghosh, Mayank Goswami, Subhabrata Paul,',
	 'category': u'Computer Science ',
	 'date': '2014-11-28',
	 'pdflink': u'http://arxiv.org/pdf/1411.7819',
	 'subjects': u'Computational Geometry (cs.CG)',
	 'title': u'\nUniformity of point samples in metric spaces using gap ratio',
	 'urllink': u'http://arxiv.org/abs/1411.7819'}
2015-04-10 11:31:03+0000 [xxu46_10] INFO: Crawled 501 pages (at 1 pages/min), scraped 494 items (at 1 items/min)
2015-04-10 11:32:03+0000 [xxu46_10] INFO: Crawled 501 pages (at 0 pages/min), scraped 494 items (at 0 items/min)
2015-04-10 11:32:21+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1411.7816> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 11:32:21+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1411.7816>
	{'abstract': u'We correct a partial mistake for a metric presented in the article "Lattice constellation and codes from quadratic number fields" [IEEE Trans. Inform. Theory, vol. 47, No. 4, May. 2001]. We show that the metric defined in the article is not true, therefore, this brings about to destroy the encoding and decoding procedures. Also, we define a proper metric for some codes defined in the article and show that there exist some error correcting perfect codes with respect to this new metric.',
	 'authors': u'Murat G\xfczeltepe,',
	 'category': u'Computer Science ',
	 'date': '2014-11-28',
	 'pdflink': u'http://arxiv.org/pdf/1411.7816',
	 'subjects': u'Information Theory (cs.IT)',
	 'title': u'\nErratum to "Lattice constellation and codes from quadratic number  fields" [IEEE Trans. Inform. Theory, vol. 47, No. 4, May. 2001]',
	 'urllink': u'http://arxiv.org/abs/1411.7816'}
2015-04-10 11:33:03+0000 [xxu46_10] INFO: Crawled 502 pages (at 1 pages/min), scraped 495 items (at 1 items/min)
2015-04-10 11:33:23+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1411.7812> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 11:33:23+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1411.7812>
	{'abstract': u'We study the computational complexity of candidate control in elections with few voters (that is, we take the number of voters as a parameter). We consider both the standard scenario of adding and deleting candidates, where one asks if a given candidate can become a winner (or, in the destructive case, can be precluded from winning) by adding/deleting some candidates, and a combinatorial scenario where adding/deleting a candidate automatically means adding/deleting a whole group of candidates. Our results show that the parameterized complexity of candidate control (with the number of voters as the parameter) is much more varied than in the setting with many voters.',
	 'authors': u'Jiehua Chen, Piotr Faliszewski, Rolf Niedermeier, Nimrod Talmon,',
	 'category': u'Computer Science ',
	 'date': '2014-11-28',
	 'pdflink': u'http://arxiv.org/pdf/1411.7812',
	 'subjects': u'Artificial Intelligence (cs.AI)',
	 'title': u'\nElections with Few Voters: Candidate Control Can Be Easy',
	 'urllink': u'http://arxiv.org/abs/1411.7812'}
2015-04-10 11:33:59+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1411.7806> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 11:33:59+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1411.7806>
	{'abstract': u'Outline of several strategies for using Gaussian processes as surrogate models for the covariance matrix adaptation evolution strategy (CMA-ES).',
	 'authors': u'Luk\xe1\u0161 Bajer, Martin Hole\u0148a,',
	 'category': u'Computer Science ',
	 'date': '2014-11-28',
	 'pdflink': u'http://arxiv.org/pdf/1411.7806',
	 'subjects': u'Neural and Evolutionary Computing (cs.NE)',
	 'title': u'\nTwo Gaussian Approaches to Black-Box Optomization',
	 'urllink': u'http://arxiv.org/abs/1411.7806'}
2015-04-10 11:34:03+0000 [xxu46_10] INFO: Crawled 504 pages (at 2 pages/min), scraped 497 items (at 2 items/min)
2015-04-10 11:35:03+0000 [xxu46_10] INFO: Crawled 504 pages (at 0 pages/min), scraped 497 items (at 0 items/min)
2015-04-10 11:35:12+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1411.7803> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 11:35:12+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1411.7803>
	{'abstract': u'With the rapid development of internet technologies, social networks, and other related areas, user authentication becomes more and more important to protect the data of the users. Password authentication is one of the widely used methods to achieve authentication for legal users and defense against intruders. There have been many password cracking methods developed during the past years, and people have been designing the countermeasures against password cracking all the time. However, we find that the survey work on the password cracking research has not been done very much. This paper is mainly to give a brief review of the password cracking methods, import technologies of password cracking, and the countermeasures against password cracking that are usually designed at two stages including the password design stage (e.g. user education, dynamic password, use of tokens, computer generations) and after the design (e.g. reactive password checking, proactive password checking, password encryption, access control). The main objective of this work is offering the abecedarian IT security professionals and the common audiences with some knowledge about the computer security and password cracking, and promoting the development of this area.',
	 'authors': u'Aaron L.-F. Han, Derek F. Wong, Lidia S. Chao,',
	 'category': u'Computer Science ',
	 'date': '2014-11-28',
	 'pdflink': u'http://arxiv.org/pdf/1411.7803',
	 'subjects': u'Cryptography and Security (cs.CR)',
	 'title': u'\nPassword Cracking and Countermeasures in Computer Security: A Survey',
	 'urllink': u'http://arxiv.org/abs/1411.7803'}
2015-04-10 11:36:03+0000 [xxu46_10] INFO: Crawled 505 pages (at 1 pages/min), scraped 498 items (at 1 items/min)
2015-04-10 11:36:09+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1411.7798> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 11:36:09+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1411.7798>
	{'abstract': u'In multimedia applications, the text and image components in a web document form a pairwise constraint that potentially indicates the same semantic concept. This paper studies cross-modal learning via the pairwise constraint, and aims to find the common structure hidden in different modalities. We first propose a compound regularization framework to deal with the pairwise constraint, which can be used as a general platform for developing cross-modal algorithms. For unsupervised learning, we propose a cross-modal subspace clustering method to learn a common structure for different modalities. For supervised learning, to reduce the semantic gap and the outliers in pairwise constraints, we propose a cross-modal matching method based on compound ?21 regularization along with an iteratively reweighted algorithm to find the global optimum. Extensive experiments demonstrate the benefits of joint text and image modeling with semantically induced pairwise constraints, and show that the proposed cross-modal methods can further reduce the semantic gap between different modalities and improve the clustering/retrieval accuracy.',
	 'authors': u'Ran He, Man Zhang, Liang Wang, Ye Ji, Qiyue Yin,',
	 'category': u'Computer Science ',
	 'date': '2014-11-28',
	 'pdflink': u'http://arxiv.org/pdf/1411.7798',
	 'subjects': u'Computer Vision and Pattern Recognition (cs.CV)',
	 'title': u'\nCross-Modal Learning via Pairwise Constraints',
	 'urllink': u'http://arxiv.org/abs/1411.7798'}
2015-04-10 11:37:03+0000 [xxu46_10] INFO: Crawled 506 pages (at 1 pages/min), scraped 499 items (at 1 items/min)
2015-04-10 11:37:28+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1411.7785> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 11:37:28+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1411.7785>
	{'abstract': u"We propose a model for heterogeneous cellular networks assuming a space-time Poisson process of call arrivals, independently marked by data volumes, and served by different types of base stations (having different transmission powers) represented by the superposition of independent Poisson processes on the plane. Each station applies a processor sharing policy to serve users arriving in its vicinity, modeled by the Voronoi cell perturbed by some random signal propagation effects (shadowing). Users' peak service rates depend on their signal-to-interference-and-noise ratios (SINR) with respect to the serving station. The mutual-dependence of the cells (due to the extra-cell interference) is captured via some system of cell-load equations impacting the spatial distribution of the SINR. We use this model to study in a semi-analytic way (involving only static simulations, with the temporal evolution handled by the queuing theoretic results) network performance metrics (cell loads, mean number of users) and the quality of service perceived by the users (mean throughput) served by different types of base stations. Our goal is to identify macroscopic laws regarding these performance metrics, involving averaging both over time and the network geometry. The reveled laws are validated against real field measurement in an operational network.",
	 'authors': u'Bartlomiej Blaszczyszyn, Miodrag Jovanovic, Mohamed Kadhem Karray,',
	 'category': u'Computer Science ',
	 'date': '2014-11-28',
	 'pdflink': u'http://arxiv.org/pdf/1411.7785',
	 'subjects': u'Networking and Internet Architecture (cs.NI)',
	 'title': u'\nPerformance laws of large heterogeneous cellular networks',
	 'urllink': u'http://arxiv.org/abs/1411.7785'}
2015-04-10 11:38:03+0000 [xxu46_10] INFO: Crawled 507 pages (at 1 pages/min), scraped 500 items (at 1 items/min)
2015-04-10 11:38:32+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1411.7766> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 11:38:32+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1411.7766>
	{'abstract': u"Predicting face attributes from web images is challenging due to background clutters and face variations. A novel deep learning framework is proposed for face attribute prediction in the wild. It cascades two CNNs (LNet and ANet) for face localization and attribute prediction respectively. These nets are trained in a cascade manner with attribute labels, but pre-trained differently. LNet is pre-trained with massive general object categories, while ANet is pre-trained with massive face identities. This framework not only outperforms state-of-the-art with large margin, but also reveals multiple valuable facts on learning face representation as below. (1) It shows how LNet and ANet can be improved by different pre-training strategies. (2) It reveals that although filters of LNet are fine-tuned by attribute labels, their response maps over the entire image have strong indication of face's location. This fact enables training LNet for face localization with only attribute tags, but without face bounding boxes (which are required by all detection works). With a novel fast feed-forward scheme, the cascade of LNet and ANet can localize faces and recognize attributes in images with arbitrary sizes in real time. (3) It also demonstrates that the high-level hidden neurons of ANet automatically discover semantic concepts after pre-training, and such concepts are significantly enriched after fine-tuning. Each attribute can be well explained by a sparse linear combination of these concepts. By analyzing such combinations, attributes show clear grouping patterns, which could be well interpreted semantically.",
	 'authors': u'Ziwei Liu, Ping Luo, Xiaogang Wang, Xiaoou Tang,',
	 'category': u'Computer Science ',
	 'date': '2014-11-28',
	 'pdflink': u'http://arxiv.org/pdf/1411.7766',
	 'subjects': u'Computer Vision and Pattern Recognition (cs.CV)',
	 'title': u'\nDeep Learning Face Attributes in the Wild',
	 'urllink': u'http://arxiv.org/abs/1411.7766'}
2015-04-10 11:39:03+0000 [xxu46_10] INFO: Crawled 508 pages (at 1 pages/min), scraped 501 items (at 1 items/min)
2015-04-10 11:39:51+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1411.7756> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 11:39:51+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1411.7756>
	{'abstract': u'secure multi-party computation is widely studied area in computer science. It is touching all most every aspect of human life. This paper demonstrates theoretical and experimental results of one of the secure multi-party computation protocols proposed by Shukla et al. implemented using visual C++. Data outflow probability is computed by changing parameters. At the end, time and space complexity is calculated using theoretical and experimental results.',
	 'authors': u'Samiksha Shukla, G. Sadashivappa, Durgesh Kumar Mishra,',
	 'category': u'Computer Science ',
	 'date': '2014-11-28',
	 'pdflink': u'http://arxiv.org/pdf/1411.7756',
	 'subjects': u'Cryptography and Security (cs.CR)',
	 'title': u'\nSimulation of Collision Resistant Secure Sum Protocol',
	 'urllink': u'http://arxiv.org/abs/1411.7756'}
2015-04-10 11:40:03+0000 [xxu46_10] INFO: Crawled 509 pages (at 1 pages/min), scraped 502 items (at 1 items/min)
2015-04-10 11:41:03+0000 [xxu46_10] INFO: Crawled 509 pages (at 0 pages/min), scraped 502 items (at 0 items/min)
2015-04-10 11:41:05+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1411.7753> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 11:41:05+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1411.7753>
	{'abstract': u'Deterministically generating near-uniform point samplings of the motion groups like SO(3), SE(3) and their n-wise products SO(3)^n, SE(3)^n is fundamental to numerous applications in computational and data sciences. The natural measure of sampling quality is discrepancy. In this work, our main goal is construct low discrepancy deterministic samplings in product spaces of the motion groups. To this end, we develop a novel strategy (using a two-step discrepancy construction) that leads to an almost exponential improvement in size (from the trivial direct product). To the best of our knowledge, this is the first nontrivial construction for SO(3)^n, SE(3)^n and the hypertorus T^n. We also construct new low discrepancy samplings of S^2 and SO(3). The central component in our construction for SO(3) is an explicit construction of N points in S^2 with discrepancy tilde(1/ sqrt) with respect to convex sets, matching the bound achieved for the special case of spherical caps in cite. We also generalize the discrepancy of Cartesian product sets cite to the discrepancy of local Cartesian product sets. The tools we develop should be useful in generating low discrepancy samplings of other complicated geometric spaces.',
	 'authors': u'Chandrajit Bajaj, Abhishek Bhowmick, Eshan Chattopadhyay, David Zuckerman,',
	 'category': u'Computer Science ',
	 'date': '2014-11-28',
	 'pdflink': u'http://arxiv.org/pdf/1411.7753',
	 'subjects': u'Computational Geometry (cs.CG)',
	 'title': u'\nOn Low Discrepancy Samplings in Product Spaces of Motion Groups',
	 'urllink': u'http://arxiv.org/abs/1411.7753'}
2015-04-10 11:42:03+0000 [xxu46_10] INFO: Crawled 510 pages (at 1 pages/min), scraped 503 items (at 1 items/min)
2015-04-10 11:42:07+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1411.7747> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 11:42:07+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1411.7747>
	{'abstract': u"We continue the study of covering complexity of constraint satisfaction problems (CSPs) initiated by Guruswami, Hastad and Sudan [SIAM J. Computing, 31(6):1663--1686, 2002] and Dinur and Kol [In Proc. th IEEE Conference on Computational Complexity, 2013]. The covering number of a CSP instance , denoted by is the smallest number of assignments to the variables of , such that each constraint of is satisfied by at least one of the assignments. We show the following results regarding how well efficient algorithms can approximate the covering number of a given CSP instance. - Assuming a covering unique games conjecture, introduced by Dinur and Kol, we show that for every non-odd predicate over any constant sized alphabet and every integer , it is NP-hard to distinguish between -CSP instances (i.e., CSP instances where all the constraints are of type ) which are coverable by a constant number of assignments and those whose covering number is at least . Previously, Dinur and Kol, using the same covering unique games conjecture, had shown a similar hardness result for every non-odd predicate over the Boolean alphabet that supports a pairwise independent distribution. Our generalization yields a complete characterization of CSPs over constant sized alphabet that are hard to cover since CSP's over odd predicates are trivially coverable with assignments. - For a large class of predicates that are contained in the -LIN predicate, we show that it is quasi-NP-hard to distinguish between instances which have covering number at most two and covering number at least . This generalizes the -LIN result of Dinur and Kol that states it is quasi-NP-hard to distinguish between -LIN-CSP instances which have covering number at most two and covering number at least .",
	 'authors': u'Amey Bhangale, Prahladh Harsha, Girish Varma,',
	 'category': u'Computer Science ',
	 'date': '2014-11-28',
	 'pdflink': u'http://arxiv.org/pdf/1411.7747',
	 'subjects': u'Computational Complexity (cs.CC)',
	 'title': u'\nA Characterization of hard-to-cover CSPs',
	 'urllink': u'http://arxiv.org/abs/1411.7747'}
2015-04-10 11:43:03+0000 [xxu46_10] INFO: Crawled 511 pages (at 1 pages/min), scraped 504 items (at 1 items/min)
2015-04-10 11:43:16+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1411.7727> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 11:43:16+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1411.7727>
	{'abstract': u'It the literature have been identified three social mechanisms explaining the similarity between people connected in the network of social relations homophily, confounding and social contagion. The article proposes a simple model for simulating mechanisms responsible for similarity of attitudes in networks of social relations; along with a measure that is able to indicate which of the three mechanisms has taken major role in the process.',
	 'authors': u'Blazej Zak, Anita Zbieg,',
	 'category': u'Computer Science ',
	 'date': '2014-11-28',
	 'pdflink': u'http://arxiv.org/pdf/1411.7727',
	 'subjects': u'Social and Information Networks (cs.SI)',
	 'title': u'\nModel for simulating mechanisms responsible of similarities between  people connected in networks of social relations',
	 'urllink': u'http://arxiv.org/abs/1411.7727'}
2015-04-10 11:44:03+0000 [xxu46_10] INFO: Crawled 512 pages (at 1 pages/min), scraped 505 items (at 1 items/min)
2015-04-10 11:44:25+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1411.7726> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 11:44:25+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1411.7726>
	{'abstract': u'Diffusion of information and viral content, social contagion and influence are still topics of broad evaluation. As theory explaining the role of influentials moves slightly to reduce their importance in the propagation of viral content, authors of the following paper have studied the information epidemic in a social networking platform in order to confirm recent theoretical findings in this area. While most of related experiments focus on the level of individuals, the elementary entities of the following analysis are dyads. The authors study behavioral motifs that are possible to observe at the dyadic level. The study shows significant differences between dyads that are more vs less engaged in the diffusion process. Dyads that fuel the diffusion proccess are characterized by stronger relationships (higher activity, more common friends), more active and networked receiving party (higher centrality measures), and higher authority centrality of person sending a viral message.',
	 'authors': u'Anita Zbieg, Blazej Zak, Jaroslaw Jankowski, Radoslaw Michalski, Sylwia Ciuberek,',
	 'category': u'Computer Science ',
	 'date': '2014-11-28',
	 'pdflink': u'http://arxiv.org/pdf/1411.7726',
	 'subjects': u'Social and Information Networks (cs.SI)',
	 'title': u'\nStudying Diffusion of Viral Content at Dyadic Level',
	 'urllink': u'http://arxiv.org/abs/1411.7726'}
2015-04-10 11:45:03+0000 [xxu46_10] INFO: Crawled 513 pages (at 1 pages/min), scraped 506 items (at 1 items/min)
2015-04-10 11:45:17+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1411.7717> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 11:45:17+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1411.7717>
	{'abstract': u'Sum Product Networks (SPNs) are a recently developed class of deep generative models which compute their associated unnormalized density functions using a special type of arithmetic circuit. When certain sufficient conditions, called the decomposability and completeness conditions (or "D&amp;C" conditions), are imposed on the structure of these circuits, marginal densities and other useful quantities, which are typically intractable for other deep generative models, can be computed by what amounts to a single evaluation of the network (which is a property known as "validity"). However, the effect that the D&amp;C conditions have on the capabilities of D&amp;C SPNs is not well understood. In this work we analyze the D&amp;C conditions, expose the various connections that D&amp;C SPNs have with multilinear arithmetic circuits, and consider the question of how well they can capture various distributions as a function of their size and depth. Among our various contributions is a result which establishes the existence of a relatively simple distribution with fully tractable marginal densities which cannot be efficiently captured by D&amp;C SPNs of any depth, but which can be efficiently captured by various other deep generative models. We also show that with each additional layer of depth permitted, the set of distributions which can be efficiently captured by D&amp;C SPNs grows in size. This kind of "depth hierarchy" property has been widely conjectured to hold for various deep models, but has never been proven for any of them. Some of our other contributions include a new characterization of the D&amp;C conditions as sufficient and necessary ones for a slightly strengthened notion of validity, and various state-machine characterizations of the types of computations that can be performed efficiently by D&amp;C SPNs.',
	 'authors': u'James Martens, Venkatesh Medabalimi,',
	 'category': u'Computer Science ',
	 'date': '2014-11-27',
	 'pdflink': u'http://arxiv.org/pdf/1411.7717',
	 'subjects': u'Learning (cs.LG)',
	 'title': u'\nOn the Expressive Efficiency of Sum Product Networks',
	 'urllink': u'http://arxiv.org/abs/1411.7717'}
2015-04-10 11:46:03+0000 [xxu46_10] INFO: Crawled 514 pages (at 1 pages/min), scraped 507 items (at 1 items/min)
2015-04-10 11:46:30+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1411.7715> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 11:46:30+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1411.7715>
	{'abstract': u'We propose an approach to detect flying objects such as UAVs and aircrafts when they occupy a small portion of the field of view, possibly moving against complex backgrounds, and are filmed by a camera that itself moves. Solving such a difficult problem requires combining both appearance and motion cues. To this end we propose a regression-based approach to motion stabilization of local image patches that allows us to achieve effective classification on spatio-temporal image cubes and outperform state-of-the-art techniques. As the problem is relatively new, we collected two challenging datasets for UAVs and Aircrafts, which can be used as benchmarks for flying objects detection and vision-guided collision avoidance.',
	 'authors': u'Artem Rozantsev, Vincent Lepetit, Pascal Fua,',
	 'category': u'Computer Science ',
	 'date': '2014-11-27',
	 'pdflink': u'http://arxiv.org/pdf/1411.7715',
	 'subjects': u'Computer Vision and Pattern Recognition (cs.CV)',
	 'title': u'\nFlying Objects Detection from a Single Moving Camera',
	 'urllink': u'http://arxiv.org/abs/1411.7715'}
2015-04-10 11:47:03+0000 [xxu46_10] INFO: Crawled 515 pages (at 1 pages/min), scraped 508 items (at 1 items/min)
2015-04-10 11:47:24+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1411.7714> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 11:47:24+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1411.7714>
	{'abstract': u'Feature selection is an essential problem in computer vision, important for category learning and recognition. Along with the rapid development of a wide variety of visual features and classifiers, there is a growing need for efficient feature selection and combination methods, to construct powerful classifiers for more complex and higher-level recognition tasks. We propose an algorithm that efficiently discovers sparse, compact representations of input features or classifiers, from a vast sea of candidates, with important optimality properties, low computational cost and excellent accuracy in practice. Different from boosting, we start with a discriminant linear classification formulation that encourages sparse solutions. Then we obtain an equivalent unsupervised clustering problem that jointly discovers ensembles of diverse features. They are independently valuable but even more powerful when united in a cluster of classifiers. We evaluate our method on the task of large-scale recognition in video and show that it significantly outperforms classical selection approaches, such as AdaBoost and greedy forward-backward selection, and powerful classifiers such as SVMs, in speed of training and performance, especially in the case of limited training data.',
	 'authors': u'Marius Leordeanu, Alexandra Radu, Rahul Sukthankar,',
	 'category': u'Computer Science ',
	 'date': '2014-11-27',
	 'pdflink': u'http://arxiv.org/pdf/1411.7714',
	 'subjects': u'Computer Vision and Pattern Recognition (cs.CV)',
	 'title': u'\nFeatures in Concert: Discriminative Feature Selection meets Unsupervised  Clustering',
	 'urllink': u'http://arxiv.org/abs/1411.7714'}
2015-04-10 11:48:03+0000 [xxu46_10] INFO: Crawled 516 pages (at 1 pages/min), scraped 509 items (at 1 items/min)
2015-04-10 11:48:31+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1411.7711> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 11:48:31+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1411.7711>
	{'abstract': u'A reliable and scalable mechanism to provide protection against a link or node failure has additional requirements in the context of SDN and OpenFlow. Not only it has to minimize the load on the controller, but it must be able to react even when the controller is unreachable. In this paper we present a protection scheme based on precomputed backup paths and inspired by MPLS "crankback" routing, that guarantees instantaneous recovery times and aims at zero packet-loss after failure detection, regardless of controller reachability, even when OpenFlow\'s "fast-failover" feature cannot be used. The proposed mechanism is based on OpenState, an OpenFlow extension that allows a programmer to specify how forwarding rules should autonomously adapt in a stateful fashion, reducing the need to rely on remote controllers. We present the scheme as well as two different formulations for the computation of backup paths.',
	 'authors': u'Antonio Capone, Carmelo Cascone, Alessandro Q.T. Nguyen, Brunilde Sans\xf2,',
	 'category': u'Computer Science ',
	 'date': '2014-11-27',
	 'pdflink': u'http://arxiv.org/pdf/1411.7711',
	 'subjects': u'Networking and Internet Architecture (cs.NI)',
	 'title': u'\nDetour Planning for Fast and Reliable Failure Recovery in SDN with  OpenState',
	 'urllink': u'http://arxiv.org/abs/1411.7711'}
2015-04-10 11:49:03+0000 [xxu46_10] INFO: Crawled 517 pages (at 1 pages/min), scraped 510 items (at 1 items/min)
2015-04-10 11:49:37+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1411.7682> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 11:49:37+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1411.7682>
	{'abstract': u'Color distortion can introduce a significant damage in visual quality perception, however, most of existing reduced-reference quality measures are designed for grayscale images. In this paper, we consider a basic extension of well-known image-statistics based quality assessment measures to color images. In order to evaluate the impact of color information on the measures efficiency, two color spaces are investigated: RGB and CIELAB. Results of an extensive evaluation using TID 2013 benchmark demonstrates that significant improvement can be achieved for a great number of distortion type when the CIELAB color representation is used.',
	 'authors': u'Mounir Omari, Mohammed El Hassouni, Hocine Cherifi, Abdelkaher Ait Abdelouahad,',
	 'category': u'Computer Science ',
	 'date': '2014-11-27',
	 'pdflink': u'http://arxiv.org/pdf/1411.7682',
	 'subjects': u'Computer Vision and Pattern Recognition (cs.CV)',
	 'title': u'\nOn color image quality assessment using natural image statistics',
	 'urllink': u'http://arxiv.org/abs/1411.7682'}
2015-04-10 11:50:03+0000 [xxu46_10] INFO: Crawled 518 pages (at 1 pages/min), scraped 511 items (at 1 items/min)
2015-04-10 11:50:34+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1411.7676> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 11:50:34+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1411.7676>
	{'abstract': u'An ideal visual representation would be a function of visual data that is a minimal sufficient statistics for the scene, and a maximal invariant to nuisance variability. We derive analytical expressions for such representations and show that, under certain assumptions underlying the Lambert-Ambient model, they are related to convolutional architectures. This link highlights the conditions under which they can be expected to perform well, and also suggests ways to improve and generalize them. This new interpretation draws connections to the classical theories of sampling, hypothesis testing and group invariance. We show that one layer of a convolutional architecture can approximate an optimal representation of one im- age, given sufficiently many receptive fields. We also show that stacking multiple layers, each of which is invariant to a small group transformation such as affine, achieves invariance to larger groups, all the way to planar diffeomorphisms, given sufficiently many layers and receptive fields.',
	 'authors': u'Stefano Soatto, Alessandro Chiuso,',
	 'category': u'Computer Science ',
	 'date': '2014-11-27',
	 'pdflink': u'http://arxiv.org/pdf/1411.7676',
	 'subjects': u'Computer Vision and Pattern Recognition (cs.CV)',
	 'title': u'\nVisual Scene Representations: Sufficiency, Minimality, Invariance and  Approximation with Deep Convolutional Networks',
	 'urllink': u'http://arxiv.org/abs/1411.7676'}
2015-04-10 11:51:03+0000 [xxu46_10] INFO: Crawled 519 pages (at 1 pages/min), scraped 512 items (at 1 items/min)
2015-04-10 11:51:24+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1411.7663> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 11:51:24+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1411.7663>
	{'abstract': u'A two step mesh deformation approach for large nodal deformations, typically arising from non-parametric shape optimization, fluid-structure interaction or computer graphics, is considered. Two major difficulties, collapsed cells and an undesirable parameterization, are overcome by considering a special form of ray tracing paired with a centroid Voronoi reparameterization. The ray direction is computed by solving an Eikonal equation. With respect to the Hadamard form of the shape derivative, both steps are within the kernel of the objective and have no negative impact on the minimizer. The paper concludes with applications in 2D and 3D fluid dynamics and automatic code generation and manages to solve these problems without any remeshing. The methodology is available as a FEniCS shape optimization add-on at this http URL',
	 'authors': u'Stephan Schmidt,',
	 'category': u'Computer Science ',
	 'date': '2014-11-27',
	 'pdflink': u'http://arxiv.org/pdf/1411.7663',
	 'subjects': u'Numerical Analysis (cs.NA)',
	 'title': u'\nA Two Stage CVT / Eikonal Convection Mesh Deformation Approach for Large  Nodal Deformations',
	 'urllink': u'http://arxiv.org/abs/1411.7663'}
2015-04-10 11:52:03+0000 [xxu46_10] INFO: Crawled 520 pages (at 1 pages/min), scraped 513 items (at 1 items/min)
2015-04-10 11:52:31+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1411.7662> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 11:52:31+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1411.7662>
	{'abstract': u'Now-a-days vehicles are one of the most important parts of our life. We need them to cross distances in our everyday life. In this paper we discuss Vehicular AdHoc Network (VANET) technology that can ensure the maintenance of traffic rules and regulation. By applying this technology we can save life, save time, corruption, vehicle security, avoid collision and so on. Vehicular Ad Hoc Network (VANET) is a part of Mobile Ad Hoc Network (MANET). Every node or vehicle can move freely and they will communicate each other by wireless technology in coverage. The main goal of this research is to study the existing routing protocols for ad-hoc network system and compared between AODV (Reactive) and DSDV (Proactive). We have studied different types of routing protocols such as topology based, position based, cluster based, geo-cast based and broadcast based. We have simulated and compared AODV (Reactive) and DSDV (Proactive) to find out their efficiency and detect their flaws.',
	 'authors': u'A. B. M. Moniruzzaman, Md. Sadekur Rahman,',
	 'category': u'Computer Science ',
	 'date': '2014-11-27',
	 'pdflink': u'http://arxiv.org/pdf/1411.7662',
	 'subjects': u'Networking and Internet Architecture (cs.NI)',
	 'title': u'\nAnalysis of Topology Based Routing Protocols for Vehicular Ad-Hoc  Network (VANET)',
	 'urllink': u'http://arxiv.org/abs/1411.7662'}
2015-04-10 11:53:03+0000 [xxu46_10] INFO: Crawled 521 pages (at 1 pages/min), scraped 514 items (at 1 items/min)
2015-04-10 11:53:21+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1411.7658> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 11:53:21+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1411.7658>
	{'abstract': u'This paper designs and implements a high availability clusters and incorporated with load balance infrastructure of web servers. The paper described system can provide full facilities to the website hosting provider and large business organizations. This system can provide continuous service though any system components fail uncertainly with the help of Linux Virtual Server (LVS) loadbalancing cluster technology and combined with virtualization as well as shared storage technology to achieve the three-tier architecture of Web server clusters. This technology not only improves availability, but also affects the security and performance of the application services being requested. Benefits of the system include node failover overcome; network failover overcome; storage limitation overcome and load distribution.',
	 'authors': u'A B M Moniruzzaman, Md. Waliullah, Md. Sadekur Rahman,',
	 'category': u'Computer Science ',
	 'date': '2014-11-27',
	 'pdflink': u'http://arxiv.org/pdf/1411.7658',
	 'subjects': u'Distributed, Parallel, and Cluster Computing (cs.DC)',
	 'title': u'\nA High Availability Clusters Model Combined with Load Balancing and  Shared Storage Technologies for Web Servers',
	 'urllink': u'http://arxiv.org/abs/1411.7658'}
2015-04-10 11:54:03+0000 [xxu46_10] INFO: Crawled 522 pages (at 1 pages/min), scraped 515 items (at 1 items/min)
2015-04-10 11:54:23+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1411.7655> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 11:54:23+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1411.7655>
	{'abstract': u'Although color is a fundamental feature of human visual perception, it has been largely unexplored in the reduced-reference (RR) image quality assessment (IQA) schemes. In this paper, we propose a natural scene statistic (NSS) method, which efficiently uses this information. It is based on the statistical deviation between the steerable pyramid coefficients of the reference color image and the degraded one. We propose and analyze the multivariate generalized Gaussian distribution (MGGD) to model the underlying statistics. In order to quantify the degradation, we develop and evaluate two measures based respectively on the Geodesic distance between two MGGDs and on the closed-form of the Kullback Leibler divergence. We performed an extensive evaluation of both metrics in various color spaces (RGB, HSV, CIELAB and YCrCb) using the TID 2008 benchmark and the FRTV Phase I validation process. Experimental results demonstrate the effectiveness of the proposed framework to achieve a good consistency with human visual perception. Furthermore, the best configuration is obtained with CIELAB color space associated to KLD deviation measure.',
	 'authors': u'Mounir Omari, Mohammed El Hassouni, Abdelkaher Ait Abdelouahad, Hocine Cherifi,',
	 'category': u'Computer Science ',
	 'date': '2014-11-27',
	 'pdflink': u'http://arxiv.org/pdf/1411.7655',
	 'subjects': u'Computer Vision and Pattern Recognition (cs.CV)',
	 'title': u'\nA statistical reduced-reference method for color image quality  assessment',
	 'urllink': u'http://arxiv.org/abs/1411.7655'}
2015-04-10 11:55:03+0000 [xxu46_10] INFO: Crawled 523 pages (at 1 pages/min), scraped 516 items (at 1 items/min)
2015-04-10 11:55:29+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1411.7647> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 11:55:29+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1411.7647>
	{'abstract': u'Although polynomial-time probabilistic Turing machines can utilize uncomputable transition probabilities to recognize uncountably many languages with bounded error when allowed to use logarithmic space, it is known that such "magic coins" give no additional computational power to constant-space versions of those machines. We show that adding a few quantum bits to the model changes the picture dramatically. For every language , there exists such a two-way quantum finite automaton that recognizes a language of the same Turing degree as with bounded error in polynomial time. When used as verifiers in public-coin interactive proof systems, such automata can verify membership in all languages with bounded error, outperforming their classical counterparts, which are known to fail for the palindromes language.',
	 'authors': u'A. C. Cem Say, Abuzer Yakaryilmaz,',
	 'category': u'Computer Science ',
	 'date': '2014-11-27',
	 'pdflink': u'http://arxiv.org/pdf/1411.7647',
	 'subjects': u'Computational Complexity (cs.CC)',
	 'title': u'\nMagic coins are useful for small-space quantum machines',
	 'urllink': u'http://arxiv.org/abs/1411.7647'}
2015-04-10 11:56:03+0000 [xxu46_10] INFO: Crawled 524 pages (at 1 pages/min), scraped 517 items (at 1 items/min)
2015-04-10 11:56:41+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1411.7640> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 11:56:41+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1411.7640>
	{'abstract': u'Kernel spectral clustering corresponds to a weighted kernel principal component analysis problem in a constrained optimization framework. The primal formulation leads to an eigen-decomposition of a centered Laplacian matrix at the dual level. The dual formulation allows to build a model on a representative subgraph of the large scale network in the training phase and the model parameters are estimated in the validation stage. The KSC model has a powerful out-of-sample extension property which allows cluster affiliation for the unseen nodes of the big data network. In this paper we exploit the structure of the projections in the eigenspace during the validation stage to automatically determine a set of increasing distance thresholds. We use these distance thresholds in the test phase to obtain multiple levels of hierarchy for the large scale network. The hierarchical structure in the network is determined in a bottom-up fashion. We empirically showcase that real-world networks have multilevel hierarchical organization which cannot be detected efficiently by several state-of-the-art large scale hierarchical community detection techniques like the Louvain, OSLOM and Infomap methods. We show a major advantage our proposed approach i.e. the ability to locate good quality clusters at both the coarser and finer levels of hierarchy using internal cluster quality metrics on 7 real-life networks.',
	 'authors': u'Raghvendra Mall, Rocco Langone, Johan A.K. Suykens,',
	 'category': u'Computer Science ',
	 'date': '2014-11-27',
	 'pdflink': u'http://arxiv.org/pdf/1411.7640',
	 'subjects': u'Social and Information Networks (cs.SI)',
	 'title': u'\nMultilevel Hierarchical Kernel Spectral Clustering for Real-Life Large  Scale Complex Networks',
	 'urllink': u'http://arxiv.org/abs/1411.7640'}
2015-04-10 11:57:03+0000 [xxu46_10] INFO: Crawled 525 pages (at 1 pages/min), scraped 518 items (at 1 items/min)
2015-04-10 11:57:58+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1411.7639> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 11:57:58+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1411.7639>
	{'abstract': u'MapReduce has been widely applied in various fields of data and compute intensive applications and also it is important programming model for cloud computing. Hadoop is an open-source implementation of MapReduce which operates on terabytes of data using commodity hardware. We have applied this Hadoop MapReduce programming model for analyzing web log files so that we could get hit count of specific web application. This system uses Hadoop file system to store log file and results are evaluated using Map and Reduce function. Experimental results show hit count for each field in log file. Also due to MapReduce runtime parallelization response time is reduced.',
	 'authors': u'Sayalee Narkhede, Trupti Baraskar, Debajyoti Mukhopadhyay,',
	 'category': u'Computer Science ',
	 'date': '2014-11-27',
	 'pdflink': u'http://arxiv.org/pdf/1411.7639',
	 'subjects': u'Distributed, Parallel, and Cluster Computing (cs.DC)',
	 'title': u'\nAnalyzing Web Application Log Files to Find Hit Count Through the  Utilization of Hadoop MapReduce in Cloud Computing Environment',
	 'urllink': u'http://arxiv.org/abs/1411.7639'}
2015-04-10 11:58:03+0000 [xxu46_10] INFO: Crawled 526 pages (at 1 pages/min), scraped 519 items (at 1 items/min)
2015-04-10 11:59:03+0000 [xxu46_10] INFO: Crawled 526 pages (at 0 pages/min), scraped 519 items (at 0 items/min)
2015-04-10 11:59:06+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1411.7631> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 11:59:06+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1411.7631>
	{'abstract': u'We show a closer algorithmic connection between constructing cut-approximating hierarchical tree decompositions and computing approximate maximum flows in undirected graphs. This leads to the first O(m polylog(n)) time algorithms for both problems.',
	 'authors': u'Richard Peng,',
	 'category': u'Computer Science ',
	 'date': '2014-11-27',
	 'pdflink': u'http://arxiv.org/pdf/1411.7631',
	 'subjects': u'Data Structures and Algorithms (cs.DS)',
	 'title': u'\nA Note on Cut-Approximators and Approximating Undirected Max Flows',
	 'urllink': u'http://arxiv.org/abs/1411.7631'}
2015-04-10 12:00:03+0000 [xxu46_10] INFO: Crawled 527 pages (at 1 pages/min), scraped 520 items (at 1 items/min)
2015-04-10 12:00:22+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1411.7630> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 12:00:22+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1411.7630>
	{'abstract': u'In this paper, we propose a compressed sensing (CS) framework that consists of three parts: a unit-norm tight frame (UTF), a random diagonal matrix and a column-wise orthonormal matrix. We prove that this structure satisfies the restricted isometry property (RIP) with high probability if the number of measurements for -sparse signals of length and if the column-wise orthonormal matrix is bounded. Some existing structured sensing models can be studied under this framework, which then gives tighter bounds on the required number of measurements to satisfy the RIP. More importantly, we propose several structured sensing models by appealing to this unified framework, such as a general sensing model with arbitrary/determinisic subsamplers, a fast and efficient block compressed sensing scheme, and structured sensing matrices with deterministic phase modulations, all of which can lead to improvements on practical applications. In particular, one of the constructions is applied to simplify the transceiver design of CS-based channel estimation for orthogonal frequency division multiplexing (OFDM) systems.',
	 'authors': u'Peng Zhang, Lu Gan, Sumei Sun, Cong Ling,',
	 'category': u'Computer Science ',
	 'date': '2014-11-27',
	 'pdflink': u'http://arxiv.org/pdf/1411.7630',
	 'subjects': u'Information Theory (cs.IT)',
	 'title': u'\nModulated Unit-Norm Tight Frames for Compressed Sensing',
	 'urllink': u'http://arxiv.org/abs/1411.7630'}
2015-04-10 12:01:03+0000 [xxu46_10] INFO: Crawled 528 pages (at 1 pages/min), scraped 521 items (at 1 items/min)
2015-04-10 12:01:37+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1411.7614> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 12:01:37+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1411.7614>
	{'abstract': u'The perfect matching problem has a randomized NC algorithm, using the celebrated Isolation Lemma of Mulmuley, Vazirani and Vazirani. The Isolation Lemma states that giving a random weight assignment to the edges of a graph, ensures that it has a unique minimum weight perfect matching, with a good probability. We derandomize this lemma for -free and -free bipartite graphs, i.e. we give a deterministic log-space construction of such a weight assignment for these graphs. Such a construction was known previously for planar bipartite graphs. Our result implies that the perfect matching problem for -free and -free bipartite graphs is in SPL. It also gives an alternate proof for an already known result -- reachability for -free and -free graphs is in UL.',
	 'authors': u'Rahul Arora, Ashu Gupta, Rohit Gurjar, Raghunath Tewari,',
	 'category': u'Computer Science ',
	 'date': '2014-11-27',
	 'pdflink': u'http://arxiv.org/pdf/1411.7614',
	 'subjects': u'Computational Complexity (cs.CC)',
	 'title': u'\nDerandomizing Isolation Lemma for $K_{3,3}$-free and $K_5$-free  Bipartite Graphs',
	 'urllink': u'http://arxiv.org/abs/1411.7614'}
2015-04-10 12:02:03+0000 [xxu46_10] INFO: Crawled 529 pages (at 1 pages/min), scraped 522 items (at 1 items/min)
2015-04-10 12:02:40+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1411.7612> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 12:02:40+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1411.7612>
	{'abstract': u'This paper presents a parallel genetic algorithm for generalised vertex cover problem (GVCP) using Hadoop Map-Reduce framework. The proposed Map-Reduce implementation helps to run the genetic algorithm for generalized vertex cover problem (GVCP) on multiple machines parallely and computes the solution in relatively short time.',
	 'authors': u'Drona Pratap Chandu,',
	 'category': u'Computer Science ',
	 'date': '2014-11-27',
	 'pdflink': u'http://arxiv.org/pdf/1411.7612',
	 'subjects': u'Distributed, Parallel, and Cluster Computing (cs.DC)',
	 'title': u'\nA Parallel Genetic Algorithm for Generalized Vertex Cover Problem',
	 'urllink': u'http://arxiv.org/abs/1411.7612'}
2015-04-10 12:03:03+0000 [xxu46_10] INFO: Crawled 530 pages (at 1 pages/min), scraped 523 items (at 1 items/min)
2015-04-10 12:03:35+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1411.7607> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 12:03:35+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1411.7607>
	{'abstract': u'This paper investigates the benefits of the side information on the universal compression of sequences from a mixture of parametric sources. The output sequence of the mixture source is chosen from the source with a -dimensional parameter vector at random according to probability vector . The average minimax redundancy of the universal compression of a new random sequence of length is derived when the encoder and the decoder have a common side information of sequences generated independently by the mixture source. Necessary and sufficient conditions on the distribution and the mixture parameter dimensions are determined such that the side information provided by the previous sequences results in a reduction in the first-order term of the average codeword length compared with the universal compression without side information. Further, it is proved that the optimal compression with side information corresponds to the clustering of the side information sequences from the mixture source. Then, a clustering technique is presented to better utilize the side information by classifying the data sequences from a mixture source. Finally, the performance of the clustering on the universal compression with side information is validated using computer simulations on real network data traces.',
	 'authors': u'Ahmad Beirami, Liling Huang, Mohsen Sardari, Faramarz Fekri,',
	 'category': u'Computer Science ',
	 'date': '2014-11-27',
	 'pdflink': u'http://arxiv.org/pdf/1411.7607',
	 'subjects': u'Information Theory (cs.IT)',
	 'title': u'\nUniversal Compression of a Mixture of Parametric Sources with Side  Information',
	 'urllink': u'http://arxiv.org/abs/1411.7607'}
2015-04-10 12:04:03+0000 [xxu46_10] INFO: Crawled 531 pages (at 1 pages/min), scraped 524 items (at 1 items/min)
2015-04-10 12:04:27+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1411.7593> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 12:04:27+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1411.7593>
	{'abstract': u'We address the problem of gauging the influence exerted by a given country on the global trade market from the viewpoint of complex networks. In particular, we apply the PWP method for computing indirect influences on the world trade network.',
	 'authors': u'Rafael Diaz, Laura Gomez,',
	 'category': u'Computer Science ',
	 'date': '2014-11-27',
	 'pdflink': u'http://arxiv.org/pdf/1411.7593',
	 'subjects': u'Social and Information Networks (cs.SI)',
	 'title': u'\nIndirect Influences in International Trade',
	 'urllink': u'http://arxiv.org/abs/1411.7593'}
2015-04-10 12:05:03+0000 [xxu46_10] INFO: Crawled 532 pages (at 1 pages/min), scraped 525 items (at 1 items/min)
2015-04-10 12:05:13+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1411.7591> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 12:05:13+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1411.7591>
	{'abstract': u'Egocentric cameras are being worn by an increasing number of users, among them many security forces worldwide. GoPro cameras already penetrated the mass market, and Google Glass may follow soon. As head-worn cameras do not capture the face and body of the wearer, it may seem that the anonymity of the wearer can be preserved even when the video is publicly distributed. We show that motion features in egocentric video provide biometric information, and the identity of the user can be determined quite reliably from a few seconds of video. Biometrics are extracted by training Convolutional Neural Network (CNN) architectures on coarse optical flow. Egocentric video biometrics can prevent theft of wearable cameras by locking the camera when worn by people other than the owner. In video sharing services, this Biometric measure can help to locate automatically all videos shot by the same user. An important message in this paper is that people should be aware that sharing egocentric video will compromise their anonymity.',
	 'authors': u'Yedid Hoshen, Shmuel Peleg,',
	 'category': u'Computer Science ',
	 'date': '2014-11-27',
	 'pdflink': u'http://arxiv.org/pdf/1411.7591',
	 'subjects': u'Computer Vision and Pattern Recognition (cs.CV)',
	 'title': u'\nEgocentric Video Biometrics',
	 'urllink': u'http://arxiv.org/abs/1411.7591'}
2015-04-10 12:06:03+0000 [xxu46_10] INFO: Crawled 533 pages (at 1 pages/min), scraped 526 items (at 1 items/min)
2015-04-10 12:06:24+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1411.7582> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 12:06:24+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1411.7582>
	{'abstract': u'This report discusses two new indices for comparing clusterings of a set of points. The motivation for looking at new ways for comparing clusterings stems from the fact that the existing clustering indices are based on set cardinality alone and do not consider the positions of data points. The new indices, namely, the Random Walk index (RWI) and Variation of Information with Neighbors (VIN), are both inspired by the clustering metric Variation of Information (VI). VI possesses some interesting theoretical properties which are also desirable in a metric for comparing clusterings. We define our indices and discuss some of their explored properties which appear relevant for a clustering index. We also include the results of these indices on clusterings of some example data sets.',
	 'authors': u'Zaeem Hussain, Marina Meila,',
	 'category': u'Computer Science ',
	 'date': '2014-11-27',
	 'pdflink': u'http://arxiv.org/pdf/1411.7582',
	 'subjects': u'Learning (cs.LG)',
	 'title': u'\nGraph Sensitive Indices for Comparing Clusterings',
	 'urllink': u'http://arxiv.org/abs/1411.7582'}
2015-04-10 12:07:03+0000 [xxu46_10] INFO: Crawled 534 pages (at 1 pages/min), scraped 527 items (at 1 items/min)
2015-04-10 12:07:42+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1411.7564> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 12:07:42+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1411.7564>
	{'abstract': u'In computer vision, many problems such as image segmentation, pixel labelling, and scene parsing can be formulated as binary quadratic programs (BQPs). For submodular problems, cuts based methods can be employed to efficiently solve large-scale problems. However, general nonsubmodular problems are significantly more challenging to solve. Finding a solution when the problem is of large size to be of practical interest, however, typically requires relaxation. Two standard relaxation methods are widely used for solving general BQPs--spectral methods and semidefinite programming (SDP), each with their own advantages and disadvantages. Spectral relaxation is simple and easy to implement, but its bound is loose. Semidefinite relaxation has a tighter bound, but its computational complexity is high, especially for large scale problems. In this work, we present a new SDP formulation for BQPs, with two desirable properties. First, it has a similar relaxation bound to conventional SDP formulations. Second, compared with conventional SDP methods, the new SDP formulation leads to a significantly more efficient and scalable dual optimization approach, which has the same degree of complexity as spectral methods. We then propose two solvers, namely, quasi-Newton and smoothing Newton methods, for the dual problem. Both of them are significantly more efficiently than standard interior-point methods. In practice, the smoothing Newton solver is faster than the quasi-Newton solver for dense or medium-sized problems, while the quasi-Newton solver is preferable for large sparse/structured problems. Our experiments on a few computer vision applications including clustering, image segmentation, co-segmentation and registration show the potential of our SDP formulation for solving large-scale BQPs.',
	 'authors': u'Peng Wang, Chunhua Shen, Anton van den Hengel,',
	 'category': u'Computer Science ',
	 'date': '2014-11-27',
	 'pdflink': u'http://arxiv.org/pdf/1411.7564',
	 'subjects': u'Computer Vision and Pattern Recognition (cs.CV)',
	 'title': u'\nLarge-scale Binary Quadratic Optimization Using Semidefinite Relaxation  and Applications',
	 'urllink': u'http://arxiv.org/abs/1411.7564'}
2015-04-10 12:08:03+0000 [xxu46_10] INFO: Crawled 535 pages (at 1 pages/min), scraped 528 items (at 1 items/min)
2015-04-10 12:08:56+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1411.7554> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 12:08:56+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1411.7554>
	{'abstract': u'Feldman et al.(2005) asked whether the performance of the LP decoder can be improved by adding redundant parity checks to tighten the LP relaxation. We prove that for LDPC codes, even if we include all redundant checks, asymptotically there is no gain in the LP decoder threshold on the BSC under certain conditions on the base Tanner graph. First, we show that if the graph has bounded check-degree and satisfies a condition which we call asymptotic strength, then including high degree redundant checks in the LP does not significantly improve the threshold in the following sense: for each constant delta&gt;0, there is a constant k&gt;0 such that the threshold of the LP decoder containing all redundant checks of degree at most k improves by at most delta upon adding to the LP all redundant checks of degree larger than k. We conclude that if the graph satisfies a rigidity condition, then including all redundant checks does not improve the threshold of the base LP. We call the graph asymptotically strong if the LP decoder corrects a constant fraction of errors even if the LLRs of the correct variables are arbitrarily small. By building on the work of Feldman et al.(2007) and Viderman(2013), we show that asymptotic strength follows from sufficiently large expansion. We also give a geometric interpretation of asymptotic strength in terms pseudocodewords. We call the graph rigid if the minimum weight of a sum of check nodes involving a cycle tends to infinity as the block length tends to infinity. Under the assumptions that the graph girth is logarithmic and the minimum check degree is at least 3, rigidity is equivalent to the nondegeneracy property that adding at least logarithmically many checks does not give a constant weight check. We argue that nondegeneracy is a typical property of random check-regular graphs.',
	 'authors': u'Louay Bazzi, Hani Audah,',
	 'category': u'Computer Science ',
	 'date': '2014-11-27',
	 'pdflink': u'http://arxiv.org/pdf/1411.7554',
	 'subjects': u'Information Theory (cs.IT)',
	 'title': u'\nImpact of redundant checks on the LP decoding thresholds of LDPC codes',
	 'urllink': u'http://arxiv.org/abs/1411.7554'}
2015-04-10 12:09:03+0000 [xxu46_10] INFO: Crawled 536 pages (at 1 pages/min), scraped 529 items (at 1 items/min)
2015-04-10 12:09:35+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1411.7542> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 12:09:35+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1411.7542>
	{'abstract': u'Estimation of Distribution Algorithms (EDAs) require flexible probability models that can be efficiently learned and sampled. Restricted Boltzmann Machines (RBMs) are generative neural networks with these desired properties. We integrate an RBM into an EDA and evaluate the performance of this system in solving combinatorial optimization problems with a single objective. We assess how the number of fitness evaluations and the CPU time scale with problem size and with problem complexity. The results are compared to the Bayesian Optimization Algorithm, a state-of-the-art EDA. Although RBM-EDA requires larger population sizes and a larger number of fitness evaluations, it outperforms BOA in terms of CPU times, in particular if the problem is large or complex. RBM-EDA requires less time for model building than BOA. These results highlight the potential of using generative neural networks for combinatorial optimization.',
	 'authors': u'Malte Probst, Franz Rothlauf, J\xf6rn Grahl,',
	 'category': u'Computer Science ',
	 'date': '2014-11-27',
	 'pdflink': u'http://arxiv.org/pdf/1411.7542',
	 'subjects': u'Neural and Evolutionary Computing (cs.NE)',
	 'title': u'\nScalability of using Restricted Boltzmann Machines for Combinatorial  Optimization',
	 'urllink': u'http://arxiv.org/abs/1411.7542'}
2015-04-10 12:10:03+0000 [xxu46_10] INFO: Crawled 537 pages (at 1 pages/min), scraped 530 items (at 1 items/min)
2015-04-10 12:10:27+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1411.7533> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 12:10:27+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1411.7533>
	{'abstract': u'We consider downlink precoding in a frequency-selective multi-user massive MIMO system with highly efficient but non-linear power amplifiers at the base station (BS). A low-complexity precoding algorithm is proposed, which generates constant-envelope (CE) transmit signals for each BS antenna. To avoid large variations in the phase angle transmitted from each antenna, the difference of the phase angles transmitted in consecutive channel uses is limited to for a fixed . To achieve a desired per-user information rate, the extra total transmit power required under the time variation constraint when compared to the special case of no time variation constraint (i.e., ), is small for many practical values of . In a i.i.d. Rayleigh fading channel with BS antennas, single-antenna users and a desired per-user information rate of bit-per-channel-use, the extra total transmit power required is less than dB when .',
	 'authors': u'Sudarshan Mukherjee, Saif Khan Mohammed,',
	 'category': u'Computer Science ',
	 'date': '2014-11-27',
	 'pdflink': u'http://arxiv.org/pdf/1411.7533',
	 'subjects': u'Information Theory (cs.IT)',
	 'title': u'\nConstant-Envelope Precoding with Time-Variation Constraint on the  Transmitted Phase Angles',
	 'urllink': u'http://arxiv.org/abs/1411.7533'}
2015-04-10 12:11:03+0000 [xxu46_10] INFO: Crawled 538 pages (at 1 pages/min), scraped 531 items (at 1 items/min)
2015-04-10 12:11:38+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1411.7529> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 12:11:38+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1411.7529>
	{'abstract': u"We consider the Multiple Input Single Output (MISO) Gaussian Broadcast channel with antennas at the base station (BS) and single-antenna users in the downlink. We propose a novel user grouping precoder which improves the sum rate performance of the Zero-Forcing (ZF) precoder specially when the channel is ill-conditioned. The proposed precoder partitions all the users into small groups of equal size. Downlink beamforming is then done in such a way that, at each user's receiver the interference from the signal intended for users not in its group is nulled out. Intra-group interference still remains, and is cancelled through successive interference pre-subtraction at the BS using Dirty Paper Coding (DPC). The proposed user grouping method is different from user selection, since it is a method for precoding of information to the selected (scheduled) users, and not for selecting which users are to be scheduled. Through analysis and simulations, the proposed user grouping based precoder is shown to achieve significant improvement in the achievable sum rate when compared to the ZF precoder. When users are paired (i.e., each group has two users), the complexity of the proposed precoder is which is the same as that of the ZF precoder.",
	 'authors': u'Saif Khan Mohammed, Erik G. Larsson,',
	 'category': u'Computer Science ',
	 'date': '2014-11-27',
	 'pdflink': u'http://arxiv.org/pdf/1411.7529',
	 'subjects': u'Information Theory (cs.IT)',
	 'title': u'\nImproving the Performance of the Zero-Forcing Multiuser MISO Downlink  Precoder through User Grouping',
	 'urllink': u'http://arxiv.org/abs/1411.7529'}
2015-04-10 12:12:03+0000 [xxu46_10] INFO: Crawled 539 pages (at 1 pages/min), scraped 532 items (at 1 items/min)
2015-04-10 12:12:29+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1411.7525> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 12:12:29+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1411.7525>
	{'abstract': u'Syllogism is a type of deductive reasoning involving quantified statements. The syllogistic reasoning scheme in the classical Aristotelian framework involves three crisp term sets and four linguistic quantifiers, for which the main support is the linguistic properties of the quantifiers. A number of fuzzy approaches for defining an approximate syllogism have been proposed for which the main support is cardinality calculus. In this paper we analyze fuzzy syllogistic models previously described by Zadeh and Dubois et al. and compare their behavior with that of the classical Aristotelian framework to check which of the 24 classical valid syllogistic reasoning patterns or moods are particular crisp cases of these fuzzy approaches. This allows us to assess to what extent these approaches can be considered as either plausible extensions of the classical crisp syllogism or a basis for a general approach to the problem of approximate syllogism.',
	 'authors': u'M. Pereira-Fari\xf1a, F. D\xedaz-Hermida, A. Bugar\xedn,',
	 'category': u'Computer Science ',
	 'date': '2014-11-27',
	 'pdflink': u'http://arxiv.org/pdf/1411.7525',
	 'subjects': u'Artificial Intelligence (cs.AI)',
	 'title': u'\nOn the analysis of set-based fuzzy quantified reasoning using classical  syllogistics',
	 'urllink': u'http://arxiv.org/abs/1411.7525'}
2015-04-10 12:13:03+0000 [xxu46_10] INFO: Crawled 540 pages (at 1 pages/min), scraped 533 items (at 1 items/min)
2015-04-10 12:13:48+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1411.7507> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 12:13:48+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1411.7507>
	{'abstract': u'Due to its advantages over traditional data centers, there has been a rapid growth in the usage of cloud infrastructures. These include public clouds (e.g., Amazon EC2), or private clouds, such as clouds deployed using OpenStack. A common factor in many of the well known infrastructures, for example OpenStack and CloudStack, is that networked storage is used for storage of persistent data. However, traditional Big Data systems, including Hadoop, store data in commodity local storage for reasons of high performance and low cost. We present an architecture for supporting Hadoop on Openstack using local storage. Subsequently, we use benchmarks on Openstack and Amazon to show that for supporting Hadoop, local storage has better performance and lower cost. We conclude that cloud systems should support local storage for persistent data (in addition to networked storage) so as to provide efficient support for Hadoop and other Big Data systems',
	 'authors': u'Akshay MS, Suhas Mohan, Vincent Kuri, Dinkar Sitaram, H. L. Phalachandra,',
	 'category': u'Computer Science ',
	 'date': '2014-11-27',
	 'pdflink': u'http://arxiv.org/pdf/1411.7507',
	 'subjects': u'Distributed, Parallel, and Cluster Computing (cs.DC)',
	 'title': u'\nEfficient Support of Big Data Storage Systems on the Cloud',
	 'urllink': u'http://arxiv.org/abs/1411.7507'}
2015-04-10 12:14:03+0000 [xxu46_10] INFO: Crawled 541 pages (at 1 pages/min), scraped 534 items (at 1 items/min)
2015-04-10 12:15:03+0000 [xxu46_10] INFO: Crawled 541 pages (at 0 pages/min), scraped 534 items (at 0 items/min)
2015-04-10 12:15:03+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1411.7493> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 12:15:03+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1411.7493>
	{'abstract': u'In this work we study the set of leader codewords of a non-binary linear code. This set has some nice properties related to the monotonicity of the weight compatible order on the generalized support of a vector in . This allows us to describe a test set, a trial set and the set zero neighbours in terms of the leader codewords.',
	 'authors': u'Mijail Borges-Quintana, Miguel Angel Borges-Trenard, Edgar Martinez-Moro,',
	 'category': u'Computer Science ',
	 'date': '2014-11-27',
	 'pdflink': u'http://arxiv.org/pdf/1411.7493',
	 'subjects': u'Information Theory (cs.IT)',
	 'title': u'\nOn zero neighbours and trial sets of linear codes',
	 'urllink': u'http://arxiv.org/abs/1411.7493'}
2015-04-10 12:16:03+0000 [xxu46_10] INFO: Crawled 542 pages (at 1 pages/min), scraped 535 items (at 1 items/min)
2015-04-10 12:16:21+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1411.7492> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 12:16:21+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1411.7492>
	{'abstract': u'In this paper we give subexponential size hitting sets for bounded depth multilinear arithmetic formulas. Using the known relation between black-box PIT and lower bounds we obtain lower bounds for these models. For depth-3 multilinear formulas, of size , we give a hitting set of size . This implies a lower bound of for depth-3 multilinear formulas, for some explicit polynomial. For depth-4 multilinear formulas, of size , we give a hitting set of size . This implies a lower bound of for depth-4 multilinear formulas, for some explicit polynomial. A regular formula consists of alternating layers of gates, where all gates at layer have the same fan-in. We give a hitting set of size (roughly) , for regular depth- multilinear formulas of size , where . This result implies a lower bound of roughly for such formulas. We note that better lower bounds are known for these models, but also that none of these bounds was achieved via construction of a hitting set. Moreover, no lower bound that implies such PIT results, even in the white-box model, is currently known. Our results are combinatorial in nature and rely on reducing the underlying formula, first to a depth-4 formula, and then to a read-once algebraic branching program (from depth-3 formulas we go straight to read-once algebraic branching programs).',
	 'authors': u'Rafael Oliveira, Amir Shpilka, Ben Lee Volk,',
	 'category': u'Computer Science ',
	 'date': '2014-11-27',
	 'pdflink': u'http://arxiv.org/pdf/1411.7492',
	 'subjects': u'Computational Complexity (cs.CC)',
	 'title': u'\nSubexponential Size Hitting Sets for Bounded Depth Multilinear Formulas',
	 'urllink': u'http://arxiv.org/abs/1411.7492'}
2015-04-10 12:17:03+0000 [xxu46_10] INFO: Crawled 543 pages (at 1 pages/min), scraped 536 items (at 1 items/min)
2015-04-10 12:17:16+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1411.7487> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 12:17:16+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1411.7487>
	{'abstract': u'In this paper, a word based chaotic image encryption scheme for gray images is proposed, that can be used in both synchronous and self-synchronous modes. The encryption scheme operates in a finite field where we have also analyzed its performance according to numerical precision used in implementation. We show that the scheme not only passes a variety of security tests, but also it is verified that the proposed scheme operates faster than other existing schemes of the same type even when using lightweight short key sizes.',
	 'authors': u'Amir Daneshgar, Behrooz Khadem,',
	 'category': u'Computer Science ',
	 'date': '2014-11-27',
	 'pdflink': u'http://arxiv.org/pdf/1411.7487',
	 'subjects': u'Cryptography and Security (cs.CR)',
	 'title': u'\nA Self-synchronized Image Encryption Scheme',
	 'urllink': u'http://arxiv.org/abs/1411.7487'}
2015-04-10 12:18:03+0000 [xxu46_10] INFO: Crawled 544 pages (at 1 pages/min), scraped 537 items (at 1 items/min)
2015-04-10 12:18:15+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1411.7482> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 12:18:15+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1411.7482>
	{'abstract': u'We have developed SmartConnect, a tool that addresses the growing need for the design and deployment of multihop wireless relay networks for connecting sensors to a control center. Given the locations of the sensors, the traffic that each sensor generates, the quality of service (QoS) requirements, and the potential locations at which relays can be placed, SmartConnect helps design and deploy a low- cost wireless multihop relay network. SmartConnect adopts a field interactive, iterative approach, with model based network design, field evaluation and relay augmentation per- formed iteratively until the desired QoS is met. The design process is based on approximate combinatorial optimization algorithms. In the paper, we provide the design choices made in SmartConnect and describe the experimental work that led to these choices. We provide results from some experimental deployments. Finally, we conduct an experimental study of the robustness of the network design over long time periods (as channel conditions slowly change), in terms of the relay augmentation and route adaptation required.',
	 'authors': u'Abhijit Bhattacharya, Sanjay Motilal Ladwa, Rachit Srivastava, Aniruddha Mallya, Akhila Rao, Easwar Vivek. M, Deeksha G. Rao Sahib, S.V.R. Anand, Anurag Kumar,',
	 'category': u'Computer Science ',
	 'date': '2014-11-27',
	 'pdflink': u'http://arxiv.org/pdf/1411.7482',
	 'subjects': u'Networking and Internet Architecture (cs.NI)',
	 'title': u'\nSmartConnect: A System for the Design and Deployment of Wireless Sensor  Networks',
	 'urllink': u'http://arxiv.org/abs/1411.7482'}
2015-04-10 12:19:03+0000 [xxu46_10] INFO: Crawled 545 pages (at 1 pages/min), scraped 538 items (at 1 items/min)
2015-04-10 12:19:20+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1411.7480> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 12:19:20+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1411.7480>
	{'abstract': u'We present ULSA, a novel stochastic local search algorithm for random binary constraint satisfaction problems (CSP). ULSA is many times faster than the prior state of the art on a widely-studied suite of random CSP benchmarks. Unlike the best previous methods for these benchmarks, ULSA is a simple unweighted method that does not require dynamic adaptation of weights or penalties. ULSA obtains new record best solutions satisfying 99 of 100 variables in the challenging frb100-40 benchmark instance.',
	 'authors': u'Christopher D. Rosin,',
	 'category': u'Computer Science ',
	 'date': '2014-11-27',
	 'pdflink': u'http://arxiv.org/pdf/1411.7480',
	 'subjects': u'Artificial Intelligence (cs.AI)',
	 'title': u'\nUnweighted Stochastic Local Search can be Effective for Random CSP  Benchmarks',
	 'urllink': u'http://arxiv.org/abs/1411.7480'}
2015-04-10 12:20:03+0000 [xxu46_10] INFO: Crawled 546 pages (at 1 pages/min), scraped 539 items (at 1 items/min)
2015-04-10 12:20:28+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1411.7477> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 12:20:28+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1411.7477>
	{'abstract': u'Applying quasi-classical perturbation theory in the corresponding Feynman\'s path-integral representation, we derive in the limit of the large signal-to-noise ratio the analytical expression for the mutual information of the nonlinear communication channel described by the nonlinear Shr "dinger equation (NLSE) with the additive Gaussian noise. The NLSE is one of the fundamental models in nonlinear physics and has a broad range of applications, including fibre-optic communications --- the backbone of the Internet. Our analytical result demonstrates that the corrections to the mutual information in the leading nonlinearity order are positive. This result is somewhat counterintuitive, that is, that the impact of nonlinearity may increase the Shannon capacity above the capacity of a corresponding linear channel.',
	 'authors': u'I. S. Terekhov, A. V. Reznichenko, S. K. Turitsyn,',
	 'category': u'Computer Science ',
	 'date': '2014-11-27',
	 'pdflink': u'http://arxiv.org/pdf/1411.7477',
	 'subjects': u'Information Theory (cs.IT)',
	 'title': u'\nMutual information in nonlinear communication channel: Analytical  results in large SNR limit',
	 'urllink': u'http://arxiv.org/abs/1411.7477'}
2015-04-10 12:21:03+0000 [xxu46_10] INFO: Crawled 547 pages (at 1 pages/min), scraped 540 items (at 1 items/min)
2015-04-10 12:21:43+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1411.7474> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 12:21:43+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1411.7474>
	{'abstract': u'Different distance measures have been used for efficiently predicting software faults at early stages of software development. One stereotyped approach for software fault prediction due to its computational efficiency is K-means clustering, which partitions the dataset into K number of clusters using any distance measure. Distance measures by using some metrics are used to extract similar data objects which help in developing efficient algorithms for clustering and classification. In this paper, we study K-means clustering with three different distance measures Euclidean, Sorensen and Canberra by using datasets that have been collected from NASA MDP (metrics data program) .Results are displayed with the help of ROC curve. The experimental results shows that K-means clustering with Sorensen distance is better than Euclidean distance and Canberra distance.',
	 'authors': u'Deepinder Kaur,',
	 'category': u'Computer Science ',
	 'date': '2014-11-27',
	 'pdflink': u'http://arxiv.org/pdf/1411.7474',
	 'subjects': u'Software Engineering (cs.SE)',
	 'title': u'\nA Comparative Study of Various Distance Measures for Software fault  prediction',
	 'urllink': u'http://arxiv.org/abs/1411.7474'}
2015-04-10 12:22:03+0000 [xxu46_10] INFO: Crawled 548 pages (at 1 pages/min), scraped 541 items (at 1 items/min)
2015-04-10 12:22:48+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1411.7472> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 12:22:48+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1411.7472>
	{'abstract': u"Time-inconsistency refers to a paradox in decision making where agents exhibit inconsistent behaviors over time. Examples are procrastination where agents tends to costly postpone easy tasks, and abandonments where agents start a plan and quit in the middle. These behaviors are undesirable in the sense that agents make clearly suboptimal decisions over optimal ones. To capture such behaviors and more importantly, to quantify inefficiency caused by such behaviors, [Kleinberg &amp; Oren 2014] propose a graph model which is essentially same as the standard planning model except for the cost structure. Using this model, they initiate the study of several interesting problems: 1) cost ratio: the worst ratio between the actual cost of the agent and the optimal cost, over all graph instances; 2) motivating subgraph: how to motivate the agent to reach the goal by deleting nodes and edges; 3) Intermediate rewards: how to motivate agents to reach the goal by placing intermediate rewards. Kleinberg and Oren give partial answers to these questions, but the main problems are still open. In fact, they raise these problems explicitly as open problems in their paper. In this paper, we give answers to all three open problems in [Kleinberg &amp; Oren 2014]. First, we show a tight upper bound of cost ratio for graphs without Akerlof's structure, thus confirm the conjecture by Kleinberg and Oren that Akerlof's structure is indeed the worst case for cost ratio. Second, we prove that finding a motivating subgraph is NP-hard, showing that it is generally inefficient to motivate agents by deleting nodes and edges in the graph. Last but not least, we show that computing a strategy to place minimum amount of total reward is also NP-hard. Therefore, it is computational inefficient to motivate agents by placing intermediate rewards. The techniques we use to prove these results are nontrivial and of independent interests.",
	 'authors': u'Pingzhong Tang, Yifeng Teng, Zihe Wang, Shenke Xiao, Yichong Xu,',
	 'category': u'Computer Science ',
	 'date': '2014-11-27',
	 'pdflink': u'http://arxiv.org/pdf/1411.7472',
	 'subjects': u'Computer Science and Game Theory (cs.GT)',
	 'title': u'\nComputational issues in time-inconsistent planning',
	 'urllink': u'http://arxiv.org/abs/1411.7472'}
2015-04-10 12:23:03+0000 [xxu46_10] INFO: Crawled 549 pages (at 1 pages/min), scraped 542 items (at 1 items/min)
2015-04-10 12:23:43+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1411.7469> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 12:23:43+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1411.7469>
	{'abstract': u'"Clustering" the significance and application of this technique is spread over various fields. Clustering is an unsupervised process in data mining, that is why the proper evaluation of the results and measuring the compactness and separability of the clusters are important issues.The procedure of evaluating the results of a clustering algorithm is known as cluster validity measure. Different types of indexes are used to solve different types of problems and indices selection depends on the kind of available data.This paper first proposes Canonical PSO based K-means clustering algorithm and also analyses some important clustering indices (intercluster, intracluster) and then evaluates the effects of those indices on real-time air pollution database,wholesale customer, wine, and vehicle datasets using typical K-means, Canonical PSO based K-means, simple PSO based K-means,DBSCAN, and Hierarchical clustering algorithms.This paper also describes the nature of the clusters and finally compares the performances of these clustering algorithms according to the validity assessment. It also defines which algorithm will be more desirable among all these algorithms to make proper compact clusters on this particular real life datasets. It actually deals with the behaviour of these clustering algorithms with respect to validation indexes and represents their results of evaluation in terms of mathematical and graphical forms.',
	 'authors': u'Lopamudra Dey, Sanjay Chakraborty,',
	 'category': u'Computer Science ',
	 'date': '2014-11-27',
	 'pdflink': u'http://arxiv.org/pdf/1411.7469',
	 'subjects': u'Databases (cs.DB)',
	 'title': u'\nCanonical PSO Based k-Means Clustering Approach for Real Datasets',
	 'urllink': u'http://arxiv.org/abs/1411.7469'}
2015-04-10 12:24:03+0000 [xxu46_10] INFO: Crawled 550 pages (at 1 pages/min), scraped 543 items (at 1 items/min)
2015-04-10 12:24:57+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1411.7466> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 12:24:57+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1411.7466>
	{'abstract': u'A number of recent studies have shown that a Deep Convolutional Neural Network (DCNN) pretrained on a large dataset can be adopted as a universal image description which leads to astounding performance in many visual classification tasks. Most of these studies, if not all, adopt activations of the fully-connected layer of a DCNN as the image or region representation and it is believed that convolutional layer activations are less discriminative. This paper, however, advocates that if used appropriately convolutional layer activations can be turned into a powerful image representation which enjoys many advantages over fully-connected layer activations. This is achieved by adopting a new technique proposed in this paper called cross-convolutional-layer pooling. More specifically, it extracts subarrays of feature maps of one convolutional layer as local features and pools the extracted features with the guidance of feature maps of the successive convolutional layer. Compared with exising methods that apply DCNNs in the local feature setting, the proposed method is significantly faster since it requires much fewer times of DCNN forward computation. Moreover, it avoids the domain mismatch issue which is usually encountered when applying fully connected layer activations to describe local regions. By applying our method to four popular visual classification tasks, it is demonstrated that the proposed method can achieve comparable or in some cases significantly better performance than existing fully-connected layer based image representations while incurring much lower computational cost.',
	 'authors': u'Lingqiao Liu, Chunhua Shen, Anton van den Hengel,',
	 'category': u'Computer Science ',
	 'date': '2014-11-27',
	 'pdflink': u'http://arxiv.org/pdf/1411.7466',
	 'subjects': u'Computer Vision and Pattern Recognition (cs.CV)',
	 'title': u'\nThe Treasure beneath Convolutional Layers: Cross-convolutional-layer  Pooling for Image Classification',
	 'urllink': u'http://arxiv.org/abs/1411.7466'}
2015-04-10 12:25:03+0000 [xxu46_10] INFO: Crawled 551 pages (at 1 pages/min), scraped 544 items (at 1 items/min)
2015-04-10 12:25:58+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1411.7462> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 12:25:58+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1411.7462>
	{'abstract': u'When fluid flow in a pipeline is suddenly halted, a pressure surge or wave is created within the pipeline. This phenomenon, called water hammer, can cause major damage to pipelines, including pipeline ruptures. In this paper, we model the problem of mitigating water hammer during valve closure by an optimal boundary control problem involving a nonlinear hyperbolic PDE system that describes the fluid flow along the pipeline. The control variable in this system represents the valve boundary actuation implemented at the pipeline terminus. To solve the boundary control problem, we first use to obtain a finite-dimensional ODE model based on the original PDE system. Then, for the boundary control design, we apply the control parameterization method to obtain an approximate optimal parameter selection problem that can be solved using nonlinear optimization techniques such as Sequential Quadratic Programming (SQP). We conclude the paper with simulation results demonstrating the capability of optimal boundary control to significantly reduce flow fluctuation.',
	 'authors': u'Tehuan Chen, Chao Xu, Zhigang Ren, Ryan Loxton,',
	 'category': u'Computer Science ',
	 'date': '2014-11-27',
	 'pdflink': u'http://arxiv.org/pdf/1411.7462',
	 'subjects': u'Computational Engineering, Finance, and Science (cs.CE)',
	 'title': u'\nOptimal Boundary Control for Water Hammer Suppression in Fluid  Transmission Pipelines',
	 'urllink': u'http://arxiv.org/abs/1411.7462'}
2015-04-10 12:26:03+0000 [xxu46_10] INFO: Crawled 552 pages (at 1 pages/min), scraped 545 items (at 1 items/min)
2015-04-10 12:27:03+0000 [xxu46_10] INFO: Crawled 552 pages (at 0 pages/min), scraped 545 items (at 0 items/min)
2015-04-10 12:27:05+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1411.7460> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 12:27:05+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1411.7460>
	{'abstract': u'The maximum clique problem is a well known NP-Hard problem with applications in data mining, network analysis, information retrieval and many other areas related to the World Wide Web. There exist several algorithms for the problem with acceptable runtimes for certain classes of graphs, but many of them are infeasible for massive graphs. We present a new exact algorithm that employs novel pruning techniques and is able to find maximum cliques in very large, sparse graphs quickly. Extensive experiments on different kinds of synthetic and real-world graphs show that our new algorithm can be orders of magnitude faster than existing algorithms. We also present a heuristic that runs orders of magnitude faster than the exact algorithm while providing optimal or near-optimal solutions. We illustrate a simple application of the algorithms in developing methods for detection of overlapping communities in networks.',
	 'authors': u'Bharath Pattabiraman, Md. Mostofa Ali Patwary, Assefaw H. Gebremedhin, Wei-keng Liao, Alok Choudhary,',
	 'category': u'Computer Science ',
	 'date': '2014-11-27',
	 'pdflink': u'http://arxiv.org/pdf/1411.7460',
	 'subjects': u'Data Structures and Algorithms (cs.DS)',
	 'title': u'\nFast Algorithms for the Maximum Clique Problem on Massive Graphs with  Applications to Overlapping Community Detection',
	 'urllink': u'http://arxiv.org/abs/1411.7460'}
2015-04-10 12:27:50+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1411.7455> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 12:27:50+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1411.7455>
	{'abstract': u'An emerging theory of "linear-algebraic pseudorandomness" aims to understand the linear-algebraic analogs of fundamental Boolean pseudorandom objects where the rank of subspaces plays the role of the size of subsets. In this work, we study and highlight the interrelationships between several such algebraic objects such as subspace designs, dimension expanders, seeded rank condensers, two-source rank condensers, and rank-metric codes. In particular, with the recent construction of near-optimal subspace designs by Guruswami and Kopparty as a starting point, we construct good (seeded) rank condensers (both lossless and lossy versions), which are a small collection of linear maps for such that for every subset of of small rank, its rank is preserved (up to a constant factor in the lossy case) by at least one of the maps. We then compose a tensoring operation with our lossy rank condenser to construct constant-degree dimension expanders over polynomially large fields. That is, we give explicit linear maps such that for any subspace of dimension at most , . Previous constructions of such constant-degree dimension expanders were based on Kazhdan\'s property (for the case when has characteristic zero) or monotone expanders (for every field ); in either case the construction was harder than that of usual vertex expanders. Our construction, on the other hand, is simpler. Via an equivalence to linear rank-metric codes, we then construct optimal lossless two-source condensers. We then use our seeded rank condensers to obtain near-optimal lossy two-source condensers for constant rank sources.',
	 'authors': u'Michael A. Forbes, Venkatesan Guruswami,',
	 'category': u'Computer Science ',
	 'date': '2014-11-27',
	 'pdflink': u'http://arxiv.org/pdf/1411.7455',
	 'subjects': u'Computational Complexity (cs.CC)',
	 'title': u'\nDimension Expanders via Rank Condensers',
	 'urllink': u'http://arxiv.org/abs/1411.7455'}
2015-04-10 12:28:03+0000 [xxu46_10] INFO: Crawled 554 pages (at 2 pages/min), scraped 547 items (at 2 items/min)
2015-04-10 12:28:50+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1411.7450> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 12:28:50+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1411.7450>
	{'abstract': u'In this paper, we propose an efficient semidefinite programming (SDP) approach to worst-case linear discriminant analysis (WLDA). Compared with the traditional LDA, WLDA considers the dimensionality reduction problem from the worst-case viewpoint, which is in general more robust for classification. However, the original problem of WLDA is non-convex and difficult to optimize. In this paper, we reformulate the optimization problem of WLDA into a sequence of semidefinite feasibility problems. To efficiently solve the semidefinite feasibility problems, we design a new scalable optimization method with quasi-Newton methods and eigen-decomposition being the core components. The proposed method is orders of magnitude faster than standard interior-point based SDP solvers. Experiments on a variety of classification problems demonstrate that our approach achieves better performance than standard LDA. Our method is also much faster and more scalable than standard interior-point SDP solvers based WLDA. The computational complexity for an SDP with constraints and matrices of size by is roughly reduced from to ( in our case).',
	 'authors': u'Hui Li, Chunhua Shen, Anton van den Hengel, Qinfeng Shi,',
	 'category': u'Computer Science ',
	 'date': '2014-11-27',
	 'pdflink': u'http://arxiv.org/pdf/1411.7450',
	 'subjects': u'Learning (cs.LG)',
	 'title': u'\nWorst-Case Linear Discriminant Analysis as Scalable Semidefinite  Feasibility Problems',
	 'urllink': u'http://arxiv.org/abs/1411.7450'}
2015-04-10 12:29:03+0000 [xxu46_10] INFO: Crawled 555 pages (at 1 pages/min), scraped 548 items (at 1 items/min)
2015-04-10 12:30:00+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1411.7445> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 12:30:00+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1411.7445>
	{'abstract': u'This paper considers a new bi-objective optimization formulation for robust RGB-D visual odometry. We investigate two methods for solving the proposed bi-objective optimization problem: the weighted sum method (in which the objective functions are combined into a single objective function) and the bounded objective method (in which one of the objective functions is optimized and the value of the other objective function is bounded via a constraint). Our experimental results for the open source TUM RGB-D dataset show that the new bi-objective optimization formulation is superior to several existing RGB-D odometry methods. In particular, the new formulation yields more accurate motion estimates and is more robust when textural or structural features in the image sequence are lacking.',
	 'authors': u'Tao Han, Chao Xu, Ryan Loxton, Lei Xie,',
	 'category': u'Computer Science ',
	 'date': '2014-11-27',
	 'pdflink': u'http://arxiv.org/pdf/1411.7445',
	 'subjects': u'Robotics (cs.RO)',
	 'title': u'\nBi-objective Optimization for Robust RGB-D Visual Odometry',
	 'urllink': u'http://arxiv.org/abs/1411.7445'}
2015-04-10 12:30:03+0000 [xxu46_10] INFO: Crawled 556 pages (at 1 pages/min), scraped 549 items (at 1 items/min)
2015-04-10 12:31:03+0000 [xxu46_10] INFO: Crawled 556 pages (at 0 pages/min), scraped 549 items (at 0 items/min)
2015-04-10 12:31:05+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1411.7443> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 12:31:05+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1411.7443>
	{'abstract': u'We introduce the diffusion and superposition distances as two metrics to compare signals supported in the nodes of a network. Both metrics consider the given vectors as initial temperature distributions and diffuse heat trough the edges of the graph. The similarity between the given vectors is determined by the similarity of the respective diffusion profiles. The superposition distance computes the instantaneous difference between the diffused signals and integrates the difference over time. The diffusion distance determines a distance between the integrals of the diffused signals. We prove that both distances define valid metrics and that they are stable to perturbations in the underlying network. We utilize numerical experiments to illustrate their utility in classifying signals in a synthetic network as well as in classifying ovarian cancer histologies using gene mutation profiles of different patients. We also reinterpret diffusion as a transformation of interrelated feature spaces and use it as preprocessing tool for learning. We use diffusion to increase the accuracy of handwritten digit classification.',
	 'authors': u'Santiago Segarra, Weiyu Huang, Alejandro Ribeiro,',
	 'category': u'Computer Science ',
	 'date': '2014-11-27',
	 'pdflink': u'http://arxiv.org/pdf/1411.7443',
	 'subjects': u'Social and Information Networks (cs.SI)',
	 'title': u'\nDiffusion and Superposition Distances for Signals Supported on Networks',
	 'urllink': u'http://arxiv.org/abs/1411.7443'}
2015-04-10 12:32:03+0000 [xxu46_10] INFO: Crawled 557 pages (at 1 pages/min), scraped 550 items (at 1 items/min)
2015-04-10 12:32:18+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1411.7441> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 12:32:18+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1411.7441>
	{'abstract': u'Identifying important components or factors in large amounts of noisy data is a key problem in machine learning and data mining. Motivated by a pattern decomposition problem in materials discovery, aimed at discovering new materials for renewable energy, e.g. for fuel and solar cells, we introduce CombiFD, a framework for factor based pattern decomposition that allows the incorporation of a-priori knowledge as constraints, including complex combinatorial constraints. In addition, we propose a new pattern decomposition algorithm, called AMIQO, based on solving a sequence of (mixed-integer) quadratic programs. Our approach considerably outperforms the state of the art on the materials discovery problem, scaling to larger datasets and recovering more precise and physically meaningful decompositions. We also show the effectiveness of our approach for enforcing background knowledge on other application domains.',
	 'authors': u'Stefano Ermon, Ronan Le Bras, Santosh K. Suram, John M. Gregoire, Carla Gomes, Bart Selman, Robert B. van Dover,',
	 'category': u'Computer Science ',
	 'date': '2014-11-27',
	 'pdflink': u'http://arxiv.org/pdf/1411.7441',
	 'subjects': u'Artificial Intelligence (cs.AI)',
	 'title': u'\nPattern Decomposition with Complex Combinatorial Constraints:  Application to Materials Discovery',
	 'urllink': u'http://arxiv.org/abs/1411.7441'}
2015-04-10 12:33:03+0000 [xxu46_10] INFO: Crawled 558 pages (at 1 pages/min), scraped 551 items (at 1 items/min)
2015-04-10 12:33:14+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1411.7439> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 12:33:15+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1411.7439>
	{'abstract': u'We propose an encoding and control strategy for the stabilization of switched systems with limited information, supposing the controller is given for each mode. Only the quantized output and the active mode of the plant at each sampling time are transmitted to the controller. Due to switching, the active mode of the plant may be different from that of the controller in the closed-loop system. Hence if switching occurs, the quantizer must recalculate a bounded set containing the estimation error for quantization at the next sampling time. We establish the global asymptotic stability under a slow-switching assumption on dwell time and average dwell time. To this end, we construct multiple discrete-time Lyapunov functions with respect to the estimated state and the size of the bounded set.',
	 'authors': u'Masashi Wakaiki, Yutaka Yamamoto,',
	 'category': u'Computer Science ',
	 'date': '2014-11-27',
	 'pdflink': u'http://arxiv.org/pdf/1411.7439',
	 'subjects': u'Systems and Control (cs.SY)',
	 'title': u'\nOutput Feedback Stabilization of Switched Linear Systems with Limited  Information',
	 'urllink': u'http://arxiv.org/abs/1411.7439'}
2015-04-10 12:34:03+0000 [xxu46_10] INFO: Crawled 559 pages (at 1 pages/min), scraped 552 items (at 1 items/min)
2015-04-10 12:34:23+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1411.7419> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 12:34:23+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1411.7419>
	{'abstract': u'The vision of -DB introduces deterministic scientific hypotheses as a kind of uncertain and probabilistic data, and opens some key technical challenges for enabling data-driven hypothesis management and analytics. The -DB system addresses those challenges throughout a design-by-synthesis pipeline that defines its architecture. It processes hypotheses from their XML-based extraction to encoding as uncertain and probabilistic U-relational data, and eventually to their conditioning in the presence of observations. In this demo we present a first prototype of the -DB system. We showcase its core innovative features by means of use case scenarios in computational science in which the hypotheses are extracted from a model repository on the web and evaluated (rated/ranked) as probabilistic data.',
	 'authors': u'Bernardo Gon\xe7alves, Frederico C. Silva, Fabio Porto,',
	 'category': u'Computer Science ',
	 'date': '2014-11-26',
	 'pdflink': u'http://arxiv.org/pdf/1411.7419',
	 'subjects': u'Databases (cs.DB)',
	 'title': u'\n$\u03a5$-DB: A system for data-driven hypothesis management and  analytics',
	 'urllink': u'http://arxiv.org/abs/1411.7419'}
2015-04-10 12:35:03+0000 [xxu46_10] INFO: Crawled 560 pages (at 1 pages/min), scraped 553 items (at 1 items/min)
2015-04-10 12:35:36+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1411.7416> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 12:35:36+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1411.7416>
	{'abstract': u'Mobile sensing has become a promising paradigm for mobile users to obtain information by task crowdsourcing. However, due to the social preferences of mobile users, the quality of sensing reports may be impacted by the underlying social attributes and selfishness of individuals. Therefore, it is crucial to consider the social impacts and trustworthiness of mobile users when selecting task participants in mobile sensing. In this paper, we propose a Social Aware Crowdsourcing with Reputation Management (SACRM) scheme to select the well-suited participants and allocate the task rewards in mobile sensing. Specifically, we consider the social attributes, task delay and reputation in crowdsourcing and propose a participant selection scheme to choose the well-suited participants for the sensing task under a fixed task budget. A report assessment and rewarding scheme is also introduced to measure the quality of the sensing reports and allocate the task rewards based the assessed report quality. In addition, we develop a reputation management scheme to evaluate the trustworthiness and cost performance ratio of mobile users for participant selection. Theoretical analysis and extensive simulations demonstrate that SACRM can efficiently improve the crowdsourcing utility and effectively stimulate the participants to improve the quality of their sensing reports.',
	 'authors': u'Ju Ren, Yaoxue Zhang, Kuan Zhang, Xuemin, Shen,',
	 'category': u'Computer Science ',
	 'date': '2014-11-26',
	 'pdflink': u'http://arxiv.org/pdf/1411.7416',
	 'subjects': u'Networking and Internet Architecture (cs.NI)',
	 'title': u'\nSACRM: Social Aware Crowdsourcing with Reputation Management in Mobile  Sensing',
	 'urllink': u'http://arxiv.org/abs/1411.7416'}
2015-04-10 12:36:03+0000 [xxu46_10] INFO: Crawled 561 pages (at 1 pages/min), scraped 554 items (at 1 items/min)
2015-04-10 12:36:29+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1411.7414> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 12:36:29+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1411.7414>
	{'abstract': u'We consider the problem of signal recovery on graphs as graphs model data with complex structure as signals on a graph. Graph signal recovery implies recovery of one or multiple smooth graph signals from noisy, corrupted, or incomplete measurements. We propose a graph signal model and formulate signal recovery as a corresponding optimization problem. We provide a general solution by using the alternating direction methods of multipliers. We next show how signal inpainting, matrix completion, robust principal component analysis, and anomaly detection all relate to graph signal recovery, and provide corresponding specific solutions and theoretical analysis. Finally, we validate the proposed methods on real-world recovery problems, including online blog classification, bridge condition identification, temperature estimation, recommender system, and expert opinion combination of online blog classification.',
	 'authors': u'Siheng Chen, Aliaksei Sandryhaila, Jos\xe9 M. F. Moura, Jelena Kova\u010devi\u0107,',
	 'category': u'Computer Science ',
	 'date': '2014-11-26',
	 'pdflink': u'http://arxiv.org/pdf/1411.7414',
	 'subjects': u'Social and Information Networks (cs.SI)',
	 'title': u'\nSignal Recovery on Graphs',
	 'urllink': u'http://arxiv.org/abs/1411.7414'}
2015-04-10 12:37:03+0000 [xxu46_10] INFO: Crawled 562 pages (at 1 pages/min), scraped 555 items (at 1 items/min)
2015-04-10 12:37:41+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1411.7406> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 12:37:41+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1411.7406>
	{'abstract': u'Unary coding has found applications in data compression, neural network training, and in explaining the production mechanism of birdsong. Unary coding is redundant; therefore it should have inherent error correction capacity. An expression for the error correction capability of unary coding for the correction of single errors has been derived in this paper.',
	 'authors': u'Pushpa Sree Potluri,',
	 'category': u'Computer Science ',
	 'date': '2014-11-26',
	 'pdflink': u'http://arxiv.org/pdf/1411.7406',
	 'subjects': u'Information Theory (cs.IT)',
	 'title': u'\nError Correction Capacity of Unary Coding',
	 'urllink': u'http://arxiv.org/abs/1411.7406'}
2015-04-10 12:38:03+0000 [xxu46_10] INFO: Crawled 563 pages (at 1 pages/min), scraped 556 items (at 1 items/min)
2015-04-10 12:38:27+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1411.7399> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 12:38:27+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1411.7399>
	{'abstract': u'In the traditional object recognition pipeline, descriptors are densely sampled over an image, pooled into a high dimensional non-linear representation and then passed to a classifier. In recent years, Fisher Vectors have proven empirically to be the leading representation for a large variety of applications. The Fisher Vector is typically taken as the gradients of the log-likelihood of descriptors, with respect to the parameters of a Gaussian Mixture Model (GMM). Motivated by the assumption that different distributions should be applied for different datasets, we present two other Mixture Models and derive their Expectation-Maximization and Fisher Vector expressions. The first is a Laplacian Mixture Model (LMM), which is based on the Laplacian distribution. The second Mixture Model presented is a Hybrid Gaussian-Laplacian Mixture Model (HGLMM) which is based on a weighted geometric mean of the Gaussian and Laplacian distribution. An interesting property of the Expectation-Maximization algorithm for the latter is that in the maximization step, each dimension in each component is chosen to be either a Gaussian or a Laplacian. Finally, by using the new Fisher Vectors derived from HGLMMs, we achieve state-of-the-art results for both the image annotation and the image search by a sentence tasks.',
	 'authors': u'Benjamin Klein, Guy Lev, Gil Sadeh, Lior Wolf,',
	 'category': u'Computer Science ',
	 'date': '2014-11-26',
	 'pdflink': u'http://arxiv.org/pdf/1411.7399',
	 'subjects': u'Computer Vision and Pattern Recognition (cs.CV)',
	 'title': u'\nFisher Vectors Derived from Hybrid Gaussian-Laplacian Mixture Models for  Image Annotation',
	 'urllink': u'http://arxiv.org/abs/1411.7399'}
2015-04-10 12:39:03+0000 [xxu46_10] INFO: Crawled 564 pages (at 1 pages/min), scraped 557 items (at 1 items/min)
2015-04-10 12:39:41+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1411.7359> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 12:39:41+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1411.7359>
	{'abstract': u'The effective use of parallel computing resources to speed up algorithms in current multi-core parallel architectures remains a difficult challenge, with ease of programming playing a key role in the eventual success of various parallel architectures. In this paper we consider an alternative view of parallelism in the form of an ultra-wide word processor. We introduce the Ultra-Wide Word architecture and model, an extension of the word-RAM model that allows for constant time operations on thousands of bits in parallel. Word parallelism as exploited by the word-RAM model does not suffer from the more difficult aspects of parallel programming, namely synchronization and concurrency. For the standard word-RAM algorithms, the speedups obtained are moderate, as they are limited by the word size. We argue that a large class of word-RAM algorithms can be implemented in the Ultra-Wide Word model, obtaining speedups comparable to multi-threaded computations while keeping the simplicity of programming of the sequential RAM model. We show that this is the case by describing implementations of Ultra-Wide Word algorithms for dynamic programming and string searching. In addition, we show that the Ultra-Wide Word model can be used to implement a nonstandard memory architecture, which enables the sidestepping of lower bounds of important data structure problems such as priority queues and dynamic prefix sums. While similar ideas about operating on large words have been mentioned before in the context of multimedia processors [Thorup 2003], it is only recently that an architecture like the one we propose has become feasible and that details can be worked out.',
	 'authors': u'Arash Farzan, Alejandro L\xf3pez-Ortiz, Patrick K. Nicholson, Alejandro Salinger,',
	 'category': u'Computer Science ',
	 'date': '2014-11-26',
	 'pdflink': u'http://arxiv.org/pdf/1411.7359',
	 'subjects': u'Data Structures and Algorithms (cs.DS)',
	 'title': u'\nAlgorithms in the Ultra-Wide Word Model',
	 'urllink': u'http://arxiv.org/abs/1411.7359'}
2015-04-10 12:40:03+0000 [xxu46_10] INFO: Crawled 565 pages (at 1 pages/min), scraped 558 items (at 1 items/min)
2015-04-10 12:40:55+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1411.7357> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 12:40:55+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1411.7357>
	{'abstract': u'Citation metrics are becoming pervasive in the quantitative evaluation of scholars, journals and institutions. More then ever before, hiring, promotion, and funding decisions rely on a variety of impact metrics that cannot disentangle quality from quantity of scientific output, and are biased by factors such as discipline and academic age. Biases affecting the evaluation of single papers are compounded when one aggregates citation-based metrics across an entire publication record. It is not trivial to compare the quality of two scholars that during their careers have published at different rates in different disciplines in different periods of time. We propose a novel solution based on the generation of a statistical baseline specifically tailored on the academic profile of each researcher. Our method can decouple the roles of quantity and quality of publications to explain how a certain level of impact is achieved. The method is flexible enough to allow for the evaluation of, and fair comparison among, arbitrary collections of papers --- scholar publication records, journals, and entire institutions; and can be extended to simultaneously suppresses any source of bias. We show that our method can capture the quality of the work of Nobel laureates irrespective of number of publications, academic age, and discipline, even when traditional metrics indicate low impact in absolute terms. We further apply our methodology to almost a million scholars and over six thousand journals to measure the impact that cannot be explained by the volume of publications alone.',
	 'authors': u'Jasleen Kaur, Emilio Ferrara, Filippo Menczer, Alessandro Flammini, Filippo Radicchi,',
	 'category': u'Computer Science ',
	 'date': '2014-11-26',
	 'pdflink': u'http://arxiv.org/pdf/1411.7357',
	 'subjects': u'Digital Libraries (cs.DL)',
	 'title': u'\nQuality versus quantity in scientific impact',
	 'urllink': u'http://arxiv.org/abs/1411.7357'}
2015-04-10 12:41:03+0000 [xxu46_10] INFO: Crawled 566 pages (at 1 pages/min), scraped 559 items (at 1 items/min)
2015-04-10 12:42:03+0000 [xxu46_10] INFO: Crawled 566 pages (at 0 pages/min), scraped 559 items (at 0 items/min)
2015-04-10 12:42:06+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1411.7346> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 12:42:06+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1411.7346>
	{'abstract': u'A recent model for property testing of probability distributions enables tremendous savings in the sample complexity of testing algorithms, by allowing them to condition the sampling on subsets of the domain. In particular, Canonne et al. showed that, in this setting, testing identity of an unknown distribution (i.e., whether for an explicitly known ) can be done with a constant number of samples, independent of the support size -- in contrast to the required in the standard sampling model. However, it was unclear whether the same held for the case of testing equivalence, where both distributions are unknown. Indeed, while the best known upper bound for equivalence testing is , whether a dependence on the domain size is necessary was still open, and explicitly posed at the Bertinoro Workshop on Sublinear Algorithms. In this work, we answer the question in the positive, showing that any testing algorithm for equivalence must make queries in the conditional sampling model. Interestingly, this demonstrates an intrinsic qualitative gap between identity and equivalence testing, absent in the standard sampling model (where both problems have sampling complexity ). Turning to another question, we strengthen a result of Ron and Tsur on support size estimation in the conditional sampling model, with an algorithm to approximate the support size of an arbitrary distribution. This result matches the previously known upper bound in the restricted case where the distribution is guaranteed to be uniform over a subset. Furthermore, we settle a related open problem of theirs, proving tight lower bounds on support size estimation with non-adaptive queries.',
	 'authors': u'Jayadev Acharya, Cl\xe9ment L. Canonne, Gautam Kamath,',
	 'category': u'Computer Science ',
	 'date': '2014-11-26',
	 'pdflink': u'http://arxiv.org/pdf/1411.7346',
	 'subjects': u'Data Structures and Algorithms (cs.DS)',
	 'title': u'\nA Chasm Between Identity and Equivalence Testing with Conditional  Queries',
	 'urllink': u'http://arxiv.org/abs/1411.7346'}
2015-04-10 12:43:03+0000 [xxu46_10] INFO: Crawled 567 pages (at 1 pages/min), scraped 560 items (at 1 items/min)
2015-04-10 12:43:08+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1411.7344> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 12:43:08+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1411.7344>
	{'abstract': u'Memory ballooning is dynamic memory management technique for virtual machines (VMs). Ballooning is a part of memory reclamation technique operations used by a hypervisor to allow the physical host system to retrieve unused memory from certain guest virtual machines (VMs) and share it with others. Memory ballooning allows the total amount ofRAM required by guest VMs to exceed the amount ofphysical RAM available on the host. Memory overcommitment enables a higher consolidation ratio in a hypervisor. Using memory overcommitment, users can consolidate VMs on a physical machine such that physical resources are utilized in an optimal manner while delivering good performance. Hence memory reclamation is an integral component ofmemory overcommitment. In this paper, we address that the basic cause of memory that ballooning is memory overcommitment from using memory-intensive virtual machines. We compared to others reclamation technique and identify Cost Associate with Memory Ballooning in state of Memory Overcommitment. The objective of this paper is to analyse memory ballooning technique for dynamic memory management of VMs. For this analysis, VMware based virtualization software e.g ESXi Server, vCenter Server, vSphere Client are installed and configured on the Centre for Innovation and Technology (CIT) Lab, DIU; for monitor and analyze VM performance for memory ballooning technique. The performance ofmemory ballooning technique is evaluated with two different test cases. The purpose is to help users understand, how this technique impact the performance. Finally, we presents the throughput ofheavy workload with different memory limits when using ballooning or swapping; and analyse VM performance issue for this technique.',
	 'authors': u'A B M Moniruzzaman,',
	 'category': u'Computer Science ',
	 'date': '2014-11-26',
	 'pdflink': u'http://arxiv.org/pdf/1411.7344',
	 'subjects': u'Distributed, Parallel, and Cluster Computing (cs.DC)',
	 'title': u'\nAnalysis of Memory Ballooning Technique for Dynamic Memory Management of  Virtual Machines (VMs)',
	 'urllink': u'http://arxiv.org/abs/1411.7344'}
2015-04-10 12:43:57+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1411.7343> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 12:43:57+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1411.7343>
	{'abstract': u'One of the key advances in resolving the big-data problem has been the emergence of an alternative database technology. Today, classic RDBMS are complemented by a rich set of alternative Data Management Systems (DMS) specially designed to handle the volume, variety, velocity and variability ofBig Data collections; these DMS include NoSQL, NewSQL and Search-based systems. NewSQL is a class of modern relational database management systems (RDBMS) that provide the same scalable performance of NoSQL systems for online transaction processing (OLTP) read-write workloads while still maintaining the ACID guarantees of a traditional database system. This paper discusses about NewSQL data management system; and compares with NoSQL and with traditional database system. This paper covers architecture, characteristics, classification of NewSQL databases for online transaction processing (OLTP) for Big data management. It also provides the list ofpopular NoSQL as well as NewSQL databases in separate categorized tables. This paper compares SQL based RDBMS, NoSQL and NewSQL databases with set of metrics; as well as, addressed some research issues ofNoSQL and NewSQL.',
	 'authors': u'A B M Moniruzzaman,',
	 'category': u'Computer Science ',
	 'date': '2014-11-26',
	 'pdflink': u'http://arxiv.org/pdf/1411.7343',
	 'subjects': u'Databases (cs.DB)',
	 'title': u'\nNewSQL: Towards Next-Generation Scalable RDBMS for Online Transaction  Processing (OLTP) for Big Data Management',
	 'urllink': u'http://arxiv.org/abs/1411.7343'}
2015-04-10 12:44:03+0000 [xxu46_10] INFO: Crawled 569 pages (at 2 pages/min), scraped 562 items (at 2 items/min)
2015-04-10 12:44:59+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1411.7341> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 12:44:59+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1411.7341>
	{'abstract': u'A read once ABP is an arithmetic branching program with each variable occurring in at most one layer. We give the first polynomial time whitebox identity test for a polynomial computed by a sum of constantly many ROABPs. We also give a corresponding blackbox algorithm with quasi-polynomial time complexity, i.e. . The motivating special case of this model is sum of constantly many set-multilinear depth- circuits. The prior results for that model were only slightly better than brute-force (i.e. exponential-time). Our techniques are a new interplay of three concepts for ROABP: low evaluation dimension, basis isolating weight assignment and low-support rank concentration.',
	 'authors': u'Rohit Gurjar, Arpita Korwar, Nitin Saxena, Thomas Thierauf,',
	 'category': u'Computer Science ',
	 'date': '2014-11-26',
	 'pdflink': u'http://arxiv.org/pdf/1411.7341',
	 'subjects': u'Computational Complexity (cs.CC)',
	 'title': u'\nDeterministic Identity Testing for Sum of Read Once ABPs',
	 'urllink': u'http://arxiv.org/abs/1411.7341'}
2015-04-10 12:45:03+0000 [xxu46_10] INFO: Crawled 570 pages (at 1 pages/min), scraped 563 items (at 1 items/min)
2015-04-10 12:46:03+0000 [xxu46_10] INFO: Crawled 570 pages (at 0 pages/min), scraped 563 items (at 0 items/min)
2015-04-10 12:46:16+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1411.7337> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 12:46:16+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1411.7337>
	{'abstract': u"We present a novel set of methods for analyzing coverage properties in dynamic sensor networks. The dynamic sensor network under consideration is studied through a series of snapshots, and is represented by a sequence of simplicial complexes, built from the communication graph at each time point. A method from computational topology called zigzag persistent homology takes this sequence of simplicial complexes as input, and returns a `barcode' containing the birth and death times of homological features in this sequence. We derive useful statistics from this output for analyzing time-varying coverage properties. Further, we propose a method which returns specific representative cycles for these homological features, at each point along the birth-death intervals. These representative cycles are then used to track coverage holes in the network, and obtain size estimates for individual holes at each time point. A weighted barcode, incorporating the size information, is then used as a visual and quantitative descriptor of the dynamic network coverage.",
	 'authors': u'Jennifer Gamble, Harish Chintakunta, Hamid Krim,',
	 'category': u'Computer Science ',
	 'date': '2014-11-26',
	 'pdflink': u'http://arxiv.org/pdf/1411.7337',
	 'subjects': u'Computational Geometry (cs.CG)',
	 'title': u'\nCoordinate-Free Quantification of Coverage in Dynamic Sensor Networks',
	 'urllink': u'http://arxiv.org/abs/1411.7337'}
2015-04-10 12:47:03+0000 [xxu46_10] INFO: Crawled 571 pages (at 1 pages/min), scraped 564 items (at 1 items/min)
2015-04-10 12:47:25+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1411.7336> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 12:47:25+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1411.7336>
	{'abstract': u'Shapes and texture image recognition usage is an essential branch of pattern recognition. It is made up of techniques that aim at extracting information from images via human knowledge and works. Local Binary Pattern (LBP) ensures encoding global and local information and scaling invariance by introducing a look-up table to reflect the uniformity structure of an object. However, edge direction matrixes (EDMS) only apply global invariant descriptor which employs first and secondary order relationships. The main idea behind this methodology is the need of improved recognition capabilities, a goal achieved by the combinative use of these descriptors. This collaboration aims to make use of the major advantages each one presents, by simultaneously complementing each other, in order to elevate their weak points. By using multiple classifier approaches such as random forest and multi-layer perceptron neural network, the proposed combinative descriptor are compared with the state of the art combinative methods based on Gray-Level Co-occurrence matrix (GLCM with EDMS), LBP and moment invariant on four benchmark dataset MPEG-7 CE-Shape-1, KTH-TIPS image, Enghlishfnt and Arabic calligraphy . The experiments have shown the superiority of the introduced descriptor over the GLCM with EDMS, LBP and moment invariants and other well-known descriptor such as Scale Invariant Feature Transform from the literature.',
	 'authors': u'Mohammed A. Talab, Siti Norul Huda Sheikh Abdullah, Bilal Bataineh,',
	 'category': u'Computer Science ',
	 'date': '2014-11-26',
	 'pdflink': u'http://arxiv.org/pdf/1411.7336',
	 'subjects': u'Computer Vision and Pattern Recognition (cs.CV)',
	 'title': u'\nEdge direction matrixes-based local binar patterns descriptor for shape  pattern recognition',
	 'urllink': u'http://arxiv.org/abs/1411.7336'}
2015-04-10 12:48:03+0000 [xxu46_10] INFO: Crawled 572 pages (at 1 pages/min), scraped 565 items (at 1 items/min)
2015-04-10 12:48:13+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1411.7315> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 12:48:13+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1411.7315>
	{'abstract': u'Given a context free language over alphabet and a string , the language edit distance (Lan-ED) problem seeks the minimum number of edits (insertions, deletions and substitutions) required to convert into a valid member of . The well-known dynamic programming algorithm solves this problem in time (ignoring grammar size) where is the string length [Aho, Peterson 1972, Myers 1985]. Despite its vast number of applications, there is no algorithm known till date that computes or approximates Lan-ED in true sub-cubic time. In this paper we give the first such algorithm that computes Lan-ED almost optimally. For any arbitrary , our algorithm runs in time and returns an estimate within a multiplicative approximation factor of , where is the exponent of ordinary matrix multiplication of dimensional square matrices. It also computes the edit script. Further, for all substrings of , we can estimate their Lan-ED within factor in time with high probability. We also design the very first sub-cubic () algorithm to handle arbitrary stochastic context free grammar (SCFG) parsing. SCFGs lie the foundation of statistical natural language processing, they generalize hidden Markov models, and have found widespread applications. To complement our upper bound result, we show that exact computation of Lan-ED in true sub-cubic time will imply a truly sub-cubic algorithm for all-pairs shortest paths. This will result in a breakthrough for a large range of problems in graphs and matrices due to sub-cubic equivalence. By a known lower bound result [Lee 2002], improving upon our time bound of for any nontrivial multiplicative approximation is (almost) not possible.',
	 'authors': u'Barna Saha,',
	 'category': u'Computer Science ',
	 'date': '2014-11-26',
	 'pdflink': u'http://arxiv.org/pdf/1411.7315',
	 'subjects': u'Data Structures and Algorithms (cs.DS)',
	 'title': u'\nLanguage Edit Distance & Maximum Likelihood Parsing of Stochastic  Grammars: Faster Algorithms & Connection to Fundamental Graph Problems',
	 'urllink': u'http://arxiv.org/abs/1411.7315'}
2015-04-10 12:49:03+0000 [xxu46_10] INFO: Crawled 573 pages (at 1 pages/min), scraped 566 items (at 1 items/min)
2015-04-10 12:49:19+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1411.7296> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 12:49:19+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1411.7296>
	{'abstract': u'We address the problem of social network de-anonymization when relationships between people are described by scale-free graphs. In particular, we propose a rigorous, asymptotic mathematical analysis of the network de-anonymization problem while capturing the impact of power-law node degree distribution, which is a fundamental and quite ubiquitous feature of many complex systems such as social networks. By applying bootstrap percolation and a novel graph slicing technique, we prove that large inhomogeneities in the node degree lead to a dramatic reduction of the initial set of nodes that must be known a priori (the seeds) in order to successfully identify all other users. We characterize the size of this set when seeds are selected using different criteria, and we show that their number can be as small as , for any small . Our results are validated through simulation experiments on a real social network graph.',
	 'authors': u'Carla Chiasserini, Michele Garetto, Emilio Leonardi,',
	 'category': u'Computer Science ',
	 'date': '2014-11-26',
	 'pdflink': u'http://arxiv.org/pdf/1411.7296',
	 'subjects': u'Social and Information Networks (cs.SI)',
	 'title': u'\nDe-anonymizing scale-free social networks by percolation graph matching',
	 'urllink': u'http://arxiv.org/abs/1411.7296'}
2015-04-10 12:50:03+0000 [xxu46_10] INFO: Crawled 574 pages (at 1 pages/min), scraped 567 items (at 1 items/min)
2015-04-10 12:50:33+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1411.7286> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 12:50:33+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1411.7286>
	{'abstract': u'Polar codes are the first provable capacity-achieving forward error correction (FEC) codes. In general polar codes can be decoded via either successive cancellation (SC) or belief propagation (BP) decoding algorithm. However, to date practical applications of polar codes have been hindered by the long decoding latency and limited error-correcting performance problems. In this paper, based on our recent proposed early stopping criteria for the BP algorithm, we propose a hybrid BP-SC decoding scheme to improve the decoding performance of polar codes with relatively short latency. Simulation results show that, for (1024, 512) polar codes the proposed approach leads to at least 0.2dB gain over the BP algorithm with the same maximum number of iterations for the entire SNR region, and also achieves 0.2dB decoding gain over the BP algorithm with the same worst-case latency in the high SNR region. Besides, compared to the SC algorithm, the proposed scheme leads to 0.2dB gain in the medium SNR region with much less average decoding latency. In addition, we also propose the low-complexity unified hardware architecture for the hybrid decoding scheme, which is able to implement SC and BP algorithms using same hardware.',
	 'authors': u'Bo Yuan, Keshab K. Parhi,',
	 'category': u'Computer Science ',
	 'date': '2014-11-26',
	 'pdflink': u'http://arxiv.org/pdf/1411.7286',
	 'subjects': u'Information Theory (cs.IT)',
	 'title': u'\nAlgorithm and Architecture for Hybrid Decoding of Polar Codes',
	 'urllink': u'http://arxiv.org/abs/1411.7286'}
2015-04-10 12:51:03+0000 [xxu46_10] INFO: Crawled 575 pages (at 1 pages/min), scraped 568 items (at 1 items/min)
2015-04-10 12:51:43+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1411.7282> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 12:51:43+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1411.7282>
	{'abstract': u'Successive cancellation list (SCL) decoding algorithm is a powerful method that can help polar codes achieve excellent error-correcting performance. However, the current SCL algorithm and decoders are based on likelihood or log-likelihood forms, which render high hardware complexity. In this paper, we propose a log-likelihood-ratio (LLR)-based SCL (LLR-SCL) decoding algorithm, which only needs half the computation and storage complexity than the conventional one. Then, based on the proposed algorithm, we develop low-complexity VLSI architectures for LLR-SCL decoders. Analysis results show that the proposed LLR-SCL decoder achieves 50% reduction in hardware and 98% improvement in hardware efficiency.',
	 'authors': u'Bo Yuan, Keshab K. Parhi,',
	 'category': u'Computer Science ',
	 'date': '2014-11-26',
	 'pdflink': u'http://arxiv.org/pdf/1411.7282',
	 'subjects': u'Information Theory (cs.IT)',
	 'title': u'\nSuccessive Cancellation List Polar Decoder using Log-likelihood Ratios',
	 'urllink': u'http://arxiv.org/abs/1411.7282'}
2015-04-10 12:52:03+0000 [xxu46_10] INFO: Crawled 576 pages (at 1 pages/min), scraped 569 items (at 1 items/min)
2015-04-10 12:52:39+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1411.7277> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 12:52:39+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1411.7277>
	{'abstract': u'In this paper, we prove that every planar graph has a 1-string -VPG representation---a string representation using paths in a rectangular grid that contain at most two bends. Furthermore, two paths representing vertices intersect precisely once whenever there is an edge between and .',
	 'authors': u'Therese Biedl, Martin Derka,',
	 'category': u'Computer Science ',
	 'date': '2014-11-26',
	 'pdflink': u'http://arxiv.org/pdf/1411.7277',
	 'subjects': u'Computational Geometry (cs.CG)',
	 'title': u'\n$1$-String $B_2$-VPG Representation of Planar Graphs',
	 'urllink': u'http://arxiv.org/abs/1411.7277'}
2015-04-10 12:53:03+0000 [xxu46_10] INFO: Crawled 577 pages (at 1 pages/min), scraped 570 items (at 1 items/min)
2015-04-10 12:53:22+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1411.7273> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 12:53:22+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1411.7273>
	{'abstract': u'We consider the RMS distance (sum of squared distances between pairs of points) under translation between two point sets in the plane, in two different setups. In the partial-matching setup, each point in the smaller set is matched to a distinct point in the bigger set. Although the problem is not known to be polynomial, we establish several structural properties of the underlying subdivision of the plane and derive improved bounds on its complexity. These results lead to the best known algorithm for finding a translation for which the partial-matching RMS distance between the point sets is minimized. In addition, we show how to compute a local minimum of the partial-matching RMS distance under translation, in polynomial time. In the Hausdorff setup, each point is paired to its nearest neighbor in the other set. We develop algorithms for finding a local minimum of the Hausdorff RMS distance in nearly linear time on the line, and in nearly quadratic time in the plane. These improve substantially the worst-case behavior of the popular ICP heuristics for solving this problem.',
	 'authors': u'Rinat Ben-Avraham, Matthias Henze, Rafel Jaume, Bal\xe1zs Keszegh, Orit E. Raz, Micha Sharir, Igor Tubis,',
	 'category': u'Computer Science ',
	 'date': '2014-11-26',
	 'pdflink': u'http://arxiv.org/pdf/1411.7273',
	 'subjects': u'Computational Geometry (cs.CG)',
	 'title': u'\nPartial-Matching and Hausdorff RMS Distance Under Translation:  Combinatorics and Algorithms',
	 'urllink': u'http://arxiv.org/abs/1411.7273'}
2015-04-10 12:54:03+0000 [xxu46_10] INFO: Crawled 578 pages (at 1 pages/min), scraped 571 items (at 1 items/min)
2015-04-10 12:54:35+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1411.7267> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 12:54:35+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1411.7267>
	{'abstract': u'Evolutionary Robotics allows robots with limited sensors and processing to tackle complex tasks by means of sensory-motor coordination. In this paper we show the first application of the Behaviour Tree framework to a real robotic platform using the Evolutionary Robotics methodology. This framework is used to improve the intelligibility of the emergent robotic behaviour as compared to the traditional Neural Network formulation. As a result, the behaviour is easier to comprehend and manually adapt when crossing the reality gap from simulation to reality. This functionality is shown by performing real-world flight tests with the 20-gram DelFly Explorer flapping wing Micro Air Vehicle equipped with a 4-gram onboard stereo vision system. The experiments show that the DelFly can fully autonomously search for and fly through a window with only its onboard sensors and processing. The success rate of the learnt behaviour in simulation 88% and the corresponding real-world performance is 54% after user adaptation. Although this leaves room for improvement, it is higher than the 46% success rate from a tuned user-defined controller.',
	 'authors': u'Kirk Y.W. Scheper, Sjoerd Tijmons, Coen C. de Visser, Guido C.H.E. de Croon,',
	 'category': u'Computer Science ',
	 'date': '2014-11-26',
	 'pdflink': u'http://arxiv.org/pdf/1411.7267',
	 'subjects': u'Robotics (cs.RO)',
	 'title': u'\nBehaviour Trees for Evolutionary Robotics',
	 'urllink': u'http://arxiv.org/abs/1411.7267'}
2015-04-10 12:55:03+0000 [xxu46_10] INFO: Crawled 579 pages (at 1 pages/min), scraped 572 items (at 1 items/min)
2015-04-10 12:55:24+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1411.7228> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 12:55:24+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1411.7228>
	{'abstract': u'SimRank, proposed by Jeh and Widom, provides a good similarity measure that has been successfully used in numerous applications. While there are many algorithms proposed for computing SimRank, their computational costs are very high. In this paper, we propose a new computational technique, "SimRank linearization," for computing SimRank, which converts the SimRank problem to a linear equation problem. By using this technique, we can solve many SimRank problems, such as single-pair compuation, single-source computation, all-pairs computation, top k searching, and similarity join problems, efficiently.',
	 'authors': u'Takanori Maehara, Mitsuru Kusumoto, Ken-ichi Kawarabayashi,',
	 'category': u'Computer Science ',
	 'date': '2014-11-26',
	 'pdflink': u'http://arxiv.org/pdf/1411.7228',
	 'subjects': u'Data Structures and Algorithms (cs.DS)',
	 'title': u'\nEfficient SimRank Computation via Linearization',
	 'urllink': u'http://arxiv.org/abs/1411.7228'}
2015-04-10 12:56:03+0000 [xxu46_10] INFO: Crawled 580 pages (at 1 pages/min), scraped 573 items (at 1 items/min)
2015-04-10 12:56:04+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1411.7225> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 12:56:04+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1411.7225>
	{'abstract': u'The Information Systems around patents are complex, their study coupled with a creative vision of "out of the box", overcomes the strict basic functions of the patent. We have, on several occasions, guiding research around the patent solely-based on information, since the writing of new patents ; invalidation of existing patents, the creation of value-added information and their links to other Information Systems. The traditional R&amp;D based on heavy investments is one type of technology transfer. But, patent information is also, another powerful tool of technology transfer, innovation and creativity. Indeed, conduct research on the patent, from an academic viewpoint, although not always focusing only on financial revenue, can be considered as a form of "Non Practicing Entities" (NPE) activity, called rightly or wrongly "Patent Trolls". We\'ll see why the term "patent troll" for this activity is controversial and inappropriate. The research we will describe in this paper falls within this context. We show two case studies of efficient use of patent information in Emerging countries, the first concern the pharmaceutical industry in Brazil and the second, the oil industry in Algeria.',
	 'authors': u'Abdelkader Baaziz, Luc Quoniam,',
	 'category': u'Computer Science ',
	 'date': '2014-11-26',
	 'pdflink': u'http://arxiv.org/pdf/1411.7225',
	 'subjects': u'Computers and Society (cs.CY)',
	 'title': u'\nPatents used by NPE as an Open Information System in Web 2.0 - Two mini  case studies',
	 'urllink': u'http://arxiv.org/abs/1411.7225'}
2015-04-10 12:57:03+0000 [xxu46_10] INFO: Crawled 581 pages (at 1 pages/min), scraped 574 items (at 1 items/min)
2015-04-10 12:57:14+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1411.7224> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 12:57:14+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1411.7224>
	{'abstract': u"In this paper we develop a novel technique to analyze both isolated and interconnected caches operating under different caching strategies and realistic traffic conditions. The main strength of our approach is the ability to consider dynamic contents which are constantly added into the system catalogue, and whose popularity evolves over time according to desired profiles. We do so while preserving the simplicity and computational efficiency of models developed under stationary popularity conditions, which are needed to analyze several caching strategies. Our main achievement is to show that the impact of content popularity dynamics on cache performance can be effectively captured into an analytical model based on a fixed content catalogue (i.e., a catalogue whose size and objects' popularity do not change over time).",
	 'authors': u'Michele Garetto, Emilio Leonardi, Stefano Traverso,',
	 'category': u'Computer Science ',
	 'date': '2014-11-26',
	 'pdflink': u'http://arxiv.org/pdf/1411.7224',
	 'subjects': u'Performance (cs.PF)',
	 'title': u'\nEfficient analysis of caching strategies under dynamic content  popularity',
	 'urllink': u'http://arxiv.org/abs/1411.7224'}
2015-04-10 12:58:03+0000 [xxu46_10] INFO: Crawled 582 pages (at 1 pages/min), scraped 575 items (at 1 items/min)
2015-04-10 12:58:18+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1411.7217> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 12:58:18+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1411.7217>
	{'abstract': u'Flexible grid optical networks allow a better exploitation of fiber capacity, by enabling a denser frequency allocation. A tighter channel spacing, however, requires narrower filters, which increase linear intersymbol interference (ISI), and may dramatically reduce system reach. Commercial coherent receivers are based on symbol by symbol detectors, which are quite sensitive to ISI. In this context, Nyquist spacing is considered as the ultimate limit to wavelength-division multiplexing (WDM) packing. In this paper, we show that by introducing a limited-complexity trellis processing at the receiver, either the reach of Nyquist WDM flexi-grid networks can be significantly extended, or a denser-than-Nyquist channel packing (i.e., a higher spectral efficiency (SE)) is possible at equal reach. By adopting well-known information-theoretic techniques, we design a limited-complexity trellis processing and quantify its SE gain in flexi-grid architectures where wavelength selective switches over a frequency grid of 12.5GHz are employed.',
	 'authors': u'Tommaso Foggi, Giulio Colavolpe, Alberto Bononi, Paolo Serena,',
	 'category': u'Computer Science ',
	 'date': '2014-11-26',
	 'pdflink': u'http://arxiv.org/pdf/1411.7217',
	 'subjects': u'Information Theory (cs.IT)',
	 'title': u'\nSpectral Efficiency Optimization in Flexi-Grid Long-Haul Optical Systems',
	 'urllink': u'http://arxiv.org/abs/1411.7217'}
2015-04-10 12:59:03+0000 [xxu46_10] INFO: Crawled 583 pages (at 1 pages/min), scraped 576 items (at 1 items/min)
2015-04-10 12:59:35+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1411.7210> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 12:59:35+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1411.7210>
	{'abstract': u"BrowserID is a complex, real-world Single Sign-On (SSO) System for web applications recently developed by Mozilla. It employs new HTML5 features (such as web messaging and web storage) and cryptographic assertions to provide decentralized login, with the intent to respect users' privacy. It can operate in a primary and a secondary identity provider mode. While in the primary mode BrowserID runs with arbitrary identity providers (IdPs), in the secondary mode there is one IdP only, namely Mozilla's default IdP. We recently proposed an expressive general model for the web infrastructure and, based on this web model, analyzed the security of the secondary IdP mode of BrowserID. The analysis revealed several severe vulnerabilities. In this paper, we complement our prior work by analyzing the even more complex primary IdP mode of BrowserID. We do not only study authentication properties as before, but also privacy properties. During our analysis we discovered new and practical attacks that do not apply to the secondary mode: an identity injection attack, which violates a central authentication property of SSO systems, and attacks that break an important privacy promise of BrowserID and which do not seem to be fixable without a major redesign of the system. Some of our attacks on privacy make use of a browser side channel that has not gained a lot of attention so far. For the authentication bug, we propose a fix and formally prove in a slight extension of our general web model that the fixed system satisfies all the requirements we consider. This constitutes the most complex formal analysis of a web application based on an expressive model of the web infrastructure so far. As another contribution, we identify and prove important security properties of generic web features in the extended web model to facilitate future analysis efforts of web standards and web applications.",
	 'authors': u'Daniel Fett, Ralf K\xfcsters, Guido Schmitz,',
	 'category': u'Computer Science ',
	 'date': '2014-11-26',
	 'pdflink': u'http://arxiv.org/pdf/1411.7210',
	 'subjects': u'Cryptography and Security (cs.CR)',
	 'title': u'\nAnalyzing the BrowserID SSO System with Primary Identity Providers Using  an Expressive Model of the Web',
	 'urllink': u'http://arxiv.org/abs/1411.7210'}
2015-04-10 13:00:03+0000 [xxu46_10] INFO: Crawled 584 pages (at 1 pages/min), scraped 577 items (at 1 items/min)
2015-04-10 13:00:53+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1411.7197> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 13:00:53+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1411.7197>
	{'abstract': u'Massive multi-user (MU) multiple-input multiple-output (MIMO) systems are one possible key technology for next generation wireless communication systems. Claims have been made that massive MU-MIMO will increase both the radiated energy efficiency as well as the sum-rate capacity by orders of magnitude, because of the high transmit directivity. However, due to the very large number of transceivers needed at each base-station (BS), a successful implementation of massive MU-MIMO will be contingent on of the availability of very cheap, compact and power-efficient radio and digital-processing hardware. This may in turn impair the quality of the modulated radio frequency (RF) signal due to an increased amount of power-amplifier distortion, phase-noise, and quantization noise. In this paper, we examine the effects of hardware impairments on a massive MU-MIMO single-cell system by means of theory and simulation. The simulations are performed using simplified, well-established statistical hardware impairment models as well as more sophisticated and realistic models based upon measurements and electromagnetic antenna array simulations.',
	 'authors': u'Ulf Gustavsson, Cesar Sanch\xe9z-Perez, Thomas Eriksson, Fredrik Athley, Giuseppe Durisi, Per Landin, Katharina Hausmair, Christian Fager, Lars Svensson,',
	 'category': u'Computer Science ',
	 'date': '2014-11-26',
	 'pdflink': u'http://arxiv.org/pdf/1411.7197',
	 'subjects': u'Information Theory (cs.IT)',
	 'title': u'\nOn the Impact of Hardware Impairments on Massive MIMO',
	 'urllink': u'http://arxiv.org/abs/1411.7197'}
2015-04-10 13:01:03+0000 [xxu46_10] INFO: Crawled 585 pages (at 1 pages/min), scraped 578 items (at 1 items/min)
2015-04-10 13:01:51+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1411.7193> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 13:01:51+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1411.7193>
	{'abstract': u'One of the most promising applications of cognitive radio networks (CRNs)is the efficient exploitation of TV white spaces (TVWSs) for enhancing the performance of wireless networks. In this paper, we propose a cross-layer design (CLD) of carrier sense multiple access with collision avoidance (CSMA/CA) mechanism at the medium access control (MAC) layer with spectrum sensing (SpSe) at the physical layer, for identifying the occupancy status of TV bands. The proposed CLD relies on a Markov chain model with a state pair containing both the SpSe and the CSMA/CA from which we derive the collision probability and the achievable throughput. Analytical and simulation results are obtained for different collision avoidance and spectrum sensing implementation scenarios by varying the contention window, backoff stage and probability of detection. The obtained results depict the achievable throughput under different collision avoidance and spectrum sensing implementation scenarios indicating thereby the performance of collision avoidance in TVWSs based cognitive radio networks.',
	 'authors': u'Fotis Foukalas, George Karetsos,',
	 'category': u'Computer Science ',
	 'date': '2014-11-26',
	 'pdflink': u'http://arxiv.org/pdf/1411.7193',
	 'subjects': u'Networking and Internet Architecture (cs.NI)',
	 'title': u'\nCollision Avoidance in TV White Spaces: A Cross-layer Design Approach  for Cognitive Radio Networks',
	 'urllink': u'http://arxiv.org/abs/1411.7193'}
2015-04-10 13:02:03+0000 [xxu46_10] INFO: Crawled 586 pages (at 1 pages/min), scraped 579 items (at 1 items/min)
2015-04-10 13:03:03+0000 [xxu46_10] INFO: Crawled 586 pages (at 0 pages/min), scraped 579 items (at 0 items/min)
2015-04-10 13:03:06+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1411.7191> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 13:03:06+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1411.7191>
	{'abstract': u'In this paper we propose a hash function for -partitioning a set into bins so that we get good concentration bounds when combining statistics from different bins. To understand this point, suppose we have a fully random hash function applied to a set of red and blue balls. We want to estimate the fraction of red balls. The idea of MinHash is to sample the ball with the smallest hash value. This sample is uniformly random and is red with probability . The standard method is to repeat the experiment times with independent hash functions to reduce variance. Consider the alternative experiment using a single hash function, where we use some bits of the hash value to partition into bins, and then use the remaining bits as a local hash value. We pick the ball with the smallest hash value in each bin. The big difference between the two schemes is that the second one runs times faster. In the first experiment, each ball participated in independent experiments, but in the second one with -partitions, each ball picks its bin, and then only participates in the local experiment for that bin. Thus, essentially, we get experiments for the price of one. However, no realistic hash function is known to give the desired concentration bounds because the contents of different bins may be too correlated even if the marginal distribution for a single bin is random. Here, we present and analyze a hash function showing that it does yields statistics similar to that of a fully random hash function when -partitioning a set into bins. In this process we also give more insight into simple tabulation and show new results regarding the power of choice and moment estimation.',
	 'authors': u'S\xf8ren Dahlgaard, Mathias B\xe6k Tejs Knudsen, Eva Rotenberg, Mikkel Thorup,',
	 'category': u'Computer Science ',
	 'date': '2014-11-26',
	 'pdflink': u'http://arxiv.org/pdf/1411.7191',
	 'subjects': u'Data Structures and Algorithms (cs.DS)',
	 'title': u'\nHashing for statistics over k-partitions',
	 'urllink': u'http://arxiv.org/abs/1411.7191'}
2015-04-10 13:04:03+0000 [xxu46_10] INFO: Crawled 587 pages (at 1 pages/min), scraped 580 items (at 1 items/min)
2015-04-10 13:04:15+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1411.7158> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 13:04:15+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1411.7158>
	{'abstract': u"Cathoristic logic is a multi-modal logic where negation is replaced by a novel operator allowing the expression of incompatible sentences. We present the syntax and semantics of the logic including complete proof rules, and establish a number of results such as compactness, a semantic characterisation of elementary equivalence, the existence of a quadratic-time decision procedure, and Brandom's incompatibility semantics property. We demonstrate the usefulness of the logic as a language for knowledge representation.",
	 'authors': u'Richard Prideaux Evans, Martin Berger,',
	 'category': u'Computer Science ',
	 'date': '2014-11-26',
	 'pdflink': u'http://arxiv.org/pdf/1411.7158',
	 'subjects': u'Logic in Computer Science (cs.LO)',
	 'title': u'\nCathoristic logic: A modal logic of incompatible propositions',
	 'urllink': u'http://arxiv.org/abs/1411.7158'}
2015-04-10 13:05:03+0000 [xxu46_10] INFO: Crawled 588 pages (at 1 pages/min), scraped 581 items (at 1 items/min)
2015-04-10 13:05:09+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1411.7149> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 13:05:09+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1411.7149>
	{'abstract': u'In this paper, a new approximate syllogistic reasoning schema is described that expands some of the approaches expounded in the literature into two ways: (i) a number of different types of quantifiers (logical, absolute, proportional, comparative and exception) taken from Theory of Generalized Quantifiers and similarity quantifiers, taken from statistics, are considered and (ii) any number of premises can be taken into account within the reasoning process. Furthermore, a systematic reasoning procedure to solve the syllogism is also proposed, interpreting it as an equivalent mathematical optimization problem, where the premises constitute the constraints of the searching space for the quantifier in the conclusion.',
	 'authors': u'M. Pereira-Fari\xf1a, Juan C. Vidal, F. D\xedaz-Hermida, A. Bugar\xedn,',
	 'category': u'Computer Science ',
	 'date': '2014-11-26',
	 'pdflink': u'http://arxiv.org/pdf/1411.7149',
	 'subjects': u'Artificial Intelligence (cs.AI)',
	 'title': u'\nA Fuzzy Syllogistic Reasoning Schema for Generalized Quantifiers',
	 'urllink': u'http://arxiv.org/abs/1411.7149'}
2015-04-10 13:06:03+0000 [xxu46_10] INFO: Crawled 589 pages (at 1 pages/min), scraped 582 items (at 1 items/min)
2015-04-10 13:06:23+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1411.7148> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 13:06:23+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1411.7148>
	{'abstract': u"The Fifth International Workshop on Domain-Specific Languages and Models for Robotic Systems (DSLRob'14) was held in conjunction with the 2014 International Conference on Simulation, Modeling, and Programming for Autonomous Robots (SIMPAR 2014), October 2014 in Bergamo, Italy. The main topics of the workshop were Domain-Specific Languages (DSLs) and Model-driven Software Development (MDSD) for robotics. A domain-specific language is a programming language dedicated to a particular problem domain that offers specific notations and abstractions that increase programmer productivity within that domain. Model-driven software development offers a high-level way for domain users to specify the functionality of their system at the right level of abstraction. DSLs and models have historically been used for programming complex systems. However recently they have garnered interest as a separate field of study. Robotic systems blend hardware and software in a holistic way that intrinsically raises many crosscutting concerns (concurrency, uncertainty, time constraints, ...), for which reason, traditional general-purpose languages often lead to a poor fit between the language features and the implementation requirements. DSLs and models offer a powerful, systematic way to overcome this problem, enabling the programmer to quickly and precisely implement novel software solutions to complex problems within the robotics domain.",
	 'authors': u'Luca Gherardi, Nico Hochgeschwender, Christian Schlegel, Ulrik Pagh Schultz, Serge Stinckwich,',
	 'category': u'Computer Science ',
	 'date': '2014-11-26',
	 'pdflink': u'http://arxiv.org/html/1411.7148',
	 'subjects': u'Robotics (cs.RO)',
	 'title': u'\nProceedings of the Fifth International Workshop on Domain-Specific  Languages and Models for Robotic Systems (DSLRob 2014)',
	 'urllink': u'http://arxiv.org/abs/1411.7148'}
2015-04-10 13:07:03+0000 [xxu46_10] INFO: Crawled 590 pages (at 1 pages/min), scraped 583 items (at 1 items/min)
2015-04-10 13:07:15+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1411.7140> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 13:07:15+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1411.7140>
	{'abstract': u"Dynamic evaluation is a paradigm in computer algebra which was introduced for computing with algebraic numbers. In linear algebra, for instance, dynamic evaluation can be used to apply programs which have been written for matrices with coefficients modulo some prime number to matrices with coefficients modulo some composite number. A way to implement dynamic evaluation in modern computing languages is to use the exceptions mechanism provided by the language. In this paper, we pesent a proof system for exceptions which involves both raising and handling, by extending Moggi's approach based on monads. Moreover, the core part of this proof system is dual to a proof system for the state effect in imperative languages, which relies on the categorical notion of comonad. Both proof systems are implemented in the Coq proof assistant, and they are combined in order to deal with both effects at the same time.",
	 'authors': u'Jean-Guillaume Dumas, Dominique Duval, Burak Ekici, Damien Pous,',
	 'category': u'Computer Science ',
	 'date': '2014-11-26',
	 'pdflink': u'http://arxiv.org/pdf/1411.7140',
	 'subjects': u'Logic in Computer Science (cs.LO)',
	 'title': u'\nProgram certification with computational effects',
	 'urllink': u'http://arxiv.org/abs/1411.7140'}
2015-04-10 13:08:03+0000 [xxu46_10] INFO: Crawled 591 pages (at 1 pages/min), scraped 584 items (at 1 items/min)
2015-04-10 13:08:17+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1411.7139> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 13:08:17+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1411.7139>
	{'abstract': u'In purely functional programming languages imperative features, more generally computational effects are prohibited. However, non-functional lan- guages do involve effects. The theory of decorated logic provides a rigorous for- malism (with a refinement in operation signatures) for proving program properties with respect to computational effects. The aim of this thesis is to first develop Coq libraries and tools for verifying program properties in decorated settings as- sociated with several effects: states, local state, exceptions, non-termination, etc. Then, these tools will be combined to deal with several effects.',
	 'authors': u'Burak Ekici,',
	 'category': u'Computer Science ',
	 'date': '2014-11-26',
	 'pdflink': u'http://arxiv.org/pdf/1411.7139',
	 'subjects': u'Logic in Computer Science (cs.LO)',
	 'title': u'\nCertification of programs with computational effects',
	 'urllink': u'http://arxiv.org/abs/1411.7139'}
2015-04-10 13:09:03+0000 [xxu46_10] INFO: Crawled 592 pages (at 1 pages/min), scraped 585 items (at 1 items/min)
2015-04-10 13:09:36+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1411.7131> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 13:09:36+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1411.7131>
	{'abstract': u'Parallel processing is considered as todays and future trend for improving performance of computers. Computing devices ranging from small embedded systems to big clusters of computers rely on parallelizing applications to reduce execution time. Many of current computing systems rely on Non-Uniform Memory Access (NUMA) based processors architectures. In these architectures, analyzing and considering the non-uniformity is of high importance for improving scalability of systems. In this paper, we analyze and develop a NUMA based approach for the OpenMP parallel programming model. Our technique applies a smart threads allocation method and an advanced tasks scheduling strategy for reducing remote memory accesses and consequently their extra time consumption. We implemented our approach within the NANOS runtime system. A set of tests was conducted using the BOTS benchmarks and results showed the capacity of our technique in improving the performance of OpenMP applications especially those dealing with a large amount of data.',
	 'authors': u'Oussama Tahan,',
	 'category': u'Computer Science ',
	 'date': '2014-11-26',
	 'pdflink': u'http://arxiv.org/pdf/1411.7131',
	 'subjects': u'Distributed, Parallel, and Cluster Computing (cs.DC)',
	 'title': u'\nTowards Efficient OpenMP Strategies for Non-Uniform Architectures',
	 'urllink': u'http://arxiv.org/abs/1411.7131'}
2015-04-10 13:10:03+0000 [xxu46_10] INFO: Crawled 593 pages (at 1 pages/min), scraped 586 items (at 1 items/min)
2015-04-10 13:10:48+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1411.7121> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 13:10:48+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1411.7121>
	{'abstract': u'In this paper, the problem of outer beamformer design based only on channel statistic information is considered for two-stage beamforming for multi-user massive MIMO downlink, and the problem is approached based on signal-to-leakage-plus-noise ratio (SLNR). To eliminate the dependence on the instantaneous channel state information, a lower bound on the average SLNR is derived by assuming zero-forcing (ZF) inner beamforming, and an outer beamformer design method that maximizes the lower bound on the average SLNR is proposed. It is shown that the proposed SLNR-based outer beamformer design problem reduces to a trace quotient problem (TQP), which is often encountered in the field of machine learning. An iterative algorithm is presented to obtain an optimal solution to the proposed TQP. The proposed method has the capability of optimally controlling the weighting factor between the signal power to the desired user and the interference leakage power to undesired users according to different channel statistics. Numerical results show that the proposed outer beamformer design method yields significant performance gain over existing methods.',
	 'authors': u'Donggun Kim, Gilwon Lee, Youngchul Sung,',
	 'category': u'Computer Science ',
	 'date': '2014-11-26',
	 'pdflink': u'http://arxiv.org/pdf/1411.7121',
	 'subjects': u'Information Theory (cs.IT)',
	 'title': u'\nTwo-Stage Beamformer Design for Massive MIMO Downlink By Trace Quotient  Formulation',
	 'urllink': u'http://arxiv.org/abs/1411.7121'}
2015-04-10 13:11:03+0000 [xxu46_10] INFO: Crawled 594 pages (at 1 pages/min), scraped 587 items (at 1 items/min)
2015-04-10 13:11:42+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1411.7113> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 13:11:42+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1411.7113>
	{'abstract': u'We present a robust and real time approach to lane marker detection in urban streets. It is based on generating a top view of the road, filtering using selective oriented Gaussian filters, using RANSAC line fitting to give initial guesses to a new and fast RANSAC algorithm for fitting Bezier Splines, which is then followed by a post-processing step. Our algorithm can detect all lanes in still images of the street in various conditions, while operating at a rate of 50 Hz and achieving comparable results to previous techniques.',
	 'authors': u'Mohamed Aly,',
	 'category': u'Computer Science ',
	 'date': '2014-11-26',
	 'pdflink': u'http://arxiv.org/pdf/1411.7113',
	 'subjects': u'Computer Vision and Pattern Recognition (cs.CV)',
	 'title': u'\nReal time Detection of Lane Markers in Urban Streets',
	 'urllink': u'http://arxiv.org/abs/1411.7113'}
2015-04-10 13:12:03+0000 [xxu46_10] INFO: Crawled 595 pages (at 1 pages/min), scraped 588 items (at 1 items/min)
2015-04-10 13:12:32+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1411.7099> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 13:12:32+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1411.7099>
	{'abstract': u"An open distributed system can be secured by requiring participants to present proof of work and rewarding them for participation. The Bitcoin digital currency introduced this mechanism, which is adopted by almost all contemporary digital currencies and related services. A natural process leads participants of such systems to form pools, where members aggregate their power and share the rewards. Experience with Bitcoin shows that the largest pools are often open, allowing anyone to join. It has long been known that a member can sabotage an open pool by seemingly joining it but never sharing its proofs of work. The pool shares its revenue with the attacker, and so each of its participants earns less. We define and analyze a game where pools use some of their participants to infiltrate other pools and perform such an attack. With any number of pools, no-pool-attacks is not a Nash equilibrium. With two pools, or any number of identical pools, there exists an equilibrium that constitutes a tragedy of the commons where the pools attack one another and all earn less than they would have if none had attacked. For two pools, the decision whether or not to attack is the miner's dilemma, an instance of the iterative prisoner's dilemma. The game is played daily by the active Bitcoin pools, which apparently choose not to attack. If this balance breaks, the revenue of open pools might diminish, making them unattractive to participants.",
	 'authors': u'Ittay Eyal,',
	 'category': u'Computer Science ',
	 'date': '2014-11-26',
	 'pdflink': u'http://arxiv.org/pdf/1411.7099',
	 'subjects': u'Cryptography and Security (cs.CR)',
	 'title': u"\nThe Miner's Dilemma",
	 'urllink': u'http://arxiv.org/abs/1411.7099'}
2015-04-10 13:13:03+0000 [xxu46_10] INFO: Crawled 596 pages (at 1 pages/min), scraped 589 items (at 1 items/min)
2015-04-10 13:13:41+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1411.7090> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 13:13:41+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1411.7090>
	{'abstract': u"Virtual companions that interact with users in a socially complex environment require a wide range of social skills. Displaying curiosity is simultaneously a factor to improve a companion's believability and to unobtrusively affect the user's activities over time. Curiosity represents a drive to know new things. It is a major driving force for engaging learners in active learning. Existing research work pays little attention in curiosity. In this paper, we enrich the social skills of a virtual companion by infusing curiosity into its mental model. A curious companion residing in a Virtual Learning Environment (VLE) to stimulate user's curiosity is proposed. The curious companion model is developed based on multidisciplinary considerations. The effectiveness of the curious companion is demonstrated by a preliminary field study.",
	 'authors': u'Han Yu, Zhiqi Shen, Qiong Wu, Chunyan Miao,',
	 'category': u'Computer Science ',
	 'date': '2014-11-26',
	 'pdflink': u'http://arxiv.org/pdf/1411.7090',
	 'subjects': u'Human-Computer Interaction (cs.HC)',
	 'title': u'\nDesigning Socially Intelligent Virtual Companions',
	 'urllink': u'http://arxiv.org/abs/1411.7090'}
2015-04-10 13:14:03+0000 [xxu46_10] INFO: Crawled 597 pages (at 1 pages/min), scraped 590 items (at 1 items/min)
2015-04-10 13:14:52+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1411.7086> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 13:14:52+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1411.7086>
	{'abstract': u'We study the problem of finding unitary submatrices of the discrete Fourier transform matrix. This problem is related to a diverse set of questions on idempotents on Z_N, tiling Z_N, difference graphs and maximal cliques. Each of these is related to the problem of interpolating a discrete bandlimited signal using an orthogonal basis.',
	 'authors': u'Aditya Siripuram, William Wu, Brad Osgood,',
	 'category': u'Computer Science ',
	 'date': '2014-11-26',
	 'pdflink': u'http://arxiv.org/pdf/1411.7086',
	 'subjects': u'Information Theory (cs.IT)',
	 'title': u'\nDiscrete Sampling and Interpolation: Orthogonal Interpolation for  Discrete Bandlimited Signals',
	 'urllink': u'http://arxiv.org/abs/1411.7086'}
2015-04-10 13:15:03+0000 [xxu46_10] INFO: Crawled 598 pages (at 1 pages/min), scraped 591 items (at 1 items/min)
2015-04-10 13:16:03+0000 [xxu46_10] INFO: Crawled 598 pages (at 0 pages/min), scraped 591 items (at 0 items/min)
2015-04-10 13:16:04+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1411.7084> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 13:16:04+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1411.7084>
	{'abstract': u'Audio splicing is one of the most common manipulation techniques in the area of audio forensics. In this paper, the magnitudes of acoustic channel impulse response and ambient noise are proposed as the environmental signature. Specifically, the spliced audio segments are detected according to the magnitude correlation between the query frames and reference frames via a statically optimal threshold. The detection accuracy is further refined by comparing the adjacent frames. The effectiveness of the proposed method is tested on two data sets. One is generated from TIMIT database, and the other one is made in four acoustic environments using a commercial grade microphones. Experimental results show that the proposed method not only detects the presence of spliced frames, but also localizes the forgery segments with near perfect accuracy. Comparison results illustrate that the identification accuracy of the proposed scheme is higher than the previous schemes. In addition, experimental results also show that the proposed scheme is robust to MP3 compression attack, which is also superior to the previous works.',
	 'authors': u'Hong Zhao, Yifan Chen, Rui Wang, Hafiz Malik,',
	 'category': u'Computer Science ',
	 'date': '2014-11-26',
	 'pdflink': u'http://arxiv.org/pdf/1411.7084',
	 'subjects': u'Cryptography and Security (cs.CR)',
	 'title': u'\nAudio Splicing Detection and Localization Using Environmental Signature',
	 'urllink': u'http://arxiv.org/abs/1411.7084'}
2015-04-10 13:17:03+0000 [xxu46_10] INFO: Crawled 599 pages (at 1 pages/min), scraped 592 items (at 1 items/min)
2015-04-10 13:17:14+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1411.7055> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 13:17:14+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1411.7055>
	{'abstract': u'For an undirected -vertex graph with non-negative edge-weights, we consider the following type of query: given two vertices and in , what is the weight of a minimum -cut in ? We solve this problem in preprocessing time for graphs of bounded genus, giving the first sub-quadratic time algorithm for this class of graphs. Our result also improves by a logarithmic factor a previous algorithm by Borradaile, Sankowski and Wulff-Nilsen (FOCS 2010) that applied only to planar graphs. Our algorithm constructs a Gomory-Hu tree for the given graph, providing a data structure with space that can answer minimum-cut queries in constant time. The dependence on the genus of the input graph in our preprocessing time is .',
	 'authors': u'Glencora Borradaile, David Eppstein, Amir Nayyeri, Christian Wulff-Nilsen,',
	 'category': u'Computer Science ',
	 'date': '2014-11-25',
	 'pdflink': u'http://arxiv.org/pdf/1411.7055',
	 'subjects': u'Computational Geometry (cs.CG)',
	 'title': u'\nAll-Pairs Minimum Cuts in Near-Linear Time for Surface-Embedded Graphs',
	 'urllink': u'http://arxiv.org/abs/1411.7055'}
2015-04-10 13:18:03+0000 [xxu46_10] INFO: Crawled 600 pages (at 1 pages/min), scraped 593 items (at 1 items/min)
2015-04-10 13:18:32+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1411.7014> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 13:18:32+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1411.7014>
	{'abstract': u'We propose an efficient family of algorithms to learn the parameters of a Bayesian network from incomplete data. In contrast to textbook approaches such as EM and the gradient method, our approach is non-iterative, yields closed form parameter estimates, and eliminates the need for inference in a Bayesian network. Our approach provides consistent parameter estimates for missing data problems that are MCAR, MAR, and in some cases, MNAR. Empirically, our approach is orders of magnitude faster than EM (as our approach requires no inference). Given sufficient data, we learn parameters that can be orders of magnitude more accurate.',
	 'authors': u'Guy Van den Broeck, Karthika Mohan, Arthur Choi, Judea Pearl,',
	 'category': u'Computer Science ',
	 'date': '2014-11-25',
	 'pdflink': u'http://arxiv.org/pdf/1411.7014',
	 'subjects': u'Learning (cs.LG)',
	 'title': u'\nEfficient Algorithms for Bayesian Network Parameter Learning from  Incomplete Data',
	 'urllink': u'http://arxiv.org/abs/1411.7014'}
2015-04-10 13:19:03+0000 [xxu46_10] INFO: Crawled 601 pages (at 1 pages/min), scraped 594 items (at 1 items/min)
2015-04-10 13:19:43+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1411.7004> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 13:19:43+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1411.7004>
	{'abstract': u'It is time to make changes to the current research evaluation system, which is built on the journal selection. In this study, we propose the idea of continuous, dynamic and comprehensive article-level-evaluation based on article-level-metrics. Different kinds of metrics are integrated into a comprehensive indicator, which could quantify both the academic and societal impact of the article. At different phases after the publication, the weights of different metrics are dynamically adjusted to mediate the long term and short term impact of the paper. Using the sample data, we make empirical study of the article-level-evaluation method.',
	 'authors': u'Xianwen Wang, Zhichao Fang, Yang Yang,',
	 'category': u'Computer Science ',
	 'date': '2014-11-25',
	 'pdflink': u'http://arxiv.org/pdf/1411.7004',
	 'subjects': u'Digital Libraries (cs.DL)',
	 'title': u'\nContinuous, Dynamic and Comprehensive Article-Level Evaluation of  Scientific Literature',
	 'urllink': u'http://arxiv.org/abs/1411.7004'}
2015-04-10 13:20:03+0000 [xxu46_10] INFO: Crawled 602 pages (at 1 pages/min), scraped 595 items (at 1 items/min)
2015-04-10 13:20:54+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1411.6998> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 13:20:54+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1411.6998>
	{'abstract': u'In railway operations, a timetable is established to determine the departure and arrival times for the trains or other rolling stock at the different stations or relevant points inside the rail network or a subset of this network. The elaboration of this timetable is done to respond to the commercial requirements for both passenger and freight traffic, but also it must respect a set of security and capacity constraints associated with the railway network, rolling stock and legislation. Combining these requirements and constraints, as well as the important number of trains and schedules to plan, makes the preparation of a feasible timetable a complex and time-consuming process, that normally takes several months to be completed. This article addresses the problem of generating periodic timetables, which means that the involved trains operate in a recurrent pattern. For instance, the trains belonging to the same train line, depart from some station every 15 minutes or one hour. To tackle the problem, we present a constraint-based model suitable for this kind of problem. Then, we propose a genetic algorithm, allowing a rapid generation of feasible periodic timetables. Finally, two case studies are presented, the first, describing a sub-set of the Netherlands rail network, and the second a large portion of the Nord-pas-de-Calais regional rail network, both of them are then solved using our algorithm and the results are presented and discussed.',
	 'authors': u'Diego Arenas, Remy Chevirer, Said Hanafi, Joaquin Rodriguez,',
	 'category': u'Computer Science ',
	 'date': '2014-11-24',
	 'pdflink': u'http://arxiv.org/pdf/1411.6998',
	 'subjects': u'Artificial Intelligence (cs.AI)',
	 'title': u'\nSolving the Periodic Timetabling Problem using a Genetic Algorithm',
	 'urllink': u'http://arxiv.org/abs/1411.6998'}
2015-04-10 13:21:03+0000 [xxu46_10] INFO: Crawled 603 pages (at 1 pages/min), scraped 596 items (at 1 items/min)
2015-04-10 13:22:03+0000 [xxu46_10] INFO: Crawled 603 pages (at 0 pages/min), scraped 596 items (at 0 items/min)
2015-04-10 13:22:13+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1411.6993> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 13:22:13+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1411.6993>
	{'abstract': u"We prove a lower estimate on the increase in entropy when two copies of a conditional random variable , with supported on for prime , are summed modulo . Specifically, given two i.i.d copies and of a pair of random variables , with taking values in , we show [ H(X_1 + X_2 mid Y_1, Y_2) - H(X|Y) ge alpha(q) cdot H(X|Y) (1-H(X|Y)) ] for some , where is the normalized (by factor ) entropy. Our motivation is an effective analysis of the finite-length behavior of polar codes, and the assumption of being prime is necessary. For supported on infinite groups without a finite subgroup and no conditioning, a sumset inequality for the absolute increase in (unnormalized) entropy was shown by Tao (2010). We use our sumset inequality to analyze Arkan's construction of polar codes and prove that for any -ary source , where is any fixed prime, and any , polar codes allow data compression of i.i.d. copies of into -ary symbols, as soon as is polynomially large in . We can get capacity-achieving source codes with similar guarantees for composite alphabets, by factoring into primes and combining different polar codes for each prime in factorization. A consequence of our result for noisy channel coding is that for discrete memoryless channels, there are explicit codes enabling reliable communication within of the symmetric Shannon capacity for a block length and decoding complexity bounded by a polynomial in . The result was previously shown for the special case of binary input channels (Guruswami-Xia '13 and Hassani-Alishahi-Urbanke '13), and this work extends the result to channels over any alphabet.",
	 'authors': u'Venkatesan Guruswami, Ameya Velingker,',
	 'category': u'Computer Science ',
	 'date': '2014-11-25',
	 'pdflink': u'http://arxiv.org/pdf/1411.6993',
	 'subjects': u'Information Theory (cs.IT)',
	 'title': u'\nAn Entropy Sumset Inequality and Polynomially Fast Convergence to  Shannon Capacity Over All Alphabets',
	 'urllink': u'http://arxiv.org/abs/1411.6993'}
2015-04-10 13:23:03+0000 [xxu46_10] INFO: Crawled 604 pages (at 1 pages/min), scraped 597 items (at 1 items/min)
2015-04-10 13:23:26+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1411.6973> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 13:23:26+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1411.6973>
	{'abstract': u'This paper examines the dynamics of power-electronic inverters in islanded microgrids that are controlled to emulate the dynamics of Van der Pol oscillators. The general strategy of controlling inverters to emulate the behavior of nonlinear oscillators presents a compelling time-domain alternative to ubiquitous droop control methods which presume the existence of a quasi-stationary sinusoidal steady state and operate on phasor quantities. We present two main results in this work. First, by leveraging the method of periodic averaging, we demonstrate that droop laws are intrinsically embedded within a slower time scale in the nonlinear dynamics of Van der Pol oscillators. Second, we establish the global convergence of amplitude and phase dynamics in a resistive network interconnecting inverters controlled as Van der Pol oscillators. Furthermore, under a set of non-restrictive decoupling approximations, we derive sufficient conditions for local exponential stability of desirable equilibria of the linearized amplitude and phase dynamics.',
	 'authors': u'Mohit Sinha, Florian Dorfler, Brian B. Johnson, Sairaj V. Dhople,',
	 'category': u'Computer Science ',
	 'date': '2014-11-21',
	 'pdflink': u'http://arxiv.org/pdf/1411.6973',
	 'subjects': u'Systems and Control (cs.SY)',
	 'title': u'\nUncovering Droop Control Laws Embedded Within the Nonlinear Dynamics of  Van der Pol Oscillators',
	 'urllink': u'http://arxiv.org/abs/1411.6973'}
2015-04-10 13:24:03+0000 [xxu46_10] INFO: Crawled 605 pages (at 1 pages/min), scraped 598 items (at 1 items/min)
2015-04-10 13:24:45+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1411.6970> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 13:24:45+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1411.6970>
	{'abstract': u'Serial section Microscopy is an established method for volumetric anatomy reconstruction. Section series imaged with Electron Microscopy are currently vital for the reconstruction of the synaptic connectivity of entire animal brains such as that of Drosophila melanogaster. The process of removing ultrathin layers from a solid block containing the specimen, however, is a fragile procedure and has limited precision with respect to section thickness. We have developed a method to estimate the relative z-position of each individual section as a function of signal change across the section series. First experiments show promising results on both serial section Transmission Electron Microscopy (ssTEM) data and Focused Ion Beam Scanning Electron Microscopy (FIB-SEM) series. We made our solution available as Open Source plugins for the TrakEM2 software and the ImageJ distribution Fiji.',
	 'authors': u'Philipp Hanslovsky, John A. Bogovic, Stephan Saalfeld,',
	 'category': u'Computer Science ',
	 'date': '2014-11-25',
	 'pdflink': u'http://arxiv.org/pdf/1411.6970',
	 'subjects': u'Computer Vision and Pattern Recognition (cs.CV)',
	 'title': u'\nPost-acquisition image based compensation for thickness variation in  microscopy section series',
	 'urllink': u'http://arxiv.org/abs/1411.6970'}
2015-04-10 13:25:03+0000 [xxu46_10] INFO: Crawled 606 pages (at 1 pages/min), scraped 599 items (at 1 items/min)
2015-04-10 13:25:19+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1411.6942> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 13:25:19+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1411.6942>
	{'abstract': u'The paper is aimed at analyzing the potential of new information networks to solve the problems of energy management network with the use of renewable energy sources. One of the basic problems of renewable energy sources is their temporal and spatial variability. It is mainly about resources based on direct solar radiation and wind speed. New computer systems that use only classical connection-based solid structure of computer network connections but also on the basis of short-range connections allow accurate prediction of the active intensity changes observed energy. Using the system thus created can control precisely the basic energy equipment / generator and operable appliances / gradient to reduce the power needed resources or from work. This approach is one of the directions of further development of smart appliances and smart elements in the energy sector.',
	 'authors': u'Kultan Jaroslav, Kultan Matej,',
	 'category': u'Computer Science ',
	 'date': '2014-11-25',
	 'pdflink': u'http://arxiv.org/pdf/1411.6942',
	 'subjects': u'Systems and Control (cs.SY)',
	 'title': u'\nComputer networks new generation in the use of RES',
	 'urllink': u'http://arxiv.org/abs/1411.6942'}
2015-04-10 13:26:03+0000 [xxu46_10] INFO: Crawled 607 pages (at 1 pages/min), scraped 600 items (at 1 items/min)
2015-04-10 13:26:33+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1411.6928> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 13:26:33+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1411.6928>
	{'abstract': u'This paper proposes a tag identify approach based on fragile Watermark that based on Least significant bit of the replacement that we first use a special way to initialize the cover to ensure that we can use random positions to embed the information of tag. Using this way enhance the security of other to get the right information of this tag. Finally as long as the covered information can be decoded, the completeness and accuracy of the tag information can be guaranteed. the result of simulation experiment show that this approach has high sensitivity and security .',
	 'authors': u'Jianbiao Lin, Ke Ji, Hui Lin, Enyan Wu, Xin Gao,',
	 'category': u'Computer Science ',
	 'date': '2014-11-25',
	 'pdflink': u'http://arxiv.org/pdf/1411.6928',
	 'subjects': u'Multimedia (cs.MM)',
	 'title': u'\nA Tag Identification Approach Based On Fragile Watermark',
	 'urllink': u'http://arxiv.org/abs/1411.6928'}
2015-04-10 13:27:03+0000 [xxu46_10] INFO: Crawled 608 pages (at 1 pages/min), scraped 601 items (at 1 items/min)
2015-04-10 13:27:19+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1411.6919> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 13:27:19+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1411.6919>
	{'abstract': u'We present in this paper a canonical form for the elements in the ring of continuous piecewise polynomial functions. This new representation is based on the use of a particular class of functions defined by C_i(P)(x)= left 0 &amp; mbox &amp; x leq alpha P(x) &amp; mbox &amp; x geq alpha end right. where is the -th real root of the polynomial . These functions will allow us to represent and manipulate easily every continuous piecewise polynomial function through the use of the corresponding canonical form. It will be also shown how to produce a "rational" representation of each function allowing its evaluation by performing only operations in and avoiding the use of any real algebraic number.',
	 'authors': u'Jorge Caravantes, M. Angeles Gomez-Molleda, Laureano Gonzalez-Vega,',
	 'category': u'Computer Science ',
	 'date': '2014-11-25',
	 'pdflink': u'http://arxiv.org/pdf/1411.6919',
	 'subjects': u'Symbolic Computation (cs.SC)',
	 'title': u'\nA canonical form for the continuous piecewise polynomial functions',
	 'urllink': u'http://arxiv.org/abs/1411.6919'}
2015-04-10 13:28:03+0000 [xxu46_10] INFO: Crawled 609 pages (at 1 pages/min), scraped 602 items (at 1 items/min)
2015-04-10 13:28:36+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1411.6915> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 13:28:36+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1411.6915>
	{'abstract': u'We consider the problem of discovering overlapping communities in networks which we model as generalizations of Graph Packing problems with overlap. We seek a collection consisting of at least sets subject to certain disjointness restrictions. In the -Set Packing with -Membership, each element of belongs to at most sets of while in -Overlap each pair of sets in overlaps in at most elements. Each set of has at most elements. Similarly, both of our graph packing problems seek a collection of at least subgraphs in a graph each isomorphic to a graph . In -Packing with -Membership, each vertex of belongs to at most subgraphs of while in -Overlap each pair of subgraphs in overlaps in at most vertices. Each member of has at most vertices and edges. We show NP-Completeness results for all of our packing problems and we give a dichotomy result for the -Packing with -Membership problem analogous to the Kirkpatrick and Hell cite. We reduce the -Set Packing with -Membership to a problem kernel with elements while we achieve a kernel with elements for the -Set Packing with -Overlap. In addition, we reduce the -Packing with -Membership and its edge version to problem kernels with and vertices, respectively. On the other hand, we achieve kernels with and vertices for the -Packing with -Overlap and its edge version, respectively. In all cases, is the input parameter while , , and are constants.',
	 'authors': u'Henning Fernau, Alejandro L\xf3pez-Ortiz, Jazm\xedn Romero,',
	 'category': u'Computer Science ',
	 'date': '2014-11-25',
	 'pdflink': u'http://arxiv.org/pdf/1411.6915',
	 'subjects': u'Data Structures and Algorithms (cs.DS)',
	 'title': u'\nKernelization Algorithms for Packing Problems Allowing Overlaps  (Extended Version)',
	 'urllink': u'http://arxiv.org/abs/1411.6915'}
2015-04-10 13:29:03+0000 [xxu46_10] INFO: Crawled 610 pages (at 1 pages/min), scraped 603 items (at 1 items/min)
2015-04-10 13:29:22+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1411.6912> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 13:29:22+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1411.6912>
	{'abstract': u'Memories in the brain are separated in two categories: short-term and long-term memories. Long-term memories remain for a lifetime, while short-term ones exist from a few milliseconds to a few minutes. Within short-term memory studies, there is debate about what neural structure could implement it. Indeed, mechanisms responsible for long-term memories appear inadequate for the task. Instead, it has been proposed that short-term memories could be sustained by the persistent activity of a group of neurons. In this work, we explore what topology could sustain short-term memories, not by designing a model from specific hypotheses, but through Darwinian evolution in order to obtain new insights into its implementation. We evolved 10 networks capable of retaining information for a fixed duration between 2 and 11s. Our main finding has been that the evolution naturally created two functional modules in the network: one which sustains the information containing primarily excitatory neurons, while the other, which is responsible for forgetting, was composed mainly of inhibitory neurons. This demonstrates how the balance between inhibition and excitation plays an important role in cognition.',
	 'authors': u'Julien Hubert, Takashi Ikegami,',
	 'category': u'Computer Science ',
	 'date': '2014-11-25',
	 'pdflink': u'http://arxiv.org/pdf/1411.6912',
	 'subjects': u'Neural and Evolutionary Computing (cs.NE)',
	 'title': u'\nShort-Term Memory Through Persistent Activity: Evolution of  Self-Stopping and Self-Sustaining Activity in Spiking Neural Networks',
	 'urllink': u'http://arxiv.org/abs/1411.6912'}
2015-04-10 13:30:03+0000 [xxu46_10] INFO: Crawled 611 pages (at 1 pages/min), scraped 604 items (at 1 items/min)
2015-04-10 13:30:31+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1411.6909> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 13:30:31+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1411.6909>
	{'abstract': u'This paper proposes direct learning of image classification from user-supplied tags, without filtering. Each tag is supplied by the user who shared the image online. Enormous numbers of these tags are freely available online, and they give insight about the image categories important to users and to image classification. Our approach is complementary to the conventional approach of manual annotation, which is extremely costly. We analyze of the Flickr 100 Million Image dataset, making several useful observations about the statistics of these tags. We introduce a large-scale robust classification algorithm, in order to handle the inherent noise in these tags, and a calibration procedure to better predict objective annotations. We show that freely available, user-supplied tags can obtain similar or superior results to large databases of costly manual annotations.',
	 'authors': u'Hamid Izadinia, Ali Farhadi, Aaron Hertzmann, Matthew D. Hoffman,',
	 'category': u'Computer Science ',
	 'date': '2014-11-25',
	 'pdflink': u'http://arxiv.org/pdf/1411.6909',
	 'subjects': u'Computer Vision and Pattern Recognition (cs.CV)',
	 'title': u'\nImage Classification and Retrieval from User-Supplied Tags',
	 'urllink': u'http://arxiv.org/abs/1411.6909'}
2015-04-10 13:31:03+0000 [xxu46_10] INFO: Crawled 612 pages (at 1 pages/min), scraped 605 items (at 1 items/min)
2015-04-10 13:31:35+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1411.6907> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 13:31:35+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1411.6907>
	{'abstract': u"Given pervasive games that maintain a virtual spatiotemporal model of the physical world, game designers must contend with space and time in the virtual and physical. Previous works on pervasive games have partially contended with these representations, but an integrated conceptual model is lacking. Because they both make use of the Earth's geography, the problem domains of GIS and pervasive games overlap. The goal here is twofold: (1) help designers contend with the spatiotemporal representations and the analysis thereof; and (2) show that Peuquet's Triad Representational Framework from the domain of GIS is applicable to specifically the sub-domain of pervasive games that maintain a virtual model of physical space and time. By borrowing the Triad framework, space and time can be conceptualized in an integrated model as the WHAT, WHEN and WHERE, allowing for spatiotemporal analysis. The framework is evaluated and validated by applying it to the pervasive game called, Codename: Heroes.",
	 'authors': u'Kim J.L. Nevelsteen,',
	 'category': u'Computer Science ',
	 'date': '2014-11-25',
	 'pdflink': u'http://arxiv.org/pdf/1411.6907',
	 'subjects': u'Other Computer Science (cs.OH)',
	 'title': u'\nApplying GIS Concepts to a Pervasive Game: Spatiotemporal Modeling and  Analysis Using the Triad Representational Framework',
	 'urllink': u'http://arxiv.org/abs/1411.6907'}
2015-04-10 13:32:03+0000 [xxu46_10] INFO: Crawled 613 pages (at 1 pages/min), scraped 606 items (at 1 items/min)
2015-04-10 13:32:44+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1411.6897> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 13:32:44+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1411.6897>
	{'abstract': u'One of the technologies with potential to provide advanced beamforming capabilities for future indoor wireless communications is time-reversal (TR). By using the channel impulse response as a pre-filter at the transmitter, TR provides automatic spatial focusing of the signal at the receiver. In this paper we present two contributions on single-user indoor wideband TR systems. First, we provide a novel analysis of a baseband time-reversal (TR) beamforming system using two propagation models commonly used in indoor wireless communications. We derive a new closed-form approximation for the inter-symbol interference (ISI) power in such scenarios, which leads to a more accurate estimation of the probability of bit error compared to previous works. We define performance parameters for the spatial focusing and time compression properties of TR beamforming and find closed-form approximations for them. We use this parameters to compare TR performance under different propagation conditions and channel models. Second, we propose an Equalized TR (ETR) technique that mitigates the ISI of conventional TR. ETR uses a zero-forcing pre-equalizer at the transmitter in cascade configuration with the TR pre-filter. We derive theoretical performance bounds for ETR and show that it greatly enhances the BER performance of conventional TR with minimal impact to its beamforming capability. By means of numerical simulations, we verify our closed-form approximations and show that the proposed ETR technique outperforms conventional TR with respect to the BER under any SNR.',
	 'authors': u'Carlos Andr\xe9s Viteri-Mera, Fernando L. Teixeira,',
	 'category': u'Computer Science ',
	 'date': '2014-11-24',
	 'pdflink': u'http://arxiv.org/pdf/1411.6897',
	 'subjects': u'Networking and Internet Architecture (cs.NI)',
	 'title': u'\nEqualized Time Reversal Beamforming for Indoor Wireless Communications',
	 'urllink': u'http://arxiv.org/abs/1411.6897'}
2015-04-10 13:33:03+0000 [xxu46_10] INFO: Crawled 614 pages (at 1 pages/min), scraped 607 items (at 1 items/min)
2015-04-10 13:33:40+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1411.6884> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 13:33:40+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1411.6884>
	{'abstract': u'A new topology optimization method called the Proportional Topology Optimization (PTO) is presented. As a non-gradient method, PTO is simple to understand, easy to implement, and is also efficient and accurate at the same time. It is implemented into two MATLAB programs to solve the stress constrained and minimum compliance problems. Descriptions of the algorithm and computer programs are provided in detail. The method is applied to solve three numerical examples for both types of problems. The method shows comparable efficiency and accuracy with an existing gradient optimality criteria method. Also, the PTO stress constrained algorithm and minimum compliance algorithm are compared by feeding output from one algorithm to the other in an alternative manner, where the former yields lower maximum stress and volume fraction but higher compliance compared to the latter. Advantages and disadvantages of the proposed method and future works are discussed. The computer programs are self-contained and publicly shared in the website www.ptomethod.org.',
	 'authors': u'Emre Biyikli, Albert C. To,',
	 'category': u'Computer Science ',
	 'date': '2014-11-21',
	 'pdflink': u'http://arxiv.org/pdf/1411.6884',
	 'subjects': u'Computational Engineering, Finance, and Science (cs.CE)',
	 'title': u'\nProportional Topology Optimization: A new non-gradient method for  solving stress constrained and minimum compliance problems and its  implementation in MATLAB',
	 'urllink': u'http://arxiv.org/abs/1411.6884'}
2015-04-10 13:34:03+0000 [xxu46_10] INFO: Crawled 615 pages (at 1 pages/min), scraped 608 items (at 1 items/min)
2015-04-10 13:34:34+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1411.6853> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 13:34:34+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1411.6853>
	{'abstract': u'Suppose that we want to patrol a fence (line segment) using k mobile agents with given speeds v_1, ..., v_k so that every point on the fence is visited by an agent at least once in every unit time period. A simple strategy where the i-th agent moves back and forth in a segment of length v_i/2 patrols the length (v_1 + ... + v_k)/2, but it has been shown recently that this is not always optimal. Thus a natural question is to determine the smallest c such that a fence of length c(v_1 + ... + v_k)/2 cannot be patrolled. We give an example showing c &gt;= 4/3 (and conjecture that this is the best possible). We also consider a variant of this problem where we want to patrol a circle and the agents can move only clockwise. We can patrol a circle of perimeter r v_r by a simple strategy where the r fastest agents move at the same speed. We give an example where we can achieve the perimeter of 1.05 max_r r v_r (and conjecture that this constant can be arbitrary big). We propose another variant where we want to patrol a single point under the constraint that each agent i = 1, ..., k can visit the point only at a predefined interval of a_i or longer. This problem can be reduced to the discretized version where the a_i are integers and the goal is to visit the point at every integer time. It is easy to see that this discretized patrolling is impossible if 1/a_1 + ... + 1/a_k &lt; 1, and that there is a simple strategy if 1/a_1 + ... + 1/a_k &gt;= 2. Thus we are interested in the smallest c such that patrolling is always possible if 1/a_1 + ... + 1/a_k &gt;= c. We prove that alpha &lt;= c &lt; 1.546, where alpha = 1.264... (and conjecture that c = alpha). We also discuss the computational complexity of related problems.',
	 'authors': u'Akitoshi Kawamura, Makoto Soejima,',
	 'category': u'Computer Science ',
	 'date': '2014-11-25',
	 'pdflink': u'http://arxiv.org/pdf/1411.6853',
	 'subjects': u'Data Structures and Algorithms (cs.DS)',
	 'title': u'\nSimple strategies versus optimal schedules in multi-agent patrolling',
	 'urllink': u'http://arxiv.org/abs/1411.6853'}
2015-04-10 13:35:03+0000 [xxu46_10] INFO: Crawled 616 pages (at 1 pages/min), scraped 609 items (at 1 items/min)
2015-04-10 13:35:53+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1411.6852> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 13:35:53+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1411.6852>
	{'abstract': u'The problem of listing the shortest simple (loopless) -paths in a graph has been studied since the early 1960s. For a non-negatively weighted graph with vertices and edges, the most efficient solution is an algorithm for directed graphs by Yen and Lawler [Management Science, 1971 and 1972], and an algorithm for the undirected version by Katoh et al. [Networks, 1982], both using space. In this work, we consider a different parameterization for this problem: instead of bounding the number of -paths output, we bound their length. For the bounded length parameterization, we propose new non-trivial algorithms matching the time complexity of the classic algorithms but using only space. Moreover, we provide a unified framework such that the solutions to both parameterizations -- the classic -shortest and the new length-bounded paths -- can be seen as two different traversals of a same tree, a Dijkstra-like and a DFS-like traversal, respectively.',
	 'authors': u'Romeo Rizzi, Gustavo Sacomoto, Marie-France Sagot,',
	 'category': u'Computer Science ',
	 'date': '2014-11-25',
	 'pdflink': u'http://arxiv.org/pdf/1411.6852',
	 'subjects': u'Data Structures and Algorithms (cs.DS)',
	 'title': u'\nEfficiently listing bounded length st-paths',
	 'urllink': u'http://arxiv.org/abs/1411.6852'}
2015-04-10 13:36:03+0000 [xxu46_10] INFO: Crawled 617 pages (at 1 pages/min), scraped 610 items (at 1 items/min)
2015-04-10 13:37:01+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1411.6850> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 13:37:01+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1411.6850>
	{'abstract': u"This paper presents a new approach for detecting outliers by introducing the notion of object's proximity. The main idea is that normal point has similar characteristics with several neighbors. So the point in not an outlier if it has a high degree of proximity and its neighbors are several. The performance of this approach is illustrated through real datasets",
	 'authors': u'Amina Dik, Khalid Jebari, Abdelaziz Bouroumi, Aziz Ettouhami,',
	 'category': u'Computer Science ',
	 'date': '2014-11-25',
	 'pdflink': u'http://arxiv.org/pdf/1411.6850',
	 'subjects': u'Computer Vision and Pattern Recognition (cs.CV)',
	 'title': u'\nSimilarity- based approach for outlier detection',
	 'urllink': u'http://arxiv.org/abs/1411.6850'}
2015-04-10 13:37:03+0000 [xxu46_10] INFO: Crawled 618 pages (at 1 pages/min), scraped 611 items (at 1 items/min)
2015-04-10 13:38:03+0000 [xxu46_10] INFO: Crawled 618 pages (at 0 pages/min), scraped 611 items (at 0 items/min)
2015-04-10 13:38:20+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1411.6841> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 13:38:20+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1411.6841>
	{'abstract': u'Binary multimedia identifiable parent property codes (binary -MIPPCs) are used in multimedia fingerprinting schemes where the identification of users taking part in the averaging collusion attack to illegally redistribute content is required. In this paper, we first introduce a binary strong multimedia identifiable parent property code (binary -SMIPPC) whose tracing algorithm is more efficient than that of a binary -MIPPC. Then a composition construction for binary -SMIPPCs from -ary -SMIPPCs is provided. Several infinite series of optimal -ary -SMIPPCs of length with are derived from the relationships among -SMIPPCs and other fingerprinting codes, such as -separable codes and -MIPPCs. Finally, combinatorial properties of -ary -SMIPPCs of length are investigated, and optimal -ary -SMIPPCs of length with are constructed.',
	 'authors': u'Jing Jiang, Minquan Cheng, Ying Miao, Dianhua Wu,',
	 'category': u'Computer Science ',
	 'date': '2014-11-25',
	 'pdflink': u'http://arxiv.org/pdf/1411.6841',
	 'subjects': u'Information Theory (cs.IT)',
	 'title': u'\nMultimedia IPP Codes with Efficient Tracing',
	 'urllink': u'http://arxiv.org/abs/1411.6841'}
2015-04-10 13:39:03+0000 [xxu46_10] INFO: Crawled 619 pages (at 1 pages/min), scraped 612 items (at 1 items/min)
2015-04-10 13:39:08+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1411.6837> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 13:39:09+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1411.6837>
	{'abstract': u'Capacitive technology allows building sensors that are small, compact and have high sensitivity. For this reason it has been widely adopted in robotics. In a previous work we presented a compliant skin system based on capacitive technology consisting of triangular modules interconnected to form a system of sensors that can be deployed on non-flat surfaces. This solution has been successfully adopted to cover various humanoid robots. The main limitation of this and all the approaches based on capacitive technology is that they require to embed a deformable dielectric layer (usually made using an elastomer) covered by a conductive layer. This complicates the production process considerably, introduces hysteresis and limits the durability of the sensors due to ageing and mechanical stress. In this paper we describe a novel solution in which the dielectric is made using a thin layer of 3D fabric which is glued to conductive and protective layers using techniques adopted in the clothing industry. As such, the sensor is easier to produce and has better mechanical properties. Furthermore, the sensor proposed in this paper embeds transducers for thermal compensation of the pressure measurements. We report experimental analysis that demonstrates that the sensor has good properties in terms of sensitivity and resolution. Remarkably we show that the sensor has very low hysteresis and effectively allows compensating drifts due to temperature variations.',
	 'authors': u'Perla Maiolino, Marco Maggiali, Giorgio Cannata, Giorgio Metta, Lorenzo Natale,',
	 'category': u'Computer Science ',
	 'date': '2014-11-25',
	 'pdflink': u'http://arxiv.org/pdf/1411.6837',
	 'subjects': u'Robotics (cs.RO)',
	 'title': u'\nA Flexible and Robust Large Scale Capacitive Tactile System for Robots',
	 'urllink': u'http://arxiv.org/abs/1411.6837'}
2015-04-10 13:39:47+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1411.6836> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 13:39:47+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1411.6836>
	{'abstract': u'Research in texture recognition often concentrates on the problem of material recognition in uncluttered conditions, an assumption rarely met by applications. In this work we conduct a first study of material and describable texture at- tributes recognition in clutter, using a new dataset derived from the OpenSurface texture repository. Motivated by the challenge posed by this problem, we propose a new texture descriptor, D-CNN, obtained by Fisher Vector pooling of a Convolutional Neural Network (CNN) filter bank. D-CNN substantially improves the state-of-the-art in texture, mate- rial and scene recognition. Our approach achieves 82.3% accuracy on Flickr material dataset and 81.1% accuracy on MIT indoor scenes, providing absolute gains of more than 10% over existing approaches. D-CNN easily trans- fers across domains without requiring feature adaptation as for methods that build on the fully-connected layers of CNNs. Furthermore, D-CNN can seamlessly incorporate multi-scale information and describe regions of arbitrary shapes and sizes. Our approach is particularly suited at lo- calizing stuff categories and obtains state-of-the-art re- sults on MSRC segmentation dataset, as well as promising results on recognizing materials and surface attributes in clutter on the OpenSurfaces dataset.',
	 'authors': u'Mircea Cimpoi, Subhransu Maji, Andrea Vedaldi,',
	 'category': u'Computer Science ',
	 'date': '2014-11-25',
	 'pdflink': u'http://arxiv.org/pdf/1411.6836',
	 'subjects': u'Computer Vision and Pattern Recognition (cs.CV)',
	 'title': u'\nDeep convolutional filter banks for texture recognition and segmentation',
	 'urllink': u'http://arxiv.org/abs/1411.6836'}
2015-04-10 13:40:03+0000 [xxu46_10] INFO: Crawled 621 pages (at 2 pages/min), scraped 614 items (at 2 items/min)
2015-04-10 13:40:50+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1411.6835> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 13:40:50+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1411.6835>
	{'abstract': u'We consider zero error function computation in a three node wireless network. Nodes A and B observe and respectively, and want to compute a function with zero error. To achieve this, nodes A and B send messages to a relay node C at rates and respectively. The relay C then broadcasts a message to A and B at rate to help them compute with zero error. We allow block coding, and study the region of rate-triples that are feasible. The rate region is characterized in terms of graph coloring of some suitably defined probabilistic graphs. We give single letter inner and outer bounds which meet for some simple examples. We provide a sufficient condition on the joint distribution under which the relay can also compute if A and B can compute it with zero error.',
	 'authors': u'Jithin Ravi, Bikash Kumar Dey,',
	 'category': u'Computer Science ',
	 'date': '2014-11-25',
	 'pdflink': u'http://arxiv.org/pdf/1411.6835',
	 'subjects': u'Information Theory (cs.IT)',
	 'title': u'\nZero-Error Function Computation through a Bidirectional Relay',
	 'urllink': u'http://arxiv.org/abs/1411.6835'}
2015-04-10 13:41:03+0000 [xxu46_10] INFO: Crawled 622 pages (at 1 pages/min), scraped 615 items (at 1 items/min)
2015-04-10 13:41:53+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1411.6831> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 13:41:53+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1411.6831>
	{'abstract': u'The Phychip project is a collaborative European research initiative to design and implement computation using the organism Physarum polycephalum; it is funded by the Seventh Framework Programme (FP7) by the European Commission within CORDIS and the FET Proactive scheme. Included in this presentation are details the development of a Physarum based biosensor and biological logic gate, offering significant advancements in the respective fields. The work demonstrates the first steps towards Physarum computation and practical Physarum Biosensor; subsequent work will focus on development of a hybrid electronic-Physarum device capable of implementing computation.',
	 'authors': u'James G.H. Whiting, Ben P.J. de Lacy Costello, Andrew Adamatzky,',
	 'category': u'Computer Science ',
	 'date': '2014-11-25',
	 'pdflink': u'http://arxiv.org/pdf/1411.6831',
	 'subjects': u'Emerging Technologies (cs.ET)',
	 'title': u'\nPhysarum Chip: Developments in growing computers from slime mould',
	 'urllink': u'http://arxiv.org/abs/1411.6831'}
2015-04-10 13:42:03+0000 [xxu46_10] INFO: Crawled 623 pages (at 1 pages/min), scraped 616 items (at 1 items/min)
2015-04-10 13:42:58+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1411.6829> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 13:42:58+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1411.6829>
	{'abstract': u'A locally-optimal structure is a combinatorial structure such as a maximal independent set that cannot be improved by certain (greedy) local moves, even though it may not be globally optimal. It is trivial to construct an independent set in a graph. It is easy to (greedily) construct a maximal independent set. However, it is NP-hard to construct a globally-optimal (maximum) independent set. In general, constructing a locally-optimal structure is somewhat more difficult than constructing an arbitrary structure, and constructing a globally-optimal structure is more difficult than constructing a locally-optimal structure. The same situation arises with listing. The differences between the problems become obscured when we move from listing to counting because nearly everything is #P-complete. However, we highlight an interesting phenomenon that arises in approximate counting, where the situation is apparently reversed. Specifically, we show that counting maximal independent sets is complete for #P with respect to approximation-preserving reductions, whereas counting all independent sets, or counting maximum independent sets is complete for an apparently smaller class, which has a prominent role in the complexity of approximate counting. Motivated by the difficulty of approximately counting maximal independent sets in bipartite graphs, we also study the problem of approximately counting other locally-optimal structures that arise in algorithmic applications, particularly problems involving minimal separators and minimal edge separators. Minimal separators have applications via fixed-parameter-tractable algorithms for constructing triangulations and phylogenetic trees. Although exact (exponential-time) algorithms exist for listing these structures, we show that the counting problems are #P-complete with respect to both exact and approximation-preserving reductions.',
	 'authors': u'Leslie Ann Goldberg, Rob Gysel, John Lapinskas,',
	 'category': u'Computer Science ',
	 'date': '2014-11-25',
	 'pdflink': u'http://arxiv.org/pdf/1411.6829',
	 'subjects': u'Computational Complexity (cs.CC)',
	 'title': u'\nApproximately counting locally-optimal structures',
	 'urllink': u'http://arxiv.org/abs/1411.6829'}
2015-04-10 13:43:03+0000 [xxu46_10] INFO: Crawled 624 pages (at 1 pages/min), scraped 617 items (at 1 items/min)
2015-04-10 13:43:44+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1411.6818> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 13:43:44+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1411.6818>
	{'abstract': u'The stable allocation problem is a many-to-many generalization of the well-known stable marriage problem, where we seek a bipartite assignment between, say, jobs (of varying sizes) and machines (of varying capacities) that is "stable" based on a set of underlying preference lists submitted by the jobs and machines. We study a natural "unsplittable" variant of this problem, where each assigned job must be fully assigned to a single machine. Such unsplittable bipartite assignment problems generally tend to be NP-hard, including previously-proposed variants of the unsplittable stable allocation problem. Our main result is to show that under an alternative model of stability, the unsplittable stable allocation problem becomes solvable in polynomial time; although this model is less likely to admit feasible solutions than the model proposed iby McDermid and Manlove, we show that in the event there is no feasible solution, our approach computes a solution of minimal total congestion (overfilling of all machines collectively beyond their capacities). We also describe a technique for rounding the solution of a stable allocation problem to produce "relaxed" unsplit solutions that are only mildly infeasible, where each machine is overcongested by at most a single job.',
	 'authors': u'\xc1gnes Cseh, Brian C. Dean,',
	 'category': u'Computer Science ',
	 'date': '2014-11-25',
	 'pdflink': u'http://arxiv.org/pdf/1411.6818',
	 'subjects': u'Data Structures and Algorithms (cs.DS)',
	 'title': u'\nImproved Algorithmic Results for Unsplittable Stable Allocation Problems',
	 'urllink': u'http://arxiv.org/abs/1411.6818'}
2015-04-10 13:44:03+0000 [xxu46_10] INFO: Crawled 625 pages (at 1 pages/min), scraped 618 items (at 1 items/min)
2015-04-10 13:44:45+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1411.6810> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 13:44:45+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1411.6810>
	{'abstract': u"We consider discretization of the 'geometric cover problem' in the plane: Given a set of points in the plane and a compact planar object , find a minimum cardinality collection of planar translates of such that the union of the translates in the collection contains all the points in . We show that the geometric cover problem can be converted to a form of the geometric set cover, which has a given finite-size collection of translates rather than the infinite continuous solution space of the former. We propose a reduced finite solution space that consists of distinct canonical translates and present polynomial algorithms to find the reduce solution space for disks, convex/non-convex polygons (including holes), and planar objects consisting of finite Jordan curves.",
	 'authors': u'Dae-Sung Jang, Han-Lim Choi,',
	 'category': u'Computer Science ',
	 'date': '2014-11-25',
	 'pdflink': u'http://arxiv.org/pdf/1411.6810',
	 'subjects': u'Computational Geometry (cs.CG)',
	 'title': u'\nDiscretization of Planar Geometric Cover Problems',
	 'urllink': u'http://arxiv.org/abs/1411.6810'}
2015-04-10 13:45:03+0000 [xxu46_10] INFO: Crawled 626 pages (at 1 pages/min), scraped 619 items (at 1 items/min)
2015-04-10 13:46:03+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1411.6804> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 13:46:03+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1411.6804>
	{'abstract': u'Binets and trinets are phylogenetic networks with two and three leaves, respectively. Here we consider the problem of deciding if there exists a binary level-1 phylogenetic network displaying a given set of binary binets or trinets over a set of taxa, and constructing such a network whenever it exists. We show that this is NP-hard for trinets but polynomial-time solvable for binets. Moreover, we show that the problem is still polynomial-time solvable for inputs consisting of binets and trinets as long as the cycles in the trinets have size three. Finally, we present an time algorithm for general sets of binets and trinets. The latter two algorithms generalise to instances containing level-1 networks with arbitrarily many leaves, and thus provide some of the first supernetwork algorithms for computing networks from a set of rooted phylogenetic networks.',
	 'authors': u'Katharina Huber, Leo van Iersel, Vincent Moulton, Celine Scornavacca, Taoyang Wu,',
	 'category': u'Computer Science ',
	 'date': '2014-11-25',
	 'pdflink': u'http://arxiv.org/pdf/1411.6804',
	 'subjects': u'Data Structures and Algorithms (cs.DS)',
	 'title': u'\nReconstructing phylogenetic level-1 networks from nondense binet and  trinet sets',
	 'urllink': u'http://arxiv.org/abs/1411.6804'}
2015-04-10 13:46:03+0000 [xxu46_10] INFO: Crawled 627 pages (at 1 pages/min), scraped 620 items (at 1 items/min)
2015-04-10 13:47:03+0000 [xxu46_10] INFO: Crawled 627 pages (at 0 pages/min), scraped 620 items (at 0 items/min)
2015-04-10 13:47:13+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1411.6794> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 13:47:13+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1411.6794>
	{'abstract': u'Two interpretations about syllogistic statements are described in this paper. One is the so-called set-based interpretation, which assumes that quantified statements and syllogisms talk about quantity-relationships between sets. The other one, the so-called conditional interpretation, assumes that quantified propositions talk about conditional propositions and how strong are the links between the antecedent and the consequent. Both interpretations are compared attending to three different questions (existential import, singular statements and non-proportional quantifiers) from the point of view of their impact on the further development of this type of reasoning.',
	 'authors': u'M. Pereira-Fari\xf1a,',
	 'category': u'Computer Science ',
	 'date': '2014-11-25',
	 'pdflink': u'http://arxiv.org/pdf/1411.6794',
	 'subjects': u'Artificial Intelligence (cs.AI)',
	 'title': u'\nSome Reflections on the Set-based and the Conditional-based  Interpretations of Statements in Syllogistic Reasoning',
	 'urllink': u'http://arxiv.org/abs/1411.6794'}
2015-04-10 13:48:03+0000 [xxu46_10] INFO: Crawled 628 pages (at 1 pages/min), scraped 621 items (at 1 items/min)
2015-04-10 13:48:29+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1411.6792> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 13:48:29+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1411.6792>
	{'abstract': u'The method for computation of conditional probability density function for the nonlinear Schr "odinger equation with additive noise is developed. We present in a constructive form the conditional probability density function in the limit of a small noise and analytically derive it in a weakly nonlinear case. The general theory results are illustrated using fibre-optic communications as a particular, albeit practically very important, example.',
	 'authors': u'I. S. Terekhov, S. S. Vergeles, S. K. Turitsyn,',
	 'category': u'Computer Science ',
	 'date': '2014-11-25',
	 'pdflink': u'http://arxiv.org/pdf/1411.6792',
	 'subjects': u'Information Theory (cs.IT)',
	 'title': u'\nConditional probability calculations for the nonlinear Schr\xf6dinger  equation with additive noise',
	 'urllink': u'http://arxiv.org/abs/1411.6792'}
2015-04-10 13:49:03+0000 [xxu46_10] INFO: Crawled 629 pages (at 1 pages/min), scraped 622 items (at 1 items/min)
2015-04-10 13:49:40+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1411.6791> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 13:49:40+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1411.6791>
	{'abstract': u"In wireless networks, node cooperation has been exploited as a data relaying mechanism for decades. However, the wireless channel allows for much richer interaction among nodes. In particular, Distributed Information SHaring (DISH) represents a new improvement to multi-channel MAC protocol design by using a cooperative element at the control plane. In this approach, nodes exchange control information to make up for other nodes' insufficient knowledge about the environment, and thereby aid in their decision making. To date, what is lacking is a theoretical understanding of DISH. In this paper, we view cooperation as a network resource and evaluate the availability of cooperation, . We first analyze in the context of a multi-channel multi-hop wireless network, and then perform simulations which show that the analysis accurately characterizes as a function of underlying network parameters. Next, we investigate the correlation between and network metrics such as collision rate, packet delay, and throughput. We find a near-linear relationship between and the metrics, which suggests that can be used as an appropriate performance indicator itself. Finally, we apply our analysis to solving a channel bandwidth allocation problem, where we derive optimal schemes and provide general guidelines on bandwidth allocation for DISH networks.",
	 'authors': u'Tie Luo, Vikram Srinivasan, Mehul Motani,',
	 'category': u'Computer Science ',
	 'date': '2014-11-25',
	 'pdflink': u'http://arxiv.org/pdf/1411.6791',
	 'subjects': u'Networking and Internet Architecture (cs.NI)',
	 'title': u'\nA Metric for DISH Networks: Analysis, Implications, and Applications',
	 'urllink': u'http://arxiv.org/abs/1411.6791'}
2015-04-10 13:50:03+0000 [xxu46_10] INFO: Crawled 630 pages (at 1 pages/min), scraped 623 items (at 1 items/min)
2015-04-10 13:50:59+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1411.6784> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 13:50:59+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1411.6784>
	{'abstract': u'Let be a -ary code of length and size , and be the set of th coordinates of . The descendant code of a sub-code is defined to be . In this paper, we introduce a multimedia analogue of codes with the identifiable parent property (IPP), called multimedia IPP codes or -MIPPC, so that given the descendant code of any sub-code of a multimedia -IPP code , one can always identify, as IPP codes do in the generic digital scenario, at least one codeword in . We first derive a general upper bound on the size of a multimedia -IPP code, and then investigate multimedia -IPP codes in more detail. We characterize a multimedia -IPP code of length in terms of a bipartite graph and a generalized packing, respectively. By means of these combinatorial characterizations, we further derive a tight upper bound on the size of a multimedia -IPP code of length , and construct several infinite families of (asymptotically) optimal multimedia -IPP codes of length .',
	 'authors': u'Minquan Cheng, Hung-Lin Fu, Jing Jiang, Yuan-Hsun Lo, Ying Miao,',
	 'category': u'Computer Science ',
	 'date': '2014-11-25',
	 'pdflink': u'http://arxiv.org/pdf/1411.6784',
	 'subjects': u'Information Theory (cs.IT)',
	 'title': u'\nCodes with the Identifiable Parent Property for Multimedia  Fingerprinting',
	 'urllink': u'http://arxiv.org/abs/1411.6784'}
2015-04-10 13:51:03+0000 [xxu46_10] INFO: Crawled 631 pages (at 1 pages/min), scraped 624 items (at 1 items/min)
2015-04-10 13:51:50+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1411.6777> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 13:51:50+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1411.6777>
	{'abstract': u'Intrusion Detection System or IDS is a software or hardware tool that repeatedly scans and monitors events that took place in a computer or a network. A set of rules are used by Signature based Network Intrusion Detection Systems or NIDS to detect hostile traffic in network segments or packets, which are so important in detecting malicious and anomalous behaviour over the network like known attacks that hackers look for new techniques to go unseen. Sometime, a single failure at any layer will cause the NIDS to miss that attack. To overcome this problem, a technique is used that will trigger a failure in that layer. Such technique is known as Evasive technique. An Evasion can be defined as any technique that modifies a visible attack into any other form in order to stay away from being detect. The proposed system is used for detecting attacks which are going on the network and also gives actual categorization of attacks. The proposed system has advantage of getting low false alarm rate and high detection rate. So that leads into decrease in complexity and overhead on the system. The paper presents the Evasion technique for customized apriori algorithm. The paper aims to make a new functional structure to evade NIDS. This framework can be used to audit NIDS. This framework shows that a proof of concept showing how to evade a self built NIDS considering two publicly available datasets.',
	 'authors': u'Laxmi Lahoti, Chaitali Chandankhede, Debajyoti Mukhopadhyay,',
	 'category': u'Computer Science ',
	 'date': '2014-11-25',
	 'pdflink': u'http://arxiv.org/pdf/1411.6777',
	 'subjects': u'Cryptography and Security (cs.CR)',
	 'title': u'\nModified Apriori Approach for Evade Network Intrusion Detection System',
	 'urllink': u'http://arxiv.org/abs/1411.6777'}
2015-04-10 13:52:03+0000 [xxu46_10] INFO: Crawled 632 pages (at 1 pages/min), scraped 625 items (at 1 items/min)
2015-04-10 13:52:22+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1411.6775> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 13:52:22+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1411.6775>
	{'abstract': u'Hadoop is a distributed batch processing infrastructure which is currently being used for big data management. The foundation of Hadoop consists of Hadoop Distributed File System or HDFS. HDFS presents a client server architecture comprised of a NameNode and many DataNodes. The NameNode stores the metadata for the DataNodes and DataNode stores application data. The NameNode holds file system metadata in memory, and thus the limit to the number of files in a file system is governed by the amount of memory on the NameNode. Thus when the memory on NameNode is full there is no further chance of increasing the cluster capacity. In this paper we have used the concept of cache memory for handling the issue of NameNode scalability. The focus of this paper is to highlight our approach that tries to enhance the current architecture and ensure that NameNode does not reach its threshold value soon.',
	 'authors': u'Debajyoti Mukhopadhyay, Chetan Agrawal, Devesh Maru, Pooja Yedale, Pranav Gadekar,',
	 'category': u'Computer Science ',
	 'date': '2014-11-25',
	 'pdflink': u'http://arxiv.org/pdf/1411.6775',
	 'subjects': u'Distributed, Parallel, and Cluster Computing (cs.DC)',
	 'title': u'\nAddressing NameNode Scalability Issue in Hadoop Distributed File System  using Cache Approach',
	 'urllink': u'http://arxiv.org/abs/1411.6775'}
2015-04-10 13:53:03+0000 [xxu46_10] INFO: Crawled 633 pages (at 1 pages/min), scraped 626 items (at 1 items/min)
2015-04-10 13:53:41+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1411.6773> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 13:53:41+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1411.6773>
	{'abstract': u'Search engines play a vital role in day to day life on internet. People use search engines to find content on internet. Cloud computing is the computing concept in which data is stored and accessed with the help of a third party server called as cloud. Data is not stored locally on our machines and the softwares and information are provided to user if user demands for it. Search queries are the most important part in searching data on internet. A search query consists of one or more than one keywords. A search query is searched from the database for exact match, and the traditional searchable schemes do not tolerate minor typos and format inconsistencies, which happen quite frequently. This drawback makes the existing techniques unsuitable and they offer very low efficiency. In this paper, we will for the first time formulate the problem of effective fuzzy search by introducing tree search methodologies. We will explore the benefits of B trees in search mechanism and use them to have an efficient keyword search. We have taken into consideration the security analysis strictly so as to get a secure and privacy-preserving system.',
	 'authors': u'Simran Bijral, Debajyoti Mukhopadhyay,',
	 'category': u'Computer Science ',
	 'date': '2014-11-25',
	 'pdflink': u'http://arxiv.org/pdf/1411.6773',
	 'subjects': u'Information Retrieval (cs.IR)',
	 'title': u'\nEfficient Fuzzy Search Engine with B-Tree Search Mechanism',
	 'urllink': u'http://arxiv.org/abs/1411.6773'}
2015-04-10 13:54:03+0000 [xxu46_10] INFO: Crawled 634 pages (at 1 pages/min), scraped 627 items (at 1 items/min)
2015-04-10 13:54:58+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1411.6771> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 13:54:58+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1411.6771>
	{'abstract': u'In todays world, Cloud computing has attracted research communities as it provides services in reduced cost due to virtualizing all the necessary resources. Even modern business architecture depends upon Cloud computing .As it is a internet based utility, which provides various services over a network, it is prone to network based attacks. Hence security in clouds is the most important in case of cloud computing. Cloud Security concerns the customer to fully rely on storing data on clouds. That is why Cloud security has attracted attention of the research community. This paper will discuss securing the data in clouds by implementing key agreement, encryption and signature verification/generation with hyperelliptic curve cryptography.',
	 'authors': u'Debajyoti Mukhopadhyay, Ashay Shirwadkar, Pratik Gaikar, Tanmay Agrawal,',
	 'category': u'Computer Science ',
	 'date': '2014-11-25',
	 'pdflink': u'http://arxiv.org/pdf/1411.6771',
	 'subjects': u'Cryptography and Security (cs.CR)',
	 'title': u'\nSecuring the Data in Clouds with Hyperelliptic Curve Cryptography',
	 'urllink': u'http://arxiv.org/abs/1411.6771'}
2015-04-10 13:55:03+0000 [xxu46_10] INFO: Crawled 635 pages (at 1 pages/min), scraped 628 items (at 1 items/min)
2015-04-10 13:56:02+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1411.6768> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 13:56:02+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1411.6768>
	{'abstract': u'This paper deals with the problem of neural code solving. On the basis of the formulated hypotheses the information model of a neuron-detector is suggested, the detector being one of the basic elements of an artificial neural network (ANN). The paper subjects the connectionist paradigm of ANN building to criticism and suggests a new presentation paradigm for ANN building and neuroelements (NE) learning. The adequacy of the suggested model is proved by the fact that is does not contradict the modern propositions of neuropsychology and neurophysiology.',
	 'authors': u'Yuri Parzhin,',
	 'category': u'Computer Science ',
	 'date': '2014-11-25',
	 'pdflink': u'http://arxiv.org/pdf/1411.6768',
	 'subjects': u'Neural and Evolutionary Computing (cs.NE)',
	 'title': u'\nHypotheses of neural code and the information model of the  neuron-detector',
	 'urllink': u'http://arxiv.org/abs/1411.6768'}
2015-04-10 13:56:03+0000 [xxu46_10] INFO: Crawled 636 pages (at 1 pages/min), scraped 629 items (at 1 items/min)
2015-04-10 13:57:02+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1411.6767> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 13:57:02+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1411.6767>
	{'abstract': u'This paper describes the design and development of a location-based mobile shopping application for bakery product shops. Whole application is deployed on cloud. The three-tier architecture consists of, front-end, middle-ware and back-end. The front-end level is a location-based mobile shopping application for android mobile devices, for purchasing bakery products of nearby places. Front-end level also displays association among the purchased products. The middle-ware level provides a web service to generate JSON (JavaScript Object Notation) output from the relational database. It exchanges information and data between mobile application and servers in cloud. The back-end level provides the Apache Tomcat Web server and MySQL database. The application also uses the Google Cloud Messaging for generating and sending notification of orders to shopkeeper.',
	 'authors': u'Vijayata Waghmare, Debajyoti Mukhopadhyay,',
	 'category': u'Computer Science ',
	 'date': '2014-11-25',
	 'pdflink': u'http://arxiv.org/pdf/1411.6767',
	 'subjects': u'Computers and Society (cs.CY)',
	 'title': u'\nMobile Agent based Market Basket Analysis on Cloud',
	 'urllink': u'http://arxiv.org/abs/1411.6767'}
2015-04-10 13:57:03+0000 [xxu46_10] INFO: Crawled 637 pages (at 1 pages/min), scraped 630 items (at 1 items/min)
2015-04-10 13:58:03+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1411.6763> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 13:58:03+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1411.6763>
	{'abstract': u'We propose techniques for processing SPARQL queries over linked data. We follow a graph-based approach where answering a query Q is equivalent to finding its matches over a distributed RDF data graph G. We adopt a "partial evaluation and assembly" framework. Partial evaluation results of query Q over each repository-called local partial match-are found. In the assembly stage, we propose a centralized and a distributed assembly strategy. We analyze our algorithms both theoretically and the experimentally. Extensive experiments over both real and benchmark RDF repositories with billion triples demonstrate the high performance and scalability of our methods compared with that of the existing solutions.',
	 'authors': u'Peng Peng, Lei Zou, M. Tamer \xd6zsu, Lei Chen, Dongyan Zhao,',
	 'category': u'Computer Science ',
	 'date': '2014-11-25',
	 'pdflink': u'http://arxiv.org/pdf/1411.6763',
	 'subjects': u'Databases (cs.DB)',
	 'title': u'\nProcessing SPARQL Queries Over Linked Data--A Distributed Graph-based  Approach',
	 'urllink': u'http://arxiv.org/abs/1411.6763'}
2015-04-10 13:58:03+0000 [xxu46_10] INFO: Crawled 638 pages (at 1 pages/min), scraped 631 items (at 1 items/min)
2015-04-10 13:59:03+0000 [xxu46_10] INFO: Crawled 638 pages (at 0 pages/min), scraped 631 items (at 0 items/min)
2015-04-10 13:59:12+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1411.6762> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 13:59:12+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1411.6762>
	{'abstract': u'Service Oriented Architecture is a loosely coupled architecture designed to tackle the problem of Business Infrastructure alignment to meet the needs of an organization. A SOA based platform enables the enterprises to develop applications in the form of independent services. To provide scalable service interactions, there is a need to maintain services performance and have a good sizing guideline of the underlying software platform. Sizing aids in finding the optimum resources required to configure and implement a system that would satisfy the requirements of Business Process Integration being planned. A web based Sizing Tool prototype is developed using Java Application Programming Interfaces to automate the process of sizing the applications deployed on SOA platform that not only scales the performance of the system but also predicts its business growth in the future.',
	 'authors': u'Debajyoti Mukhopadhyay, Juhi Jariwala, Payal Innani, Sheetal Bablani, Sushama Kothawale,',
	 'category': u'Computer Science ',
	 'date': '2014-11-25',
	 'pdflink': u'http://arxiv.org/pdf/1411.6762',
	 'subjects': u'Software Engineering (cs.SE)',
	 'title': u'\nA Tool to Automate the Sizing of Application Process for SOA based  Platform',
	 'urllink': u'http://arxiv.org/abs/1411.6762'}
2015-04-10 14:00:03+0000 [xxu46_10] INFO: Crawled 639 pages (at 1 pages/min), scraped 632 items (at 1 items/min)
2015-04-10 14:00:20+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1411.6757> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 14:00:20+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1411.6757>
	{'abstract': u'Recurrent networks that have transfer functions that fulfill the Lipschitz continuity with L=1, may be echo state networks if certain limitations on the recurrent connectivity are applied. Initially it has been shown that it is sufficient if the largest singular value of the recurrent connectivity is smaller than 1. Here it is investigated under which conditions it still can be shown that the network is an an echo state network even if S=1.',
	 'authors': u'Norbert Michael Mayer,',
	 'category': u'Computer Science ',
	 'date': '2014-11-25',
	 'pdflink': u'http://arxiv.org/pdf/1411.6757',
	 'subjects': u'Neural and Evolutionary Computing (cs.NE)',
	 'title': u'\nProof about Weakly Universal State Contracting Networks',
	 'urllink': u'http://arxiv.org/abs/1411.6757'}
2015-04-10 14:01:03+0000 [xxu46_10] INFO: Crawled 640 pages (at 1 pages/min), scraped 633 items (at 1 items/min)
2015-04-10 14:01:38+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1411.6756> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 14:01:38+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1411.6756>
	{'abstract': u"The notion of a -representative set for a family of subsets has recently proven to be very useful in the design of parameterized and exact algorithms. We generalize this notion to families of . We also give an efficient way to find a representative set for a family of multisets. As an application we give a deterministic algorithm for minimal weight r-SIMPLE k-PATH running in time for . This extends a result of Abasi et. al [ABGH14] that gave a emph algorithm of similar running time for the non-weighted case. We derive other algorithms for problems that can be viewed as augmenting a parameterized problem with a `relaxation' parameter. A corollary of our construction is an improved explicit construction of [FLS14] for a certain range of parameters.",
	 'authors': u'Ariel Gabizon, Daniel Lokshtanov, Michal Pilipczuk,',
	 'category': u'Computer Science ',
	 'date': '2014-11-25',
	 'pdflink': u'http://arxiv.org/pdf/1411.6756',
	 'subjects': u'Data Structures and Algorithms (cs.DS)',
	 'title': u'\nRepresentative sets for multisets',
	 'urllink': u'http://arxiv.org/abs/1411.6756'}
2015-04-10 14:02:03+0000 [xxu46_10] INFO: Crawled 641 pages (at 1 pages/min), scraped 634 items (at 1 items/min)
2015-04-10 14:02:34+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1411.6754> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 14:02:34+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1411.6754>
	{'abstract': u'Nowadays, online clothes-selling business has become popular and extremely attractive because of its convenience and cheap-and-fine price. Good examples of these successful Web sites include Yintai.com, Vancl.com and Shop.vipshop.com which provide thousands of clothes for online shoppers. The challenge for online shoppers lies on how to find a good product from lots of options. In this article, we propose a collaborative clothes recommender for easy shopping. One of the unique features of this system is the ability to recommend clothes in terms of both user ratings and clothing attributes. Experiments in our simulation environment show that the proposed recommender can better satisfy the needs of users.',
	 'authors': u'Xiaosong Hu, Wen Zhu, Qing Li,',
	 'category': u'Computer Science ',
	 'date': '2014-11-25',
	 'pdflink': u'http://arxiv.org/pdf/1411.6754',
	 'subjects': u'Artificial Intelligence (cs.AI)',
	 'title': u'\nHCRS: A hybrid clothes recommender system based on user ratings and  product features',
	 'urllink': u'http://arxiv.org/abs/1411.6754'}
2015-04-10 14:03:03+0000 [xxu46_10] INFO: Crawled 642 pages (at 1 pages/min), scraped 635 items (at 1 items/min)
2015-04-10 14:03:10+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1411.6753> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 14:03:10+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1411.6753>
	{'abstract': u'The Dynamic Scalability of resources, a problem in Infrastructure as a Service (IaaS) has been the hotspot for research and industry communities. The heterogeneous and dynamic nature of the Cloud workloads depends on the Quality of Service (QoS) allocation of appropriate workloads to appropriate resources. A workload is an abstraction of work that instance or set of instances that are going to perform. Running a web service or being a Hadoop data node is valid workloads. The efficient management of dynamic nature resources can be done with the help of workloads. Until workload is considered a fundamental capability, the Cloud resources cannot be utilized in an efficient manner. In this paper, different workloads have been identified and categorized along with their characteristics and constraints. The metrics based on Quality of Service (QoS) requirements have been identified for each workload and have been analyzed for creating better application design.',
	 'authors': u'Sukhpal Singh, Inderveer Chana,',
	 'category': u'Computer Science ',
	 'date': '2014-11-25',
	 'pdflink': u'http://arxiv.org/pdf/1411.6753',
	 'subjects': u'Distributed, Parallel, and Cluster Computing (cs.DC)',
	 'title': u'\nMetrics based Workload Analysis Technique for IaaS Cloud',
	 'urllink': u'http://arxiv.org/abs/1411.6753'}
2015-04-10 14:04:03+0000 [xxu46_10] INFO: Crawled 643 pages (at 1 pages/min), scraped 636 items (at 1 items/min)
2015-04-10 14:04:26+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1411.6749> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 14:04:26+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1411.6749>
	{'abstract': u'For long, node cooperation has been exploited as a data relaying mechanism. However, the wireless channel allows for much richer interaction between nodes. One such scenario is in a multi-channel environment, where transmitter-receiver pairs may make incorrect decisions (e.g., in selecting channels) but idle neighbors could help by sharing information to prevent undesirable consequences (e.g., data collisions). This represents a Distributed Information SHaring (DISH) mechanism for cooperation and suggests new ways of designing cooperative protocols. However, what is lacking is a theoretical understanding of this new notion of cooperation. In this paper, we view cooperation as a network resource and evaluate the availability of cooperation via a metric, , the probability of obtaining cooperation. First, we analytically evaluate in the context of multi-channel multi-hop wireless networks. Second, we verify our analysis via simulations and the results show that our analysis accurately characterizes the behavior of as a function of underlying network parameters. This step also yields important insights into DISH with respect to network dynamics. Third, we investigate the correlation between and network performance in terms of collision rate, packet delay, and throughput. The results indicate a near-linear relationship, which may significantly simplify performance analysis for cooperative networks and suggests that be used as an appropriate performance indicator itself. Throughout this work, we utilize, as appropriate, three different DISH contexts --- model-based DISH, ideal DISH, and real DISH --- to explore .',
	 'authors': u'Tie Luo, Mehul Motani, Vikram Srinivasan,',
	 'category': u'Computer Science ',
	 'date': '2014-11-25',
	 'pdflink': u'http://arxiv.org/pdf/1411.6749',
	 'subjects': u'Networking and Internet Architecture (cs.NI)',
	 'title': u'\nAnalyzing DISH for Multi-Channel MAC Protocols in Wireless Networks',
	 'urllink': u'http://arxiv.org/abs/1411.6749'}
2015-04-10 14:05:03+0000 [xxu46_10] INFO: Crawled 644 pages (at 1 pages/min), scraped 637 items (at 1 items/min)
2015-04-10 14:05:26+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1411.6741> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 14:05:26+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1411.6741>
	{'abstract': u'Conventional NMF methods for source separation factorize the matrix of spectral magnitudes. Spectral Phase is not included in the decomposition process of these methods. However, phase of the speech mixture is generally used in reconstructing the target speech signal. This results in undesired traces of interfering sources in the target signal. In this paper the spectral phase is incorporated in the decomposition process itself. Additionally, the complex matrix factorization problem is reduced to an NMF problem using simple transformations. This results in effective separation of speech mixtures since both magnitude and phase are utilized jointly in the separation process. Improvement in source separation results are demonstrated using objective quality evaluations on the GRID corpus.',
	 'authors': u'Chaitanya Ahuja, Karan Nathwani, Rajesh M. Hegde,',
	 'category': u'Computer Science ',
	 'date': '2014-11-25',
	 'pdflink': u'http://arxiv.org/pdf/1411.6741',
	 'subjects': u'Sound (cs.SD)',
	 'title': u'\nA Complex Matrix Factorization approach to Joint Modeling of Magnitude  and Phase for Source Separation',
	 'urllink': u'http://arxiv.org/abs/1411.6741'}
2015-04-10 14:06:03+0000 [xxu46_10] INFO: Crawled 645 pages (at 1 pages/min), scraped 638 items (at 1 items/min)
2015-04-10 14:06:23+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1411.6739> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 14:06:23+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1411.6739>
	{'abstract': u'Massive MIMO systems have made significant progress in increasing spectral and energy efficiency over traditional MIMO systems by exploiting large antenna arrays. In this paper we consider the joint maximum likelihood (ML) channel estimation and data detection problem for massive SIMO (single input multiple output) wireless systems. Despite the large number of unknown channel coefficients for massive SIMO systems, we improve an algorithm to achieve the exact ML non-coherent data detection with a low expected complexity. We show that the expected computational complexity of this algorithm is linear in the number of receive antennas and polynomial in channel coherence time. Simulation results show the performance gain of the optimal non-coherent data detection with a low computational complexity.',
	 'authors': u'Haider Ali Jasim Alshamary, Tareq Al-Naffouri, Alam Zaib, Weiyu Xu,',
	 'category': u'Computer Science ',
	 'date': '2014-11-25',
	 'pdflink': u'http://arxiv.org/pdf/1411.6739',
	 'subjects': u'Information Theory (cs.IT)',
	 'title': u'\nOptimal non-coherent data detection for massive SIMO wireless systems: A  polynomial complexity solution',
	 'urllink': u'http://arxiv.org/abs/1411.6739'}
2015-04-10 14:07:03+0000 [xxu46_10] INFO: Crawled 646 pages (at 1 pages/min), scraped 639 items (at 1 items/min)
2015-04-10 14:07:25+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1411.6725> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 14:07:25+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1411.6725>
	{'abstract': u"The growing amount of high dimensional data in different machine learning applications requires more efficient and scalable optimization algorithms. In this work, we consider combining two techniques, parallelism and Nesterov's acceleration, to design faster algorithms for L1-regularized loss. We first simplify BOOM, a variant of gradient descent, and study it in a unified framework, which allows us to not only propose a refined measurement of sparsity to improve BOOM, but also show that BOOM is provably slower than FISTA. Moving on to parallel coordinate descent methods, we then propose an efficient accelerated version of Shotgun, improving the convergence rate from to . Our algorithm enjoys a concise form and analysis compared to previous work, and also allows one to study several connected work in a unified way.",
	 'authors': u'Haipeng Luo, Patrick Haffner, Jean-Francois Paiement,',
	 'category': u'Computer Science ',
	 'date': '2014-11-25',
	 'pdflink': u'http://arxiv.org/pdf/1411.6725',
	 'subjects': u'Learning (cs.LG)',
	 'title': u'\nAccelerated Parallel Optimization Methods for Large Scale Machine  Learning',
	 'urllink': u'http://arxiv.org/abs/1411.6725'}
2015-04-10 14:08:03+0000 [xxu46_10] INFO: Crawled 647 pages (at 1 pages/min), scraped 640 items (at 1 items/min)
2015-04-10 14:08:35+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1411.6721> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 14:08:35+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1411.6721>
	{'abstract': u"More users and companies make use of cloud services every day. They all expect a perfect performance and any issue to remain transparent to them. This last statement is very challenging to perform. A user's activities in our cloud can affect the overall performance of our servers, having an impact on other resources. We can consider these kind of activities as fraudulent. They can be either illegal activities, such as launching a DDoS attack or just activities which are undesired by the cloud provider, such as Bitcoin mining, which uses substantial power, reduces the life of the hardware and can possibly slow down other user's activities. This article discusses a method to detect such activities by using non-intrusive, privacy-friendly data: billing data. We use OpenStack as an example with data provided by Telemetry, the component in charge of measuring resource usage for billing purposes. Results will be shown proving the efficiency of this method and ways to improve it will be provided as well as its advantages and disadvantages.",
	 'authors': u'Marc Solanas, Julio Hernandez-Castro, Debojyoti Dutta,',
	 'category': u'Computer Science ',
	 'date': '2014-11-25',
	 'pdflink': u'http://arxiv.org/pdf/1411.6721',
	 'subjects': u'Cryptography and Security (cs.CR)',
	 'title': u'\nDetecting fraudulent activity in a cloud using privacy-friendly data  aggregates',
	 'urllink': u'http://arxiv.org/abs/1411.6721'}
2015-04-10 14:09:03+0000 [xxu46_10] INFO: Crawled 648 pages (at 1 pages/min), scraped 641 items (at 1 items/min)
2015-04-10 14:09:40+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1411.6718> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 14:09:40+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1411.6718>
	{'abstract': u'We introduce LABR, the largest sentiment analysis dataset to-date for the Arabic language. It consists of over 63,000 book reviews, each rated on a scale of 1 to 5 stars. We investigate the properties of the the dataset, and present its statistics. We explore using the dataset for two tasks: sentiment polarity classification and ratings classification. We provide standard splits of the dataset into training, validation and testing, for both polarity and ratings classification, in both balanced and unbalanced settings. We extend the work done in Aly and Atiya [2013] by performing a comprehensive analysis on the dataset. In particular, we perform an extended survey of the different classifiers typically used for the sentiment polarity classification problem. Also we construct a sentiment lexicon from the dataset that contains both single and compound sentiment words and we explore its effectiveness.',
	 'authors': u'Mahmoud Nabil, Mohamed Aly, Amir Atiya,',
	 'category': u'Computer Science ',
	 'date': '2014-11-25',
	 'pdflink': u'http://arxiv.org/pdf/1411.6718',
	 'subjects': u'Computation and Language (cs.CL)',
	 'title': u'\nLABR: A Large Scale Arabic Book Reviews Dataset',
	 'urllink': u'http://arxiv.org/abs/1411.6718'}
2015-04-10 14:10:03+0000 [xxu46_10] INFO: Crawled 649 pages (at 1 pages/min), scraped 642 items (at 1 items/min)
2015-04-10 14:10:37+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1411.6714> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 14:10:37+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1411.6714>
	{'abstract': u'Although computer scientists are generally familiar with the achievements of computer vision technology in art history, these accomplishments are little known and often misunderstood by scholars in the humanities. To clarify the parameters of this seeming disjuncture, we have addressed the concerns that one example of the digitization of the humanities poses on social, philosophical, and practical levels. In support of our assessment of the perceptions held by computer scientists and art historians about the use of computer vision technology to examine art, we based our interpretations on two surveys that were distributed in August 2014. In this paper, the development of these surveys and their results are discussed in the context of the major philosophical conclusions of our research in this area to date.',
	 'authors': u'Emily L. Spratt, Ahmed Elgammal,',
	 'category': u'Computer Science ',
	 'date': '2014-11-25',
	 'pdflink': u'http://arxiv.org/pdf/1411.6714',
	 'subjects': u'Computers and Society (cs.CY)',
	 'title': u'\nThe Digital Humanities Unveiled: Perceptions Held by Art Historians and  Computer Scientists about Computer Vision Technology',
	 'urllink': u'http://arxiv.org/abs/1411.6714'}
2015-04-10 14:11:03+0000 [xxu46_10] INFO: Crawled 650 pages (at 1 pages/min), scraped 643 items (at 1 items/min)
2015-04-10 14:11:21+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1411.6712> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 14:11:21+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1411.6712>
	{'abstract': u'The square root rank of a nonnegative matrix is the minimum rank of a matrix such that , where denotes entrywise product. We show that the square root rank of the slack matrix of the correlation polytope is exponential. Our main technique is a way to lower bound the rank of certain matrices under arbitrary sign changes of the entries using properties of the roots of polynomials in number fields. The square root rank is an upper bound on the positive semidefinite rank of a matrix, and corresponds the special case where all matrices in the factorization are rank-one.',
	 'authors': u'Troy Lee, Zhaohui Wei,',
	 'category': u'Computer Science ',
	 'date': '2014-11-25',
	 'pdflink': u'http://arxiv.org/pdf/1411.6712',
	 'subjects': u'Computational Complexity (cs.CC)',
	 'title': u'\nThe square root rank of the correlation polytope is exponential',
	 'urllink': u'http://arxiv.org/abs/1411.6712'}
2015-04-10 14:12:03+0000 [xxu46_10] INFO: Crawled 651 pages (at 1 pages/min), scraped 644 items (at 1 items/min)
2015-04-10 14:12:30+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1411.6704> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 14:12:30+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1411.6704>
	{'abstract': u"Correctness of SQL queries is usually tested by executing the queries on one or more datasets. Erroneous queries are often the results of small changes, or mutations of the correct query. A mutation Q' of a query Q is killed by a dataset D, if Q(D) Q'(D). Earlier work on the XData system showed how to generate datasets that kill all mutations in a class of mutations that included join type and comparison operation mutations. In this paper we extend the XData data generation techniques to handle a wider variety of SQL queries and a much larger class of mutations. We have also built a system for grading SQL queries using the datasets generated by XData. We present a study of the effectiveness of the datasets generated by the extended XData approach, using a variety of queries including queries submitted by students as part of a database course. We show that the XData datasets outperform predefined datasets as well as manual grading done earlier by teaching assistants, while also avoiding the drudgery of manual correction. Thus, we believe that our techniques will be of great value to database course instructors and TAs, particularly to those of MOOCs. It will also be valuable to database application developers and testers for testing SQL queries.",
	 'authors': u'Bikash Chandra, Amol Bhangdia, Bhupesh Chawda, Biplab Kar, K. V. Maheshwara Reddy, Shetal Shah, S. Sudarshan,',
	 'category': u'Computer Science ',
	 'date': '2014-11-25',
	 'pdflink': u'http://arxiv.org/pdf/1411.6704',
	 'subjects': u'Databases (cs.DB)',
	 'title': u'\nData Generation for Testing and Grading SQL Queries',
	 'urllink': u'http://arxiv.org/abs/1411.6704'}
2015-04-10 14:13:03+0000 [xxu46_10] INFO: Crawled 652 pages (at 1 pages/min), scraped 645 items (at 1 items/min)
2015-04-10 14:13:32+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1411.6699> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 14:13:32+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1411.6699>
	{'abstract': u'Discourse relations bind smaller linguistic units into coherent texts. However, automatically identifying discourse relations is difficult, because it requires understanding the semantics of the linked arguments. A more subtle challenge is that it is not enough to represent the meaning of each argument of a discourse relation, because the relation may depend on links between lower-level components, such as entity mentions. Our solution computes distributional meaning representations by composition up the syntactic parse tree. A key difference from previous work on compositional distributional semantics is that we also compute representations for entity mentions, using a novel downward compositional pass. Discourse relations are predicted from the distributional representations of the arguments, and also of their coreferent entity mentions. The resulting system obtains substantial improvements over the previous state-of-the-art in predicting implicit discourse relations in the Penn Discourse Treebank.',
	 'authors': u'Yangfeng Ji, Jacob Eisenstein,',
	 'category': u'Computer Science ',
	 'date': '2014-11-25',
	 'pdflink': u'http://arxiv.org/pdf/1411.6699',
	 'subjects': u'Computation and Language (cs.CL)',
	 'title': u'\nOne Vector is Not Enough: Entity-Augmented Distributional Semantics for  Discourse Relations',
	 'urllink': u'http://arxiv.org/abs/1411.6699'}
2015-04-10 14:14:03+0000 [xxu46_10] INFO: Crawled 653 pages (at 1 pages/min), scraped 646 items (at 1 items/min)
2015-04-10 14:14:44+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1411.6685> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 14:14:44+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1411.6685>
	{'abstract': u"Recent experimental studies confirm the prevalence of the widely known performance anomaly problem and the severe network utility degradation this inflicts on current Wi-Fi networks. Although a large body of work addressed this issue, we attribute refusal of prior solutions to their poor implementation feasibility with off-the-shelf hardware and their imprecise modelling of the 802.11 protocol. Their applicability is further challenged today by very high throughput enhancements (802.11n/ac) whereby link speeds can vary by two orders of magnitude. Unlike earlier approaches, in this paper we introduce the first rigorous analytical model of 802.11 stations' throughput and airtime in multi-rate settings, without sacrificing accuracy for tractability. We use the proportional-fair allocation criterion to formulate network utility maximisation as a convex optimisation problem for which we give a closed-form solution. We present a fully functional light-weight implementation of our scheme on commodity access points and evaluate this extensively via experiments in a real deployment over a broad range of network conditions. Results demonstrate our proposal achieves up to 100 % utility gains, can double video streaming goodput and reduces TCP download times by 8x.",
	 'authors': u'Paul Patras, Andres Garcia-Saavedra, David Malone, Douglas J. Leith,',
	 'category': u'Computer Science ',
	 'date': '2014-11-24',
	 'pdflink': u'http://arxiv.org/pdf/1411.6685',
	 'subjects': u'Networking and Internet Architecture (cs.NI)',
	 'title': u'\nRigorous and Practical Proportional-fair Allocation for Multi-rate Wi-Fi',
	 'urllink': u'http://arxiv.org/abs/1411.6685'}
2015-04-10 14:15:03+0000 [xxu46_10] INFO: Crawled 654 pages (at 1 pages/min), scraped 647 items (at 1 items/min)
2015-04-10 14:15:42+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1411.6675> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 14:15:42+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1411.6675>
	{'abstract': u"Summarising the new orphan works law of Italy, we show how it makes the public interest prevail and allows libraries and other beneficiaries to improve their services. We then argue that such services are part of their mission towards the public domain and are a first step for its complete accomplishment, by the work of each and a reform of european copyright. Failing that, European culture will disappear. -- Sintetizzando le nuove norme sulle opere orfane, mostriamo come esse affermino la prevalenza dell'interesse pubblico e consentano a biblioteche e altri enti beneficiari di migliorare i propri servizi. Sosteniamo quindi che questi si inquadrano nella loro missione nei confronti del pubblico dominio e sono un primo passo per la sua completa realizzazione, mediante il lavoro di ciascuno e la riforma del diritto d'autore europeo. In caso contrario, la cultura europea sparir `a.",
	 'authors': u'Federico Leva,',
	 'category': u'Computer Science ',
	 'date': '2014-11-24',
	 'pdflink': u'http://arxiv.org/pdf/1411.6675',
	 'subjects': u'Digital Libraries (cs.DL)',
	 'title': u"\nFrom orphan works, a new role of libraries for the public domain and  public interest (Dalle opere orfane, un nuovo ruolo delle biblioteche per il  pubblico dominio e l'utilit\xe0 sociale)",
	 'urllink': u'http://arxiv.org/abs/1411.6675'}
2015-04-10 14:16:03+0000 [xxu46_10] INFO: Crawled 655 pages (at 1 pages/min), scraped 648 items (at 1 items/min)
2015-04-10 14:16:52+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1411.6673> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 14:16:52+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1411.6673>
	{'abstract': u'We study the problem of counting the number of copies of a given graph, say , in the input graph, say . In general, it is believed that polynomial time algorithms that solve this problem exactly are unlikely to exist. So, a lot of work has gone into designing efficient , especially, when is a perfect matching. In this work, we present efficient approximation schemes to count -Cliques, -Independent sets and -Clique covers in random graphs. We present (fpras) to count -Cliques and -Independent sets in a random graph on vertices when is at most , and -Clique covers when is a constant. [Grimmett and McDiarmid, 1975] present a simple greedy algorithm that a clique (independent set) of size in with high probability. No algorithm is known to detect a clique or an independent set of larger size with non-vanishing probability. Furthermore, [Coja-Oghlan and Efthymiou, 2011] present some evidence that one cannot hope to easily improve a similar, almost 40 years old bound for sparse random graphs. Therefore, our results are unlikely to be easily improved. We use a novel approach to obtain a recurrence corresponding to the variance of each estimator. Then we upper bound the variance using the corresponding recurrence. This leads us to obtain a polynomial upper bound on the critical ratio. As an aside, we also obtain an alternate derivation of the closed form expression for the -th moment of a binomial random variable using our techniques. The previous derivation [Knoblauch (2008)] was based on the moment generating function of a binomial random variable.',
	 'authors': u'Kashyap Dixit, Martin F\xfcrer,',
	 'category': u'Computer Science ',
	 'date': '2014-11-24',
	 'pdflink': u'http://arxiv.org/pdf/1411.6673',
	 'subjects': u'Data Structures and Algorithms (cs.DS)',
	 'title': u'\nCounting cliques and clique covers in random graphs',
	 'urllink': u'http://arxiv.org/abs/1411.6673'}
2015-04-10 14:17:03+0000 [xxu46_10] INFO: Crawled 656 pages (at 1 pages/min), scraped 649 items (at 1 items/min)
2015-04-10 14:17:59+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1411.6672> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 14:17:59+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1411.6672>
	{'abstract': u'We show that in the hierarchical tile assembly model, if there is a producible assembly that overlaps a nontrivial translation of itself consistently (i.e., the pattern of tile types in the overlap region is identical in both translations), then arbitrarily large assemblies are producible. The significance of this result is that tile systems intended to controllably produce finite structures must avoid pattern repetition in their producible assemblies that would lead to such overlap. This answers an open question of Chen and Doty (SODA 2012), who showed that so-called "partial-order" systems producing a unique finite assembly *and" avoiding such overlaps must require time linear in the assembly diameter. An application of our main result is that any system producing a unique finite assembly is automatically guaranteed to avoid such overlaps, simplifying the hypothesis of Chen and Doty\'s main theorem.',
	 'authors': u'Ho-Lin Chen, David Doty, J\xe1n Ma\u0148uch, Arash Rafiey, Ladislav Stacho,',
	 'category': u'Computer Science ',
	 'date': '2014-11-24',
	 'pdflink': u'http://arxiv.org/pdf/1411.6672',
	 'subjects': u'Computational Geometry (cs.CG)',
	 'title': u'\nPattern overlap implies runaway growth in hierarchical tile systems',
	 'urllink': u'http://arxiv.org/abs/1411.6672'}
2015-04-10 14:18:03+0000 [xxu46_10] INFO: Crawled 657 pages (at 1 pages/min), scraped 650 items (at 1 items/min)
2015-04-10 14:19:03+0000 [xxu46_10] INFO: Crawled 657 pages (at 0 pages/min), scraped 650 items (at 0 items/min)
2015-04-10 14:19:08+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1411.6667> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 14:19:08+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1411.6667>
	{'abstract': u'The noise model of deletions poses significant challenges in coding theory, with basic questions like the capacity of the binary deletion channel still being open. In this paper, we study the harder model of worst-case deletions, with a focus on constructing efficiently decodable codes for the two extreme regimes of high-noise and high-rate. Specifically, we construct polynomial-time decodable codes with the following trade-offs (for any eps &gt; 0): (1) Codes that can correct a fraction 1-eps of deletions with rate poly(eps) over an alphabet of size poly(1/eps); (2) Binary codes of rate 1-O~(sqrt(eps)) that can correct a fraction eps of deletions; and (3) Binary codes that can be list decoded from a fraction (1/2-eps) of deletions with rate poly(eps) Our work is the first to achieve the qualitative goals of correcting a deletion fraction approaching 1 over bounded alphabets, and correcting a constant fraction of bit deletions with rate aproaching 1. The above results bring our understanding of deletion code constructions in these regimes to a similar level as worst-case errors.',
	 'authors': u'Venkatesan Guruswami, Carol Wang,',
	 'category': u'Computer Science ',
	 'date': '2014-11-24',
	 'pdflink': u'http://arxiv.org/pdf/1411.6667',
	 'subjects': u'Information Theory (cs.IT)',
	 'title': u'\nDeletion codes in the high-noise and high-rate regimes',
	 'urllink': u'http://arxiv.org/abs/1411.6667'}
2015-04-10 14:19:46+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1411.6660> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 14:19:46+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1411.6660>
	{'abstract': u'Most state-of-the-art action feature extractors involve differential operators, which act as highpass filters and tend to attenuate low frequency action information. This attenuation introduces bias to the resulting features and generates ill-conditioned feature matrices. The Gaussian Pyramid has been used as a feature enhancing technique that encodes scale-invariant characteristics into the feature space in an attempt to deal with this attenuation. However, at the core of the Gaussian Pyramid is a convolutional smoothing operation, which makes it incapable of generating new features at coarse scales. In order to address this problem, we propose a novel feature enhancing technique called Multi-skIp Feature Stacking (MIFS), which stacks features extracted using a family of differential filters parameterized with multiple time skips and encodes shift-invariance into the frequency space. MIFS compensates for information lost from using differential operators by recapturing information at coarse scales. This recaptured information allows us to match actions at different speeds and ranges of motion. We prove that MIFS enhances the learnability of differential-based features exponentially. The resulting feature matrices from MIFS have a much smaller conditional numbers and variances than those from conventional methods. Experimental results show significantly improved performance on challenging action recognition and event detection tasks. Specifically, our method exceeds the state-of-the-arts on HMDB51, Hollywood2, UCF101, UCF50 and Olympics Sports datasets.',
	 'authors': u'Zhenzhong Lan, Ming Lin, Xuanchong Li, Alexander G. Hauptmann, Bhiksha Raj,',
	 'category': u'Computer Science ',
	 'date': '2014-11-24',
	 'pdflink': u'http://arxiv.org/pdf/1411.6660',
	 'subjects': u'Computer Vision and Pattern Recognition (cs.CV)',
	 'title': u'\nBeyond Gaussian Pyramid: Multi-skip Feature Stacking for Action  Recognition',
	 'urllink': u'http://arxiv.org/abs/1411.6660'}
2015-04-10 14:20:03+0000 [xxu46_10] INFO: Crawled 659 pages (at 2 pages/min), scraped 652 items (at 2 items/min)
2015-04-10 14:20:57+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1411.6651> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 14:20:57+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1411.6651>
	{'abstract': u'In this report paper we first present a report of the Advanced Machine Learning Course Project on the provided data set and then present a novel heuristic algorithm for exact Bayesian network (BN) structure discovery that uses decomposable scoring functions. Our algorithm follows a different approach to solve the problem of BN structure discovery than the previously used methods such as Dynamic Programming (DP) and Branch and Bound to reduce the search space and find the global optima space for the problem. The algorithm we propose has some degree of flexibility that can make it more or less greedy. The more the algorithm is set to be greedy, the more the speed of the algorithm will be, and the less optimal the final structure. Our algorithm runs in a much less time than the previously known methods and guarantees to have an optimality of close to 99%. Therefore, it sacrifices less than one percent of score of an optimal structure in order to gain a much lower running time and make the algorithm feasible for large data sets (we may note that we never used any toolbox except for result validation)',
	 'authors': u'Amir Arsalan Soltani,',
	 'category': u'Computer Science ',
	 'date': '2014-11-24',
	 'pdflink': u'http://arxiv.org/pdf/1411.6651',
	 'subjects': u'Artificial Intelligence (cs.AI)',
	 'title': u'\nA Greedy, Flexible Algorithm to Learn an Optimal Bayesian Network  Structure',
	 'urllink': u'http://arxiv.org/abs/1411.6651'}
2015-04-10 14:21:03+0000 [xxu46_10] INFO: Crawled 660 pages (at 1 pages/min), scraped 653 items (at 1 items/min)
2015-04-10 14:22:03+0000 [xxu46_10] INFO: Crawled 660 pages (at 0 pages/min), scraped 653 items (at 0 items/min)
2015-04-10 14:22:04+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1411.6646> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 14:22:04+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1411.6646>
	{'abstract': u'We introduce session automata, an automata model to process data words, i.e., words over an infinite alphabet. Session automata support the notion of fresh data values, which are well suited for modeling protocols in which sessions using fresh values are of major interest, like in security protocols or ad-hoc networks. Session automata have an expressiveness partly extending, partly reducing that of classical register automata. We show that, unlike register automata and their various extensions, session automata are robust: They (i) are closed under intersection, union, and (resource-sensitive) complementation, (ii) admit a symbolic regular representation, (iii) have a decidable inclusion problem (unlike register automata), and (iv) enjoy logical characterizations. Using these results, we establish a learning algorithm to infer session automata through membership and equivalence queries.',
	 'authors': u'Benedikt Bollig, Peter Habermehl, Martin Leucker, Benjamin Monmege,',
	 'category': u'Computer Science ',
	 'date': '2014-11-24',
	 'pdflink': u'http://arxiv.org/pdf/1411.6646',
	 'subjects': u'Logic in Computer Science (cs.LO)',
	 'title': u'\nA Robust Class of Data Languages and an Application to Learning',
	 'urllink': u'http://arxiv.org/abs/1411.6646'}
2015-04-10 14:22:52+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1411.6593> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 14:22:52+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1411.6593>
	{'abstract': u'Recent advances in metareasoning for search has shown its usefulness in improving numerous search algorithms. This paper applies rational metareasoning to IDA* when several admissible heuristics are available. The obvious basic approach of taking the maximum of the heuristics is improved upon by lazy evaluation of the heuristics, resulting in a variant known as Lazy IDA*. We introduce a rational version of lazy IDA* that decides whether to compute the more expensive heuristics or to bypass it, based on a myopic expected regret estimate. Empirical evaluation in several domains supports the theoretical results, and shows that rational lazy IDA* is a state-of-the-art heuristic combination method.',
	 'authors': u'David Tolpin, Oded Betzalel, Ariel Felner, Solomon Eyal Shimony,',
	 'category': u'Computer Science ',
	 'date': '2014-11-24',
	 'pdflink': u'http://arxiv.org/pdf/1411.6593',
	 'subjects': u'Artificial Intelligence (cs.AI)',
	 'title': u'\nRational Deployment of Multiple Heuristics in IDA*',
	 'urllink': u'http://arxiv.org/abs/1411.6593'}
2015-04-10 14:23:03+0000 [xxu46_10] INFO: Crawled 662 pages (at 2 pages/min), scraped 655 items (at 2 items/min)
2015-04-10 14:24:00+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1411.6591> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 14:24:00+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1411.6591>
	{'abstract': u'Despite the prevalence of collaborative filtering in recommendation systems, there has been little theoretical development on why and how well it works, especially in the "online" setting, where items are recommended to users over time. We address this theoretical gap by introducing a model for online recommendation systems, cast item recommendation under the model as a learning problem, and analyze the performance of a cosine-similarity collaborative filtering method. In our model, each of users either likes or dislikes each of items. We assume there to be types of users, and all the users of a given type share a common string of probabilities determining the chance of liking each item. At each time step, we recommend an item to each user, where a key distinction from related bandit literature is that once a user consumes an item (e.g., watches a movie), then that item cannot be recommended to the same user again. The goal is to maximize the number of likable items recommended to users over time. Our main result establishes that after nearly initial learning time steps, a simple collaborative filtering algorithm achieves essentially optimal performance without knowing . The algorithm has an exploitation step that uses cosine similarity and two types of exploration steps, one to explore the space of items (standard in the literature) and the other to explore similarity between users (novel to this work).',
	 'authors': u'Guy Bresler, George H. Chen, Devavrat Shah,',
	 'category': u'Computer Science ',
	 'date': '2014-10-31',
	 'pdflink': u'http://arxiv.org/pdf/1411.6591',
	 'subjects': u'Learning (cs.LG)',
	 'title': u'\nA Latent Source Model for Online Collaborative Filtering',
	 'urllink': u'http://arxiv.org/abs/1411.6591'}
2015-04-10 14:24:03+0000 [xxu46_10] INFO: Crawled 663 pages (at 1 pages/min), scraped 656 items (at 1 items/min)
2015-04-10 14:25:03+0000 [xxu46_10] INFO: Crawled 663 pages (at 0 pages/min), scraped 656 items (at 0 items/min)
2015-04-10 14:25:07+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1411.6587> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 14:25:07+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1411.6587>
	{'abstract': u'As technology grows, higher frequency signals are required to be processed in various applications. In order to digitize such signals, conventional analog to digital convertors are facing implementation challenges due to the higher sampling rates. Hence, lower sampling rates (i.e., sub-Nyquist) are considered to be cost efficient. A well-known approach is to consider sparse signals that have fewer nonzero frequency components compared to the highest frequency component. For the prior knowledge of the sparse positions, well-established methods already exist. However, there are applications where such information is not available. For such cases, a number of approaches have recently been proposed. In this paper, we propose several random sampling recovery algorithms which do not require any anti-aliasing filter. Moreover, we offer certain conditions under which these recovery techniques converge to the signal. Finally, we also confirm the performance of the above methods through extensive simulations.',
	 'authors': u'Amir Zandieh, Alireza Zareian, Masoumeh Azghani, Farokh Marvasti,',
	 'category': u'Computer Science ',
	 'date': '2014-11-8',
	 'pdflink': u'http://arxiv.org/pdf/1411.6587',
	 'subjects': u'Information Theory (cs.IT)',
	 'title': u'\nReconstruction of Sub-Nyquist Random Sampling for Sparse and Multi-Band  Signals',
	 'urllink': u'http://arxiv.org/abs/1411.6587'}
2015-04-10 14:26:03+0000 [xxu46_10] INFO: Crawled 664 pages (at 1 pages/min), scraped 657 items (at 1 items/min)
2015-04-10 14:26:22+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1411.6581> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 14:26:22+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1411.6581>
	{'abstract': u'In this paper we consider various encoding problems for range queries on arrays. In these problems, the goal is that the encoding occupies the information theoretic minimum space required to answer a particular set of range queries. Given an array a range top- query on an arbitrary range asks us to return the ordered set of indices such that is the -th largest element in . We present optimal encodings for range top- queries, as well as for a new problem which we call range min-max, in which the goal is to return the indices of both the minimum and maximum element in a range.',
	 'authors': u'Pawel Gawrychowski, Patrick K. Nicholson,',
	 'category': u'Computer Science ',
	 'date': '2014-11-24',
	 'pdflink': u'http://arxiv.org/pdf/1411.6581',
	 'subjects': u'Data Structures and Algorithms (cs.DS)',
	 'title': u'\nOptimal Encodings for Range Min-Max and Top-k',
	 'urllink': u'http://arxiv.org/abs/1411.6581'}
2015-04-10 14:27:03+0000 [xxu46_10] INFO: Crawled 665 pages (at 1 pages/min), scraped 658 items (at 1 items/min)
2015-04-10 14:27:40+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1411.6580> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 14:27:40+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1411.6580>
	{'abstract': u"An original approach to solving rather difficult probabilistic problems arising in studying the readout of random discrete fields and having no exact analytical solutions at the moment is proposed. Several algorithms for direct, iterative, and combinatorial-recursive calculations of multidimensional integral expressions, which can describe partial solutions of these problems, are presented (these solutions are further used to search for the common closed analytical regularities). The huge volume of necessary calculations forced us to formalize completely the algorithms and to transfer all the burden of routine analytical transforms to a computer. The calculations performed helped us to establish (and to prove later) a number of new earlier unknown probabilistic formulas responsible for random division of an interval. One more important feature of this study is the fact that we introduced a new concept of 'three-dimensional generalized Catalan numbers' and found their explicit form in solving problems associated with random division of an interval.",
	 'authors': u'Aleksander Reznik, Vitaly Efimov, Aleksander Soloview, Andrey Torgov,',
	 'category': u'Computer Science ',
	 'date': '2014-11-20',
	 'pdflink': u'http://arxiv.org/pdf/1411.6580',
	 'subjects': u'Other Computer Science (cs.OH)',
	 'title': u'\nThe Solving of the Problems with Random Division of an Interval with Use  of Computer Analytic Programs',
	 'urllink': u'http://arxiv.org/abs/1411.6580'}
2015-04-10 14:28:03+0000 [xxu46_10] INFO: Crawled 666 pages (at 1 pages/min), scraped 659 items (at 1 items/min)
2015-04-10 14:28:57+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1411.6574> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 14:28:57+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1411.6574>
	{'abstract': u'Natural disasters affect hundreds of millions of people worldwide every year. Emergency response efforts depend upon the availability of timely information, such as information concerning the movements of affected populations. The analysis of aggregated and anonymized Call Detail Records (CDR) captured from the mobile phone infrastructure provides new possibilities to characterize human behavior during critical events. In this work, we investigate the viability of using CDR data combined with other sources of information to characterize the floods that occurred in Tabasco, Mexico in 2009. An impact map has been reconstructed using Landsat-7 images to identify the floods. Within this frame, the underlying communication activity signals in the CDR data have been analyzed and compared against rainfall levels extracted from data of the NASA-TRMM project. The variations in the number of active phones connected to each cell tower reveal abnormal activity patterns in the most affected locations during and after the floods that could be used as signatures of the floods - both in terms of infrastructure impact assessment and population information awareness. The representativeness of the analysis has been assessed using census data and civil protection records. While a more extensive validation is required, these early results suggest high potential in using cell tower activity information to improve early warning and emergency management mechanisms.',
	 'authors': u'David Pastor-Escuredo, Alfredo Morales-Guzm\xe1n, Yolanda Torres-Fern\xe1ndez, Jean-Martin Bauer, Amit Wadhwa, Carlos Castro-Correa, Liudmyla Romanoff, Jong Gun Lee, Alex Rutherford, Vanessa Frias-Martinez, Nuria Oliver, Enrique Frias-Martinez, Miguel Luengo-Oroz,',
	 'category': u'Computer Science ',
	 'date': '2014-11-24',
	 'pdflink': u'http://arxiv.org/pdf/1411.6574',
	 'subjects': u'Computers and Society (cs.CY)',
	 'title': u'\nFlooding through the lens of mobile phone activity',
	 'urllink': u'http://arxiv.org/abs/1411.6574'}
2015-04-10 14:29:03+0000 [xxu46_10] INFO: Crawled 667 pages (at 1 pages/min), scraped 660 items (at 1 items/min)
2015-04-10 14:30:03+0000 [xxu46_10] INFO: Crawled 667 pages (at 0 pages/min), scraped 660 items (at 0 items/min)
2015-04-10 14:30:05+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1411.6573> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 14:30:05+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1411.6573>
	{'abstract': u'Intense vehicular traffic is recognized as a global societal problem, with a multifaceted influence on the quality of life of a person. Intelligent Transportation Systems (ITS) can play an important role in combating such problem, decreasing pollution levels and, consequently, their negative effects. One of the goals of ITSs, in fact, is that of controlling traffic flows, measuring traffic states, providing vehicles with routes that globally pursue low pollution conditions. How such systems measure and enforce given traffic states has been at the center of multiple research efforts in the past few years. Although many different solutions have been proposed, very limited effort has been devoted to exploring the potential of social network analysis in such context. Social networks, in general, provide direct feedback from people and, as such, potentially very valuable information. A post that tells, for example, how a person feels about pollution at a given time in a given location, could be put to good use by an environment aware ITS aiming at minimizing contaminant emissions in residential areas. This work verifies the feasibility of using pollution related social network feeds into ITS operations. In particular, it concentrates on understanding how reliable such information is, producing an analysis that confronts over 1,500,000 posts and pollution data obtained from on-the- field sensors over a one-year span.',
	 'authors': u'Rita Tse, Yubin Xiao, Giovanni Pau, Marco Roccetti, Serge Fdida, Gustavo Marfia,',
	 'category': u'Computer Science ',
	 'date': '2014-11-24',
	 'pdflink': u'http://arxiv.org/pdf/1411.6573',
	 'subjects': u'Social and Information Networks (cs.SI)',
	 'title': u'\nOn the Feasibility of Social Network-based Pollution Sensing in ITSs',
	 'urllink': u'http://arxiv.org/abs/1411.6573'}
2015-04-10 14:30:46+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1411.6562> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 14:30:46+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1411.6562>
	{'abstract': u'Worker quality control is a crucial aspect of crowdsourcing systems; typically occupying a large fraction of the time and money invested on crowdsourcing. In this work, we devise techniques to generate confidence intervals for worker error rate estimates, thereby enabling a better evaluation of worker quality. We show that our techniques generate correct confidence intervals on a range of real-world datasets, and demonstrate wide applicability by using them to evict poorly performing workers, and provide confidence intervals on the accuracy of the answers.',
	 'authors': u'Manas Joglekar, Hector Garcia-Molina, Aditya Parameswaran,',
	 'category': u'Computer Science ',
	 'date': '2014-11-12',
	 'pdflink': u'http://arxiv.org/pdf/1411.6562',
	 'subjects': u'Databases (cs.DB)',
	 'title': u'\nEvaluating the Crowd with Confidence',
	 'urllink': u'http://arxiv.org/abs/1411.6562'}
2015-04-10 14:31:03+0000 [xxu46_10] INFO: Crawled 669 pages (at 2 pages/min), scraped 662 items (at 2 items/min)
2015-04-10 14:31:35+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1411.6550> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 14:31:35+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1411.6550>
	{'abstract': u'A power spectral density based on the theory of weak wave turbulence is suggested for calculating the interference power in dense wavelength-division multiplexed optical systems. This power spectrum, termed Kolmogorov-Zakharov (KZ) model, results in a better estimate of the signal spectrum in optical fiber, compared with the so-called Gaussian noise (GN) model.',
	 'authors': u'Mansoor I. Yousefi, Frank R. Kschischang, Gerhard Kramer,',
	 'category': u'Computer Science ',
	 'date': '2014-11-24',
	 'pdflink': u'http://arxiv.org/pdf/1411.6550',
	 'subjects': u'Information Theory (cs.IT)',
	 'title': u'\nKolmogorov-Zakharov Model for Optical Fiber Communication',
	 'urllink': u'http://arxiv.org/abs/1411.6550'}
2015-04-10 14:32:03+0000 [xxu46_10] INFO: Crawled 670 pages (at 1 pages/min), scraped 663 items (at 1 items/min)
2015-04-10 14:32:21+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1411.6549> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 14:32:21+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1411.6549>
	{'abstract': u'A secure set in a graph is defined as a set of vertices such that for any the majority of vertices in the neighborhood of belongs to . It is known that deciding whether a set is secure in a graph is co-NP-complete. However, it is still open how this result contributes to the actual complexity of deciding whether for a given graph and integer , a non-empty secure set for of size at most exists. While membership in the class is rather easy to see for this existence problem, showing -hardness is quite involved. In this paper, we provide such a hardness result, hence classifying the secure set existence problem as -complete. We do so by first showing hardness for a variant of the problem, which we then reduce step-by-step to secure set existence. In total, we obtain eight new completeness results for different variants of the secure set existence problem.',
	 'authors': u'Bernhard Bliem, Stefan Woltran,',
	 'category': u'Computer Science ',
	 'date': '2014-11-24',
	 'pdflink': u'http://arxiv.org/pdf/1411.6549',
	 'subjects': u'Computational Complexity (cs.CC)',
	 'title': u'\nComplexity of Secure Sets',
	 'urllink': u'http://arxiv.org/abs/1411.6549'}
2015-04-10 14:33:03+0000 [xxu46_10] INFO: Crawled 671 pages (at 1 pages/min), scraped 664 items (at 1 items/min)
2015-04-10 14:33:32+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1411.6538> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 14:33:32+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1411.6538>
	{'abstract': u'Biobjective mixed integer linear programs (BOMILP) are optimization problems where two linear objectives are optimized over a polyhedron while restricting some of the variables to be integer. Since many of the techniques for solving BOMILP (or approximating its solution set) are iterative processes which utilize data discovered during early iterations to aid in the discovery of improved data during later iterations, it is highly desirable to efficiently store the nondominated subset of a given set of data. This problem has not received considerable attention in the context of BOMILP; only naive methods have been implemented. We seek to bridge this gap by presenting a new data structure in the form of a modified binary tree that stores, updates, searches and returns nondominated solutions. This structure takes points and line segments in as input and stores the nondominated subset of this input. We note that when used alongside an exact solution procedure, such as branch-and-bound (BB), at termination the data stored by this structure is precisely the set of Pareto optimal solutions. We perform two experiments. The first is designed to compare the utility of our structure for storing nondominated data to that of a dynamic list which updates via pairwise comparison. In the second we use our data structure alongside the biobjective BB techniques available in the literature and solve specific instances of BOMILP. The results of our first experiment suggest that the data structure performs reasonably well in handling input of up to points or segments and does so much more efficiently than a dynamic list. The results of the second experiment show that when our structure is utilized alongside BB fathoming is enhanced and running times improve slightly.',
	 'authors': u'Nathan Adelgren, Pietro Belotti, Akshay Gupte,',
	 'category': u'Computer Science ',
	 'date': '2014-11-24',
	 'pdflink': u'http://arxiv.org/pdf/1411.6538',
	 'subjects': u'Data Structures and Algorithms (cs.DS)',
	 'title': u'\nEfficient storage of Pareto points in biobjective mixed integer  programming',
	 'urllink': u'http://arxiv.org/abs/1411.6538'}
2015-04-10 14:34:03+0000 [xxu46_10] INFO: Crawled 672 pages (at 1 pages/min), scraped 665 items (at 1 items/min)
2015-04-10 14:34:43+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1411.6529> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 14:34:43+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1411.6529>
	{'abstract': u'This paper considers the arbitrary-proportional finite-set-partitioning problem which involves partitioning a finite set into multiple subsets with respect to arbitrary nonnegative proportions. This is the core art of many fundamental problems such as determining quotas for different individuals of different weights or sampling from a discrete-valued weighted sample set to get a new identically distributed but non-weighted sample set (e.g. the resampling needed in the particle filter). The challenge raises as the size of each subset must be an integer while its unbiased expectation is often not. To solve this problem, a metric (cost function) is defined on their discrepancies and correspondingly a solution is proposed to determine the sizes of each subsets, gaining the minimal bias. Theoretical proof and simulation demonstrations are provided to demonstrate the optimality of the scheme in the sense of the proposed metric.',
	 'authors': u'Tiancheng Li,',
	 'category': u'Computer Science ',
	 'date': '2014-11-24',
	 'pdflink': u'http://arxiv.org/pdf/1411.6529',
	 'subjects': u'Numerical Analysis (cs.NA)',
	 'title': u'\nThe Optimal Arbitrary-Proportional Finite-Set-Partitioning',
	 'urllink': u'http://arxiv.org/abs/1411.6529'}
2015-04-10 14:35:03+0000 [xxu46_10] INFO: Crawled 673 pages (at 1 pages/min), scraped 666 items (at 1 items/min)
2015-04-10 14:35:59+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1411.6521> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 14:35:59+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1411.6521>
	{'abstract': u'Distributed Information SHaring (DISH) is a new cooperative approach to designing multi-channel MAC protocols. It aids nodes in their decision making processes by compensating for their missing information via information sharing through other neighboring nodes. This approach was recently shown to significantly boost the throughput of multi-channel MAC protocols. However, a critical issue for ad hoc communication devices, i.e., energy efficiency, has yet to be addressed. In this paper, we address this issue by developing simple solutions which (1) reduce the energy consumption (2) without compromising the throughput performance, and meanwhile (3) maximize cost efficiency. We propose two energy-efficient strategies: in-situ energy conscious DISH which uses existing nodes only, and altruistic DISH which needs additional nodes called altruists. We compare five protocols with respect to the strategies and identify altruistic DISH to be the right choice in general: it (1) conserves 40-80% of energy, (2) maintains the throughput advantage gained from the DISH approach, and (3) more than doubles the cost efficiency compared to protocols without applying the strategy. On the other hand, our study shows that in-situ energy conscious DISH is suitable only in certain limited scenarios.',
	 'authors': u'Tie Luo, Mehul Motani, Vikram Srinivasan,',
	 'category': u'Computer Science ',
	 'date': '2014-11-24',
	 'pdflink': u'http://arxiv.org/pdf/1411.6521',
	 'subjects': u'Networking and Internet Architecture (cs.NI)',
	 'title': u'\nEnergy-Efficient Strategies for Cooperative Multi-Channel MAC Protocols',
	 'urllink': u'http://arxiv.org/abs/1411.6521'}
2015-04-10 14:36:03+0000 [xxu46_10] INFO: Crawled 674 pages (at 1 pages/min), scraped 667 items (at 1 items/min)
2015-04-10 14:37:03+0000 [xxu46_10] INFO: Crawled 674 pages (at 0 pages/min), scraped 667 items (at 0 items/min)
2015-04-10 14:37:13+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1411.6509> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 14:37:13+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1411.6509>
	{'abstract': u'Supervised training of a convolutional network for object classification should make explicit any information related to the class of objects and disregard any auxiliary information associated with the capture of the image or the variation within the object class. Does this happen in practice? Although this seems to pertain to the very final layers in the network, if we look at earlier layers we find that this is not the case. Surprisingly, strong spatial information is implicit. This paper addresses this, in particular, exploiting the image representation at the first fully connected layer, i.e. the global image descriptor which has been recently shown to be most effective in a range of visual recognition tasks. We empirically demonstrate evidences for the finding in the contexts of four different tasks: 2d landmark detection, 2d object keypoints prediction, estimation of the RGB values of input image, and recovery of semantic label of each pixel. We base our investigation on a simple framework with ridge rigression commonly across these tasks, and show results which all support our insight. Such spatial information can be used for computing correspondence of landmarks to a good accuracy, but should potentially be useful for improving the training of the convolutional nets for classification purposes.',
	 'authors': u'Ali Sharif Razavian, Hossein Azizpour, Atsuto Maki, Josephine Sullivan, Carl Henrik Ek, Stefan Carlsson,',
	 'category': u'Computer Science ',
	 'date': '2014-11-24',
	 'pdflink': u'http://arxiv.org/pdf/1411.6509',
	 'subjects': u'Computer Vision and Pattern Recognition (cs.CV)',
	 'title': u'\nPersistent Evidence of Local Image Properties in Generic ConvNets',
	 'urllink': u'http://arxiv.org/abs/1411.6509'}
2015-04-10 14:37:52+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1411.6498> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 14:37:52+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1411.6498>
	{'abstract': u'It has been pointed out by counterexamples in a 2013 paper in the IEEE Transactions on Computers [1], that there is an error in the previously ibid. in 2005 published paper [2] on the construction of valid digit selection tables for SRT type division and square root algorithms. The error has been corrected, and new results found on selection constants for maximally redundant digit sets.',
	 'authors': u'Peter Kornerup,',
	 'category': u'Computer Science ',
	 'date': '2014-11-24',
	 'pdflink': u'http://arxiv.org/pdf/1411.6498',
	 'subjects': u'Hardware Architecture (cs.AR)',
	 'title': u'\nCorrection to the 2005 paper: "Digit Selection for SRT Division and  Square Root"',
	 'urllink': u'http://arxiv.org/abs/1411.6498'}
2015-04-10 14:38:03+0000 [xxu46_10] INFO: Crawled 676 pages (at 2 pages/min), scraped 669 items (at 2 items/min)
2015-04-10 14:38:59+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1411.6496> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 14:38:59+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1411.6496>
	{'abstract': u'Automatic summarization generation of sports video content has been object of great interest for many years. Although semantic descriptions techniques have been proposed, many of the approaches still rely on low-level video descriptors that render quite limited results due to the complexity of the problem and to the low capability of the descriptors to represent semantic content. In this paper, a new approach for automatic highlights summarization generation of soccer videos using audio-visual descriptors is presented. The approach is based on the segmentation of the video sequence into shots that will be further analyzed to determine its relevance and interest. Of special interest in the approach is the use of the audio information that provides additional robustness to the overall performance of the summarization system. For every video shot a set of low and mid level audio-visual descriptors are computed and lately adequately combined in order to obtain different relevance measures based on empirical knowledge rules. The final summary is generated by selecting those shots with highest interest according to the specifications of the user and the results of relevance measures. A variety of results are presented with real soccer video sequences that prove the validity of the approach.',
	 'authors': u'Arnau Raventos, Raul Quijada, Luis Torres, Francesc Tarres,',
	 'category': u'Computer Science ',
	 'date': '2014-11-24',
	 'pdflink': u'http://arxiv.org/pdf/1411.6496',
	 'subjects': u'Information Retrieval (cs.IR)',
	 'title': u'\nAutomatic Summarization of Soccer Highlights Using Audio-visual  Descriptors',
	 'urllink': u'http://arxiv.org/abs/1411.6496'}
2015-04-10 14:39:03+0000 [xxu46_10] INFO: Crawled 677 pages (at 1 pages/min), scraped 670 items (at 1 items/min)
2015-04-10 14:40:03+0000 [xxu46_10] INFO: Crawled 677 pages (at 0 pages/min), scraped 670 items (at 0 items/min)
2015-04-10 14:40:04+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1411.6478> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 14:40:04+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1411.6478>
	{'abstract': u'Over the last thirty years, numerous consistency conditions for replicated data have been proposed and implemented. Popular examples of such conditions include linearizability (or atomicity), sequential consistency, causal consistency, and eventual consistency. These consistency conditions are usually defined independently from the computing entities (nodes) that manipulate the replicated data; i.e., they do not take into account how computing entities might be linked to one another, or geographically distributed. To address this lack, as a first contribution, this paper introduces the notion of proximity graph between computing nodes. If two nodes are connected in this graph, their operations must satisfy a strong consistency condition, while the operations invoked by other nodes are allowed to satisfy a weaker condition. The second contribution is the use of such a graph to provide a generic approach to the hybridization of data consistency conditions into the same system. We illustrate this approach on sequential consistency and causal consistency, and present a model in which all data operations are causally consistent, while operations by neighboring processes in the proximity graph are sequentially consistent. The third contribution of the paper is the design and the proof of a distributed algorithm based on this proximity graph, which combines sequential consistency and causal consistency (the resulting condition is called fisheye consistency). In doing so the paper not only extends the domain of consistency conditions, but provides a generic provably correct solution of direct relevance to modern georeplicated systems.',
	 'authors': u'Roy Friedman, Michel Raynal, Fran\xe7ois Ta\xefani,',
	 'category': u'Computer Science ',
	 'date': '2014-11-24',
	 'pdflink': u'http://arxiv.org/pdf/1411.6478',
	 'subjects': u'Distributed, Parallel, and Cluster Computing (cs.DC)',
	 'title': u'\nFisheye Consistency: Keeping Data in Synch in a Georeplicated World',
	 'urllink': u'http://arxiv.org/abs/1411.6478'}
2015-04-10 14:41:03+0000 [xxu46_10] INFO: Crawled 678 pages (at 1 pages/min), scraped 671 items (at 1 items/min)
2015-04-10 14:41:22+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1411.6469> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 14:41:22+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1411.6469>
	{'abstract': u'Throughput and energy efficiency in 3-way relay channels are studied in this paper. Unlike previous contributions, we consider a circular message exchange. First, an outer bound and achievable sum rate expressions for different relaying protocols are derived for 3-way relay channels. The sum capacity is characterized for certain SNR regimes. Next, leveraging the derived achievable sum rate expressions, cooperative and competitive maximization of the energy efficiency are considered. For the cooperative case, both low-complexity and globally optimal algorithms for joint power allocation at the users and at the relay are designed so as to maximize the system global energy efficiency. For the competitive case, a game theoretic approach is taken, and it is shown that the best response dynamics is guaranteed to converge to a Nash equilibrium. A power consumption model for mmWave board-to-board communications is developed, and numerical results are provided to corroborate and provide insight on the theoretical findings.',
	 'authors': u'Bho Matthiesen, Alessio Zappone, Eduard A. Jorswieck,',
	 'category': u'Computer Science ',
	 'date': '2014-11-24',
	 'pdflink': u'http://arxiv.org/pdf/1411.6469',
	 'subjects': u'Information Theory (cs.IT)',
	 'title': u'\nResource Allocation for Energy-Efficient 3-Way Relay Channels',
	 'urllink': u'http://arxiv.org/abs/1411.6469'}
2015-04-10 14:42:03+0000 [xxu46_10] INFO: Crawled 679 pages (at 1 pages/min), scraped 672 items (at 1 items/min)
2015-04-10 14:42:39+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1411.6466> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 14:42:39+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1411.6466>
	{'abstract': u'In this letter, we propose the interference cancellation through interference alignment at the downlink of cognitive cellular networks. Interference alignment helps the spatial resources to be shared among primary and secondary cells and thus, it can provide higher degrees of freedom through interference cancellation. We derive and depict the achievable degrees of freedom. We also analyse and calculate the achievable sum rates applying water-filling optimal power allocation.',
	 'authors': u'May Moussa, Fotis Foukalas, Tamer Khattab,',
	 'category': u'Computer Science ',
	 'date': '2014-11-24',
	 'pdflink': u'http://arxiv.org/pdf/1411.6466',
	 'subjects': u'Information Theory (cs.IT)',
	 'title': u'\nInterference Cancellation trough Interference Alignment for Downlink of  Cognitive Cellular Networks',
	 'urllink': u'http://arxiv.org/abs/1411.6466'}
2015-04-10 14:43:03+0000 [xxu46_10] INFO: Crawled 680 pages (at 1 pages/min), scraped 673 items (at 1 items/min)
2015-04-10 14:43:57+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1411.6463> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 14:43:57+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1411.6463>
	{'abstract': u'The quotient operation, which is dual to the composition, is crucial in specification theories as it allows the synthesis of missing specifications and thus enables incremental design. In this paper, we consider a specification theory based on marked acceptance specifications (MAS) which are automata enriched with variability information encoded by acceptance sets and with reachability constraints on states. We define a sound and complete quotient for MAS hence ensuring reachability properties by construction.',
	 'authors': u'Guillaume Verdier, Jean-Baptiste Raclet,',
	 'category': u'Computer Science ',
	 'date': '2014-11-24',
	 'pdflink': u'http://arxiv.org/pdf/1411.6463',
	 'subjects': u'Formal Languages and Automata Theory (cs.FL)',
	 'title': u'\nQuotient of Acceptance Specifications under Reachability Constraints',
	 'urllink': u'http://arxiv.org/abs/1411.6463'}
2015-04-10 14:44:03+0000 [xxu46_10] INFO: Crawled 681 pages (at 1 pages/min), scraped 674 items (at 1 items/min)
2015-04-10 14:45:03+0000 [xxu46_10] INFO: Crawled 681 pages (at 0 pages/min), scraped 674 items (at 0 items/min)
2015-04-10 14:45:04+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1411.6462> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 14:45:04+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1411.6462>
	{'abstract': u"Modern society habitually uses online social media services to publicly share observations, thoughts, opinions, and beliefs at any time and from any location. These geotagged social media posts may provide aggregate insights into people's perceptions on a bad range of topics across a given geographical area beyond what is currently possible through services such as Yelp and Foursquare. This paper develops probabilistic language models to investigate whether collective, topic-based perceptions within a geographical area can be extracted from the content of geotagged Twitter posts. The capability of the methodology is illustrated using tweets from three areas of different sizes. An application of the approach to support power grid restoration following a storm is presented.",
	 'authors': u'Derek Doran, Swapna Gokhale, Aldo Dagnino,',
	 'category': u'Computer Science ',
	 'date': '2014-11-24',
	 'pdflink': u'http://arxiv.org/pdf/1411.6462',
	 'subjects': u'Social and Information Networks (cs.SI)',
	 'title': u'\nUnderstanding Common Perceptions from Online Social Media',
	 'urllink': u'http://arxiv.org/abs/1411.6462'}
2015-04-10 14:45:53+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1411.6447> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 14:45:53+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1411.6447>
	{'abstract': u'Fine-grained classification is challenging because categories can only be discriminated by subtle and local differences. Variances in the pose, scale or rotation usually make the problem more difficult. Most fine-grained classification systems follow the pipeline of finding foreground object or object parts (where) to extract discriminative features (what). In this paper, we propose to apply visual attention to fine-grained classification task using deep neural network. Our pipeline integrates three types of attention: the bottom-up attention that propose candidate patches, the object-level top-down attention that selects relevant patches to a certain object, and the part-level top-down attention that localizes discriminative parts. We combine these attentions to train domain-specific deep nets, then use it to improve both the what and where aspects. Importantly, we avoid using expensive annotations like bounding box or part information from end-to-end. The weak supervision constraint makes our work easier to generalize. We have verified the effectiveness of the method on the subsets of ILSVRC2012 dataset and CUB200_2011 dataset. Our pipeline delivered significant improvements and achieved the best accuracy under the weakest supervision condition. The performance is competitive against other methods that rely on additional annotations.',
	 'authors': u'Tianjun Xiao, Yichong Xu, Kuiyuan Yang, Jiaxing Zhang, Yuxin Peng, Zheng Zhang,',
	 'category': u'Computer Science ',
	 'date': '2014-11-24',
	 'pdflink': u'http://arxiv.org/pdf/1411.6447',
	 'subjects': u'Computer Vision and Pattern Recognition (cs.CV)',
	 'title': u'\nThe Application of Two-level Attention Models in Deep Convolutional  Neural Network for Fine-grained Image Classification',
	 'urllink': u'http://arxiv.org/abs/1411.6447'}
2015-04-10 14:46:03+0000 [xxu46_10] INFO: Crawled 683 pages (at 2 pages/min), scraped 676 items (at 2 items/min)
2015-04-10 14:46:53+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1411.6432> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 14:46:53+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1411.6432>
	{'abstract': u'Apart from a brief look at applications (Relational Databases, Formal Concept Analysis et al.) this article is devoted to the mathematical t h e o r y of implications (=pure Horn formulas). It is mainly a survey of results obtained in the last thirty years, but features a few novelties as well. Some keywords: The Duquenne-Guiges (implicational) base, the canonical direct base, prime implicates, the consensus method, implications and meet irreducible closed sets, optimum bases for certain lattices, component-wise quadratic pure Horn functions, ordered direct bases, generating all closed sets, general (i.e. impure) Horn functions. We pose six open problems to stimulate further research.',
	 'authors': u'Marcel Wild,',
	 'category': u'Computer Science ',
	 'date': '2014-11-24',
	 'pdflink': u'http://arxiv.org/pdf/1411.6432',
	 'subjects': u'Logic in Computer Science (cs.LO)',
	 'title': u'\nThe joy of implications, aka pure Horn functions: mainly a survey',
	 'urllink': u'http://arxiv.org/abs/1411.6432'}
2015-04-10 14:47:03+0000 [xxu46_10] INFO: Crawled 684 pages (at 1 pages/min), scraped 677 items (at 1 items/min)
2015-04-10 14:48:03+0000 [xxu46_10] INFO: Crawled 684 pages (at 0 pages/min), scraped 677 items (at 0 items/min)
2015-04-10 14:48:11+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1411.6409> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 14:48:11+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1411.6409>
	{'abstract': u'Secure communications are playing increasing roles in society, particularly in finance, journalism, and military projects. Current methods of securing e-mail and similar messaging methods rely on encryption of the message body, but the header with addressing information remains plaintext. This allows third party eavesdroppers to collect and analyse the header metadata and construct a network model of the participants in conversations (who, where, when, subject). In this article, we describe a method of communication where the header is also encrypted, hindering the assembly of the communication network models, which is verified with a working prototype application. This provides a useful tool to journalists and proponents of free speech in oppressed countries, protecting both the messages and their sources.',
	 'authors': u'H. Bjorgvinsdottir, P. M. Bentley,',
	 'category': u'Computer Science ',
	 'date': '2014-11-24',
	 'pdflink': u'http://arxiv.org/pdf/1411.6409',
	 'subjects': u'Cryptography and Security (cs.CR)',
	 'title': u'\nWarp2: A Method of Email and Messaging with Encrypted Addressing and  Headers',
	 'urllink': u'http://arxiv.org/abs/1411.6409'}
2015-04-10 14:49:03+0000 [xxu46_10] INFO: Crawled 685 pages (at 1 pages/min), scraped 678 items (at 1 items/min)
2015-04-10 14:49:23+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1411.6408> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 14:49:23+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1411.6408>
	{'abstract': u'This paper studies properties of the back end of a sorting network and illustrates the utility of these in the search for networks of optimal size or depth. All previous works focus on properties of the front end of networks and on how to apply these to break symmetries in the search. The new properties help shed understanding on how sorting networks sort and speed-up solvers for both optimal size and depth by an order of magnitude.',
	 'authors': u'Michael Codish, Lu\xeds Cruz-Filipe, Peter Schneider-Kamp,',
	 'category': u'Computer Science ',
	 'date': '2014-11-24',
	 'pdflink': u'http://arxiv.org/pdf/1411.6408',
	 'subjects': u'Data Structures and Algorithms (cs.DS)',
	 'title': u'\nSorting Networks: the End Game',
	 'urllink': u'http://arxiv.org/abs/1411.6408'}
2015-04-10 14:50:03+0000 [xxu46_10] INFO: Crawled 686 pages (at 1 pages/min), scraped 679 items (at 1 items/min)
2015-04-10 14:50:08+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1411.6406> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 14:50:08+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1411.6406>
	{'abstract': u'Deriving from the gradient vector of a generative model of local features, Fisher vector coding (FVC) has been identified as an effective coding method for image classification. Most, if not all, % FVC implementations employ the Gaussian mixture model (GMM) to characterize the generation process of local features. This choice has shown to be sufficient for traditional low dimensional local features, e.g., SIFT; and typically, good performance can be achieved with only a few hundred Gaussian distributions. However, the same number of Gaussians is insufficient to model the feature space spanned by higher dimensional local features, which have become popular recently. In order to improve the modeling capacity for high dimensional features, it turns out to be inefficient and computationally impractical to simply increase the number of Gaussians. In this paper, we propose a model in which each local feature is drawn from a Gaussian distribution whose mean vector is sampled from a subspace. With certain approximation, this model can be converted to a sparse coding procedure and the learning/inference problems can be readily solved by standard sparse coding methods. By calculating the gradient vector of the proposed model, we derive a new fisher vector encoding strategy, termed Sparse Coding based Fisher Vector Coding (SCFVC). Moreover, we adopt the recently developed Deep Convolutional Neural Network (CNN) descriptor as a high dimensional local feature and implement image classification with the proposed SCFVC. Our experimental evaluations demonstrate that our method not only significantly outperforms the traditional GMM based Fisher vector encoding but also achieves the state-of-the-art performance in generic object recognition, indoor scene, and fine-grained image classification problems.',
	 'authors': u'Lingqiao Liu, Chunhua Shen, Lei Wang, Anton van den Hengel, Chao Wang,',
	 'category': u'Computer Science ',
	 'date': '2014-11-24',
	 'pdflink': u'http://arxiv.org/pdf/1411.6406',
	 'subjects': u'Computer Vision and Pattern Recognition (cs.CV)',
	 'title': u'\nEncoding High Dimensional Local Features by Sparse Coding Based Fisher  Vectors',
	 'urllink': u'http://arxiv.org/abs/1411.6406'}
2015-04-10 14:51:03+0000 [xxu46_10] INFO: Crawled 687 pages (at 1 pages/min), scraped 680 items (at 1 items/min)
2015-04-10 14:51:22+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1411.6387> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 14:51:22+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1411.6387>
	{'abstract': u'We consider the problem of depth estimation from a single monocular image in this work. It is a challenging task as no reliable depth cues are available, e.g., stereo correspondences, motions, etc. Previous efforts have been focusing on exploiting geometric priors or additional sources of information, with all using hand-crafted features. Recently, there is mounting evidence that features from deep convolutional neural networks (CNN) are setting new records for various vision applications. On the other hand, considering the continuous characteristic of the depth values, depth estimations can be naturally formulated into a continuous conditional random field (CRF) learning problem. Therefore, we in this paper present a deep convolutional neural field model for estimating depths from a single image, aiming to jointly explore the capacity of deep CNN and continuous CRF. Specifically, we propose a deep structured learning scheme which learns the unary and pairwise potentials of continuous CRF in a unified deep CNN framework. The proposed method can be used for depth estimations of general scenes with no geometric priors nor any extra information injected. In our case, the integral of the partition function can be analytically calculated, thus we can exactly solve the log-likelihood optimization. Moreover, solving the MAP problem for predicting depths of a new image is highly efficient as closed-form solutions exist. We experimentally demonstrate that the proposed method outperforms state-of-the-art depth estimation methods on both indoor and outdoor scene datasets.',
	 'authors': u'Fayao Liu, Chunhua Shen, Guosheng Lin,',
	 'category': u'Computer Science ',
	 'date': '2014-11-24',
	 'pdflink': u'http://arxiv.org/pdf/1411.6387',
	 'subjects': u'Computer Vision and Pattern Recognition (cs.CV)',
	 'title': u'\nDeep Convolutional Neural Fields for Depth Estimation from a Single  Image',
	 'urllink': u'http://arxiv.org/abs/1411.6387'}
2015-04-10 14:52:03+0000 [xxu46_10] INFO: Crawled 688 pages (at 1 pages/min), scraped 681 items (at 1 items/min)
2015-04-10 14:52:25+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1411.6382> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 14:52:25+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1411.6382>
	{'abstract': u'Mid-level visual element discovery aims to find clusters of image patches that are both representative and discriminative. In this work, we study this problem from the prospective of pattern mining while relying on the recently popularized Convolutional Neural Networks (CNNs). Specifically, we find that for an image patch, activations extracted from the first fully-connected layer of CNNs have two appealing properties which enable its seamless integration with pattern mining. Patterns are then discovered from a large number of CNN activations of image patches through the well-known association rule mining. When we retrieve and visualize image patches with the same pattern, surprisingly, they are not only visually similar but also semantically consistent. We apply our approach to scene and object classification tasks, and demonstrate that our approach outperforms all previous works on mid-level visual element discovery by a sizeable margin with far fewer elements being used. Our approach also outperforms or matches recent works using CNN for these tasks. Source code of the complete system is available online.',
	 'authors': u'Yao Li, Lingqiao Liu, Chunhua Shen, Anton van den Hengel,',
	 'category': u'Computer Science ',
	 'date': '2014-11-24',
	 'pdflink': u'http://arxiv.org/pdf/1411.6382',
	 'subjects': u'Computer Vision and Pattern Recognition (cs.CV)',
	 'title': u'\nMid-level Deep Pattern Mining',
	 'urllink': u'http://arxiv.org/abs/1411.6382'}
2015-04-10 14:53:03+0000 [xxu46_10] INFO: Crawled 689 pages (at 1 pages/min), scraped 682 items (at 1 items/min)
2015-04-10 14:53:25+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1411.6372> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 14:53:26+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1411.6372>
	{'abstract': u"The performance of molecular communication is significantly impacted by the reception process of the messenger molecules. The receptors' size and density, however, have yet to be investigated. In this letter, we analyze the effect of receptor density and size on the signal reception of an absorbing receiver with receptors. The results show that, when the total receptor area is the same, better hitting probability is achieved by using a higher number of relatively small receptors. In addition, deploying receptors, which cover a small percentage of the receiver surface, is able to create an effective communication channel that has a detectable signal level.",
	 'authors': u'Ali Akkaya, H. Birkan Yilmaz, Chan-Byoung Chae, Tuna Tugcu,',
	 'category': u'Computer Science ',
	 'date': '2014-11-24',
	 'pdflink': u'http://arxiv.org/pdf/1411.6372',
	 'subjects': u'Emerging Technologies (cs.ET)',
	 'title': u'\nEffect of Receptor Density and Size on Signal Reception in Molecular  Communication via Diffusion with an Absorbing Receiver',
	 'urllink': u'http://arxiv.org/abs/1411.6372'}
2015-04-10 14:54:03+0000 [xxu46_10] INFO: Crawled 690 pages (at 1 pages/min), scraped 683 items (at 1 items/min)
2015-04-10 14:54:36+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1411.6371> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 14:54:36+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1411.6371>
	{'abstract': u'In this paper, we study how to fold a specified origami crease pattern in order to minimize the impact of paper thickness. Specifically, origami designs are often expressed by a mountain-valley pattern (plane graph of creases with relative fold orientations), but in general this specification is consistent with exponentially many possible folded states. We analyze the complexity of finding the best consistent folded state according to two metrics: minimizing the total number of layers in the folded state (so that a "flat folding" is indeed close to flat), and minimizing the total amount of paper required to execute the folding (where "thicker" creases consume more paper). We prove both problems strongly NP-complete even for 1D folding. On the other hand, we prove the first problem fixed-parameter tractable in 1D with respect to the number of layers.',
	 'authors': u'Erik D. Demaine, David Eppstein, Adam Hesterberg, Hiro Ito, Anna Lubiw, Ryuhei Uehara, Yushi Uno,',
	 'category': u'Computer Science ',
	 'date': '2014-11-24',
	 'pdflink': u'http://arxiv.org/pdf/1411.6371',
	 'subjects': u'Data Structures and Algorithms (cs.DS)',
	 'title': u'\nFolding a Paper Strip to Minimize Thickness',
	 'urllink': u'http://arxiv.org/abs/1411.6371'}
2015-04-10 14:55:03+0000 [xxu46_10] INFO: Crawled 691 pages (at 1 pages/min), scraped 684 items (at 1 items/min)
2015-04-10 14:55:30+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1411.6370> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 14:55:30+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1411.6370>
	{'abstract': u'Explosive growth in data and availability of cheap computing resources have sparked increasing interest in Big learning, an emerging subfield that studies scalable machine learning algorithms, systems, and applications with Big Data. Bayesian methods represent one important class of statistic methods for machine learning, with substantial recent developments on adaptive, flexible and scalable Bayesian learning. This article provides a survey of the recent advances in Big learning with Bayesian methods, termed Big Bayesian Learning, including nonparametric Bayesian methods for adaptively inferring model complexity, regularized Bayesian inference for improving the flexibility via posterior regularization, and scalable algorithms and systems based on stochastic subsampling and distributed computing for dealing with large-scale applications.',
	 'authors': u'Jun Zhu, Jianfei Chen, Wenbo Hu,',
	 'category': u'Computer Science ',
	 'date': '2014-11-24',
	 'pdflink': u'http://arxiv.org/pdf/1411.6370',
	 'subjects': u'Learning (cs.LG)',
	 'title': u'\nBig Learning with Bayesian Methods',
	 'urllink': u'http://arxiv.org/abs/1411.6370'}
2015-04-10 14:56:03+0000 [xxu46_10] INFO: Crawled 692 pages (at 1 pages/min), scraped 685 items (at 1 items/min)
2015-04-10 14:56:45+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1411.6369> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 14:56:45+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1411.6369>
	{'abstract': u'Even though convolutional neural networks (CNN) has achieved near-human performance in various computer vision tasks, its ability to tolerate scale variations is limited. The popular practise is making the model bigger first, and then train it with data augmentation using extensive scale-jittering. In this paper, we propose a scaleinvariant convolutional neural network (SiCNN), a modeldesigned to incorporate multi-scale feature exaction and classification into the network structure. SiCNN uses a multi-column architecture, with each column focusing on a particular scale. Unlike previous multi-column strategies, these columns share the same set of filter parameters by a scale transformation among them. This design deals with scale variation without blowing up the model size. Experimental results show that SiCNN detects features at various scales, and the classification result exhibits strong robustness against object scale variations.',
	 'authors': u'Yichong Xu, Tianjun Xiao, Jiaxing Zhang, Kuiyuan Yang, Zheng Zhang,',
	 'category': u'Computer Science ',
	 'date': '2014-11-24',
	 'pdflink': u'http://arxiv.org/pdf/1411.6369',
	 'subjects': u'Computer Vision and Pattern Recognition (cs.CV)',
	 'title': u'\nScale-Invariant Convolutional Neural Networks',
	 'urllink': u'http://arxiv.org/abs/1411.6369'}
2015-04-10 14:57:03+0000 [xxu46_10] INFO: Crawled 693 pages (at 1 pages/min), scraped 686 items (at 1 items/min)
2015-04-10 14:57:56+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1411.6365> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 14:57:56+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1411.6365>
	{'abstract': u"B 'ezier splines are widely available in various systems with the curves and surface designs. In general, the B 'ezier spline can be specified with the B 'ezier curve segments and a B 'ezier curve segment can be fitted to any number of control points. The number of control points determines the degree of the B 'ezier polynomial. This paper presents a method which determines control points for B 'ezier curves approximating segments of obtained image outline(non-parametric curve) by using the properties of cubic B 'ezier curves. Proposed method is a technique to determine the control points that has generality and reduces the error of the B 'ezier curve approximation. Main advantage of proposed method is that it has higher accuracy and compression rate than previous methods. The cubic B 'ezier spline is obtained from cubic B 'ezier curve segments. To demonstrate the various performances of the proposed algorithm, experimental results are compared.",
	 'authors': u'Ha Jong Won, Choe Chun Hwa, Li Kum Song,',
	 'category': u'Computer Science ',
	 'date': '2014-11-24',
	 'pdflink': u'http://arxiv.org/pdf/1411.6365',
	 'subjects': u'Computer Vision and Pattern Recognition (cs.CV)',
	 'title': u'\nOn the mathematic modeling of non-parametric curves based on cubic  B\xe9zier curves',
	 'urllink': u'http://arxiv.org/abs/1411.6365'}
2015-04-10 14:58:03+0000 [xxu46_10] INFO: Crawled 694 pages (at 1 pages/min), scraped 687 items (at 1 items/min)
2015-04-10 14:59:03+0000 [xxu46_10] INFO: Crawled 694 pages (at 0 pages/min), scraped 687 items (at 0 items/min)
2015-04-10 14:59:15+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1411.6361> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 14:59:15+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1411.6361>
	{'abstract': u'Profile-Guided Optimization (PGO) is an excellent means to improve the performance of a compiled program. Indeed, the execution path data it provides helps the compiler to generate better code and better cacheline packing. At the time of this writing, compilers only support instrumentation-based PGO. This proved effective for optimizing programs. However, few projects use it, due to its complicated dual-compilation model and its high overhead. Our solution of sampling Hardware Performance Counters overcome these drawbacks. In this paper, we propose a PGO solution for GCC by sampling Last Branch Record (LBR) events and using debug symbols to recreate source locations of binary instructions. By using LBR-Sampling, the generated profiles are very accurate. This solution achieved an average of 83% of the gains obtained with instrumentation-based PGO and 93% on C++ benchmarks only. The profiling overhead is only 1.06% on average whereas instrumentation incurs a 16% overhead on average.',
	 'authors': u'Baptiste Wicht, Roberto A. Vitillo, Dehao Chen, David Levinthal,',
	 'category': u'Computer Science ',
	 'date': '2014-11-24',
	 'pdflink': u'http://arxiv.org/pdf/1411.6361',
	 'subjects': u'Programming Languages (cs.PL)',
	 'title': u'\nHardware Counted Profile-Guided Optimization',
	 'urllink': u'http://arxiv.org/abs/1411.6361'}
2015-04-10 15:00:03+0000 [xxu46_10] INFO: Crawled 695 pages (at 1 pages/min), scraped 688 items (at 1 items/min)
2015-04-10 15:00:25+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1411.6359> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 15:00:25+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1411.6359>
	{'abstract': u'The existence of considerable amount of redundancy in the Internet traffic at the packet level has stimulated the deployment of packet-level redundancy elimination techniques within the network by enabling network nodes to memorize data packets. Redundancy elimination results in traffic reduction which in turn improves the efficiency of network links. In this paper, the concept of network compression is introduced that aspires to exploit the statistical correlation beyond removing large duplicate strings from the flow to better suppress redundancy. In the first part of the paper, we introduce "memory-assisted compression", which utilizes the memorized content within the network to learn the statistics of the information source generating the packets which can then be used toward reducing the length of codewords describing the packets emitted by the source. Using simulations on data gathered from real network traces, we show that memory-assisted compression can result in significant traffic reduction. In the second part of the paper, we study the scaling of the average network-wide benefits of memory-assisted compression. We discuss routing and memory placement problems in network for the reduction of overall traffic. We derive a closed-form expression for the scaling of the gain in Erdos-Renyi random network graphs, where obtain a threshold value for the number of memories deployed in a random graph beyond which network-wide benefits start to shine. Finally, the network-wide benefits are studied on Internet-like scale-free networks. We show that non-vanishing network compression gain is obtained even when only a tiny fraction of the total number of nodes in the network are memory-enabled.',
	 'authors': u'Ahmad Beirami, Mohsen Sardari, Faramarz Fekri,',
	 'category': u'Computer Science ',
	 'date': '2014-11-24',
	 'pdflink': u'http://arxiv.org/pdf/1411.6359',
	 'subjects': u'Networking and Internet Architecture (cs.NI)',
	 'title': u'\nPacket-Level Network Compression: Realization and Scaling of the  Network-Wide Benefits',
	 'urllink': u'http://arxiv.org/abs/1411.6359'}
2015-04-10 15:01:03+0000 [xxu46_10] INFO: Crawled 696 pages (at 1 pages/min), scraped 689 items (at 1 items/min)
2015-04-10 15:01:44+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1411.6358> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 15:01:44+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1411.6358>
	{'abstract': u'Currently, many machine learning algorithms contain lots of iterations. When it comes to existing large-scale distributed systems, some slave nodes may break down or have lower efficiency. Therefore traditional machine learning algorithm may fail because of the instability of distributed system.We presents a hybrid approach which not only own a high fault-tolerant but also achieve a balance of performance and efficiency.For each iteration, the result of slow machines will be abandoned. Then, we discuss the relationship between accuracy and abandon rate. Next we debate the convergence speed of this process. Finally, our experiments demonstrate our idea can dramatically reduce calculation time and be used in many platforms.',
	 'authors': u'Junxiong Wang, Hongzhi Wang, Chenxu Zhao,',
	 'category': u'Computer Science ',
	 'date': '2014-11-24',
	 'pdflink': u'http://arxiv.org/pdf/1411.6358',
	 'subjects': u'Distributed, Parallel, and Cluster Computing (cs.DC)',
	 'title': u'\nA Hybrid Solution to improve Iteration Efficiency in the Distributed  Learning',
	 'urllink': u'http://arxiv.org/abs/1411.6358'}
2015-04-10 15:02:03+0000 [xxu46_10] INFO: Crawled 697 pages (at 1 pages/min), scraped 690 items (at 1 items/min)
2015-04-10 15:02:50+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1411.6340> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 15:02:50+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1411.6340>
	{'abstract': u'While widely acknowledged as highly effective in computer vision, multi-label MRFs with non-convex priors are difficult to optimize. To tackle this, we introduce an algorithm that iteratively approximates the original energy with an appropriately weighted surrogate energy that is easier to minimize. Our algorithm guarantees that the original energy decreases at each iteration. In particular, we consider the scenario where the global minimizer of the weighted surrogate energy can be obtained by a multi-label graph cut algorithm, and show that our algorithm then lets us handle of large variety of non-convex priors. We demonstrate the benefits of our method over state-of-the-art MRF energy minimization techniques on stereo and inpainting problems.',
	 'authors': u'Thalaiyasingam Ajanthan, Richard Hartley, Mathieu Salzmann, Hongdong Li,',
	 'category': u'Computer Science ',
	 'date': '2014-11-24',
	 'pdflink': u'http://arxiv.org/pdf/1411.6340',
	 'subjects': u'Computer Vision and Pattern Recognition (cs.CV)',
	 'title': u'\nIteratively Reweighted Graph Cut for Multi-label MRFs with Non-convex  Priors',
	 'urllink': u'http://arxiv.org/abs/1411.6340'}
2015-04-10 15:03:03+0000 [xxu46_10] INFO: Crawled 698 pages (at 1 pages/min), scraped 691 items (at 1 items/min)
2015-04-10 15:03:35+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1411.6335> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 15:03:35+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1411.6335>
	{'abstract': u'Although SPARQL has been the predominant query language over RDF graphs, some query intentions cannot be well captured by only using SPARQL syntax. On the other hand, the keyword search enjoys widespread usage because of its intuitive way of specifying information needs but suffers from the problem of low precision. To maximize the advantages of both SPARQL and keyword search, we introduce a novel paradigm that combines both of them and propose a hybrid query (called an SK query) that integrates SPARQL and keyword search. In order to answer SK queries efficiently, a structural index is devised, based on a novel integrated query algorithm is proposed. We evaluate our method in large real RDF graphs and experiments demonstrate both effectiveness and efficiency of our method.',
	 'authors': u'Peng Peng, Lei Zou, Dongyan Zhao,',
	 'category': u'Computer Science ',
	 'date': '2014-11-24',
	 'pdflink': u'http://arxiv.org/pdf/1411.6335',
	 'subjects': u'Databases (cs.DB)',
	 'title': u'\nOn The Marriage of SPARQL and Keywords',
	 'urllink': u'http://arxiv.org/abs/1411.6335'}
2015-04-10 15:04:03+0000 [xxu46_10] INFO: Crawled 699 pages (at 1 pages/min), scraped 692 items (at 1 items/min)
2015-04-10 15:04:43+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1411.6328> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 15:04:43+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1411.6328>
	{'abstract': u'MDS codes are erasure-correcting codes that can correct the maximum number of erasures for a given number of redundancy or parity symbols. If an MDS code has parities and no more than erasures occur, then by transmitting all the remaining data in the code, the original information can be recovered. However, it was shown that in order to recover a single symbol erasure, only a fraction of of the information needs to be transmitted. This fraction is called the repair bandwidth (fraction). Explicit code constructions were given in previous works. If we view each symbol in the code as a vector or a column over some field, then the code forms a 2D array and such codes are especially widely used in storage systems. In this paper, we address the following question: given the length of the column , number of parities , can we construct high-rate MDS array codes with optimal repair bandwidth of , whose code length is as long as possible? In this paper, we give code constructions such that the code length is .',
	 'authors': u'Zhiying Wang, Itzhak Tamo, Jehoshua Bruck,',
	 'category': u'Computer Science ',
	 'date': '2014-11-24',
	 'pdflink': u'http://arxiv.org/pdf/1411.6328',
	 'subjects': u'Information Theory (cs.IT)',
	 'title': u'\nExplicit MDS Codes for Optimal Repair Bandwidth',
	 'urllink': u'http://arxiv.org/abs/1411.6328'}
2015-04-10 15:05:03+0000 [xxu46_10] INFO: Crawled 700 pages (at 1 pages/min), scraped 693 items (at 1 items/min)
2015-04-10 15:05:43+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1411.6326> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 15:05:43+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1411.6326>
	{'abstract': u'Cameras provide a rich source of information while being passive, cheap and lightweight for small and medium Unmanned Aerial Vehicles (UAVs). In this work we present the first implementation of receding horizon control, which is widely used in ground vehicles, with monocular vision as the only sensing mode for autonomous UAV flight in dense clutter. We make it feasible on UAVs via a number of contributions: novel coupling of perception and control via relevant and diverse, multiple interpretations of the scene around the robot, leveraging recent advances in machine learning to showcase anytime budgeted cost-sensitive feature selection, and fast non-linear regression for monocular depth prediction. We empirically demonstrate the efficacy of our novel pipeline via real world experiments of more than 2 kms through dense trees with a quadrotor built from off-the-shelf parts. Moreover our pipeline is designed to combine information from other modalities like stereo and lidar as well if available.',
	 'authors': u'Debadeepta Dey, Kumar Shaurya Shankar, Sam Zeng, Rupesh Mehta, M. Talha Agcayazi, Christopher Eriksen, Shreyansh Daftry, Martial Hebert, J. Andrew Bagnell,',
	 'category': u'Computer Science ',
	 'date': '2014-11-24',
	 'pdflink': u'http://arxiv.org/pdf/1411.6326',
	 'subjects': u'Robotics (cs.RO)',
	 'title': u'\nVision and Learning for Deliberative Monocular Cluttered Flight',
	 'urllink': u'http://arxiv.org/abs/1411.6326'}
2015-04-10 15:06:03+0000 [xxu46_10] INFO: Crawled 701 pages (at 1 pages/min), scraped 694 items (at 1 items/min)
2015-04-10 15:06:41+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1411.6320> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 15:06:41+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1411.6320>
	{'abstract': u'Mobility is one of the basic features that define an ad hoc network, an asset that leaves the field free for the nodes to move. The most important aspect of this kind of network turns into a great disadvantage when it comes to commercial applications, take as an example: the automotive networks that allow communication between a groups of vehicles. The ad hoc on-demand distance vector (AODV) routing protocol, designed for mobile ad hoc networks, has two main functions. First, it enables route establishment between a source and a destination node by initiating a route discovery process. Second, it maintains the active routes, which means finding alternative routes in a case of a link failure and deleting routes when they are no longer desired. In a highly mobile network those are demanding tasks to be performed efficiently and accurately. In this paper, we focused in the first point to enhance the local decision of each node in the network by the quantification of the mobility of their neighbours. Quantification is made around RSSI algorithm a well known distance estimation method.',
	 'authors': u'Meryem Saadoune, Abdelmajid Hajami, Hakim Allali,',
	 'category': u'Computer Science ',
	 'date': '2014-11-24',
	 'pdflink': u'http://arxiv.org/pdf/1411.6320',
	 'subjects': u'Networking and Internet Architecture (cs.NI)',
	 'title': u"\nDistance's Quantification Algorithm in AODV Protocol",
	 'urllink': u'http://arxiv.org/abs/1411.6320'}
2015-04-10 15:07:03+0000 [xxu46_10] INFO: Crawled 702 pages (at 1 pages/min), scraped 695 items (at 1 items/min)
2015-04-10 15:07:32+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1411.6317> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 15:07:32+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1411.6317>
	{'abstract': u'We introduce a method for proving lower bounds on the efficacy of semidefinite programming (SDP) relaxations for combinatorial problems. In particular, we show that the cut, TSP, and stable set polytopes on -vertex graphs are not the linear image of the feasible region of any SDP (i.e., any spectrahedron) of dimension less than , for some constant . This result yields the first super-polynomial lower bounds on the semidefinite extension complexity of any explicit family of polytopes. Our results follow from a general technique for proving lower bounds on the positive semidefinite rank of a matrix. To this end, we establish a close connection between arbitrary SDPs and those arising from the sum-of-squares SDP hierarchy. For approximating maximum constraint satisfaction problems, we prove that SDPs of polynomial-size are equivalent in power to those arising from degree- sum-of-squares relaxations. This result implies, for instance, that no family of polynomial-size SDP relaxations can achieve better than a 7/8-approximation for MAX-3-SAT.',
	 'authors': u'James R. Lee, Prasad Raghavendra, David Steurer,',
	 'category': u'Computer Science ',
	 'date': '2014-11-24',
	 'pdflink': u'http://arxiv.org/pdf/1411.6317',
	 'subjects': u'Computational Complexity (cs.CC)',
	 'title': u'\nLower bounds on the size of semidefinite programming relaxations',
	 'urllink': u'http://arxiv.org/abs/1411.6317'}
2015-04-10 15:08:03+0000 [xxu46_10] INFO: Crawled 703 pages (at 1 pages/min), scraped 696 items (at 1 items/min)
2015-04-10 15:08:50+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1411.6308> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 15:08:50+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1411.6308>
	{'abstract': u'Spectral clustering is a fundamental technique in the field of data mining and information processing. Most existing spectral clustering algorithms integrate dimensionality reduction into the clustering process assisted by manifold learning in the original space. However, the manifold in reduced-dimensional subspace is likely to exhibit altered properties in contrast with the original space. Thus, applying manifold information obtained from the original space to the clustering process in a low-dimensional subspace is prone to inferior performance. Aiming to address this issue, we propose a novel convex algorithm that mines the manifold structure in the low-dimensional subspace. In addition, our unified learning process makes the manifold learning particularly tailored for the clustering. Compared with other related methods, the proposed algorithm results in more structured clustering result. To validate the efficacy of the proposed algorithm, we perform extensive experiments on several benchmark datasets in comparison with some state-of-the-art clustering approaches. The experimental results demonstrate that the proposed algorithm has quite promising clustering performance.',
	 'authors': u'Xiaojun Chang, Feiping Nie, Zhigang Ma, Yi Yang, Xiaofang Zhou,',
	 'category': u'Computer Science ',
	 'date': '2014-11-23',
	 'pdflink': u'http://arxiv.org/pdf/1411.6308',
	 'subjects': u'Learning (cs.LG)',
	 'title': u'\nA Convex Formulation for Spectral Shrunk Clustering',
	 'urllink': u'http://arxiv.org/abs/1411.6308'}
2015-04-10 15:09:03+0000 [xxu46_10] INFO: Crawled 704 pages (at 1 pages/min), scraped 697 items (at 1 items/min)
2015-04-10 15:09:52+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1411.6307> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 15:09:52+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1411.6307>
	{'abstract': u'We propose a novel diverse feature selection method based on determinantal point processes (DPPs). Our model enables one to flexibly define diversity based on the covariance of features (similar to orthogonal matching pursuit) or alternatively based on side information. We introduce our approach in the context of Bayesian sparse regression, employing a DPP as a variational approximation to the true spike and slab posterior distribution. We subsequently show how this variational DPP approximation generalizes and extends mean-field approximation, and can be learned efficiently by exploiting the fast sampling properties of DPPs. Our motivating application comes from bioinformatics, where we aim to identify a diverse set of genes whose expression profiles predict a tumor type where the diversity is defined with respect to a gene-gene interaction network. We also explore an application in spatial statistics. In both cases, we demonstrate that the proposed method yields significantly more diverse feature sets than classic sparse methods, without compromising accuracy.',
	 'authors': u'Nematollah Kayhan Batmanghelich, Gerald Quon, Alex Kulesza, Manolis Kellis, Polina Golland, Luke Bornn,',
	 'category': u'Computer Science ',
	 'date': '2014-11-23',
	 'pdflink': u'http://arxiv.org/pdf/1411.6307',
	 'subjects': u'Learning (cs.LG)',
	 'title': u'\nDiversifying Sparsity Using Variational Determinantal Point Processes',
	 'urllink': u'http://arxiv.org/abs/1411.6307'}
2015-04-10 15:10:03+0000 [xxu46_10] INFO: Crawled 705 pages (at 1 pages/min), scraped 698 items (at 1 items/min)
2015-04-10 15:11:03+0000 [xxu46_10] INFO: Crawled 705 pages (at 0 pages/min), scraped 698 items (at 0 items/min)
2015-04-10 15:11:09+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1411.6305> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 15:11:09+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1411.6305>
	{'abstract': u'We study revenue optimization learning algorithms for posted-price auctions with strategic buyers. We analyze a very broad family of monotone regret minimization algorithms for this problem, which includes the previously best known algorithm, and show that no algorithm in that family admits a strategic regret more favorable than . We then introduce a new algorithm that achieves a strategic regret differing from the lower bound only by a factor in , an exponential improvement upon the previous best algorithm. Our new algorithm admits a natural analysis and simpler proofs, and the ideas behind its design are general. We also report the results of empirical evaluations comparing our algorithm with the previous state of the art and show a consistent exponential improvement in several different scenarios.',
	 'authors': u'Mehryar Mohri, Andres Mu\xf1oz Medina,',
	 'category': u'Computer Science ',
	 'date': '2014-11-23',
	 'pdflink': u'http://arxiv.org/pdf/1411.6305',
	 'subjects': u'Learning (cs.LG)',
	 'title': u'\nRevenue Optimization in Posted-Price Auctions with Strategic Buyers',
	 'urllink': u'http://arxiv.org/abs/1411.6305'}
2015-04-10 15:12:03+0000 [xxu46_10] INFO: Crawled 706 pages (at 1 pages/min), scraped 699 items (at 1 items/min)
2015-04-10 15:12:09+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1411.6300> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 15:12:09+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1411.6300>
	{'abstract': u'In a Bayesian network, we wish to evaluate the marginal probability of a query variable, which may be conditioned on the observed values of some evidence variables. Here we first present our "border algorithm," which converts a BN into a directed chain. For the polytrees, we then present in details, with some modifications and within the border algorithm framework, the "revised polytree algorithm" by Peot &amp; Shachter (1991). Finally, we present our "parentless polytree method," which, coupled with the border algorithm, converts any Bayesian network into a polytree, rendering the complexity of our inferences independent of the size of network, and linear with the number of its evidence and query variables. All quantities in this paper have probabilistic interpretations.',
	 'authors': u'Do Le Paul Minh,',
	 'category': u'Computer Science ',
	 'date': '2014-11-23',
	 'pdflink': u'http://arxiv.org/pdf/1411.6300',
	 'subjects': u'Artificial Intelligence (cs.AI)',
	 'title': u'\nDiscrete Bayesian Networks: The Exact Posterior Marginal Distributions',
	 'urllink': u'http://arxiv.org/abs/1411.6300'}
2015-04-10 15:13:03+0000 [xxu46_10] INFO: Crawled 707 pages (at 1 pages/min), scraped 700 items (at 1 items/min)
2015-04-10 15:13:07+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1411.6299> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 15:13:07+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1411.6299>
	{'abstract': u'Halfspaces or linear threshold functions are widely studied in complexity theory, learning theory and algorithm design. In this work we study the natural problem of constructing pseudorandom generators (PRGs) for halfspaces over the sphere, aka spherical caps, which besides being interesting and basic geometric objects, also arise frequently in the analysis of various randomized algorithms (e.g., randomized rounding). We give an explicit PRG which fools spherical caps within error and has an almost optimal seed-length of . For an inverse-polynomially growing error , our generator has a seed-length optimal up to a factor of . The most efficient PRG previously known (due to Kane, 2012) requires a seed-length of in this setting. We also obtain similar constructions to fool halfspaces with respect to the Gaussian distribution. Our construction and analysis are significantly different from previous works on PRGs for halfspaces and build on the iterative dimension reduction ideas of Kane et. al. (2011) and Celis et. al. (2013), the emph from probability theory and explicit constructions of emph based on the seminal work of Bourgain and Gamburd (2011) on expansion in Lie groups.',
	 'authors': u'Pravesh Kothari, Raghu Meka,',
	 'category': u'Computer Science ',
	 'date': '2014-11-23',
	 'pdflink': u'http://arxiv.org/pdf/1411.6299',
	 'subjects': u'Computational Complexity (cs.CC)',
	 'title': u'\nAlmost Optimal Pseudorandom Generators for Spherical Caps',
	 'urllink': u'http://arxiv.org/abs/1411.6299'}
2015-04-10 15:14:03+0000 [xxu46_10] INFO: Crawled 708 pages (at 1 pages/min), scraped 701 items (at 1 items/min)
2015-04-10 15:14:19+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1411.6296> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 15:14:19+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1411.6296>
	{'abstract': u'If a tensor with various symmetries is properly unfolded, then the resulting matrix inherits those symmetries. As tensor computations become increasingly important it is imperative that we develop efficient structure preserving methods for matrices with multiple symmetries. In this paper we consider how to exploit and preserve structure in the pivoted Cholesky factorization when approximating a matrix that is both symmetric () and what we call , or . The latter property means that where is a permutation with the property that if is the vec of a symmetric matrix and if is the vec of a skew-symmetric matrix. Matrices with this structure can arise when an order-4 tensor is unfolded and its elements satisfy This is the case in certain quantum chemistry applications where the tensor entries are electronic repulsion integrals. Our technique involves a closed-form block diagonalization followed by one or two half-sized pivoted Cholesky factorizations. This framework allows for a lazy evaluation feature that is important if the entries in are expensive to compute. In addition to being a structure preserving rank reduction technique, we find that this approach for obtaining the Cholesky factorization reduces the work by up to a factor of 4.',
	 'authors': u'Charles Van Loan, Joseph Vokt,',
	 'category': u'Computer Science ',
	 'date': '2014-11-23',
	 'pdflink': u'http://arxiv.org/pdf/1411.6296',
	 'subjects': u'Numerical Analysis (cs.NA)',
	 'title': u'\nApproximating Matrices with Multiple Symmetries',
	 'urllink': u'http://arxiv.org/abs/1411.6296'}
2015-04-10 15:15:03+0000 [xxu46_10] INFO: Crawled 709 pages (at 1 pages/min), scraped 702 items (at 1 items/min)
2015-04-10 15:15:29+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1411.6279> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 15:15:29+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1411.6279>
	{'abstract': u'We present Dynamic Epistemic Temporal Logic, a framework for reasoning about operations on multi-agent Kripke models that contain a designated temporal relation. These operations are natural extensions of the well-known "action models" from Dynamic Epistemic Logic. Our "temporal action models" may be used to define a number of informational actions that can modify the "objective" temporal structure of a model along with the agents\' basic and higher-order knowledge and beliefs about this structure, including their beliefs about the time. In essence, this approach provides one way to extend the domain of action model-style operations from atemporal Kripke models to temporal Kripke models in a manner that allows actions to control the flow of time. We present a number of examples to illustrate the subtleties involved in interpreting the effects of our extended action models on temporal Kripke models. We also study preservation of important epistemic-temporal properties of temporal Kripke models under temporal action model-induced operations, provide complete axiomatizations for two theories of temporal action models, and connect our approach with previous work on time in Dynamic Epistemic Logic.',
	 'authors': u'Bryan Renne, Joshua Sack, Audrey Yap,',
	 'category': u'Computer Science ',
	 'date': '2014-11-23',
	 'pdflink': u'http://arxiv.org/pdf/1411.6279',
	 'subjects': u'Logic in Computer Science (cs.LO)',
	 'title': u'\nLogics of Temporal-Epistemic Actions',
	 'urllink': u'http://arxiv.org/abs/1411.6279'}
2015-04-10 15:16:03+0000 [xxu46_10] INFO: Crawled 710 pages (at 1 pages/min), scraped 703 items (at 1 items/min)
2015-04-10 15:16:32+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1411.6276> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 15:16:32+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1411.6276>
	{'abstract': u'Understanding the epidemic dynamics, and finding out efficient techniques to control it, is a challenging issue. A lot of research has been done on targeted immunization strategies, exploiting various global network topological properties. However, in practice, information about the global structure of the contact network may not be available. Therefore, immunization strategies that can deal with a limited knowledge of the network structure are required. In this paper, we propose targeted immunization strategies that require information only at the community level. Results of our investigations on the SIR epidemiological model, using a realistic synthetic benchmark with controlled community structure, show that the community structure plays an important role in the epidemic dynamics. An extensive comparative evaluation demonstrates that the proposed strategies are as efficient as the most influential global centrality based immunization strategies, despite the fact that they use a limited amount of information. Furthermore, they outperform alternative local strategies, which are agnostic about the network structure, and make decisions based on random walks.',
	 'authors': u'Naveen Gupta, Anurag Singh, Hocine Cherifi,',
	 'category': u'Computer Science ',
	 'date': '2014-11-23',
	 'pdflink': u'http://arxiv.org/pdf/1411.6276',
	 'subjects': u'Social and Information Networks (cs.SI)',
	 'title': u'\nCommunity-based Immunization Strategies for Epidemic Control',
	 'urllink': u'http://arxiv.org/abs/1411.6276'}
2015-04-10 15:17:03+0000 [xxu46_10] INFO: Crawled 711 pages (at 1 pages/min), scraped 704 items (at 1 items/min)
2015-04-10 15:17:41+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1411.6275> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 15:17:41+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1411.6275>
	{'abstract': u"Interfaces based on projection screens have become increasingly more popular in recent years, mainly due to the large screen size and resolution that they provide, as well as their stereo-vision capabilities. This work shows a local method for real-time detection of non-stationary photometric perturbations in projected images by means of computer vision techniques. The method is based on the computation of differences between the images in the projector's frame buffer and the corresponding images on the projection screen observed by the camera. It is robust under spatial variations in the intensity of light emitted by the projector on the projection surface and also robust under stationary photometric perturbations caused by external factors. Moreover, we describe the experiments carried out to show the reliability of the method.",
	 'authors': u'Miguel Casta\xf1eda-Garay, Oscar Belmonte-Fern\xe1ndez, Hebert P\xe9rez-Ros\xe9s, Antonio Diaz-Tula,',
	 'category': u'Computer Science ',
	 'date': '2014-11-23',
	 'pdflink': u'http://arxiv.org/pdf/1411.6275',
	 'subjects': u'Computer Vision and Pattern Recognition (cs.CV)',
	 'title': u'\nDetection of Non-Stationary Photometric Perturbations on Projection  Screens',
	 'urllink': u'http://arxiv.org/abs/1411.6275'}
2015-04-10 15:18:03+0000 [xxu46_10] INFO: Crawled 712 pages (at 1 pages/min), scraped 705 items (at 1 items/min)
2015-04-10 15:18:58+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1411.6273> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 15:18:58+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1411.6273>
	{'abstract': u'In many simulation studies involving networks there is the need to rely on a sample network to perform the simulation experiments. In many cases, real network data is not available due to privacy concerns. In that case we can recourse to synthetic data sets with similar properties to the real data. In this paper we discuss the problem of generating synthetic data sets for a certain kind of online social network, for simulation purposes. Some popular online social networks, such as LinkedIn and ResearchGate, allow user endorsements for specific skills. For each particular skill, the endorsements give rise to a directed subgraph of the corresponding network, where the nodes correspond to network members or users, and the arcs represent endorsement relations. Modelling these endorsement digraphs can be done by formulating an optimization problem, which is amenable to different heuristics. Our construction method consists of two stages: The first one simulates the growth of the network, and the second one solves the aforementioned optimization problem to construct the endorsements.',
	 'authors': u'Hebert P\xe9rez-Ros\xe9s, Francesc Seb\xe9,',
	 'category': u'Computer Science ',
	 'date': '2014-11-23',
	 'pdflink': u'http://arxiv.org/pdf/1411.6273',
	 'subjects': u'Social and Information Networks (cs.SI)',
	 'title': u'\nSynthetic Generation of Social Network Data With Endorsements',
	 'urllink': u'http://arxiv.org/abs/1411.6273'}
2015-04-10 15:19:03+0000 [xxu46_10] INFO: Crawled 713 pages (at 1 pages/min), scraped 706 items (at 1 items/min)
2015-04-10 15:20:03+0000 [xxu46_10] INFO: Crawled 713 pages (at 0 pages/min), scraped 706 items (at 0 items/min)
2015-04-10 15:20:17+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1411.6272> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 15:20:17+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1411.6272>
	{'abstract': u'In this paper we study the identification of a time-varying linear system whose response is a weighted superposition of time and frequency shifted versions of the input signal. This problem arises in a multitude of applications such as wireless communications and radar imaging. Due to practical constraints, the input signal has finite bandwidth B, and the received signal is observed over a finite time interval of length T only. This gives rise to a time and frequency resolution of 1/B and 1/T. We show that this resolution limit can be overcome, i.e., we can recover the exact (continuous) time-frequency shifts and the corresponding attenuation factors, by essentially solving a simple convex optimization problem. This result holds provided that the distance between the time-frequency shifts is at least 2.37/B and 2.37/T, in time and frequency. Furthermore, this result allows the total number of time-frequency shifts to be linear (up to a log-factor) in BT, the dimensionality of the response of the system. More generally, we show that we can estimate the time-frequency components of a signal that is S-sparse in the continuous dictionary of time-frequency shifts of a random (window) function, from a number of measurements, that is linear (up to a log-factor) in S.',
	 'authors': u'Reinhard Heckel, Veniamin I. Morgenshtern, Mahdi Soltanolkotabi,',
	 'category': u'Computer Science ',
	 'date': '2014-11-23',
	 'pdflink': u'http://arxiv.org/pdf/1411.6272',
	 'subjects': u'Information Theory (cs.IT)',
	 'title': u'\nSuper-Resolution Radar',
	 'urllink': u'http://arxiv.org/abs/1411.6272'}
2015-04-10 15:21:03+0000 [xxu46_10] INFO: Crawled 714 pages (at 1 pages/min), scraped 707 items (at 1 items/min)
2015-04-10 15:21:19+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1411.6262> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 15:21:20+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1411.6262>
	{'abstract': u'Consider the -th integrator , where , , is the -th Jordan block and . We provide easily implementable state feedback laws which not only render the closed-loop system globally asymptotically stable but also are finite-gain -stabilizing with arbitrarily small gain. These -stabilizing state feedbacks are built from homogeneous feedbacks appearing in finite-time stabilization of linear systems. We also provide additional -stabilization results for the case of both internal and external disturbances of the -th integrator, namely for the perturbed system where and .',
	 'authors': u'Yacine Chitour, Mohamed Harmouche, Salah Laghrouche,',
	 'category': u'Computer Science ',
	 'date': '2014-11-23',
	 'pdflink': u'http://arxiv.org/pdf/1411.6262',
	 'subjects': u'Systems and Control (cs.SY)',
	 'title': u'\n$L_p$-stabilization of integrator chains subject to input saturation  using Lyapunov-based homogeneous design',
	 'urllink': u'http://arxiv.org/abs/1411.6262'}
2015-04-10 15:22:03+0000 [xxu46_10] INFO: Crawled 715 pages (at 1 pages/min), scraped 708 items (at 1 items/min)
2015-04-10 15:22:36+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1411.6243> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 15:22:36+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1411.6243>
	{'abstract': u'While there are many studies on weight regularization, the study on structure regularization is rare. Many existing systems on structured prediction focus on increasing the level of structural dependencies within the model. However, this trend could have been misdirected, because our study suggests that complex structures are actually harmful to generalization ability in structured prediction. To control structure-based overfitting, we propose a structure regularization framework via emph, which decomposes training samples into mini-samples with simpler structures, deriving a model with better generalization power. We show both theoretically and empirically that structure regularization can effectively control overfitting risk and lead to better accuracy. As a by-product, the proposed method can also substantially accelerate the training speed. The method and the theoretical results can apply to general graphical models with arbitrary structures. Experiments on well-known tasks demonstrate that our method can easily beat the benchmark systems on those highly-competitive tasks, achieving state-of-the-art accuracies yet with substantially faster training speed.',
	 'authors': u'Xu Sun,',
	 'category': u'Computer Science ',
	 'date': '2014-11-23',
	 'pdflink': u'http://arxiv.org/pdf/1411.6243',
	 'subjects': u'Learning (cs.LG)',
	 'title': u'\nStructure Regularization for Structured Prediction: Theories and  Experiments',
	 'urllink': u'http://arxiv.org/abs/1411.6243'}
2015-04-10 15:23:03+0000 [xxu46_10] INFO: Crawled 716 pages (at 1 pages/min), scraped 709 items (at 1 items/min)
2015-04-10 15:23:41+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1411.6241> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 15:23:41+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1411.6241>
	{'abstract': u"Spectral clustering is a key research topic in the field of machine learning and data mining. Most of the existing spectral clustering algorithms are built upon Gaussian Laplacian matrices, which are sensitive to parameters. We propose a novel parameter free, distance consistent Locally Linear Embedding. The proposed distance consistent LLE promises that edges between closer data points have greater weight.Furthermore, we propose a novel improved spectral clustering via embedded label propagation. Our algorithm is built upon two advancements of the state of the art:1) label propagation,which propagates a node 's labels to neighboring nodes according to their proximity; and 2) manifold learning, which has been widely used in its capacity to leverage the manifold structure of data points. First we perform standard spectral clustering on original data and assign each cluster to k nearest data points. Next, we propagate labels through dense, unlabeled data regions. Extensive experiments with various datasets validate the superiority of the proposed algorithm compared to current state of the art spectral algorithms.",
	 'authors': u'Xiaojun Chang, Feiping Nie, Yi Yang, Heng Huang,',
	 'category': u'Computer Science ',
	 'date': '2014-11-23',
	 'pdflink': u'http://arxiv.org/pdf/1411.6241',
	 'subjects': u'Learning (cs.LG)',
	 'title': u'\nImproved Spectral Clustering via Embedded Label Propagation',
	 'urllink': u'http://arxiv.org/abs/1411.6241'}
2015-04-10 15:24:03+0000 [xxu46_10] INFO: Crawled 717 pages (at 1 pages/min), scraped 710 items (at 1 items/min)
2015-04-10 15:24:39+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1411.6235> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 15:24:39+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1411.6235>
	{'abstract': u'Clustering is an effective technique in data mining to generate groups that are the matter of interest. Among various clustering approaches, the family of k-means algorithms and min-cut algorithms gain most popularity due to their simplicity and efficacy. The classical k-means algorithm partitions a number of data points into several subsets by iteratively updating the clustering centers and the associated data points. By contrast, a weighted undirected graph is constructed in min-cut algorithms which partition the vertices of the graph into two sets. However, existing clustering algorithms tend to cluster minority of data points into a subset, which shall be avoided when the target dataset is balanced. To achieve more accurate clustering for balanced dataset, we propose to leverage exclusive lasso on k-means and min-cut to regulate the balance degree of the clustering results. By optimizing our objective functions that build atop the exclusive lasso, we can make the clustering result as much balanced as possible. Extensive experiments on several large-scale datasets validate the advantage of the proposed algorithms compared to the state-of-the-art clustering algorithms.',
	 'authors': u'Xiaojun Chang, Feiping Nie, Zhigang Ma, Yi Yang,',
	 'category': u'Computer Science ',
	 'date': '2014-11-23',
	 'pdflink': u'http://arxiv.org/pdf/1411.6235',
	 'subjects': u'Learning (cs.LG)',
	 'title': u'\nBalanced k-Means and Min-Cut Clustering',
	 'urllink': u'http://arxiv.org/abs/1411.6235'}
2015-04-10 15:25:03+0000 [xxu46_10] INFO: Crawled 718 pages (at 1 pages/min), scraped 711 items (at 1 items/min)
2015-04-10 15:25:46+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1411.6233> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 15:25:46+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1411.6233>
	{'abstract': u'Principal component analysis (PCA) has been widely applied to dimensionality reduction and data pre-processing for different applications in engineering, biology and social science. Classical PCA and its variants seek for linear projections of the original variables to obtain a low dimensional feature representation with maximal variance. One limitation is that it is very difficult to interpret the results of PCA. In addition, the classical PCA is vulnerable to certain noisy data. In this paper, we propose a convex sparse principal component analysis (CSPCA) algorithm and apply it to feature analysis. First we show that PCA can be formulated as a low-rank regression optimization problem. Based on the discussion, the l 2 , 1 -norm minimization is incorporated into the objective function to make the regression coefficients sparse, thereby robust to the outliers. In addition, based on the sparse model used in CSPCA, an optimal weight is assigned to each of the original feature, which in turn provides the output with good interpretability. With the output of our CSPCA, we can effectively analyze the importance of each feature under the PCA criteria. The objective function is convex, and we propose an iterative algorithm to optimize it. We apply the CSPCA algorithm to feature selection and conduct extensive experiments on six different benchmark datasets. Experimental results demonstrate that the proposed algorithm outperforms state-of-the-art unsupervised feature selection algorithms.',
	 'authors': u'Xiaojun Chang, Feiping Nie, Yi Yang, Heng Huang,',
	 'category': u'Computer Science ',
	 'date': '2014-11-23',
	 'pdflink': u'http://arxiv.org/pdf/1411.6233',
	 'subjects': u'Learning (cs.LG)',
	 'title': u'\nA Convex Sparse PCA for Feature Analysis',
	 'urllink': u'http://arxiv.org/abs/1411.6233'}
2015-04-10 15:26:03+0000 [xxu46_10] INFO: Crawled 719 pages (at 1 pages/min), scraped 712 items (at 1 items/min)
2015-04-10 15:26:58+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1411.6232> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 15:26:58+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1411.6232>
	{'abstract': u'In this paper, we propose a novel semi-supervised feature selection framework by mining correlations among multiple tasks and apply it to different multimedia applications. Instead of independently computing the importance of features for each task, our algorithm leverages shared knowledge from multiple related tasks, thus, improving the performance of feature selection. Note that we build our algorithm on assumption that different tasks share common structures. The proposed algorithm selects features in a batch mode, by which the correlations between different features are taken into consideration. Besides, considering the fact that labeling a large amount of training data in real world is both time-consuming and tedious, we adopt manifold learning which exploits both labeled and unlabeled training data for feature space analysis. Since the objective function is non-smooth and difficult to solve, we propose an iterative algorithm with fast convergence. Extensive experiments on different applications demonstrate that our algorithm outperforms other state-of-the-art feature selection algorithms.',
	 'authors': u'Xiaojun Chang, Yi Yang,',
	 'category': u'Computer Science ',
	 'date': '2014-11-23',
	 'pdflink': u'http://arxiv.org/pdf/1411.6232',
	 'subjects': u'Learning (cs.LG)',
	 'title': u'\nSemi-supervised Feature Analysis by Mining Correlations among Multiple  Tasks',
	 'urllink': u'http://arxiv.org/abs/1411.6232'}
2015-04-10 15:27:03+0000 [xxu46_10] INFO: Crawled 720 pages (at 1 pages/min), scraped 713 items (at 1 items/min)
2015-04-10 15:28:03+0000 [xxu46_10] INFO: Crawled 720 pages (at 0 pages/min), scraped 713 items (at 0 items/min)
2015-04-10 15:28:17+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1411.6231> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 15:28:17+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1411.6231>
	{'abstract': u'In many real-world applications, data are represented by matrices or high-order tensors. Despite the promising performance, the existing two-dimensional discriminant analysis algorithms employ a single projection model to exploit the discriminant information for projection, making the model less flexible. In this paper, we propose a novel Compound Rank-k Projection (CRP) algorithm for bilinear analysis. CRP deals with matrices directly without transforming them into vectors, and it therefore preserves the correlations within the matrix and decreases the computation complexity. Different from the existing two dimensional discriminant analysis algorithms, objective function values of CRP increase monotonically.In addition, CRP utilizes multiple rank-k projection models to enable a larger search space in which the optimal solution can be found. In this way, the discriminant ability is enhanced.',
	 'authors': u'Xiaojun Chang, Haoquan Shen, Feiping Nie, Sen Wang, Yi Yang, Xiaofang Zhou,',
	 'category': u'Computer Science ',
	 'date': '2014-11-23',
	 'pdflink': u'http://arxiv.org/pdf/1411.6231',
	 'subjects': u'Learning (cs.LG)',
	 'title': u'\nCompound Rank-k Projections for Bilinear Analysis',
	 'urllink': u'http://arxiv.org/abs/1411.6231'}
2015-04-10 15:29:03+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1411.6228> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 15:29:03+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1411.6228>
	{'abstract': u'We are interested in inferring object segmentation by leveraging only object class information, and by considering only minimal priors on the object segmentation task. This problem could be viewed as a kind of weakly supervised segmentation task, and naturally fits the Multiple Instance Learning (MIL) framework: every training image is known to have (or not) at least one pixel corresponding to the image class label, and the segmentation task can be rewritten as inferring the pixels belonging to the class of the object (given one image, and its object class). We propose a Convolutional Neural Network-based model, which is constrained during training to put more weight on pixels which are important for classifying the image. We show that at test time, the model has learned to discriminate the right pixels well enough, such that it performs very well on an existing segmentation benchmark, by adding only few smoothing priors. Our system is trained using a subset of the Imagenet dataset and the segmentation experiments are performed on the challenging Pascal VOC dataset (with no fine-tuning of the model on Pascal VOC). Our model beats the state of the art results in weakly supervised object segmentation task by a large margin. We also compare the performance of our model with state of the art fully-supervised segmentation approaches.',
	 'authors': u'Pedro O. Pinheiro, Ronan Collobert,',
	 'category': u'Computer Science ',
	 'date': '2014-11-23',
	 'pdflink': u'http://arxiv.org/pdf/1411.6228',
	 'subjects': u'Computer Vision and Pattern Recognition (cs.CV)',
	 'title': u'\nWeakly Supervised Semantic Segmentation with Convolutional Networks',
	 'urllink': u'http://arxiv.org/abs/1411.6228'}
2015-04-10 15:29:03+0000 [xxu46_10] INFO: Crawled 722 pages (at 2 pages/min), scraped 715 items (at 2 items/min)
2015-04-10 15:30:03+0000 [xxu46_10] INFO: Crawled 722 pages (at 0 pages/min), scraped 715 items (at 0 items/min)
2015-04-10 15:30:18+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1411.6224> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 15:30:18+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1411.6224>
	{'abstract': u'Apriori Algorithm is one of the most important algorithm which is used to extract frequent itemsets from large database and get the association rule for discovering the knowledge. It basically requires two important things: minimum support and minimum confidence. First, we check whether the items are greater than or equal to the minimum support and we find the frequent itemsets respectively. Secondly, the minimum confidence constraint is used to form association rules. Based on this algorithm, this paper indicates the limitation of the original Apriori algorithm of wasting time and space for scanning the whole database searching on the frequent itemsets, and present an improvement on Apriori.',
	 'authors': u'Akshita Bhandari, Ashutosh Gupta, Debasis Das,',
	 'category': u'Computer Science ',
	 'date': '2014-11-23',
	 'pdflink': u'http://arxiv.org/pdf/1411.6224',
	 'subjects': u'Databases (cs.DB)',
	 'title': u'\nImprovised Apriori Algorithm using frequent pattern tree for real time  applications in data mining',
	 'urllink': u'http://arxiv.org/abs/1411.6224'}
2015-04-10 15:31:03+0000 [xxu46_10] INFO: Crawled 723 pages (at 1 pages/min), scraped 716 items (at 1 items/min)
2015-04-10 15:31:31+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1411.6206> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 15:31:31+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1411.6206>
	{'abstract': u'It has been recently shown that incorporating priori knowledge significantly improves the performance of basic compressive sensing based approaches. We have managed to successfully exploit this idea for recovering a matrix as a summation of a Low-rank and a Sparse component from compressive measurements. When applied to the problem of construction of 4D Cardiac MR image sequences in real-time from highly under-sampled space data, our proposed method achieves superior reconstruction quality compared to the other state-of-the-art methods.',
	 'authors': u'Dornoosh Zonoobi, Shahrooz Faghih Roohi, Ashraf A. Kassim,',
	 'category': u'Computer Science ',
	 'date': '2014-11-23',
	 'pdflink': u'http://arxiv.org/pdf/1411.6206',
	 'subjects': u'Computer Vision and Pattern Recognition (cs.CV)',
	 'title': u'\nLow-Rank and Sparse Matrix Decomposition with a-priori knowledge for  Dynamic 3D MRI reconstruction',
	 'urllink': u'http://arxiv.org/abs/1411.6206'}
2015-04-10 15:32:03+0000 [xxu46_10] INFO: Crawled 724 pages (at 1 pages/min), scraped 717 items (at 1 items/min)
2015-04-10 15:32:49+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1411.6202> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 15:32:49+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1411.6202>
	{'abstract': u'It has been widely recognized that the performance of a multi-agent system is highly affected by its organization. A large scale system may have billions of possible ways of organization, which makes it impractical to find an optimal choice of organization using exhaustive search methods. In this paper, we propose a genetic algorithm aided optimization scheme for designing hierarchical structures of multi-agent systems. We introduce a novel algorithm, called the hierarchical genetic algorithm, in which hierarchical crossover with a repair strategy and mutation of small perturbation are used. The phenotypic hierarchical structure space is translated to the genome-like array representation space, which makes the algorithm genetic-operator-literate. A case study with 10 scenarios of a hierarchical information retrieval model is provided. Our experiments have shown that competitive baseline structures which lead to the optimal organization in terms of utility can be found by the proposed algorithm during the evolutionary search. Compared with the traditional genetic operators, the newly introduced operators produced better organizations of higher utility more consistently in a variety of test cases. The proposed algorithm extends of the search processes of the state-of-the-art multi-agent organization design methodologies, and is more computationally efficient in a large search space.',
	 'authors': u'Zhiqi Shen, Ling Yu, Han Yu,',
	 'category': u'Computer Science ',
	 'date': '2014-11-23',
	 'pdflink': u'http://arxiv.org/pdf/1411.6202',
	 'subjects': u'Multiagent Systems (cs.MA)',
	 'title': u'\nAn Evolutionary Approach for Optimizing Hierarchical Multi-Agent System  Organization',
	 'urllink': u'http://arxiv.org/abs/1411.6202'}
2015-04-10 15:33:03+0000 [xxu46_10] INFO: Crawled 725 pages (at 1 pages/min), scraped 718 items (at 1 items/min)
2015-04-10 15:33:51+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1411.6201> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 15:33:51+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1411.6201>
	{'abstract': u"Agile Software Development (ASD) methodology has become widely used in the industry. Understanding the challenges facing software engineering students is important to designing effective training methods to equip students with proper skills required for effectively using the ASD techniques. Existing empirical research mostly focused on eXtreme Programming (XP) based ASD methodologies. There is a lack of empirical studies about Scrum-based ASD programming which has become the most popular agile methodology among industry practitioners. In this paper, we present empirical findings regarding the aspects of task allocation decision-making, collaboration, and team morale related to the Scrum ASD process which have not yet been well studied by existing research. We draw our findings from a 12 week long course work project in 2014 involving 125 undergraduate software engineering students from a renowned university working in 21 Scrum teams. Instead of the traditional survey or interview based methods, which suffer from limitations in scale and level of details, we obtain fine grained data through logging students' activities in our online agile project management (APM) platform - HASE. During this study, the platform logged over 10,000 ASD activities. Deviating from existing preconceptions, our results suggest negative correlations between collaboration and team performance as well as team morale.",
	 'authors': u'Jun Lin, Han Yu, Zhiqi Shen,',
	 'category': u'Computer Science ',
	 'date': '2014-11-23',
	 'pdflink': u'http://arxiv.org/pdf/1411.6201',
	 'subjects': u'Software Engineering (cs.SE)',
	 'title': u'\nAn Empirical Analysis of Task Allocation in Scrum-based Agile  Programming',
	 'urllink': u'http://arxiv.org/abs/1411.6201'}
2015-04-10 15:34:03+0000 [xxu46_10] INFO: Crawled 726 pages (at 1 pages/min), scraped 719 items (at 1 items/min)
2015-04-10 15:34:56+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1411.6197> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 15:34:56+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1411.6197>
	{'abstract': u"For software development companies, one of the most important objectives is to identify and acquire talented software engineers in order to maintain a skilled team that can produce competitive products. Traditional approaches for finding talented young software engineers are mainly through programming contests of various forms which mostly test participants' programming skills. However, successful software engineering in practice requires a wider range of skills from team members including analysis, design, programming, testing, communication, collaboration, and self-management, etc. In this paper, we explore potential ways to identify talented software engineering students in a data-driven manner through an Agile Project Management (APM) platform. Through our proposed HASE online APM tool, we conducted a study involving 21 Scrum teams consisting of over 100 undergraduate software engineering students in multi-week coursework projects in 2014. During this study, students performed over 10,000 ASD activities logged by HASE. We demonstrate the possibility and potentials of this new research direction, and discuss its implications for software engineering education and industry recruitment.",
	 'authors': u'Jun Lin, Han Yu, Zhiqi Shen,',
	 'category': u'Computer Science ',
	 'date': '2014-11-23',
	 'pdflink': u'http://arxiv.org/pdf/1411.6197',
	 'subjects': u'Software Engineering (cs.SE)',
	 'title': u'\nIdentifying Talented Software Engineering Students through Data-driven  Skill Assessment',
	 'urllink': u'http://arxiv.org/abs/1411.6197'}
2015-04-10 15:35:03+0000 [xxu46_10] INFO: Crawled 727 pages (at 1 pages/min), scraped 720 items (at 1 items/min)
2015-04-10 15:36:03+0000 [xxu46_10] INFO: Crawled 727 pages (at 0 pages/min), scraped 720 items (at 0 items/min)
2015-04-10 15:36:05+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1411.6191> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 15:36:05+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1411.6191>
	{'abstract': u"Error backpropagation is an extremely effective algorithm for assigning credit in artificial neural networks. However, weight updates under Backprop depend on lengthy recursive computations and require separate output and error messages -- features not shared by biological neurons, that are perhaps unnecessary. In this paper, we revisit Backprop and the credit assignment problem. We first decompose Backprop into a collection of interacting learning algorithms; provide regret bounds on the performance of these sub-algorithms; and factorize Backprop's error signals. Using these results, we derive a new credit assignment algorithm for nonparametric regression, Kickback, that is significantly simpler than Backprop. Finally, we provide a sufficient condition for Kickback to follow error gradients, and show that Kickback matches Backprop's performance on real-world regression benchmarks.",
	 'authors': u'David Balduzzi, Hastagiri Vanchinathan, Joachim Buhmann,',
	 'category': u'Computer Science ',
	 'date': '2014-11-23',
	 'pdflink': u'http://arxiv.org/pdf/1411.6191',
	 'subjects': u'Learning (cs.LG)',
	 'title': u"\nKickback cuts Backprop's red-tape: Biologically plausible credit  assignment in neural networks",
	 'urllink': u'http://arxiv.org/abs/1411.6191'}
2015-04-10 15:37:02+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1411.6189> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 15:37:02+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1411.6189>
	{'abstract': u'In the past few decades, the world has witnessed a rapid growth in mobile communication and reaped great benefits from it. Even though the fourth generation (4G) mobile communication system is just being deployed worldwide, proliferating mobile demands call for newer wireless communication technologies with even better performance. Consequently, the fifth generation (5G) system is already emerging in the research field. However, simply evolving the current mobile networks can hardly meet such great expectations, because over the years the infrastructures have generally become ossified, closed, and vertically constructed. Aiming to establish a new paradigm for 5G mobile networks, in this article, we propose a cross-layer software-defined 5G network architecture. By jointly considering both the network layer and the physical layer together, we establish the two software-defined programmable components, the control plane and the cloud computing pool, which enable an effective control of the mobile network from the global perspective and benefit technological innovations. Specifically, by the cross-layer design for software-defining, the logically centralized and programmable control plane abstracts the control functions from the network layer down to the physical layer, through which we achieve the fine-grained controlling of mobile network, while the cloud computing pool provides powerful computing capability to implement the baseband data processing of multiple heterogeneous networks. We discuss the main challenges of our architecture, including the fine-grained control strategies, network virtualization, and programmability. The architecture significantly benefits the convergence towards heterogeneous networks and it enables much more controllable, programmable and evolvable mobile networks.',
	 'authors': u'Mao Yang, Yong Li, Long Hu, Bo Li, Depeng Jin, Sheng Chen, Zhongjiang Yan,',
	 'category': u'Computer Science ',
	 'date': '2014-11-23',
	 'pdflink': u'http://arxiv.org/pdf/1411.6189',
	 'subjects': u'Networking and Internet Architecture (cs.NI)',
	 'title': u'\nCross-Layer Software-Defined 5G Network',
	 'urllink': u'http://arxiv.org/abs/1411.6189'}
2015-04-10 15:37:03+0000 [xxu46_10] INFO: Crawled 729 pages (at 2 pages/min), scraped 722 items (at 2 items/min)
2015-04-10 15:37:39+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1411.6188> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 15:37:39+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1411.6188>
	{'abstract': u'We propose a secure data aggregation (SDA) framework for mobile sensor networks whose topology changes dynamically with time. The SDA framework (designed to be resilient to both insider and outsider attacks) comprises of a pair-wise key establishment mechanism run along the edges of a data gathering tree and a distributed trust evaluation model that is tightly integrated with the data aggregation process itself. If an aggregator node already shares a secret key with its child node, the two nodes locally coordinate to refresh and establish a new pair-wise secret key; otherwise, the aggregator node requests the sink to send a seed-secret key message that is used as the basis to establish a new pair-wise secret key. The trust evaluation model uses the two-sided Grubbs test to identify outlier data in the periodic beacons collected from the child nodes (neighbor) nodes. Once the estimated trust score for a neighbor node falls below a threshold, the sensor node locally classifies its neighbor node as a Compromised or Faulty (CF) node, and discards the data or aggregated data received from the CF node. This way, the erroneous data generated by the CF nodes could be filtered at various levels of the data gathering tree and are prevented from reaching the root node (sink node). Finally, we assess the effectiveness of our trust evaluation model through a comprehensive simulation study.',
	 'authors': u'Natarajan Meghanathan,',
	 'category': u'Computer Science ',
	 'date': '2014-11-23',
	 'pdflink': u'http://arxiv.org/pdf/1411.6188',
	 'subjects': u'Networking and Internet Architecture (cs.NI)',
	 'title': u'\nA Pair-wise Key Distribution Mechanism and Distributed Trust Evaluation  Model for Secure Data Aggregation in Mobile Sensor Networks',
	 'urllink': u'http://arxiv.org/abs/1411.6188'}
2015-04-10 15:38:03+0000 [xxu46_10] INFO: Crawled 730 pages (at 1 pages/min), scraped 723 items (at 1 items/min)
2015-04-10 15:38:48+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1411.6186> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 15:38:48+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1411.6186>
	{'abstract': u'We consider the problem of morphing between two planar drawings of the same triangulated graph, maintaining straight-line planarity. A paper in SODA 2013 gave a morph that consists of steps where each step is a linear morph that moves each of the vertices in a straight line at uniform speed. However, their method imitates edge contractions so the grid size of the intermediate drawings is not bounded and the morphs are not good for visualization purposes. Using Schnyder embeddings, we are able to morph in linear morphing steps and improve the grid size to for a significant class of drawings of triangulations, namely the class of weighted Schnyder drawings. The morphs are visually attractive. Our method involves implementing the basic "flip" operations of Schnyder woods as linear morphs.',
	 'authors': u'Fidel Barrera-Cruz, Penny Haxell, Anna Lubiw,',
	 'category': u'Computer Science ',
	 'date': '2014-11-23',
	 'pdflink': u'http://arxiv.org/pdf/1411.6186',
	 'subjects': u'Computational Geometry (cs.CG)',
	 'title': u'\nMorphing Schnyder drawings of planar triangulations',
	 'urllink': u'http://arxiv.org/abs/1411.6186'}
2015-04-10 15:39:03+0000 [xxu46_10] INFO: Crawled 731 pages (at 1 pages/min), scraped 724 items (at 1 items/min)
2015-04-10 15:39:51+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1411.6185> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 15:39:51+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1411.6185>
	{'abstract': u"Alamdari et al. showed that given two straight-line planar drawings of a graph, there is a morph between them that preserves planarity and consists of a polynomial number of steps where each step is a emph that moves each vertex at constant speed along a straight line. An important step in their proof consists of converting a emph (in which contractions are allowed) to a true morph. Here we introduce the notion of emph step, where the vertices move along lines that all have the same direction. Our main result is to show that any planarity preserving pseudo-morph consisting of unidirectional steps and contraction of low degree vertices can be turned into a true morph without increasing the number of steps. Using this, we strengthen Alamdari et al.'s result to use only unidirectional morphs, and in the process we simplify the proof.",
	 'authors': u'Fidel Barrera-Cruz, Penny Haxell, Anna Lubiw,',
	 'category': u'Computer Science ',
	 'date': '2014-11-23',
	 'pdflink': u'http://arxiv.org/pdf/1411.6185',
	 'subjects': u'Computational Geometry (cs.CG)',
	 'title': u'\nMorphing Planar Graph Drawings with Unidirectional Moves',
	 'urllink': u'http://arxiv.org/abs/1411.6185'}
2015-04-10 15:40:03+0000 [xxu46_10] INFO: Crawled 732 pages (at 1 pages/min), scraped 725 items (at 1 items/min)
2015-04-10 15:41:03+0000 [xxu46_10] INFO: Crawled 732 pages (at 0 pages/min), scraped 725 items (at 0 items/min)
2015-04-10 15:41:11+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1411.6172> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 15:41:11+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1411.6172>
	{'abstract': u'We study the problem of broadcasting packets in wireless networks. At each time slot, a network controller activates non-interfering links and forwards packets to all nodes at a common rate; the maximum rate is referred to as the broadcast capacity of the wireless network. Existing policies achieve the broadcast capacity by balancing traffic over a set of spanning trees, which are difficult to maintain in a large and time-varying wireless network. We propose a new dynamic algorithm that achieves the broadcast capacity when the underlying network topology is a directed acyclic graph (DAG). This algorithm utilizes local queue-length information, does not use any global topological structures such as spanning trees, and uses the idea of in-order packet delivery to all network nodes. Although the in-order packet delivery constraint leads to degraded throughput in cyclic graphs, we show that it is throughput optimal in DAGs and can be exploited to simplify the design and analysis of optimal algorithms. Our simulation results show that the proposed algorithm has superior delay performance as compared to tree-based approaches.',
	 'authors': u'Abhishek Sinha, Georgios Paschos, Chih-ping Li, Eytan Modiano,',
	 'category': u'Computer Science ',
	 'date': '2014-11-22',
	 'pdflink': u'http://arxiv.org/pdf/1411.6172',
	 'subjects': u'Networking and Internet Architecture (cs.NI)',
	 'title': u'\nThroughput-Optimal Broadcast on Directed Acyclic Graphs',
	 'urllink': u'http://arxiv.org/abs/1411.6172'}
2015-04-10 15:42:03+0000 [xxu46_10] INFO: Crawled 733 pages (at 1 pages/min), scraped 726 items (at 1 items/min)
2015-04-10 15:42:30+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1411.6156> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 15:42:30+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1411.6156>
	{'abstract': u'We consider the problem of reconstructing the graph underlying an Ising model from i.i.d. samples. Over the last fifteen years this problem has been of significant interest in the statistics, machine learning, and statistical physics communities, and much of the effort has been directed towards finding algorithms with low computational cost for various restricted classes of models. Nevertheless, for learning Ising models on general graphs with nodes of degree at most , it is not known whether or not it is possible to improve upon the computation needed to exhaustively search over all possible neighborhoods for each node. In this paper we show that a simple greedy procedure allows to learn the structure of an Ising model on an arbitrary bounded-degree graph in time on the order of . We make no assumptions on the parameters except what is necessary for identifiability of the model, and in particular the results hold at low-temperatures as well as for highly non-uniform models. The proof rests on a new structural property of Ising models: we show that for any node there exists at least one neighbor with which it has a high mutual information. This structural property may be of independent interest.',
	 'authors': u'Guy Bresler,',
	 'category': u'Computer Science ',
	 'date': '2014-11-22',
	 'pdflink': u'http://arxiv.org/pdf/1411.6156',
	 'subjects': u'Learning (cs.LG)',
	 'title': u'\nEfficiently learning Ising models on arbitrary graphs',
	 'urllink': u'http://arxiv.org/abs/1411.6156'}
2015-04-10 15:43:03+0000 [xxu46_10] INFO: Crawled 734 pages (at 1 pages/min), scraped 727 items (at 1 items/min)
2015-04-10 15:43:49+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1411.6148> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 15:43:49+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1411.6148>
	{'abstract': u"Recently, there has been a number of papers relating mechanism design and privacy (e.g., see cite). All of these papers consider a worst-case setting where there is no probabilistic information about the players' types. In this paper, we investigate mechanism design and privacy in the emph setting, where the players' types are drawn from some common distribution. We adapt the notion of emph to the Bayesian mechanism design setting, obtaining emph. We also define a robust notion of approximate truthfulness for Bayesian mechanisms, which we call emph. We give several classes of mechanisms (e.g., social welfare mechanisms and histogram mechanisms) that achieve both Bayesian differential privacy and persistent approximate truthfulness. These classes of mechanisms can achieve optimal (economic) efficiency, and do not use any payments. We also demonstrate that by considering the above mechanisms in a modified mechanism design model, the above mechanisms can achieve actual truthfulness.",
	 'authors': u'Samantha Leung, Edward Lui,',
	 'category': u'Computer Science ',
	 'date': '2014-11-22',
	 'pdflink': u'http://arxiv.org/pdf/1411.6148',
	 'subjects': u'Computer Science and Game Theory (cs.GT)',
	 'title': u'\nBayesian Mechanism Design with Efficiency, Privacy, and Approximate  Truthfulness',
	 'urllink': u'http://arxiv.org/abs/1411.6148'}
2015-04-10 15:44:03+0000 [xxu46_10] INFO: Crawled 735 pages (at 1 pages/min), scraped 728 items (at 1 items/min)
2015-04-10 15:45:03+0000 [xxu46_10] INFO: Crawled 735 pages (at 0 pages/min), scraped 728 items (at 0 items/min)
2015-04-10 15:45:07+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1411.6147> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 15:45:07+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1411.6147>
	{'abstract': u'Contractive interference functions introduced by Feyzmahdavian et al. is the newest approach in the analysis and design of distributed power control laws. This approach can be extended to several cases of distributed power control. One of the distributed power control scenarios wherein the contractive interference functions have not been employed is the power control in MIMO systems. In this paper, this scenario will be analyzed. In addition, the optimal linear precoder is employed in each user to achieve maximum point-to-point information rate. In our approach, we use the same amount of signaling as the previous methods did. However, we show that the uniqueness of Nash equilibria is more probable in our approach, suggesting that our proposed method improves the convergence performance of distributed power control in MIMO systems. We also show that the proposed power control algorithm can be implemented asynchronously, which gives a noticeable flexibility to our algorithm given the practical communication limitations.',
	 'authors': u'Peyman Siyari, Hassan Aghaeinia,',
	 'category': u'Computer Science ',
	 'date': '2014-11-22',
	 'pdflink': u'http://arxiv.org/pdf/1411.6147',
	 'subjects': u'Information Theory (cs.IT)',
	 'title': u'\nDistributed Power Control in Multiuser MIMO Networks with Optimal Linear  Precoding',
	 'urllink': u'http://arxiv.org/abs/1411.6147'}
2015-04-10 15:45:57+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1411.6137> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 15:45:57+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1411.6137>
	{'abstract': u'The very original concept of cognitive radio (CR) raised by Mitola targets at all the environment parameters, including those in physical layer, MAC layer, application layer as well as the information extracted from reasoning. Hence the first CR is also referred to as "full cognitive radio". However, due to its difficult implementation, FCC and Simon Haykin separately proposed a much more simplified definition, in which CR mainly detects one single parameter, i.e., spectrum occupancy, and is also called as "spectrum sensing cognitive radio". With the rapid development of wireless communication, the infrastructure of a wireless system becomes much more complicated while the functionality at every node is desired to be as intelligent as possible, say the self-organized capability in the approaching 5G cellular networks. It is then interesting to re-look into Mitola\'s definition and think whether one could, besides obtaining the "on/off" status of the licensed user only, achieve more parameters in a cognitive way. In this article, we propose a new cognitive architecture targeting at multiple parameters in future cellular networks, which is a one step further towards the "full cognition" compared to the most existing CR research. The new architecture is elaborated in detailed stages, and three representative examples are provided based on the recent research progress to illustrate the feasibility as well as the validity of the proposed architecture.',
	 'authors': u'Kaiqing Zhang, Feifei Gao, Qihui Wu,',
	 'category': u'Computer Science ',
	 'date': '2014-11-22',
	 'pdflink': u'http://arxiv.org/pdf/1411.6137',
	 'subjects': u'Information Theory (cs.IT)',
	 'title': u'\nEnhanced Multi-Parameter Cognitive Architecture for Future Wireless  Communications',
	 'urllink': u'http://arxiv.org/abs/1411.6137'}
2015-04-10 15:46:03+0000 [xxu46_10] INFO: Crawled 737 pages (at 2 pages/min), scraped 730 items (at 2 items/min)
2015-04-10 15:47:03+0000 [xxu46_10] INFO: Crawled 737 pages (at 0 pages/min), scraped 730 items (at 0 items/min)
2015-04-10 15:47:11+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1411.6135> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 15:47:11+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1411.6135>
	{'abstract': u'Different Boolean networks may reveal similar dynamics although their definition differs, then preventing their distinction from the observations. This raises the question about the sufficiency of a particular Boolean network for properly reproducing a modeled phenomenon to make realistic predictions. The question actually depends on the invariant properties of behaviorally similar Boolean networks. In this article, we address this issue by considering that the similarity is formalized by isomorphism on graphs modeling their dynamics. The similarity also depends on the parameter governing the updating policy, called the mode. We define a general characterization of the group of isomorphism preserving the mode. From this characterization, we deduce invariant structural properties of the interaction graph and conditions to maintain an equivalence through mode variation.',
	 'authors': u'Franck Delaplace,',
	 'category': u'Computer Science ',
	 'date': '2014-11-22',
	 'pdflink': u'http://arxiv.org/pdf/1411.6135',
	 'subjects': u'Discrete Mathematics (cs.DM)',
	 'title': u'\nAnalogous Dynamics of Boolean Network',
	 'urllink': u'http://arxiv.org/abs/1411.6135'}
2015-04-10 15:48:03+0000 [xxu46_10] INFO: Crawled 738 pages (at 1 pages/min), scraped 731 items (at 1 items/min)
2015-04-10 15:48:26+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1411.6130> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 15:48:26+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1411.6130>
	{'abstract': u'When power grids are heavily stressed with a bulk power transfer, it is useful to have a fast indication of the increased stress when multiple line outages occur. Reducing the bulk power transfer when the outages are severe could forestall further cascading of the outages. We show that synchrophasor measurements of voltage angles at all the area tie lines can be used to indicate the severity of multiple outages. These synchrophasor measurements are readily combined into an "area angle" that can quickly track the severity of multiple outages after they occur. We present a procedure to define thresholds for the area angle that relate to the maximum power that can be transferred through the area until a line limit is reached. Then in real time we would monitor the area angle and compare it to the thresholds when line outages occur to determine the urgency (or not) of actions to reduce the bulk transfer of power through the area. The procedure also identifies exceptional cases in which separate actions to resolve local power distribution problems are needed. We illustrate the thresholds and monitoring with the area angle across several states of Northwestern USA.',
	 'authors': u'Atena Darvishi, Ian Dobson,',
	 'category': u'Computer Science ',
	 'date': '2014-11-22',
	 'pdflink': u'http://arxiv.org/pdf/1411.6130',
	 'subjects': u'Systems and Control (cs.SY)',
	 'title': u'\nThreshold-based monitoring of cascading outages with PMU measurements of  area angle',
	 'urllink': u'http://arxiv.org/abs/1411.6130'}
2015-04-10 15:49:03+0000 [xxu46_10] INFO: Crawled 739 pages (at 1 pages/min), scraped 732 items (at 1 items/min)
2015-04-10 15:49:41+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1411.6127> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 15:49:41+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1411.6127>
	{'abstract': u'In this note, we have revisited the previously published paper "Particle Filtering for Attitude Estimation Using a Minimal Local-Error Representation". In the revisit, we point out that the quaternion particle filtering based on the local/global representation structure has not made full use of the advantage of the particle filtering in terms of accuracy and robustness. Moreover, a normalized quaternion determining procedure based on the minimum mean-square error approach has been investigated into the quaternion-based particle filtering to obtain the fiducial quaternion for the transformation between quaternion and modified Rodrigues parameter. The modification investigated in this note is expected to make the quaternion particle filtering based on the local/global representation structure more strict.',
	 'authors': u'Lubin Chang,',
	 'category': u'Computer Science ',
	 'date': '2014-11-22',
	 'pdflink': u'http://arxiv.org/pdf/1411.6127',
	 'subjects': u'Robotics (cs.RO)',
	 'title': u'\nParticle Filtering for Attitude Estimation Using a Minimal Local-Error  Representation: A Revisit',
	 'urllink': u'http://arxiv.org/abs/1411.6127'}
2015-04-10 15:50:03+0000 [xxu46_10] INFO: Crawled 740 pages (at 1 pages/min), scraped 733 items (at 1 items/min)
2015-04-10 15:50:47+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1411.6118> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 15:50:47+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1411.6118>
	{'abstract': u'We propose and explore a new paradigm that considers every software artifact such as a class as an artificially intelligent and a socially active entity. In this Software Artifact Choreographed Software Engineering (SACSE) paradigm, the humanized artifacts themselves take the lead and choreograph (socially, in collaboration with other intelligent software artifacts, humans, and artifact/human organizations) software engineering solutions to the many software development and maintenance challenges such as (automatic) code reuse, documentation, testing, patching, and refactoring. In this paper, we discuss the implications of seeing software artifacts as our new intelligent friends.',
	 'authors': u'Mithun P. Acharya,',
	 'category': u'Computer Science ',
	 'date': '2014-11-22',
	 'pdflink': u'http://arxiv.org/pdf/1411.6118',
	 'subjects': u'Software Engineering (cs.SE)',
	 'title': u'\nHumanizing Software Artifacts: Software Engineering with Intelligent and  Social Software Artifacts as our Friends',
	 'urllink': u'http://arxiv.org/abs/1411.6118'}
2015-04-10 15:51:03+0000 [xxu46_10] INFO: Crawled 741 pages (at 1 pages/min), scraped 734 items (at 1 items/min)
2015-04-10 15:52:03+0000 [xxu46_10] INFO: Crawled 741 pages (at 0 pages/min), scraped 734 items (at 0 items/min)
2015-04-10 15:52:06+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1411.6114> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 15:52:06+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1411.6114>
	{'abstract': u'With the advancement of Cloud Computing over the past few years, there has been a massive shift from traditional data centers to cloud enabled data centers. The enterprises with cloud data centers are focusing their attention on energy savings through effective utilization of resources. In this work, we propose algorithms which try to minimize the energy consumption in the data center duly maintaining the SLA guarantees. The algorithms try to utilize least number of physical machines in the data center by dynamically rebalancing the physical machines based on their resource utilization. The algorithms also perform an optimal consolidation of virtual machines on a physical machine, minimizing SLA violations. In extensive simulation, our algorithms achieve savings of about 21% in terms of energy consumption and in terms of maintaining the SLAs, it performs 60% better than Single Threshold algorithm.',
	 'authors': u'Radheshyam Nanduri, Dharmesh Kakadia, Vasudeva Varma,',
	 'category': u'Computer Science ',
	 'date': '2014-11-22',
	 'pdflink': u'http://arxiv.org/pdf/1411.6114',
	 'subjects': u'Distributed, Parallel, and Cluster Computing (cs.DC)',
	 'title': u'\nEnergy and SLA aware VM Scheduling',
	 'urllink': u'http://arxiv.org/abs/1411.6114'}
2015-04-10 15:53:03+0000 [xxu46_10] INFO: Crawled 742 pages (at 1 pages/min), scraped 735 items (at 1 items/min)
2015-04-10 15:53:23+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1411.6091> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 15:53:23+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1411.6091>
	{'abstract': u'All that structure from motion algorithms "see" are sets of 2D points. We show that these impoverished views of the world can be faked for the purpose of reconstructing objects in challenging settings, such as from a single image, or from a few ones far apart, by recognizing the object and getting help from a collection of images of other objects from the same class. We synthesize virtual views by computing geodesics on novel networks connecting objects with similar viewpoints, and introduce techniques to increase the specificity and robustness of factorization-based object reconstruction in this setting. We report accurate object shape reconstruction from a single image on challenging PASCAL VOC data, which suggests that the current domain of applications of rigid structure-from-motion techniques may be significantly extended.',
	 'authors': u'Jo\xe3o Carreira, Abhishek Kar, Shubham Tulsiani, Jitendra Malik,',
	 'category': u'Computer Science ',
	 'date': '2014-11-22',
	 'pdflink': u'http://arxiv.org/pdf/1411.6091',
	 'subjects': u'Computer Vision and Pattern Recognition (cs.CV)',
	 'title': u'\nVirtual View Networks for Object Reconstruction',
	 'urllink': u'http://arxiv.org/abs/1411.6091'}
2015-04-10 15:54:03+0000 [xxu46_10] INFO: Crawled 743 pages (at 1 pages/min), scraped 736 items (at 1 items/min)
2015-04-10 15:54:24+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1411.6087> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 15:54:24+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1411.6087>
	{'abstract': u'Harvesting energy from ambient environment is a new promising solution to free electronic devices from electric wire or limited-lifetime battery, which may find very significant applications in sensor networks and body-area networks. This paper mainly investigate the fundamental limits of information transmission in wireless communication system with RF-based energy harvesting, in which a master node acts not only as an information source but also an energy source for child node while only information is transmitted back from child to master node. Three typical structures: optimum receiver, orthogonal receiver and power splitting receiver are considered where two way information transmission between two nodes under an unique external power supply constraint at master node are jointly investigated in the viewpoint of systemic level. We explicitly characterize the achievable capacity-rate region and also discuss the effect of signal processing power consumption at child node. The optimal transmission strategy corresponding to the most energy-efficient status, namely the point on the boundary of achievable capacity-rate region, is derived with help of conditional capacity function. Simulation confirms the substantial gains of employing optimal transmission strategy and optimum receiver structure. Besides, a typical application on minimizing required transmit power to green system is presented.',
	 'authors': u'Tao Li, Pingyi Fan, Khaled Ben Letaief,',
	 'category': u'Computer Science ',
	 'date': '2014-11-22',
	 'pdflink': u'http://arxiv.org/pdf/1411.6087',
	 'subjects': u'Information Theory (cs.IT)',
	 'title': u'\nWireless Communication System with RF-based Energy Harvesting: From  Information Theory to Green System',
	 'urllink': u'http://arxiv.org/abs/1411.6087'}
2015-04-10 15:55:03+0000 [xxu46_10] INFO: Crawled 744 pages (at 1 pages/min), scraped 737 items (at 1 items/min)
2015-04-10 15:55:18+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1411.6082> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 15:55:18+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1411.6082>
	{'abstract': u'The structure entropy is one of the most important parameters to describe the structure property of the complex networks. Most of the existing struc- ture entropies are based on the degree or the betweenness centrality. In order to describe the structure property of the complex networks more reasonably, a new structure entropy of the complex networks based on the Tsallis nonextensive statistical mechanics is proposed in this paper. The influence of the degree and the betweenness centrality on the structure property is combined in the proposed structure entropy. Compared with the existing structure entropy, the proposed structure entropy is more reasonable to describe the structure property of the complex networks in some situations.',
	 'authors': u'Qi Zhang, Xi Lu, Meizhu Li, Yong Deng, Sankaran Mahadevan,',
	 'category': u'Computer Science ',
	 'date': '2014-11-22',
	 'pdflink': u'http://arxiv.org/pdf/1411.6082',
	 'subjects': u'Social and Information Networks (cs.SI)',
	 'title': u'\nA new structure entropy of complex networks based on Tsallis  nonextensive statistical mechanics',
	 'urllink': u'http://arxiv.org/abs/1411.6082'}
2015-04-10 15:56:03+0000 [xxu46_10] INFO: Crawled 745 pages (at 1 pages/min), scraped 738 items (at 1 items/min)
2015-04-10 15:56:17+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1411.6081> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 15:56:17+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1411.6081>
	{'abstract': u'In this paper, we consider the matrix completion problem when the observations are one-bit measurements of some underlying matrix M, and in particular the observed samples consist only of ones and no zeros. This problem is motivated by modern applications such as recommender systems and social networks where only "likes" or "friendships" are observed. The problem of learning from only positive and unlabeled examples, called PU (positive-unlabeled) learning, has been studied in the context of binary classification. We consider the PU matrix completion problem, where an underlying real-valued matrix M is first quantized to generate one-bit observations and then a subset of positive entries is revealed. Under the assumption that M has bounded nuclear norm, we provide recovery guarantees for two different observation models: 1) M parameterizes a distribution that generates a binary matrix, 2) M is thresholded to obtain a binary matrix. For the first case, we propose a "shifted matrix completion" method that recovers M using only a subset of indices corresponding to ones, while for the second case, we propose a "biased matrix completion" method that recovers the (thresholded) binary matrix. Both methods yield strong error bounds --- if M is n by n, the Frobenius error is bounded as O(1/((1-rho)n), where 1-rho denotes the fraction of ones observed. This implies a sample complexity of O(n log n) ones to achieve a small error, when M is dense and n is large. We extend our methods and guarantees to the inductive matrix completion problem, where rows and columns of M have associated features. We provide efficient and scalable optimization procedures for both the methods and demonstrate the effectiveness of the proposed methods for link prediction (on real-world networks consisting of over 2 million nodes and 90 million links) and semi-supervised clustering tasks.',
	 'authors': u'Cho-Jui Hsieh, Nagarajan Natarajan, Inderjit S. Dhillon,',
	 'category': u'Computer Science ',
	 'date': '2014-11-22',
	 'pdflink': u'http://arxiv.org/pdf/1411.6081',
	 'subjects': u'Learning (cs.LG)',
	 'title': u'\nPU Learning for Matrix Completion',
	 'urllink': u'http://arxiv.org/abs/1411.6081'}
2015-04-10 15:57:03+0000 [xxu46_10] INFO: Crawled 746 pages (at 1 pages/min), scraped 739 items (at 1 items/min)
2015-04-10 15:57:16+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1411.6079> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 15:57:16+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1411.6079>
	{'abstract': u'Recent research advances have revealed the computational secrecy of the compressed sensing (CS) paradigm. Perfect secrecy can also be achieved by normalizing the CS measurement vector. However, these findings are established on real measurements while digital devices can only store measurements at a finite precision. Based on the distribution of measurements of natural images sensed by structurally random ensemble, a joint quantization and diffusion approach is proposed for these real-valued measurements. In this way, a nonlinear cryptographic diffusion is intrinsically imposed on the CS process and the overall security level is thus enhanced. Security analyses show that the proposed scheme is able to resist known-plaintext attack while the original CS scheme without quantization cannot. Experimental results demonstrate that the reconstruction quality of our scheme is comparable to that of the original one.',
	 'authors': u'Leo Yu Zhang, Kwok-Wo Wong, Yushu Zhang, Qiuzhen Lin,',
	 'category': u'Computer Science ',
	 'date': '2014-11-22',
	 'pdflink': u'http://arxiv.org/pdf/1411.6079',
	 'subjects': u'Cryptography and Security (cs.CR)',
	 'title': u'\nJoint Quantization and Diffusion for Compressed Sensing Measurements of  Natural Images',
	 'urllink': u'http://arxiv.org/abs/1411.6079'}
2015-04-10 15:58:03+0000 [xxu46_10] INFO: Crawled 747 pages (at 1 pages/min), scraped 740 items (at 1 items/min)
2015-04-10 15:58:29+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1411.6069> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 15:58:29+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1411.6069>
	{'abstract': u'Object reconstruction from a single image -- in the wild -- is a problem where we can make progress and get meaningful results today. This is the main message of this paper, which introduces the first fully automatic pipeline having pixels as inputs and dense 3D surfaces of various rigid categories as outputs in images of realistic scenes. At the core of our approach are novel deformable 3D models that can be learned from 2D annotations available in existing object detection datasets, that can be driven by noisy automatic object segmentations and which we complement with a bottom-up module for recovering high-frequency shape details. We perform a comprehensive quantitative analysis and ablation study of our approach using the recently introduced PASCAL 3D+ dataset and show very encouraging automatic reconstructions on PASCAL VOC.',
	 'authors': u'Abhishek Kar, Shubham Tulsiani, Jo\xe3o Carreira, Jitendra Malik,',
	 'category': u'Computer Science ',
	 'date': '2014-11-22',
	 'pdflink': u'http://arxiv.org/pdf/1411.6069',
	 'subjects': u'Computer Vision and Pattern Recognition (cs.CV)',
	 'title': u'\nCategory-Specific Object Reconstruction from a Single Image',
	 'urllink': u'http://arxiv.org/abs/1411.6069'}
2015-04-10 15:59:03+0000 [xxu46_10] INFO: Crawled 748 pages (at 1 pages/min), scraped 741 items (at 1 items/min)
2015-04-10 15:59:49+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1411.6067> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 15:59:49+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1411.6067>
	{'abstract': u'We characterize the problem of pose estimation for rigid objects in terms of determining viewpoint to explain coarse pose and keypoint prediction to capture the finer details. We address both these tasks in two different settings - the constrained setting with known bounding boxes as well as the more challenging detection setting where the aim is to simultaneously detect and correctly estimate pose of objects. We present Convolutional Neural Network based architectures for these and demonstrate that leveraging viewpoint estimates can substantially improve local appearance based keypoint predictions. In addition to achieving significant improvements over state-of-the-art in the above tasks, we present a study of error modes and effect of object characteristics on performance to guide future efforts towards this goal.',
	 'authors': u'Shubham Tulsiani, Jitendra Malik,',
	 'category': u'Computer Science ',
	 'date': '2014-11-22',
	 'pdflink': u'http://arxiv.org/pdf/1411.6067',
	 'subjects': u'Computer Vision and Pattern Recognition (cs.CV)',
	 'title': u'\nViewpoints and Keypoints',
	 'urllink': u'http://arxiv.org/abs/1411.6067'}
2015-04-10 16:00:03+0000 [xxu46_10] INFO: Crawled 749 pages (at 1 pages/min), scraped 742 items (at 1 items/min)
2015-04-10 16:00:20+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1411.6061> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 16:00:20+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1411.6061>
	{'abstract': u"The structure of real-world social networks in large part determines the evolution of social phenomena, including opinion formation, diffusion of information and influence, and the spread of disease. Globally, network structure is characterized by features such as degree distribution, degree assortativity, and clustering coefficient. However, information about global structure is usually not available to each vertex. Instead, each vertex's knowledge is generally limited to the locally observable portion of the network consisting of the subgraph over its immediate neighbors. Such subgraphs, known as ego networks, have properties that can differ substantially from those of the global network. In this paper, we study the structural properties of ego networks and show how they relate to the global properties of networks from which they are derived. Through empirical comparisons and mathematical derivations, we show that structural features, similar to static attributes, suffer from paradoxes. We quantify the differences between global information about network structure and local estimates. This knowledge allows us to better identify and correct the biases arising from incomplete local information.",
	 'authors': u'Sidharth Gupta, Xiaoran Yan, Kristina Lerman,',
	 'category': u'Computer Science ',
	 'date': '2014-11-22',
	 'pdflink': u'http://arxiv.org/pdf/1411.6061',
	 'subjects': u'Social and Information Networks (cs.SI)',
	 'title': u'\nStructural Properties of Ego Networks',
	 'urllink': u'http://arxiv.org/abs/1411.6061'}
2015-04-10 16:01:03+0000 [xxu46_10] INFO: Crawled 750 pages (at 1 pages/min), scraped 743 items (at 1 items/min)
2015-04-10 16:01:25+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1411.6049> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 16:01:25+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1411.6049>
	{'abstract': u'In computational complexity, a significant amount of useful theory is built on a foundation of widely-believed conjectures about the separation of complexity classes, the most famous of which is P NP. In this work, we examine the consequences of one such conjecture on the combinatorics of 3-manifold diagrams. We use basic tools from quantum computation to give a simple (and unconditional) proof that the Witten-Reshetikhin-Turaev invariant of 3-manifolds is P-hard to calculate. We then use this fact to show that, if NP P, then there exist infinitely many 3-manifold diagrams which cannot be made logarithmically "thin" (relative to their overall size) except perhaps by an exponentially large number of local moves. The latter theorem is an analogue of a result of Freedman for the Jones Polynomial.',
	 'authors': u'Gorjan Alagic, Catharine Lo,',
	 'category': u'Computer Science ',
	 'date': '2014-11-21',
	 'pdflink': u'http://arxiv.org/pdf/1411.6049',
	 'subjects': u'Computational Complexity (cs.CC)',
	 'title': u'\n3-manifold diagrams and NP vs $\\#$P',
	 'urllink': u'http://arxiv.org/abs/1411.6049'}
2015-04-10 16:02:03+0000 [xxu46_10] INFO: Crawled 751 pages (at 1 pages/min), scraped 744 items (at 1 items/min)
2015-04-10 16:02:42+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1411.6031> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 16:02:42+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1411.6031>
	{'abstract': u'We address the problem of action detection in videos. Driven by the latest progress in object detection from 2D images, we build action models using rich feature hierarchies derived from shape and kinematic cues. We incorporate appearance and motion in two ways. First, starting from image region proposals we select those that are motion salient and thus are more likely to contain the action. This leads to a significant reduction in the number of regions being processed and allows for faster computations. Second, we extract spatio-temporal feature representations to build strong classifiers using Convolutional Neural Networks. We link our predictions to produce detections consistent in time, which we call action tubes. We show that our approach outperforms other techniques in the task of action detection.',
	 'authors': u'Georgia Gkioxari, Jitendra Malik,',
	 'category': u'Computer Science ',
	 'date': '2014-11-21',
	 'pdflink': u'http://arxiv.org/pdf/1411.6031',
	 'subjects': u'Computer Vision and Pattern Recognition (cs.CV)',
	 'title': u'\nFinding Action Tubes',
	 'urllink': u'http://arxiv.org/abs/1411.6031'}
2015-04-10 16:03:03+0000 [xxu46_10] INFO: Crawled 752 pages (at 1 pages/min), scraped 745 items (at 1 items/min)
2015-04-10 16:03:30+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1411.6027> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 16:03:30+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1411.6027>
	{'abstract': u'We present a new and powerful class of automata which are explicitly concurrent and allow a very simple definition of composition. The novelty of these automata is their time-synchronous message-asynchronous communication mechanism. Time synchrony is obtained by using global clock. Message asynchrony is obtained by requiring the automata to react to every input. Explicit concurrency is obtained by marking each transition with a set of input and output messages. We compare these automata with a history based approach which uses the same communication mechanism and show that they are equivalent.',
	 'authors': u'Radu Grosu, Bernhard Rumpe,',
	 'category': u'Computer Science ',
	 'date': '2014-11-10',
	 'pdflink': u'http://arxiv.org/pdf/1411.6027',
	 'subjects': u'Formal Languages and Automata Theory (cs.FL)',
	 'title': u'\nConcurrent Timed Port Automata',
	 'urllink': u'http://arxiv.org/abs/1411.6027'}
2015-04-10 16:04:03+0000 [xxu46_10] INFO: Crawled 753 pages (at 1 pages/min), scraped 746 items (at 1 items/min)
2015-04-10 16:04:29+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1411.6021> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 16:04:29+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1411.6021>
	{'abstract': u'In this paper we explore the use of full-duplex radio to improve the spectrum efficiency in a two-way relay channel where two sources exchange information through an multi-antenna relay, and all nodes work in the full-duplex mode. The full-duplex operation can reduce the overall communication to only one phase but suffers from the self-interference. Instead of purely suppressing the self-interference, we aim to maximize the end-to-end performance by jointly optimizing the beamforming matrix at the relay which uses the amplify-and-forward protocol as well as the power control at the sources. To be specific, we propose iterative algorithms and 1-D search to solve two problems: finding the achievable rate region and maximizing the sum rate. At each iteration, either the analytical solution or convex formulation is obtained. We compare the proposed full-duplex two-way relaying with the conventional half-duplex two-way relaying, a full-duplex one-way relaying and a performance upper bound. Numerical results show that the proposed full-duplex scheme significantly improves the achievable data rates over the conventional scheme.',
	 'authors': u'Gan Zheng,',
	 'category': u'Computer Science ',
	 'date': '2014-11-21',
	 'pdflink': u'http://arxiv.org/pdf/1411.6021',
	 'subjects': u'Information Theory (cs.IT)',
	 'title': u'\nJoint Beamforming Optimization and Power Control for Full-Duplex MIMO  Two-way Relay Channel',
	 'urllink': u'http://arxiv.org/abs/1411.6021'}
2015-04-10 16:05:03+0000 [xxu46_10] INFO: Crawled 754 pages (at 1 pages/min), scraped 747 items (at 1 items/min)
2015-04-10 16:05:42+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1411.5995> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 16:05:42+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1411.5995>
	{'abstract': u"Due to popularity surge social networks became lucrative targets for spammers and guerilla marketers, who are trying to game ranking systems and broadcast their messages at little to none cost. Ranking systems, for example Twitter's Trends, can be gamed by scripted users also called bots, who are automatically or semi-automatically twitting essentially the same message. Judging by the prices and abundance of supply from PR firms this is an easy to implement and widely used tactic, at least in Russian blogosphere. Aggregative analysis of social networks should at best mark those messages as spam or at least correctly downplay their importance as they represent opinions only of a few, if dedicated, users. Hence bot detection plays a crucial role in social network mining and analysis. In this paper we propose technique called RepRank which could be viewed as Markov chain based model for reputation propagation on graphs utilizing simultaneous trust and anti-trust propagation and provide effective numerical approach for its computation. Comparison with another models such as TrustRank and some of its modifications on sample of 320000 Russian speaking Twitter users is presented. The dataset is presented as well.",
	 'authors': u'G.V. Ovchinnikov, D.A. Kolesnikov, I.V. Oseledets,',
	 'category': u'Computer Science ',
	 'date': '2014-11-20',
	 'pdflink': u'http://arxiv.org/pdf/1411.5995',
	 'subjects': u'Social and Information Networks (cs.SI)',
	 'title': u'\nAlgebraic reputation model RepRank and its application to spambot  detection',
	 'urllink': u'http://arxiv.org/abs/1411.5995'}
2015-04-10 16:06:03+0000 [xxu46_10] INFO: Crawled 755 pages (at 1 pages/min), scraped 748 items (at 1 items/min)
2015-04-10 16:06:49+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1411.5993> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 16:06:49+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1411.5993>
	{'abstract': u'Being able to reverse engineer from point cloud data to obtain 3D models is important in modeling. As our main contribution, we present a new method to obtain a tensor product B-spline representation from point cloud data by fitting surfaces to appropriately segmented data. By blending multiple local fits our method is more efficient than existing techniques, with the ability to deal with more detail by efficiently introducing a high number of knots. Further point cloud data obtained by digitizing 3D data, typically presents many associated complications like noise and missing data. As our second contribution, we propose an end-to-end framework for smoothing, hole filling, parameterization, knot selection and B-spline fitting that addresses these issues, works robustly with large irregularly shaped data containing holes and is straightforward to implement.',
	 'authors': u'Lavanya Sita Tekumalla, Elaine Cohen,',
	 'category': u'Computer Science ',
	 'date': '2014-11-21',
	 'pdflink': u'http://arxiv.org/pdf/1411.5993',
	 'subjects': u'Graphics (cs.GR)',
	 'title': u'\nReverse Engineering Point Clouds to Fit Tensor Product B-Spline Surfaces  by Blending Local Fits',
	 'urllink': u'http://arxiv.org/abs/1411.5993'}
2015-04-10 16:07:03+0000 [xxu46_10] INFO: Crawled 756 pages (at 1 pages/min), scraped 749 items (at 1 items/min)
2015-04-10 16:08:03+0000 [xxu46_10] INFO: Crawled 756 pages (at 0 pages/min), scraped 749 items (at 0 items/min)
2015-04-10 16:08:03+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1411.5988> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 16:08:03+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1411.5988>
	{'abstract': u'In this thesis, we propose several modelling strategies to tackle evolving data in different contexts. In the framework of static clustering, we start by introducing a soft kernel spectral clustering (SKSC) algorithm, which can better deal with overlapping clusters with respect to kernel spectral clustering (KSC) and provides more interpretable outcomes. Afterwards, a whole strategy based upon KSC for community detection of static networks is proposed, where the extraction of a high quality training sub-graph, the choice of the kernel function, the model selection and the applicability to large-scale data are key aspects. This paves the way for the development of a novel clustering algorithm for the analysis of evolving networks called kernel spectral clustering with memory effect (MKSC), where the temporal smoothness between clustering results in successive time steps is incorporated at the level of the primal optimization problem, by properly modifying the KSC formulation. Later on, an application of KSC to fault detection of an industrial machine is presented. Here, a smart pre-processing of the data by means of a proper windowing operation is necessary to catch the ongoing degradation process affecting the machine. In this way, in a genuinely unsupervised manner, it is possible to raise an early warning when necessary, in an online fashion. Finally, we propose a new algorithm called incremental kernel spectral clustering (IKSC) for online learning of non-stationary data. This ambitious challenge is faced by taking advantage of the out-of-sample property of kernel spectral clustering (KSC) to adapt the initial model, in order to tackle merging, splitting or drifting of clusters across time. Real-world applications considered in this thesis include image segmentation, time-series clustering, community detection of static and evolving networks.',
	 'authors': u'Rocco Langone,',
	 'category': u'Computer Science ',
	 'date': '2014-11-20',
	 'pdflink': u'http://arxiv.org/pdf/1411.5988',
	 'subjects': u'Social and Information Networks (cs.SI)',
	 'title': u'\nClustering evolving data using kernel-based methods',
	 'urllink': u'http://arxiv.org/abs/1411.5988'}
2015-04-10 16:09:03+0000 [xxu46_10] INFO: Crawled 757 pages (at 1 pages/min), scraped 750 items (at 1 items/min)
2015-04-10 16:09:16+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1411.5951> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 16:09:16+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1411.5951>
	{'abstract': u'Bloxorz is an online puzzle game where players move a 1 by 1 by 2 block by tilting it on a subset of the two dimensional grid. Bloxorz features switches that open and close trapdoors. The puzzle is to move the block from its initial position to an upright position on the destination square. We show that the problem of deciding whether a given Bloxorz level is solvable is PSPACE-complete and that this remains so even when all trapdoors are initially closed or all trapdoors are initially open. We also answer an open question of Viglietta, showing that 2-buttons are sufficient for PSPACE-hardness of general puzzle games. We also examine the hardness of some variants of Bloxorz, including variants where the block is a 1 by 1 by 1 cube, and variants with single-use tiles.',
	 'authors': u'Tom C. van der Zanden, Hans L. Bodlaender,',
	 'category': u'Computer Science ',
	 'date': '2014-11-21',
	 'pdflink': u'http://arxiv.org/pdf/1411.5951',
	 'subjects': u'Computational Complexity (cs.CC)',
	 'title': u'\nPSPACE-completeness of Bloxorz and of Games with 2-Buttons',
	 'urllink': u'http://arxiv.org/abs/1411.5951'}
2015-04-10 16:09:56+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1411.5935> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 16:09:56+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1411.5935>
	{'abstract': u"Current approaches to semantic image and scene understanding typically employ rather simple object representations such as 2D or 3D bounding boxes. While such coarse models are robust and allow for reliable object detection, they discard much of the information about objects' 3D shape and pose, and thus do not lend themselves well to higher-level reasoning. Here, we propose to base scene understanding on a high-resolution object representation. An object class - in our case cars - is modeled as a deformable 3D wireframe, which enables fine-grained modeling at the level of individual vertices and faces. We augment that model to explicitly include vertex-level occlusion, and embed all instances in a common coordinate frame, in order to infer and exploit object-object interactions. Specifically, from a single view we jointly estimate the shapes and poses of multiple objects in a common 3D frame. A ground plane in that frame is estimated by consensus among different objects, which significantly stabilizes monocular 3D pose estimation. The fine-grained model, in conjunction with the explicit 3D scene model, further allows one to infer part-level occlusions between the modeled objects, as well as occlusions by other, unmodeled scene elements. To demonstrate the benefits of such detailed object class models in the context of scene understanding we systematically evaluate our approach on the challenging KITTI street scene dataset. The experiments show that the model's ability to utilize image evidence at the level of individual parts improves monocular 3D pose estimation w.r.t. both location and (continuous) viewpoint.",
	 'authors': u'M.Zeeshan Zia, Michael Stark, Konrad Schindler,',
	 'category': u'Computer Science ',
	 'date': '2014-11-18',
	 'pdflink': u'http://arxiv.org/pdf/1411.5935',
	 'subjects': u'Computer Vision and Pattern Recognition (cs.CV)',
	 'title': u'\nTowards Scene Understanding with Detailed 3D Object Representations',
	 'urllink': u'http://arxiv.org/abs/1411.5935'}
2015-04-10 16:10:03+0000 [xxu46_10] INFO: Crawled 759 pages (at 2 pages/min), scraped 752 items (at 2 items/min)
2015-04-10 16:11:03+0000 [xxu46_10] INFO: Crawled 759 pages (at 0 pages/min), scraped 752 items (at 0 items/min)
2015-04-10 16:11:09+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1411.5928> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 16:11:09+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1411.5928>
	{'abstract': u'We train a generative convolutional neural network which is able to generate images of objects given object type, viewpoint, and color. We train the network in a supervised manner on a dataset of rendered 3D chair models. Our experiments show that the network does not merely learn all images by heart, but rather finds a meaningful representation of a 3D chair model allowing it to assess the similarity of different chairs, interpolate between given viewpoints to generate the missing ones, or invent new chair styles by interpolating between chairs from the training set. We show that the network can be used to find correspondences between different chairs from the dataset, outperforming existing approaches on this task.',
	 'authors': u'Alexey Dosovitskiy, Jost Tobias Springenberg, Thomas Brox,',
	 'category': u'Computer Science ',
	 'date': '2014-11-21',
	 'pdflink': u'http://arxiv.org/pdf/1411.5928',
	 'subjects': u'Computer Vision and Pattern Recognition (cs.CV)',
	 'title': u'\nLearning to Generate Chairs with Convolutional Neural Networks',
	 'urllink': u'http://arxiv.org/abs/1411.5928'}
2015-04-10 16:12:03+0000 [xxu46_10] INFO: Crawled 760 pages (at 1 pages/min), scraped 753 items (at 1 items/min)
2015-04-10 16:12:27+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1411.5923> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 16:12:27+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1411.5923>
	{'abstract': u'We address a class of Markov jump linear systems that are characterized by the underlying Markov process being time-inhomogeneous with a priori unknown transition probabilities. Necessary and sufficient conditions for uniform stochastic stability and uniform stochastic disturbance attenuation are reported. In both cases, conditions are expressed as a set of finite-dimensional linear matrix inequalities that can be solved efficiently.',
	 'authors': u'Collin C. Lutz, Daniel J. Stilwell,',
	 'category': u'Computer Science ',
	 'date': '2014-11-21',
	 'pdflink': u'http://arxiv.org/pdf/1411.5923',
	 'subjects': u'Systems and Control (cs.SY)',
	 'title': u'\nStability and disturbance attenuation for a switched Markov jump linear  system',
	 'urllink': u'http://arxiv.org/abs/1411.5923'}
2015-04-10 16:13:03+0000 [xxu46_10] INFO: Crawled 761 pages (at 1 pages/min), scraped 754 items (at 1 items/min)
2015-04-10 16:13:36+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1411.5915> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 16:13:36+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1411.5915>
	{'abstract': u"Recent developments in system identification have brought attention to regularized kernel-based methods. This type of approach has been proven to compare favorably with classic parametric methods. However, current formulations are not robust with respect to outliers. In this paper, we introduce a novel method to robustify kernel-based system identification methods. To this end, we model the output measurement noise using random variables with heavy-tailed probability density functions (pdfs), focusing on the Laplacian and the Student's t distributions. Exploiting the representation of these pdfs as scale mixture of Gaussians, we cast our system identification problem into a Gaussian process regression framework, which requires estimating a number of hyperparameters of the data size order. To overcome this difficulty, we design a new maximum a posteriori (MAP) estimator of the hyperparameters based on an Expectation-Maximization (EM) method. In presence of outliers, numerical experiments show a substantial performance improvement compared to currently used kernel-based methods for linear system identification.",
	 'authors': u'Giulio Bottegal, Aleksandr Y. Aravkin, H\xe5kan Hjalmarsson, Gianluigi Pillonetto,',
	 'category': u'Computer Science ',
	 'date': '2014-11-21',
	 'pdflink': u'http://arxiv.org/pdf/1411.5915',
	 'subjects': u'Systems and Control (cs.SY)',
	 'title': u'\nRobust EM kernel-based methods for linear system identification',
	 'urllink': u'http://arxiv.org/abs/1411.5915'}
2015-04-10 16:14:03+0000 [xxu46_10] INFO: Crawled 762 pages (at 1 pages/min), scraped 755 items (at 1 items/min)
2015-04-10 16:14:41+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1411.5908> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 16:14:41+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1411.5908>
	{'abstract': u'Despite the importance of image representations such as histograms of oriented gradients and deep Convolutional Neural Networks (CNN), our theoretical understanding of them remains limited. Aiming at filling this gap, we investigate three key mathematical properties of representations: equivariance, invariance, and equivalence. Equivariance studies how transformations of the input image are encoded by the representation, invariance being a special case where a transformation has no effect. Equivalence studies whether two representations, for example two different parametrisations of a CNN, capture the same visual information or not. A number of methods to establish these properties empirically are proposed, including introducing transformation and stitching layers in CNNs. These methods are then applied to popular representations to reveal insightful aspects of their structure, including clarifying at which layers in a CNN certain geometric invariances are achieved. While the focus of the paper is theoretical, direct applications to structured-output regression are demonstrated too.',
	 'authors': u'Karel Lenc, Andrea Vedaldi,',
	 'category': u'Computer Science ',
	 'date': '2014-11-21',
	 'pdflink': u'http://arxiv.org/pdf/1411.5908',
	 'subjects': u'Computer Vision and Pattern Recognition (cs.CV)',
	 'title': u'\nUnderstanding image representations by measuring their equivariance and  equivalence',
	 'urllink': u'http://arxiv.org/abs/1411.5908'}
2015-04-10 16:15:03+0000 [xxu46_10] INFO: Crawled 763 pages (at 1 pages/min), scraped 756 items (at 1 items/min)
2015-04-10 16:15:25+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1411.5899> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 16:15:25+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1411.5899>
	{'abstract': u'Falling rule lists are classification models consisting of an ordered list of if-then rules, where (i) the order of rules determines which example should be classified by each rule, and (ii) the estimated probability of success decreases monotonically down the list. These kinds of rule lists are inspired by healthcare applications where patients would be stratified into risk sets and the highest at-risk patients should be considered first. We provide a Bayesian framework for learning falling rule lists that does not rely on traditional greedy decision tree learning methods.',
	 'authors': u'Fulton Wang, Cynthia Rudin,',
	 'category': u'Computer Science ',
	 'date': '2014-11-21',
	 'pdflink': u'http://arxiv.org/pdf/1411.5899',
	 'subjects': u'Artificial Intelligence (cs.AI)',
	 'title': u'\nFalling Rule Lists',
	 'urllink': u'http://arxiv.org/abs/1411.5899'}
2015-04-10 16:16:03+0000 [xxu46_10] INFO: Crawled 764 pages (at 1 pages/min), scraped 757 items (at 1 items/min)
2015-04-10 16:16:33+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1411.5881> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 16:16:33+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1411.5881>
	{'abstract': u'This paper presents a spike-based model which employs neurons with functionally distinct dendritic compartments for classifying high dimensional binary patterns. The synaptic inputs arriving on each dendritic subunit are nonlinearly processed before being linearly integrated at the soma, giving the neuron a capacity to perform a large number of input-output mappings. The model utilizes sparse synaptic connectivity; where each synapse takes a binary value. The optimal connection pattern of a neuron is learned by using a simple hardware-friendly, margin enhancing learning algorithm inspired by the mechanism of structural plasticity in biological neurons. The learning algorithm groups correlated synaptic inputs on the same dendritic branch. Since the learning results in modified connection patterns, it can be incorporated into current event-based neuromorphic systems with little overhead. This work also presents a branch-specific spike-based version of this structural plasticity rule. The proposed model is evaluated on benchmark binary classification problems and its performance is compared against that achieved using Support Vector Machine (SVM) and Extreme Learning Machine (ELM) techniques. Our proposed method attains comparable performance while utilizing 10 to 50% less computational resources than the other reported techniques.',
	 'authors': u'Shaista Hussain, Shih-Chii Liu, Arindam Basu,',
	 'category': u'Computer Science ',
	 'date': '2014-11-20',
	 'pdflink': u'http://arxiv.org/pdf/1411.5881',
	 'subjects': u'Neural and Evolutionary Computing (cs.NE)',
	 'title': u'\nHardware-Amenable Structural Learning for Spike-based Pattern  Classification using a Simple Model of Active Dendrites',
	 'urllink': u'http://arxiv.org/abs/1411.5881'}
2015-04-10 16:17:03+0000 [xxu46_10] INFO: Crawled 765 pages (at 1 pages/min), scraped 758 items (at 1 items/min)
2015-04-10 16:17:40+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1411.5879> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 16:17:40+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1411.5879>
	{'abstract': u'We propose a method that learns a discriminative yet semantic space for object categorization, where we also embed auxiliary semantic entities such as supercategories and attributes. Contrary to prior work which only utilized them as side information, we explicitly embed the semantic entities into the same space where we embed categories, which enables us to represent a category as their linear combination. By exploiting such a unified model for semantics, we enforce each category to be represented by a supercategory + sparse combination of attributes, with an additional exclusive regularization to learn discriminative composition.',
	 'authors': u'Sung Ju Hwang, Leonid Sigal,',
	 'category': u'Computer Science ',
	 'date': '2014-11-18',
	 'pdflink': u'http://arxiv.org/pdf/1411.5879',
	 'subjects': u'Computer Vision and Pattern Recognition (cs.CV)',
	 'title': u'\nA Unified Semantic Embedding: Relating Taxonomies and Attributes',
	 'urllink': u'http://arxiv.org/abs/1411.5879'}
2015-04-10 16:18:03+0000 [xxu46_10] INFO: Crawled 766 pages (at 1 pages/min), scraped 759 items (at 1 items/min)
2015-04-10 16:18:28+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1411.5878> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 16:18:28+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1411.5878>
	{'abstract': u'Detecting and segmenting salient objects in natural scenes, also known as salient object detection, has attracted a lot of focused research in computer vision and has resulted in many applications. However, while many such models exist, a deep understanding of achievements and issues is lacking. We aim to provide a comprehensive review of the recent progress in this field. We situate salient object detection among other closely related areas such as generic scene segmentation, object proposal generation, and saliency for fixation prediction. Covering 256 publications we survey i) roots, key concepts, and tasks, ii) core techniques and main modeling trends, and iii) datasets and evaluation metrics in salient object detection. We also discuss open problems such as evaluation metrics and dataset bias in model performance and suggest future research directions.',
	 'authors': u'Ali Borji, Ming-Ming Cheng, Huaizu Jiang, Jia Li,',
	 'category': u'Computer Science ',
	 'date': '2014-11-18',
	 'pdflink': u'http://arxiv.org/pdf/1411.5878',
	 'subjects': u'Computer Vision and Pattern Recognition (cs.CV)',
	 'title': u'\nSalient Object Detection: A Survey',
	 'urllink': u'http://arxiv.org/abs/1411.5878'}
2015-04-10 16:19:03+0000 [xxu46_10] INFO: Crawled 767 pages (at 1 pages/min), scraped 760 items (at 1 items/min)
2015-04-10 16:19:14+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1411.5869> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 16:19:14+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1411.5869>
	{'abstract': u'In this paper, the optimization-based alignment (OBA) methods are investigated with main focus on the vector observations construction procedures for the strapdown inertial navigation system (SINS). The contributions of this study are twofold. First the OBA method is extended to be able to estimate the gyroscopes biases coupled with the attitude based on the construction process of the existing OBA methods. This extension transforms the initial alignment into an attitude estimation problem which can be solved using the nonlinear filtering algorithms. The second contribution is the comprehensive evaluation of the OBA methods and their extensions with different vector observations construction procedures in terms of convergent speed and steady-state estimate using field test data collected from different grades of SINS. This study is expected to facilitate the selection of appropriate OBA methods for different grade SINS.',
	 'authors': u'Lubin Chang, Jingshu Li, Kailong Li,',
	 'category': u'Computer Science ',
	 'date': '2014-11-21',
	 'pdflink': u'http://arxiv.org/pdf/1411.5869',
	 'subjects': u'Robotics (cs.RO)',
	 'title': u'\nOptimization-based Alignment for Strapdown Inertial Navigation System  Comparison and Extension',
	 'urllink': u'http://arxiv.org/abs/1411.5869'}
2015-04-10 16:20:03+0000 [xxu46_10] INFO: Crawled 768 pages (at 1 pages/min), scraped 761 items (at 1 items/min)
2015-04-10 16:20:17+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1411.5867> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 16:20:17+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1411.5867>
	{'abstract': u"We show how to represent a planar digraph in linear space so that distance queries can be answered in constant time. The data structure can be constructed in linear time. This representation of reachability is thus optimal in both time and space, and has optimal construction time. The previous best solution used space for constant query time [Thorup FOCS'01].",
	 'authors': u'Jacob Holm, Eva Rotenberg, Mikkel Thorup,',
	 'category': u'Computer Science ',
	 'date': '2014-11-21',
	 'pdflink': u'http://arxiv.org/pdf/1411.5867',
	 'subjects': u'Data Structures and Algorithms (cs.DS)',
	 'title': u'\nPlanar Reachability in Linear Space and Constant Time',
	 'urllink': u'http://arxiv.org/abs/1411.5867'}
2015-04-10 16:21:03+0000 [xxu46_10] INFO: Crawled 769 pages (at 1 pages/min), scraped 762 items (at 1 items/min)
2015-04-10 16:21:21+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1411.5861> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 16:21:21+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1411.5861>
	{'abstract': u"We consider lattice coset-coded transmissions over a wiretap channel with additive white Gaussian noise (AWGN). Examining a function that can be interpreted as either the legitimate receiver's error probability or the eavesdropper's correct decision probability, we rigorously show that, albeit offering simple bit labeling, orthogonal nested lattices are suboptimal for coset coding in terms of both the legitimate receiver's and the eavesdropper's probabilities.",
	 'authors': u'Alex Karrila, Camilla Hollanti,',
	 'category': u'Computer Science ',
	 'date': '2014-11-21',
	 'pdflink': u'http://arxiv.org/pdf/1411.5861',
	 'subjects': u'Information Theory (cs.IT)',
	 'title': u'\nA Comparison of Skewed and Orthogonal Lattices in Gaussian Wiretap  Channels',
	 'urllink': u'http://arxiv.org/abs/1411.5861'}
2015-04-10 16:22:03+0000 [xxu46_10] INFO: Crawled 770 pages (at 1 pages/min), scraped 763 items (at 1 items/min)
2015-04-10 16:22:31+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1411.5853> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 16:22:31+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1411.5853>
	{'abstract': u'It is fairly easy to show that every regular set is an almost-confluent congruential language (ACCL), and it is known that every regular set is a Church-Rosser congruential language (CRCL). Whether there exists an ACCL, which is not a CRCL, seems to remain an open question. In this note we present one such ACCL.',
	 'authors': u'Colm \xd3 D\xfanlaing,',
	 'category': u'Computer Science ',
	 'date': '2014-11-21',
	 'pdflink': u'http://arxiv.org/pdf/1411.5853',
	 'subjects': u'Logic in Computer Science (cs.LO)',
	 'title': u'\nAn ACCL which is not a CRCL',
	 'urllink': u'http://arxiv.org/abs/1411.5853'}
2015-04-10 16:23:03+0000 [xxu46_10] INFO: Crawled 771 pages (at 1 pages/min), scraped 764 items (at 1 items/min)
2015-04-10 16:23:46+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1411.5849> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 16:23:46+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1411.5849>
	{'abstract': u'Many hard graph problems, such as Hamiltonian Cycle, become FPT when parameterized by treewidth, a parameter that is bounded only on sparse graphs. When parameterized by the more general parameter clique-width, Hamiltonian Cycle becomes W[1]-hard, as shown by Fomin et al. [5]. Sther and Telle address this problem in their paper [13] by introducing a new parameter, split-matching-width, which lies between treewidth and clique-width in terms of generality. They show that even though graphs of restricted split-matching-width might be dense, solving problems such as Hamiltonian Cycle can be done in FPT time. Recently, it was shown that Hamiltonian Cycle parameterized by treewidth is in EPT [1, 6], meaning it can be solved in -time. In this paper, using tools from [6], we show that also parameterized by split-matching-width Hamiltonian Cycle is EPT. To the best of our knowledge, this is the first EPT algorithm for any "globally constrained" graph problem parameterized by a non-trivial and non-sparse structural parameter. To accomplish this, we also give an algorithm constructing a branch decomposition approximating the minimum split-matching-width to within a constant factor. Combined, these results show that the algorithms in [13] for Edge Dominating Set, Chromatic Number and Max Cut all can be improved. We also show that for Hamiltonian Cycle and Max Cut the resulting algorithms are asymptotically optimal under the Exponential Time Hypothesis.',
	 'authors': u'Sigve Hortemo S\xe6ther,',
	 'category': u'Computer Science ',
	 'date': '2014-11-21',
	 'pdflink': u'http://arxiv.org/pdf/1411.5849',
	 'subjects': u'Data Structures and Algorithms (cs.DS)',
	 'title': u'\nSolving Hamiltonian Cycle by an EPT Algorithm for a Non-sparse Parameter',
	 'urllink': u'http://arxiv.org/abs/1411.5849'}
2015-04-10 16:24:03+0000 [xxu46_10] INFO: Crawled 772 pages (at 1 pages/min), scraped 765 items (at 1 items/min)
2015-04-10 16:24:58+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1411.5847> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 16:24:58+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1411.5847>
	{'abstract': u'Affine forms are a common way to represent convex sets of using a base of error terms . Quadratic forms are an extension of affine forms enabling the use of quadratic error terms . In static analysis, the zonotope domain, a relational abstract domain based on affine forms has been used in a wide set of settings, e.g. set-based simulation for hybrid systems, or floating point analysis, providing relational abstraction of functions with a cost linear in the number of errors terms. In this paper, we propose a quadratic version of zonotopes. We also present a new algorithm based on semi-definite programming to project a quadratic zonotope, and therefore quadratic forms, to intervals. All presented material has been implemented and applied on representative examples.',
	 'authors': u'Assal\xe9 Adj\xe9, Pierre-Lo\xefc Garoche, Alexis Werey,',
	 'category': u'Computer Science ',
	 'date': '2014-11-21',
	 'pdflink': u'http://arxiv.org/pdf/1411.5847',
	 'subjects': u'Logic in Computer Science (cs.LO)',
	 'title': u'\nQuadratic Zonotopes:An extension of Zonotopes to Quadratic Arithmetics',
	 'urllink': u'http://arxiv.org/abs/1411.5847'}
2015-04-10 16:25:03+0000 [xxu46_10] INFO: Crawled 773 pages (at 1 pages/min), scraped 766 items (at 1 items/min)
2015-04-10 16:25:59+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1411.5825> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 16:25:59+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1411.5825>
	{'abstract': u'The proliferative activity of breast tumors, which is routinely estimated by counting of mitotic figures in hematoxylin and eosin stained histology sections, is considered to be one of the most important prognostic markers. However, mitosis counting is laborious, subjective and may suffer from low inter-observer agreement. With the wider acceptance of whole slide images in pathology labs, automatic image analysis has been proposed as a potential solution for these issues. In this paper, the results from the Assessment of Mitosis Detection Algorithms 2013 (AMIDA13) challenge are described. The challenge was based on a data set consisting of 12 training and 11 testing subjects, with more than one thousand annotated mitotic figures by multiple observers. Short descriptions and results from the evaluation of eleven methods are presented. The top performing method has an error rate that is comparable to the inter-observer agreement among pathologists.',
	 'authors': u'Mitko Veta, Paul J. van Diest, Stefan M. Willems, Haibo Wang, Anant Madabhushi, Angel Cruz-Roa, Fabio Gonzalez, Anders B. L. Larsen, Jacob S. Vestergaard, Anders B. Dahl, Dan C. Cire\u015fan, J\xfcrgen Schmidhuber, Alessandro Giusti, Luca M. Gambardella, F. Boray Tek, Thomas Walter, Ching-Wei Wang, Satoshi Kondo, Bogdan J. Matuszewski, Frederic Precioso, Violet Snell, Josef Kittler, Teofilo E. de Campos, Adnan M. Khan, Nasir M. Rajpoot, Evdokia Arkoumani, Miangela M. Lacle, Max A. Viergever, Josien P.W. Pluim,',
	 'category': u'Computer Science ',
	 'date': '2014-11-21',
	 'pdflink': u'http://arxiv.org/pdf/1411.5825',
	 'subjects': u'Computer Vision and Pattern Recognition (cs.CV)',
	 'title': u'\nAssessment of algorithms for mitosis detection in breast cancer  histopathology images',
	 'urllink': u'http://arxiv.org/abs/1411.5825'}
2015-04-10 16:26:03+0000 [xxu46_10] INFO: Crawled 774 pages (at 1 pages/min), scraped 767 items (at 1 items/min)
2015-04-10 16:27:03+0000 [xxu46_10] INFO: Crawled 774 pages (at 0 pages/min), scraped 767 items (at 0 items/min)
2015-04-10 16:27:18+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1411.5822> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 16:27:18+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1411.5822>
	{'abstract': u'A -ary code of length , size , and minimum distance is called an code. An code is called a maximum distance separable (MDS) code. In this work, some MDS codes over small alphabets are classified. It is shown that every code with , , is equivalent to a linear code with the same parameters. This implies that the code and the MDS codes for are unique. The classification of one-error-correcting -ary MDS codes is also finished; there are , , , and equivalence classes of codes for , respectively. One of the equivalence classes of perfect codes corresponds to the Hamming code and the other three are nonlinear codes for which there exists no previously known construction.',
	 'authors': u'Janne I. Kokkala, Denis S. Krotov, Patric R. J. \xd6sterg\xe5rd,',
	 'category': u'Computer Science ',
	 'date': '2014-11-21',
	 'pdflink': u'http://arxiv.org/pdf/1411.5822',
	 'subjects': u'Information Theory (cs.IT)',
	 'title': u'\nOn the Classification of MDS Codes',
	 'urllink': u'http://arxiv.org/abs/1411.5822'}
2015-04-10 16:28:03+0000 [xxu46_10] INFO: Crawled 775 pages (at 1 pages/min), scraped 768 items (at 1 items/min)
2015-04-10 16:28:16+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1411.5820> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 16:28:16+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1411.5820>
	{'abstract': u'Infinite games where several players seek to coordinate under imperfect information are believed to be intractable, unless the information is hierarchically ordered among the players. We identify a class of games for which joint winning strategies can be constructed effectively without restricting the direction of information flow. Instead, our condition requires that the players attain common knowledge about the actual state of the game over and over again along every play. We show that it is decidable whether a given game satisfies the condition, and prove tight complexity bounds for the strategy synthesis problem under parity winning conditions.',
	 'authors': u'Dietmar Berwanger, Anup Basil Mathew,',
	 'category': u'Computer Science ',
	 'date': '2014-11-21',
	 'pdflink': u'http://arxiv.org/pdf/1411.5820',
	 'subjects': u'Computer Science and Game Theory (cs.GT)',
	 'title': u'\nInfinite games with finite knowledge gaps',
	 'urllink': u'http://arxiv.org/abs/1411.5820'}
2015-04-10 16:29:03+0000 [xxu46_10] INFO: Crawled 776 pages (at 1 pages/min), scraped 769 items (at 1 items/min)
2015-04-10 16:29:18+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1411.5797> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 16:29:18+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1411.5797>
	{'abstract': u'Synchronization underlies biological and man-made phenomena including memory and perception in the brain, coordinated motion of animal flocks, and stability of the power grid, and is often modeled through networks of phase-coupled oscillating nodes. Heterogeneity in the node dynamics, however, may prevent such networks from achieving the required level of synchronization. In order to guarantee synchronization, external inputs can be used to pin a subset of nodes to a reference frequency, while the remaining nodes are steered toward synchronization via local coupling. In this paper, we present a submodular optimization framework for selecting a set of nodes to act as external inputs in order to achieve synchronization from almost any initial network state. We derive threshold-based sufficient conditions for synchronization, and then prove that these conditions are equivalent to connectivity of a class of augmented network graphs. Based on this connection, we map the sufficient conditions for synchronization to constraints on submodular functions, leading to efficient algorithms with provable optimality bounds for selecting input nodes. We illustrate our approach via numerical studies of synchronization in networks from power systems, wireless networks, and neuronal networks.',
	 'authors': u'Andrew Clark, Basel Alomair, Linda Bushnell, Radha Poovendran,',
	 'category': u'Computer Science ',
	 'date': '2014-11-21',
	 'pdflink': u'http://arxiv.org/pdf/1411.5797',
	 'subjects': u'Systems and Control (cs.SY)',
	 'title': u'\nGlobal Practical Node and Edge Synchronization in Kuramoto Networks: A  Submodular Optimization Framework',
	 'urllink': u'http://arxiv.org/abs/1411.5797'}
2015-04-10 16:30:03+0000 [xxu46_10] INFO: Crawled 777 pages (at 1 pages/min), scraped 770 items (at 1 items/min)
2015-04-10 16:30:19+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1411.5796> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 16:30:19+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1411.5796>
	{'abstract': u'This paper describes pre-processing phase of ontology graph generation system from Punjabi text documents of different domains. This research paper focuses on pre-processing of Punjabi text documents. Pre-processing is structured representation of the input text. Pre-processing of ontology graph generation includes allowing input restrictions to the text, removal of special symbols and punctuation marks, removal of duplicate terms, removal of stop words, extract terms by matching input terms with dictionary and gazetteer lists terms.',
	 'authors': u'Rajveer Kaur, Saurabh Sharma,',
	 'category': u'Computer Science ',
	 'date': '2014-11-21',
	 'pdflink': u'http://arxiv.org/pdf/1411.5796',
	 'subjects': u'Computation and Language (cs.CL)',
	 'title': u'\nPre-processing of Domain Ontology Graph Generation System in Punjabi',
	 'urllink': u'http://arxiv.org/abs/1411.5796'}
2015-04-10 16:31:03+0000 [xxu46_10] INFO: Crawled 778 pages (at 1 pages/min), scraped 771 items (at 1 items/min)
2015-04-10 16:31:38+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1411.5795> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 16:31:38+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1411.5795>
	{'abstract': u"Participatory sensing has emerged recently as a promising approach to large-scale data collection. However, without incentives for users to regularly contribute good quality data, this method is unlikely to be viable in the long run. In this paper, we link incentive to users' demand for consuming compelling services, as an approach complementary to conventional credit or reputation based approaches. With this demand-based principle, we design two incentive schemes, Incentive with Demand Fairness (IDF) and Iterative Tank Filling (ITF), for maximizing fairness and social welfare, respectively. Our study shows that the IDF scheme is max-min fair and can score close to 1 on the Jain's fairness index, while the ITF scheme maximizes social welfare and achieves a unique Nash equilibrium which is also Pareto and globally optimal. We adopted a game theoretic approach to derive the optimal service demands. Furthermore, to address practical considerations, we use a stochastic programming technique to handle uncertainty that is often encountered in real life situations.",
	 'authors': u'Tie Luo, Chen-Khong Tham,',
	 'category': u'Computer Science ',
	 'date': '2014-11-21',
	 'pdflink': u'http://arxiv.org/pdf/1411.5795',
	 'subjects': u'Computer Science and Game Theory (cs.GT)',
	 'title': u'\nFairness and Social Welfare in Incentivizing Participatory Sensing',
	 'urllink': u'http://arxiv.org/abs/1411.5795'}
2015-04-10 16:32:03+0000 [xxu46_10] INFO: Crawled 779 pages (at 1 pages/min), scraped 772 items (at 1 items/min)
2015-04-10 16:32:55+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1411.5784> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 16:32:55+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1411.5784>
	{'abstract': u"In high speed railways (HSRs) communication system, when a train travels along the railway with high velocity, the wireless channel between the train and base station varies strenuously, which makes it essential to implement appropriate power allocations to guarantee system performance. What's more, how to evaluate the performance limits in this new scenario is also needed to consider. To this end, this paper investigates the performance limits of wireless communication in HSRs scenario. Since the hybrid information transmitted between train and base station usually has diverse quality of service (QoS) requirements, QoS-based achievable rate region is utilized to characterize the transmission performance in this paper. It is proved that traditional ergodic capacity and outage capacity with unique QoS requirement can be regarded as two extreme cases of the achievable rate region proposed in this paper. The corresponding optimal power allocation strategy is also given to achieve the maximal boundary of achievable rate region. Compared with conventional strategies, the advantages of the proposed strategy are validated in terms of green communication, namely minimizing average transmit power. Besides, the hybrid information transmission in a non-uniform generalized motion scenario is analyzed to confirm the robust performance of proposed strategy. The performance loss caused by non-uniform motion compared with that in uniform motion is also indicated, where a deterministic worst case for instantaneous speed realization is proposed to serve as the lower bound for system performance.",
	 'authors': u'Tao Li, Pingyi Fan, Ke Xiong, K. B. Letaief,',
	 'category': u'Computer Science ',
	 'date': '2014-11-21',
	 'pdflink': u'http://arxiv.org/pdf/1411.5784',
	 'subjects': u'Information Theory (cs.IT)',
	 'title': u'\nQoS-distinguished Achievable Rate Region for High Speed Railway Wireless  Communications',
	 'urllink': u'http://arxiv.org/abs/1411.5784'}
2015-04-10 16:33:03+0000 [xxu46_10] INFO: Crawled 780 pages (at 1 pages/min), scraped 773 items (at 1 items/min)
2015-04-10 16:33:54+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1411.5782> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 16:33:54+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1411.5782>
	{'abstract': u'Frameproof codes are used to fingerprint digital data. It can prevent copyrighted materials from unauthorized use. In this paper, we study upper and lower bounds for -frameproof codes of length over an alphabet of size . The upper bound is based on a combinatorial approach and the lower bound is based on a probabilistic construction. Both bounds can improve previous results when is small compared to , say for some constant . Furthermore, we pay special attention to binary frameproof codes. We show a binary -frameproof code of length can not have more than codewords if .',
	 'authors': u'Chong Shangguan, Xin Wang, Gennian Ge, Ying Miao,',
	 'category': u'Computer Science ',
	 'date': '2014-11-21',
	 'pdflink': u'http://arxiv.org/pdf/1411.5782',
	 'subjects': u'Information Theory (cs.IT)',
	 'title': u'\nNew Bounds For Frameproof Codes',
	 'urllink': u'http://arxiv.org/abs/1411.5782'}
2015-04-10 16:34:03+0000 [xxu46_10] INFO: Crawled 781 pages (at 1 pages/min), scraped 774 items (at 1 items/min)
2015-04-10 16:34:46+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1411.5768> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 16:34:46+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1411.5768>
	{'abstract': u'Packing and vehicle routing problems play an important role in the area of supply chain management. In this paper, we introduce a non-linear knapsack problem that occurs when packing items along a fixed route and taking into account travel time. We investigate constrained and unconstrained versions of the problem and show that both are NP-hard. In order to solve the problems, we provide a pre-processing scheme as well as exact and approximate mixed integer programming (MIP) solutions. Our experimental results show the effectiveness of the MIP solutions and in particular point out that the approximate MIP approach often leads to near optimal results within far less computation time than the exact approach.',
	 'authors': u'Sergey Polyakovskiy, Frank Neumann,',
	 'category': u'Computer Science ',
	 'date': '2014-11-21',
	 'pdflink': u'http://arxiv.org/pdf/1411.5768',
	 'subjects': u'Data Structures and Algorithms (cs.DS)',
	 'title': u'\nPacking While Traveling: Mixed Integer Programming for a Class of  Nonlinear Knapsack Problems',
	 'urllink': u'http://arxiv.org/abs/1411.5768'}
2015-04-10 16:35:03+0000 [xxu46_10] INFO: Crawled 782 pages (at 1 pages/min), scraped 775 items (at 1 items/min)
2015-04-10 16:36:02+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1411.5767> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 16:36:02+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1411.5767>
	{'abstract': u"This paper studies a Shannon-theoretic version of the generalized distribution preserving quantization problem where a stationary and memoryless source is encoded subject to a distortion constraint and the additional requirement that the reproduction also be stationary and memoryless with a given distribution. The encoder and decoder are stochastic and assumed to have access to independent common randomness. Recent work has characterized the minimum achievable coding rate at a given distortion level when unlimited common randomness is available. Here we consider the general case where the available common randomness may be rate limited. Our main result completely characterizes the set of achievable coding and common randomness rate pairs at any distortion level, thereby providing the optimal tradeoff between these two rate quantities. We also consider two variations of this problem where we investigate the effect of relaxing the strict output distribution constraint and the role of `private randomness' used by the decoder on the rate region. Our results have strong connections with Cuff's recent work on distributed channel synthesis. In particular, our achievability proof combines a coupling argument with the approach developed by Cuff, where instead of explicitly constructing the encoder-decoder pair, a joint distribution is constructed from which a desired encoder-decoder pair is established. We show however that for our problem, the separated solution of first finding an optimal channel and then synthesizing this channel results in a suboptimal rate region.",
	 'authors': u'Naci Saldi, Tam\xe1s Linder, Serdar Y\xfcksel,',
	 'category': u'Computer Science ',
	 'date': '2014-11-21',
	 'pdflink': u'http://arxiv.org/pdf/1411.5767',
	 'subjects': u'Information Theory (cs.IT)',
	 'title': u'\nOutput Constrained Lossy Source Coding with Limited Common Randomness',
	 'urllink': u'http://arxiv.org/abs/1411.5767'}
2015-04-10 16:36:03+0000 [xxu46_10] INFO: Crawled 783 pages (at 1 pages/min), scraped 776 items (at 1 items/min)
2015-04-10 16:37:03+0000 [xxu46_10] INFO: Crawled 783 pages (at 0 pages/min), scraped 776 items (at 0 items/min)
2015-04-10 16:37:11+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1411.5765> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 16:37:11+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1411.5765>
	{'abstract': u'We prove that completing an untimed, unbounded track in TrackMania Nations Forever is NP-complete by using a reduction from 3-SAT and showing that a solution can be checked in polynomial time.',
	 'authors': u'Franck Dernoncourt,',
	 'category': u'Computer Science ',
	 'date': '2014-11-21',
	 'pdflink': u'http://arxiv.org/pdf/1411.5765',
	 'subjects': u'Computational Complexity (cs.CC)',
	 'title': u'\nTrackMania is NP-complete',
	 'urllink': u'http://arxiv.org/abs/1411.5765'}
2015-04-10 16:38:03+0000 [xxu46_10] INFO: Crawled 784 pages (at 1 pages/min), scraped 777 items (at 1 items/min)
2015-04-10 16:38:24+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1411.5752> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 16:38:24+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1411.5752>
	{'abstract': u'Recognition algorithms based on convolutional networks (CNNs) typically use the output of the last layer as feature representation. However, the information in this layer may be too coarse to allow precise localization. On the contrary, earlier layers may be precise in localization but will not capture semantics. To get the best of both worlds, we define the hypercolumn at a pixel as the vector of activations of all CNN units above that pixel. Using hypercolumns as pixel descriptors, we show results on three fine-grained localization tasks: simultaneous detection and segmentation[21], where we improve state-of-the-art from 49.7[21] mean AP^r to 59.0, keypoint localization, where we get a 3.3 point boost over[19] and part labeling, where we show a 6.6 point gain over a strong baseline.',
	 'authors': u'Bharath Hariharan, Pablo Arbel\xe1ez, Ross Girshick, Jitendra Malik,',
	 'category': u'Computer Science ',
	 'date': '2014-11-21',
	 'pdflink': u'http://arxiv.org/pdf/1411.5752',
	 'subjects': u'Computer Vision and Pattern Recognition (cs.CV)',
	 'title': u'\nHypercolumns for Object Segmentation and Fine-grained Localization',
	 'urllink': u'http://arxiv.org/abs/1411.5752'}
2015-04-10 16:39:03+0000 [xxu46_10] INFO: Crawled 785 pages (at 1 pages/min), scraped 778 items (at 1 items/min)
2015-04-10 16:39:27+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1411.5739> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 16:39:27+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1411.5739>
	{'abstract': u'Given a universe of elements and a collection of subsets of , the maximum disjoint set cover problem (DSCP) is to partition into as many set covers as possible, where a set cover is defined as a collection of subsets whose union is . We consider the online DSCP, in which the subsets arrive one by one (possibly in an order chosen by an adversary), and must be irrevocably assigned to some partition on arrival with the objective of minimizing the competitive ratio. The competitive ratio of an online DSCP algorithm is defined as the maximum ratio of the number of disjoint set covers obtained by the optimal offline algorithm to the number of disjoint set covers obtained by across all inputs. We propose an online algorithm for solving the DSCP with competitive ratio . We then show a lower bound of on the competitive ratio for any online DSCP algorithm. The online disjoint set cover problem has wide ranging applications in practice, including the online crowd-sourcing problem, the online coverage lifetime maximization problem in wireless sensor networks, and in online resource allocation problems.',
	 'authors': u'Ashwin Pananjady, Vivek Kumar Bagaria, Rahul Vaze,',
	 'category': u'Computer Science ',
	 'date': '2014-11-21',
	 'pdflink': u'http://arxiv.org/pdf/1411.5739',
	 'subjects': u'Data Structures and Algorithms (cs.DS)',
	 'title': u'\nThe Online Disjoint Set Cover Problem and its Applications',
	 'urllink': u'http://arxiv.org/abs/1411.5739'}
2015-04-10 16:40:03+0000 [xxu46_10] INFO: Crawled 786 pages (at 1 pages/min), scraped 779 items (at 1 items/min)
2015-04-10 16:40:26+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1411.5737> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 16:40:26+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1411.5737>
	{'abstract': u'In this paper, we describe an algorithm FARDiff (Fuzzy Adaptive Resonance Dif- fusion) which combines Diffusion Maps and Fuzzy Adaptive Resonance Theory to do clustering on high dimensional data. We describe some applications of this method and some problems for future research.',
	 'authors': u'S. B. Damelin, Y. Gu, D. C. Wunsch II, R. Xu,',
	 'category': u'Computer Science ',
	 'date': '2014-11-21',
	 'pdflink': u'http://arxiv.org/pdf/1411.5737',
	 'subjects': u'Neural and Evolutionary Computing (cs.NE)',
	 'title': u'\nFuzzy Adaptive Resonance Theory, Diffusion Maps and their applications  to Clustering and Biclustering',
	 'urllink': u'http://arxiv.org/abs/1411.5737'}
2015-04-10 16:41:03+0000 [xxu46_10] INFO: Crawled 787 pages (at 1 pages/min), scraped 780 items (at 1 items/min)
2015-04-10 16:41:13+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1411.5735> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 16:41:13+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1411.5735>
	{'abstract': u'We study the minimization problem of a non-convex sparsity promoting penalty function, the transformed (TL1), and its application in compressed sensing (CS). The TL1 penalty interpolates and norms through a nonnegative parameter , similar to with . TL1 is known in the statistics literature to enjoy three desired properties: unbiasedness, sparsity and Lipschitz continuity. We first consider the constrained minimization problem and prove the uniqueness of global minimizer and its equivalence to norm minimization if the sensing matrix satisfies a restricted isometry property (RIP) and if , where depends only on . The solution is stable under noisy measurement. For general sensing matrix , we show that the support set of a local minimizer corresponds to linearly independent columns of , and recall sufficient conditions for a critical point to be a local minimum. Next, we present difference of convex algorithms for TL1 (DCATL1) in computing TL1-regularized constrained and unconstrained problems in CS. For the unconstrained problem, we prove convergence of DCALT1 to a stationary point satisfying the first order optimality condition. Finally in numerical experiments, we identify the optimal value , and compare DCATL1 with other CS algorithms on three classes of sensing matrices: Gaussian random matrices, over-sampled discrete cosine transform matrices (ODCT), and uniformly distributed M-sphere matrices. We find that for all three classes of sensing matrices, the performance of DCATL1 algorithm (initiated with minimization) always ranks near the top (if not the top), and is the most robust choice insensitive to RIP (incoherence) of the underlying CS problems.',
	 'authors': u'Shuai Zhang, Jack Xin,',
	 'category': u'Computer Science ',
	 'date': '2014-11-21',
	 'pdflink': u'http://arxiv.org/pdf/1411.5735',
	 'subjects': u'Information Theory (cs.IT)',
	 'title': u'\nMinimization of Transformed L_1 Penalty: Theory, Difference of Convex  Function Algorithm, and Robust Application in Compressed Sensing',
	 'urllink': u'http://arxiv.org/abs/1411.5735'}
2015-04-10 16:42:03+0000 [xxu46_10] INFO: Crawled 788 pages (at 1 pages/min), scraped 781 items (at 1 items/min)
2015-04-10 16:42:22+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1411.5732> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 16:42:22+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1411.5732>
	{'abstract': u'Estimating the difficulty level of math word problems is an important task for many educational applications. Identification of relevant and irrelevant sentences in math word problems is an important step for calculating the difficulty levels of such problems. This paper addresses a novel application of text categorization to identify two types of sentences in mathematical word problems, namely relevant and irrelevant sentences. A novel joint probabilistic classification model is proposed to estimate the joint probability of classification decisions for all sentences of a math word problem by utilizing the correlation among all sentences along with the correlation between the question sentence and other sentences, and sentence text. The proposed model is compared with i) a SVM classifier which makes independent classification decisions for individual sentences by only using the sentence text and ii) a novel SVM classifier that considers the correlation between the question sentence and other sentences along with the sentence text. An extensive set of experiments demonstrates the effectiveness of the joint probabilistic classification model for identifying relevant and irrelevant sentences as well as the novel SVM classifier that utilizes the correlation between the question sentence and other sentences. Furthermore, empirical results and analysis show that i) it is highly beneficial not to remove stopwords and ii) utilizing part of speech tagging does not make a significant improvement although it has been shown to be effective for the related task of math word problem type classification.',
	 'authors': u'Suleyman Cetintas, Luo Si, Yan Ping Xin, Dake Zhang, Joo Young Park, Ron Tzur,',
	 'category': u'Computer Science ',
	 'date': '2014-11-21',
	 'pdflink': u'http://arxiv.org/pdf/1411.5732',
	 'subjects': u'Computation and Language (cs.CL)',
	 'title': u'\nA Joint Probabilistic Classification Model of Relevant and Irrelevant  Sentences in Mathematical Word Problems',
	 'urllink': u'http://arxiv.org/abs/1411.5732'}
2015-04-10 16:43:03+0000 [xxu46_10] INFO: Crawled 789 pages (at 1 pages/min), scraped 782 items (at 1 items/min)
2015-04-10 16:43:04+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1411.5731> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 16:43:04+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1411.5731>
	{'abstract': u'Images have become one of the most popular types of media through which users convey their emotions within online social networks. Although vast amount of research is devoted to sentiment analysis of textual data, there has been very limited work that focuses on analyzing sentiment of image data. In this work, we propose a novel visual sentiment prediction framework that performs image understanding with Deep Convolutional Neural Networks (CNN). Specifically, the proposed sentiment prediction framework performs transfer learning from a CNN with millions of parameters, which is pre-trained on large-scale data for object recognition. Experiments conducted on two real-world datasets from Twitter and Tumblr demonstrate the effectiveness of the proposed visual sentiment analysis framework.',
	 'authors': u'Can Xu, Suleyman Cetintas, Kuang-Chih Lee, Li-Jia Li,',
	 'category': u'Computer Science ',
	 'date': '2014-11-21',
	 'pdflink': u'http://arxiv.org/pdf/1411.5731',
	 'subjects': u'Computer Vision and Pattern Recognition (cs.CV)',
	 'title': u'\nVisual Sentiment Prediction with Deep Convolutional Neural Networks',
	 'urllink': u'http://arxiv.org/abs/1411.5731'}
2015-04-10 16:44:03+0000 [xxu46_10] INFO: Crawled 790 pages (at 1 pages/min), scraped 783 items (at 1 items/min)
2015-04-10 16:44:18+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1411.5726> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 16:44:18+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1411.5726>
	{'abstract': u'Automatically describing an image with a sentence is a long-standing challenge in computer vision and natural language processing. Due to recent progress in object detection, attribute classification, action recognition, etc., there is renewed interest in this area. However, evaluating the quality of descriptions has proven to be challenging. We propose a novel paradigm for evaluating image descriptions that uses human consensus. This paradigm consists of three main parts: a new triplet-based method of collecting human annotations to measure consensus, a new automated metric that captures consensus, and two new datasets: PASCAL-50S and ABSTRACT-50S that contain 50 sentences describing each image. Our simple metric captures human judgment of consensus better than existing metrics across sentences generated by various sources. We also evaluate five state-of-the-art image description approaches using this new protocol and provide a benchmark for future comparisons.',
	 'authors': u'Ramakrishna Vedantam, C. Lawrence Zitnick, Devi Parikh,',
	 'category': u'Computer Science ',
	 'date': '2014-11-20',
	 'pdflink': u'http://arxiv.org/pdf/1411.5726',
	 'subjects': u'Computer Vision and Pattern Recognition (cs.CV)',
	 'title': u'\nCIDEr: Consensus-based Image Description Evaluation',
	 'urllink': u'http://arxiv.org/abs/1411.5726'}
2015-04-10 16:45:03+0000 [xxu46_10] INFO: Crawled 791 pages (at 1 pages/min), scraped 784 items (at 1 items/min)
2015-04-10 16:45:07+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1411.5712> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 16:45:07+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1411.5712>
	{'abstract': u'We study strong equilibria in symmetric capacitated cost-sharing games. In these games, a graph with designated source and sink is given, and each edge is associated with some cost. Each agent chooses strategically an - path, knowing that the cost of each edge is shared equally between all agents using it. Two variants of cost-sharing games have been previously studied: (i) games where coalitions can form, and (ii) games where edges are associated with capacities; both variants are inspired by real-life scenarios. In this work we combine these variants and analyze strong equilibria (profiles where no coalition can deviate) in capacitated games. This combination gives rise to new phenomena that do not occur in the previous variants. Our contribution is two-fold. First, we provide a topological characterization of networks that always admit a strong equilibrium. Second, we establish tight bounds on the efficiency loss that may be incurred due to strategic behavior, as quantified by the strong price of anarchy (and stability) measures. Interestingly, our results are qualitatively different than those obtained in the analysis of each variant alone, and the combination of coalitions and capacities entails the introduction of more refined topology classes than previously studied.',
	 'authors': u'Michal Feldman, Ofir Geri,',
	 'category': u'Computer Science ',
	 'date': '2014-11-20',
	 'pdflink': u'http://arxiv.org/pdf/1411.5712',
	 'subjects': u'Computer Science and Game Theory (cs.GT)',
	 'title': u'\nDo Capacity Constraints Constrain Coalitions?',
	 'urllink': u'http://arxiv.org/abs/1411.5712'}
2015-04-10 16:45:45+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1411.5681> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 16:45:45+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1411.5681>
	{'abstract': u'The grid theorem, originally proved by Robertson and Seymour in Graph Minors V in 1986, is one of the most central results in the study of graph minors. It has found numerous applications in algorithmic graph structure theory, for instance in bidimensionality theory, and it is the basis for several other structure theorems developed in the graph minors project. In the mid-90s, Reed and Johnson, Robertson, Seymour and Thomas (see [Reed 97, Johnson, Robertson, Seymour, Thomas 01]), independently, conjectured an analogous theorem for directed graphs, i.e. the existence of a function f : N -&gt; N such that every digraph of directed tree-width at least f(k) contains a directed grid of order k. In an unpublished manuscript from 2001, Johnson, Robertson, Seymour and Thomas give a proof of this conjecture for planar digraphs. But for over a decade, this was the most general case proved for the Reed, Johnson, Robertson, Seymour and Thomas conjecture. Only very recently, this result has been extended to all classes of digraphs excluding a fixed undirected graph as a minor (see [Kawarabayashi, Kreutzer 14]). In this paper, nearly two decades after the conjecture was made, we are finally able to confirm the Reed, Johnson, Robertson, Seymour and Thomas conjecture in full generality and to prove the directed grid theorem. As consequence of our results we are able to improve results in Reed et al. in 1996 [Reed, Robertson, Seymour, Thomas 96] (see also [Open Problem Garden]) on disjoint cycles of length at least l and in [Kawarabayashi, Kobayashi, Kreutzer 14] on quarter-integral disjoint paths. We expect many more algorithmic results to follow from the grid theorem.',
	 'authors': u'Ken-ichi Kawarabayashi, Stephan Kreutzer,',
	 'category': u'Computer Science ',
	 'date': '2014-11-20',
	 'pdflink': u'http://arxiv.org/pdf/1411.5681',
	 'subjects': u'Discrete Mathematics (cs.DM)',
	 'title': u'\nThe Directed Grid Theorem',
	 'urllink': u'http://arxiv.org/abs/1411.5681'}
2015-04-10 16:46:03+0000 [xxu46_10] INFO: Crawled 793 pages (at 2 pages/min), scraped 786 items (at 2 items/min)
2015-04-10 16:46:48+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1411.5679> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 16:46:48+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1411.5679>
	{'abstract': u'This paper explores and clarifies several issues surrounding Zeno machines and the issue of running a Turing machine for infinite time. Without a minimum hypothetical bound on physical conditions, any magical machine can be created, and therefore, a thesis on the bound is formulated. This paper then proves that the halting problem algorithm for every Turing-recognizable program and every input cannot be devised whatever method is used to exploit infinite running-time of Turing machine.',
	 'authors': u'Bryce M. Kim,',
	 'category': u'Computer Science ',
	 'date': '2014-11-20',
	 'pdflink': u'http://arxiv.org/pdf/1411.5679',
	 'subjects': u'Formal Languages and Automata Theory (cs.FL)',
	 'title': u'\nZeno machines and Running Turing machine for infinite time',
	 'urllink': u'http://arxiv.org/abs/1411.5679'}
2015-04-10 16:47:03+0000 [xxu46_10] INFO: Crawled 794 pages (at 1 pages/min), scraped 787 items (at 1 items/min)
2015-04-10 16:48:03+0000 [xxu46_10] INFO: Crawled 794 pages (at 0 pages/min), scraped 787 items (at 0 items/min)
2015-04-10 16:48:07+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1411.5661> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 16:48:07+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1411.5661>
	{'abstract': u"An edge-coloring of a graph with colors is an interval -coloring if all colors are used, and the colors of edges incident to each vertex of are distinct and form an interval of integers. A graph is interval colorable if it has an interval -coloring for some positive integer . For an interval colorable graph , denotes the greatest value of for which has an interval -coloring. It is known that the complete graph is interval colorable if and only if the number of its vertices is even. However, the exact value of is known only for . The second author showed that if , where is odd and is nonnegative, then . Later, he conjectured that if , then , where is the number of 's in the binary representation of . In this paper we introduce a new technique to construct interval colorings of complete graphs based on their 1-factorizations, which is used to disprove the conjecture, improve lower and upper bounds on and determine its exact values for .",
	 'authors': u'Hrant H. Khachatrian, Petros A. Petrosyan,',
	 'category': u'Computer Science ',
	 'date': '2014-11-20',
	 'pdflink': u'http://arxiv.org/pdf/1411.5661',
	 'subjects': u'Discrete Mathematics (cs.DM)',
	 'title': u'\nInterval edge-colorings of complete graphs',
	 'urllink': u'http://arxiv.org/abs/1411.5661'}
2015-04-10 16:49:03+0000 [xxu46_10] INFO: Crawled 795 pages (at 1 pages/min), scraped 788 items (at 1 items/min)
2015-04-10 16:49:22+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1411.5654> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 16:49:22+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1411.5654>
	{'abstract': u'In this paper we explore the bi-directional mapping between images and their sentence-based descriptions. We propose learning this mapping using a recurrent neural network. Unlike previous approaches that map both sentences and images to a common embedding, we enable the generation of novel sentences given an image. Using the same model, we can also reconstruct the visual features associated with an image given its visual description. We use a novel recurrent visual memory that automatically learns to remember long-term visual concepts to aid in both sentence generation and visual feature reconstruction. We evaluate our approach on several tasks. These include sentence generation, sentence retrieval and image retrieval. State-of-the-art results are shown for the task of generating novel image descriptions. When compared to human generated captions, our automatically generated captions are preferred by humans over of the time. Results are better than or comparable to state-of-the-art results on the image and sentence retrieval tasks for methods using similar visual features.',
	 'authors': u'Xinlei Chen, C. Lawrence Zitnick,',
	 'category': u'Computer Science ',
	 'date': '2014-11-20',
	 'pdflink': u'http://arxiv.org/pdf/1411.5654',
	 'subjects': u'Computer Vision and Pattern Recognition (cs.CV)',
	 'title': u'\nLearning a Recurrent Visual Representation for Image Caption Generation',
	 'urllink': u'http://arxiv.org/abs/1411.5654'}
2015-04-10 16:50:03+0000 [xxu46_10] INFO: Crawled 796 pages (at 1 pages/min), scraped 789 items (at 1 items/min)
2015-04-10 16:50:05+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1411.5649> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 16:50:05+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1411.5649>
	{'abstract': u"In the convex optimization approach to online regret minimization, many methods have been developed to guarantee a regret bound for subdifferentiable convex loss functions with bounded subgradients by means of a reduction to bounded linear loss functions. This suggests that the latter tend to be the hardest loss functions to learn against. We investigate this question in a systematic fashion as a function of the decision set and the environment's set of moves. On the one hand, we exhibit a localization property for linear losses leading to learning rates and provide examples where this property holds. On the other hand, we establish lower bounds on the minimum achievable regret for a class of piecewise linear loss functions that subsumes the class of bounded linear loss functions and for polyhedral decision sets. These results hold in a completely adversarial setting. In contrast, we show that the minimum achievable regret can be significantly smaller when the opponent is greedy.",
	 'authors': u'Arthur Flajolet, Patrick Jaillet,',
	 'category': u'Computer Science ',
	 'date': '2014-11-20',
	 'pdflink': u'http://arxiv.org/pdf/1411.5649',
	 'subjects': u'Learning (cs.LG)',
	 'title': u'\nNo-Regret Learnability for Piecewise Linear Losses',
	 'urllink': u'http://arxiv.org/abs/1411.5649'}
2015-04-10 16:51:03+0000 [xxu46_10] INFO: Crawled 797 pages (at 1 pages/min), scraped 790 items (at 1 items/min)
2015-04-10 16:51:08+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1411.5635> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 16:51:08+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1411.5635>
	{'abstract': u'An answer set is a plain set of literals which has no further structure that would explain why certain literals are part of it and why others are not. We show how argumentation theory can help to explain why a literal is or is not contained in a given answer set by defining two justification methods, both of which make use of the correspondence between answer sets of a logic program and stable extensions of the Assumption-Based Argumentation (ABA) framework constructed from the same logic program. Attack Trees justify a literal in argumentation-theoretic terms, i.e. using arguments and attacks between them, whereas ABA-Based Answer Set Justifications express the same justification structure in logic programming terms, that is using literals and their relationships. Interestingly, an ABA-Based Answer Set Justification corresponds to an admissible fragment of the answer set in question, and an Attack Tree corresponds to an admissible fragment of the stable extension corresponding to this answer set.',
	 'authors': u'Claudia Schulz, Francesca Toni,',
	 'category': u'Computer Science ',
	 'date': '2014-11-20',
	 'pdflink': u'http://arxiv.org/pdf/1411.5635',
	 'subjects': u'Artificial Intelligence (cs.AI)',
	 'title': u'\nJustifying Answer Sets using Argumentation',
	 'urllink': u'http://arxiv.org/abs/1411.5635'}
2015-04-10 16:52:03+0000 [xxu46_10] INFO: Crawled 798 pages (at 1 pages/min), scraped 791 items (at 1 items/min)
2015-04-10 16:52:11+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1411.5630> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 16:52:11+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1411.5630>
	{'abstract': u'In this paper, we give a constant approximation for Capacitated -median( CKM), by violating the cardinality constraint by a factor of . This generalizes the result of [Li15], which only works for the uniform capacitated case. Our algorithm gives the first constant approximation for general CKM that only opens facilities. Indeed, most previous algorithms for CKM are based on the natural LP relaxation for the problem, which has unbounded integrality gap even if facilities can be opened. Instead, our algorithm is based on a novel configuration LP for the problem. For each set of potential facilities, we try to characterize the convex hull of all valid integral solutions restricted to the instance defined by and all clients. In order to reduce the size of the polytope, we cut the polytope into two parts: conditioned on the event that a few facilities are open in we have the exact polytope; conditioned on the event that many facilities are open, we only have a relaxed polytope. This LP can not be solved efficiently as there are exponential number of sets . Instead, we use the standard trick: that given a fractional solution, our rounding algorithm either succeeds, or finds a set such that the fractional solution is not valid for the set . This allows us to combine the rounding algorithm with the ellipsoid method.',
	 'authors': u'Shi Li,',
	 'category': u'Computer Science ',
	 'date': '2014-11-20',
	 'pdflink': u'http://arxiv.org/pdf/1411.5630',
	 'subjects': u'Data Structures and Algorithms (cs.DS)',
	 'title': u'\nApproximating capacitated $k$-median with $(1+\u03b5)k$ open  facilities',
	 'urllink': u'http://arxiv.org/abs/1411.5630'}
2015-04-10 16:53:03+0000 [xxu46_10] INFO: Crawled 799 pages (at 1 pages/min), scraped 792 items (at 1 items/min)
2015-04-10 16:53:06+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1411.5611> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 16:53:06+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1411.5611>
	{'abstract': u'These proceedings are gathering twelve different research papers developping the theory of recognizability for various kinds of discrete objects: words. terms, graphs, etc...',
	 'authors': u'G\xe9raud S\xe9nizergues,',
	 'category': u'Computer Science ',
	 'date': '2014-11-20',
	 'pdflink': u'http://arxiv.org/html/1411.5611',
	 'subjects': u'Formal Languages and Automata Theory (cs.FL)',
	 'title': u'\nFREC 14: FRontiers of RECognizability',
	 'urllink': u'http://arxiv.org/abs/1411.5611'}
2015-04-10 16:53:55+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1411.5599> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 16:53:55+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1411.5599>
	{'abstract': u'We propose a communication-driven mechanism for predicting triadic closure in complex networks. It is mathematically formulated on the basis of communicability distance functions that account for the "goodness" of communication between nodes in the network. We study real-world networks and show that the proposed method predicts correctly of triadic closures in these networks, in contrast to the predicted by a random mechanism. We also show that the communication-driven method outperforms the random mechanism in explaining the clustering coefficient, average path length, average communicability, and degree heterogeneity of a network. The new method also displays some interesting features toward its use for optimizing communication and degree heterogeneity in networks.',
	 'authors': u'Ernesto Estrada, Francesca Arrigo,',
	 'category': u'Computer Science ',
	 'date': '2014-11-20',
	 'pdflink': u'http://arxiv.org/pdf/1411.5599',
	 'subjects': u'Social and Information Networks (cs.SI)',
	 'title': u'\nPredicting triadic closure in networks using communicability distance  functions',
	 'urllink': u'http://arxiv.org/abs/1411.5599'}
2015-04-10 16:54:03+0000 [xxu46_10] INFO: Crawled 801 pages (at 2 pages/min), scraped 794 items (at 2 items/min)
2015-04-10 16:55:03+0000 [xxu46_10] INFO: Crawled 801 pages (at 0 pages/min), scraped 794 items (at 0 items/min)
2015-04-10 16:55:10+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1411.5595> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 16:55:10+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1411.5595>
	{'abstract': u'The Global Vectors for word representation (GloVe), introduced by Jeffrey Pennington et al. is reported to be an efficient and effective method for learning vector representations of words. State-of-the-art performance is also provided by skip-gram with negative-sampling (SGNS) implemented in the word2vec tool. In this note, we explain the similarities between the training objectives of the two models, and show that the objective of SGNS is similar to the objective of a specialized form of GloVe, though their cost functions are defined differently.',
	 'authors': u'Tianze Shi, Zhiyuan Liu,',
	 'category': u'Computer Science ',
	 'date': '2014-11-20',
	 'pdflink': u'http://arxiv.org/pdf/1411.5595',
	 'subjects': u'Computation and Language (cs.CL)',
	 'title': u'\nLinking GloVe with word2vec',
	 'urllink': u'http://arxiv.org/abs/1411.5595'}
2015-04-10 16:56:03+0000 [xxu46_10] INFO: Crawled 802 pages (at 1 pages/min), scraped 795 items (at 1 items/min)
2015-04-10 16:56:08+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1411.5573> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 16:56:08+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1411.5573>
	{'abstract': u'In order to achieve competitive performance, abstract machines for Prolog and related languages end up being large and intricate, and incorporate sophisticated optimizations, both at the design and at the implementation levels. At the same time, efficiency considerations make it necessary to use low-level languages in their implementation. This makes them laborious to code, optimize, and, especially, maintain and extend. Writing the abstract machine (and ancillary code) in a higher-level language can help tame this inherent complexity. We show how the semantics of most basic components of an efficient virtual machine for Prolog can be described using (a variant of) Prolog. These descriptions are then compiled to C and assembled to build a complete bytecode emulator. Thanks to the high level of the language used and its closeness to Prolog, the abstract machine description can be manipulated using standard Prolog compilation and optimization techniques with relative ease. We also show how, by applying program transformations selectively, we obtain abstract machine implementations whose performance can match and even exceed that of state-of-the-art, highly-tuned, hand-crafted emulators.',
	 'authors': u'Jose F. Morales, Manuel Carro, Manuel Hermenegildo,',
	 'category': u'Computer Science ',
	 'date': '2014-11-20',
	 'pdflink': u'http://arxiv.org/pdf/1411.5573',
	 'subjects': u'Programming Languages (cs.PL)',
	 'title': u'\nDescription and Optimization of Abstract Machines in a Dialect of Prolog',
	 'urllink': u'http://arxiv.org/abs/1411.5573'}
2015-04-10 16:57:03+0000 [xxu46_10] INFO: Crawled 803 pages (at 1 pages/min), scraped 796 items (at 1 items/min)
2015-04-10 16:57:14+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1411.5563> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 16:57:14+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1411.5563>
	{'abstract': u'Relationships between objects constitute our notion of space. When these relationships change we interpret this as the passage of time. Observer interpretations are essential to the way we understand these relationships. Hence observer semantics are an integral part of what we mean by spacetime. Semantics make up the essential difference in how one describes and uses the concept of space in physics, chemistry, biology and technology. In these notes, I have tried to assemble what seems to be a set of natural, and pragmatic, considerations about discrete, finite spacetimes, to unify descriptions of these areas. It reviews familiar notions of spacetime, and brings them together into a less familiar framework of promise theory (autonomous agents), in order to illuminate the goal of encoding the semantics of observers into a description of spacetime itself. Autonomous agents provide an exacting atomic and local model for finite spacetime, which quickly reveals the issues of incomplete information and non-locality. From this we should be able to reconstruct all other notions of spacetime. The aim of this exercise is to apply related tools and ideas to an initial unification of real and artificial spaces, e.g. databases and information webs with natural spacetime. By reconstructing these spaces from autonomous agents, we may better understand naming and coordinatization of semantic spaces, from crowds and swarms to datacentres and libraries, as well as the fundamental arena of natural science.',
	 'authors': u'Mark Burgess,',
	 'category': u'Computer Science ',
	 'date': '2014-11-20',
	 'pdflink': u'http://arxiv.org/pdf/1411.5563',
	 'subjects': u'Multiagent Systems (cs.MA)',
	 'title': u'\nSpacetimes with Semantics',
	 'urllink': u'http://arxiv.org/abs/1411.5563'}
2015-04-10 16:58:03+0000 [xxu46_10] INFO: Crawled 804 pages (at 1 pages/min), scraped 797 items (at 1 items/min)
2015-04-10 16:58:30+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1411.5555> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 16:58:30+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1411.5555>
	{'abstract': u'We explore the problems of classification of composite object (images, speech signals) with low number of models per class. We study the question of improving recognition performance for medium-sized database (thousands of classes). The key issue of fast approximate nearest-neighbor methods widely applied in this task is their heuristic nature. It is possible to strongly prove their efficiency by using the theory of algorithms only for simple similarity measures and artificially generated tasks. On the contrary, in this paper we propose an alternative, statistically optimal greedy algorithm. At each step of this algorithm joint density (likelihood) of distances to previously checked models is estimated for each class. The next model to check is selected from the class with the maximal likelihood. The latter is estimated based on the asymptotic properties of the Kullback-Leibler information discrimination and mathematical model of piecewise-regular object with distribution of each regular segment of exponential type. Experimental results in face recognition for FERET dataset prove that the proposed method is much more effective than not only brute force and the baseline (directed enumeration method) but also approximate nearest neighbor methods from FLANN and NonMetricSpaceLib libraries (randomized kd-tree, composite index, perm-sort).',
	 'authors': u'Andrey Savchenko,',
	 'category': u'Computer Science ',
	 'date': '2014-11-20',
	 'pdflink': u'http://arxiv.org/pdf/1411.5555',
	 'subjects': u'Computer Vision and Pattern Recognition (cs.CV)',
	 'title': u'\nMaximum Likelihood Directed Enumeration Method in Piecewise-Regular  Object Recognition',
	 'urllink': u'http://arxiv.org/abs/1411.5555'}
2015-04-10 16:59:03+0000 [xxu46_10] INFO: Crawled 805 pages (at 1 pages/min), scraped 798 items (at 1 items/min)
2015-04-10 16:59:50+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1411.5548> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 16:59:50+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1411.5548>
	{'abstract': u'In this article, we focus on inter-cell interference coordination (ICIC) techniques in heterogeneous network (Het-Net) deployments, whereby macro- and picocells autonomously optimize their downlink transmissions, with loose coordination. We model this strategic coexistence as a multi-agent system, aiming at joint interference management and cell association. Using tools from Reinforcement Learning (RL), agents (i.e., macro- and picocells) sense their environment, and self-adapt based on local information so as to maximize their network performance. Specifically, we explore both time- and frequency domain ICIC scenarios, and propose a two-level RL formulation. Here, picocells learn their optimal cell range expansion (CRE) bias and transmit power allocation, as well as appropriate frequency bands for multi-flow transmissions, in which a user equipment (UE) can be simultaneously served by two or more base stations (BSs) from macro- and pico-layers. To substantiate our theoretical findings, Long Term Evolution Advanced (LTEA) based system level simulations are carried out in which our proposed approaches are compared with a number of baseline approaches, such as resource partitioning (RP), static CRE, and single-flow Carrier Aggregation (CA). Our proposed solutions yield substantial gains up to 125% compared to static ICIC approaches in terms of average UE throughput in the timedomain. In the frequency-domain our proposed solutions yield gains up to 240% in terms of cell-edge UE throughput.',
	 'authors': u'Meryem Simsek, Mehdi Bennis, Ismail Guvenc,',
	 'category': u'Computer Science ',
	 'date': '2014-11-20',
	 'pdflink': u'http://arxiv.org/pdf/1411.5548',
	 'subjects': u'Networking and Internet Architecture (cs.NI)',
	 'title': u'\nLearning Based Frequency- and Time-Domain Inter-Cell Interference  Coordination in HetNets',
	 'urllink': u'http://arxiv.org/abs/1411.5548'}
2015-04-10 17:00:03+0000 [xxu46_10] INFO: Crawled 806 pages (at 1 pages/min), scraped 799 items (at 1 items/min)
2015-04-10 17:00:56+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1411.5547> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 17:00:56+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1411.5547>
	{'abstract': u'The explosive growth of content-on-the-move, such as video streaming to mobile devices, has propelled research on multimedia broadcast and multicast schemes. Multi-rate transmission strategies have been proposed as a means of delivering layered services to users experiencing different downlink channel conditions. In this paper, we consider Point-to-Multipoint layered service delivery across a generic cellular system and improve it by applying different random linear network coding approaches. We derive packet error probability expressions and use them as performance metrics in the formulation of resource allocation frameworks. The aim of these frameworks is both the optimization of the transmission scheme and the minimization of the number of broadcast packets on each downlink channel, while offering service guarantees to a predetermined fraction of users. As a case of study, our proposed frameworks are then adapted to the LTE-A standard and the eMBMS technology. We focus on the delivery of a video service based on the H.264/SVC standard and demonstrate the advantages of layered network coding over multi-rate transmission. Furthermore, we establish that the choice of both the network coding technique and resource allocation method play a critical role on the network footprint, and the quality of each received video layer.',
	 'authors': u'Andrea Tassi, Ioannis Chatzigeorgiou, Dejan Vukobratovi\u0107,',
	 'category': u'Computer Science ',
	 'date': '2014-11-20',
	 'pdflink': u'http://arxiv.org/pdf/1411.5547',
	 'subjects': u'Information Theory (cs.IT)',
	 'title': u'\nResource Allocation Frameworks for Network-coded Layered Multimedia  Multicast Services',
	 'urllink': u'http://arxiv.org/abs/1411.5547'}
2015-04-10 17:01:03+0000 [xxu46_10] INFO: Crawled 807 pages (at 1 pages/min), scraped 800 items (at 1 items/min)
2015-04-10 17:02:03+0000 [xxu46_10] INFO: Crawled 807 pages (at 0 pages/min), scraped 800 items (at 0 items/min)
2015-04-10 17:02:11+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1411.5494> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 17:02:11+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1411.5494>
	{'abstract': u'We present new results on the size of OBDD representations of structurally characterized classes of CNF formulas. First, we identify a natural sufficient condition, which we call the few subterms property, for a class of CNFs to have polynomial OBDD size; we then prove that CNFs whose incidence graphs are variable convex have few subterms (and hence have polynomial OBDD size), and observe that the few subterms property also explains the known fact that classes of CNFs of bounded treewidth have polynomial OBDD size. Second, we prove an exponential lower bound on the OBDD size of a family of CNF classes with incidence graphs of bounded degree, exploiting the combinatorial properties of expander graphs.',
	 'authors': u'Simone Bova, Friedrich Slivovsky,',
	 'category': u'Computer Science ',
	 'date': '2014-11-20',
	 'pdflink': u'http://arxiv.org/pdf/1411.5494',
	 'subjects': u'Logic in Computer Science (cs.LO)',
	 'title': u'\nOn Compiling Structured CNFs to OBDDs',
	 'urllink': u'http://arxiv.org/abs/1411.5494'}
2015-04-10 17:03:03+0000 [xxu46_10] INFO: Crawled 808 pages (at 1 pages/min), scraped 801 items (at 1 items/min)
2015-04-10 17:03:15+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1411.5474> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 17:03:15+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1411.5474>
	{'abstract': u'We present a new, dynamical way to study powers (or repetitions) in Sturmian words based on results from Diophantine approximation theory. As a result, we provide an alternative and shorter proof of a result by Damanik and Lenz characterizing powers in Sturmian words [Powers in Sturmian Sequences, European J. Combin. 24 (2003), 377--390]. Further, as a consequence, we obtain a previously known formula for the fractional index of a Sturmian word based on the continued fraction expansion of its slope.',
	 'authors': u'Jarkko Peltom\xe4ki,',
	 'category': u'Computer Science ',
	 'date': '2014-11-20',
	 'pdflink': u'http://arxiv.org/pdf/1411.5474',
	 'subjects': u'Discrete Mathematics (cs.DM)',
	 'title': u'\nCharacterization of Repetitions in Sturmian Words: A New Proof',
	 'urllink': u'http://arxiv.org/abs/1411.5474'}
2015-04-10 17:04:03+0000 [xxu46_10] INFO: Crawled 809 pages (at 1 pages/min), scraped 802 items (at 1 items/min)
2015-04-10 17:04:32+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1411.5472> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 17:04:32+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1411.5472>
	{'abstract': u'The -skeleton for a point set V is a family of geometric graphs, defined by the notion of neighborhoods parameterized by real number . By using the distance-based version definition of -skeletons we study those graphs for a set of points in space with and metrics. We present algorithms for the entire spectrum of values and we discuss properties of lens-based and circle-based -skeletons in those metrics. Let in metric be a set of points in general position. Then, for lens-based -skeleton can be computed in time. For there exists an time algorithm that constructs -skeleton for the set . We show that in with metric, for -skeleton for points can be computed in time. For there exists an time algorithm. In with metric for a set of points in arbitrary position -skeleton can be computed in time.',
	 'authors': u'Miros\u0142aw Kowaluk, Gabriela Majewska,',
	 'category': u'Computer Science ',
	 'date': '2014-11-20',
	 'pdflink': u'http://arxiv.org/pdf/1411.5472',
	 'subjects': u'Computational Geometry (cs.CG)',
	 'title': u'\nMultidimensional $\u03b2$-skeletons in $L_1$ and $L_{\\infty}$ metric',
	 'urllink': u'http://arxiv.org/abs/1411.5472'}
2015-04-10 17:05:03+0000 [xxu46_10] INFO: Crawled 810 pages (at 1 pages/min), scraped 803 items (at 1 items/min)
2015-04-10 17:05:48+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1411.5465> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 17:05:48+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1411.5465>
	{'abstract': u"Malicious anchor nodes will constantly hinder genuine and appropriate localization. Discovering the malicious or vulnerable anchor node is an essential problem in Wireless Sensor Networks (WSNs). In wireless sensor networks, anchor nodes are the nodes that know its current location. Neighbouring nodes or non-anchor nodes calculate its location (or its location reference) with the help of anchor nodes. Ingenuous localization is not possible in the presence of a cheating anchor node or a cheating node. Nowadays, it's a challenging task to identify the cheating anchor node or cheating node in a network. Even after finding out the location of the cheating anchor node, there is no assurance, that the identified node is legitimate or not. This paper aims to localize the cheating anchor nodes using trilateration algorithm and later associate it with maximum likelihood expectation technique (MLE), and Mahalanobis distance to obtain maximum accuracy in identifying malicious or cheating anchor nodes during localization. We were able to attain a considerable reduction in the error achieved during localization. For implementation purpose we simulated our scheme using ns-3 network simulator.",
	 'authors': u'Jeril Kuriakose, V. Amruth, Swathy Nandhini, V. Abhilash,',
	 'category': u'Computer Science ',
	 'date': '2014-11-20',
	 'pdflink': u'http://arxiv.org/pdf/1411.5465',
	 'subjects': u'Cryptography and Security (cs.CR)',
	 'title': u'\nIdentifying Cheating Anchor Nodes using Maximum Likelihood and  Mahalanobis Distance',
	 'urllink': u'http://arxiv.org/abs/1411.5465'}
2015-04-10 17:06:03+0000 [xxu46_10] INFO: Crawled 811 pages (at 1 pages/min), scraped 804 items (at 1 items/min)
2015-04-10 17:06:53+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1411.5461> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 17:06:53+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1411.5461>
	{'abstract': u'This paper investigates the capacity region of the three-receiver AWGN broadcast channel where the receivers (i) have private-message requests and (ii) may know some of the messages requested by other receivers as side information. We first classify all 64 possible side information configurations into eight groups, each consisting of eight members. We next construct transmission schemes, and derive new inner and outer bounds for the groups. This establishes the capacity region for 52 out of all 64 possible side information configurations. For six groups (i.e., groups 1, 2, 3, 5, 6, and 8 in our terminology), we establish the capacity region for all their members, and show that it tightens both the best known inner and outer bounds. For group 4, our inner and outer bounds tighten the best known inner bound and/or outer bound for all the group members. Moreover, our bounds coincide at certain regions, which can be characterized by two thresholds. For group 7, our inner and outer bounds coincide for four members, thereby establishing the capacity region. For the remaining four members, our bounds tighten both the best known inner and outer bounds.',
	 'authors': u'Behzad Asadi, Lawrence Ong, Sarah J. Johnson,',
	 'category': u'Computer Science ',
	 'date': '2014-11-20',
	 'pdflink': u'http://arxiv.org/pdf/1411.5461',
	 'subjects': u'Information Theory (cs.IT)',
	 'title': u'\nOptimal Coding Schemes for the Three-Receiver AWGN Broadcast Channel  with Receiver Message Side Information',
	 'urllink': u'http://arxiv.org/abs/1411.5461'}
2015-04-10 17:07:03+0000 [xxu46_10] INFO: Crawled 812 pages (at 1 pages/min), scraped 805 items (at 1 items/min)
2015-04-10 17:07:39+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1411.5459> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 17:07:39+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1411.5459>
	{'abstract': u'We present a new algorithm for lune-based -skeletons for sets of points in the plane, for , the only case when optimal algorithms are not known. The running time of the algorithm is , which is the best known and is an improvement of Rao and Mukhopadhyay cite result. The method is based on point location in monotonic subdivisions of arrangements of curve segments.',
	 'authors': u'Miros\u0142aw Kowaluk,',
	 'category': u'Computer Science ',
	 'date': '2014-11-20',
	 'pdflink': u'http://arxiv.org/pdf/1411.5459',
	 'subjects': u'Computational Geometry (cs.CG)',
	 'title': u'\nPlanar $\u03b2$-skeletons via point location in monotone subdivisions of  subset of lunes',
	 'urllink': u'http://arxiv.org/abs/1411.5459'}
2015-04-10 17:08:03+0000 [xxu46_10] INFO: Crawled 813 pages (at 1 pages/min), scraped 806 items (at 1 items/min)
2015-04-10 17:08:58+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1411.5458> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 17:08:58+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1411.5458>
	{'abstract': u"In this paper, we describe a new neuro-inspired, hardware-friendly readout stage for the liquid state machine (LSM), a popular model for reservoir computing. Compared to the parallel perceptron architecture trained by the p-delta algorithm, which is the state of the art in terms of performance of readout stages, our readout architecture and learning algorithm can attain better performance with significantly less synaptic resources making it attractive for VLSI implementation. Inspired by the nonlinear properties of dendrites in biological neurons, our readout stage incorporates neurons having multiple dendrites with a lumped nonlinearity. The number of synaptic connections on each branch is significantly lower than the total number of connections from the liquid neurons and the learning algorithm tries to find the best 'combination' of input connections on each branch to reduce the error. Hence, the learning involves network rewiring (NRW) of the readout network similar to structural plasticity observed in its biological counterparts. We show that compared to a single perceptron using analog weights, this architecture for the readout can attain, even by using the same number of binary valued synapses, up to 3.3 times less error for a two-class spike train classification problem and 2.4 times less error for an input rate approximation task. Even with 60 times larger synapses, a group of 60 parallel perceptrons cannot attain the performance of the proposed dendritically enhanced readout. An additional advantage of this method for hardware implementations is that the 'choice' of connectivity can be easily implemented exploiting address event representation (AER) protocols commonly used in current neuromorphic systems where the connection matrix is stored in memory. Also, due to the use of binary synapses, our proposed method is more robust against statistical variations.",
	 'authors': u'Subhrajit Roy, Amitava Banerjee, Arindam Basu,',
	 'category': u'Computer Science ',
	 'date': '2014-11-20',
	 'pdflink': u'http://arxiv.org/pdf/1411.5458',
	 'subjects': u'Emerging Technologies (cs.ET)',
	 'title': u'\nLiquid State Machine with Dendritically Enhanced Readout for Low-power,  Neuromorphic VLSI Implementations',
	 'urllink': u'http://arxiv.org/abs/1411.5458'}
2015-04-10 17:09:03+0000 [xxu46_10] INFO: Crawled 814 pages (at 1 pages/min), scraped 807 items (at 1 items/min)
2015-04-10 17:10:03+0000 [xxu46_10] INFO: Crawled 814 pages (at 0 pages/min), scraped 807 items (at 0 items/min)
2015-04-10 17:10:13+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1411.5457> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 17:10:13+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1411.5457>
	{'abstract': u'We generalize -skeletons, well-known neighborhood graphs for point sets, to sets of line segments and present algorithms for computing such skeletons, both circle and lens-based, for the entire range of values. In particular, for the -skeleton for a set on segments in the Euclidean plane can be constructed in time and the construction relies on the Delaunay triangulation for . When , the -skeleton can be constructed in a time. In a special case of , which is a generalization of Gabriel Graph, the construction can be carried out in a time. Additionally, we show that inclusions between Gabriel Graph (), Relative Neighborhood Graph (), and the Delaunay triangulation known to hold for sets of points, generalize to those graphs constructed for sets of segments.',
	 'authors': u'Miros\u0142aw Kowaluk, Gabriela Majewska,',
	 'category': u'Computer Science ',
	 'date': '2014-11-20',
	 'pdflink': u'http://arxiv.org/pdf/1411.5457',
	 'subjects': u'Computational Geometry (cs.CG)',
	 'title': u'\n$\u03b2$-skeletons for a set of line segments in $R^2$',
	 'urllink': u'http://arxiv.org/abs/1411.5457'}
2015-04-10 17:11:03+0000 [xxu46_10] INFO: Crawled 815 pages (at 1 pages/min), scraped 808 items (at 1 items/min)
2015-04-10 17:11:27+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1411.5455> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 17:11:27+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1411.5455>
	{'abstract': u'-skeletons, a prominent member of the neighborhood graph family, have interesting geometric properties and various applications ranging from geographic networks to archeology. This paper focuses on developing a new, more general than the present one, definition of -skeletons based only on the distance criterion. It allows us to consider them in many different cases, e.g. for weighted graphs or objects other than points. Two types of -skeletons are especially well-known: the Gabriel Graph (for ) and the Relative Neighborhood Graph (for ). The new definition retains relations between those graphs and the other well-known ones (minimum spanning tree and Delaunay triangulation). We also show several new algorithms finding -skeletons.',
	 'authors': u'Miros\u0142aw Kowaluk, Gabriela Majewska,',
	 'category': u'Computer Science ',
	 'date': '2014-11-20',
	 'pdflink': u'http://arxiv.org/pdf/1411.5455',
	 'subjects': u'Computational Geometry (cs.CG)',
	 'title': u'\nGeneralized $\u03b2$-skeletons',
	 'urllink': u'http://arxiv.org/abs/1411.5455'}
2015-04-10 17:12:03+0000 [xxu46_10] INFO: Crawled 816 pages (at 1 pages/min), scraped 809 items (at 1 items/min)
2015-04-10 17:12:39+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1411.5451> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 17:12:39+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1411.5451>
	{'abstract': u'The Name-Letter Effect states that people have a preference for brands, places, and even jobs that start with the same letter as their own first name. So Sam might like Snickers and live in Seattle. We use social network data from Twitter and Google+ to replicate this effect in a new environment. We find limited to no support for the Name-Letter Effect on social networks. We do, however, find a very robust Same-Name Effect where, say, Michaels would be more likely to link to other Michaels than Johns. This effect persists when accounting for gender, nationality, race, and age. The fundamentals behind these effects have implications beyond psychology as understanding how a positive self-image is transferred to other entities is important in domains ranging from studying homophily to personalized advertising and to link formation in social networks.',
	 'authors': u'Farshad Kooti, Gabriel Magno, Ingmar Weber,',
	 'category': u'Computer Science ',
	 'date': '2014-11-20',
	 'pdflink': u'http://arxiv.org/pdf/1411.5451',
	 'subjects': u'Social and Information Networks (cs.SI)',
	 'title': u'\nThe Social Name-Letter Effect on Online Social Networks',
	 'urllink': u'http://arxiv.org/abs/1411.5451'}
2015-04-10 17:13:03+0000 [xxu46_10] INFO: Crawled 817 pages (at 1 pages/min), scraped 810 items (at 1 items/min)
2015-04-10 17:13:43+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1411.5442> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 17:13:43+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1411.5442>
	{'abstract': u'Persistent homology and zigzag persistent homology are techniques which track the homology over a sequence of spaces, outputting a set of intervals corresponding to birth and death times of homological features in the sequence. This paper presents a method for choosing a homology class to correspond to each of the intervals at each time point. For each homology class a specific representative cycle is stored, with the choice of homology class and representative cycle being both geometrically relevant and compatible with the birth-death interval decomposition. After describing the method in detail and proving its correctness, we illustrate the utility of the method by applying it to the study of coverage holes in time-varying sensor networks.',
	 'authors': u'Jennifer Gamble, Harish Chintakunta, Hamid Krim,',
	 'category': u'Computer Science ',
	 'date': '2014-11-20',
	 'pdflink': u'http://arxiv.org/pdf/1411.5442',
	 'subjects': u'Computational Geometry (cs.CG)',
	 'title': u'\nAdaptive tracking of representative cycles in regular and zigzag  persistent homology',
	 'urllink': u'http://arxiv.org/abs/1411.5442'}
2015-04-10 17:14:03+0000 [xxu46_10] INFO: Crawled 818 pages (at 1 pages/min), scraped 811 items (at 1 items/min)
2015-04-10 17:14:54+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1411.5437> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 17:14:54+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1411.5437>
	{'abstract': u'In a typical regular expression (regex) crossword puzzle, you are given two nonempty lists and of regular expressions over some alphabet, and your goal is to fill in an grid with letters from that alphabet so that the string formed by the th row is in , and the string formed by the th column is in , for all and . Such a grid is a solution to the puzzle. It is known that determining whether a solution exists is NP-complete. We consider a number of restrictions and variants to this problem where all the are equal to some regular expression , and all the are equal to some regular expression . We call the solution to such a puzzle an -crossword. Our main results are the following: 1. There exists a fixed regular expression over the alphabet such that the following problem is NP-complete: "Given a regular expression over and positive integers and given in unary, does an -crossword exist?" This improves the result mentioned above. 2. The following problem is NP-hard: "Given a regular expression over and positive integers and given in unary, does an -crossword exist?" 3. There exists a fixed regular expression over such that the following problem is undecidable (equivalent to the Halting Problem): "Given a regular expression over , does an -crossword exist (of any size)?" 4. The following problem is undecidable (equivalent to the Halting Problem): "Given a regular expression over , does an -crossword exist (of any size)?"',
	 'authors': u'Stephen A. Fenner,',
	 'category': u'Computer Science ',
	 'date': '2014-11-20',
	 'pdflink': u'http://arxiv.org/pdf/1411.5437',
	 'subjects': u'Computational Complexity (cs.CC)',
	 'title': u'\nThe complexity of some regex crossword problems',
	 'urllink': u'http://arxiv.org/abs/1411.5437'}
2015-04-10 17:15:03+0000 [xxu46_10] INFO: Crawled 819 pages (at 1 pages/min), scraped 812 items (at 1 items/min)
2015-04-10 17:16:03+0000 [xxu46_10] INFO: Crawled 819 pages (at 0 pages/min), scraped 812 items (at 0 items/min)
2015-04-10 17:16:11+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1411.5433> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 17:16:11+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1411.5433>
	{'abstract': u"In this paper we describe the volunteer computing project SAT@home, developed and maintained by us. This project is aimed at solving hard instances of the Boolean satisfiability problem (SAT). We believe that this project can be a useful tool for computational study of inversion problems of some cryptographic functions. In particular we describe a series of experiments performed in SAT@home on the cryptanalysis of the widely known keystream generator A5/1. In all experiments we analyzed one known burst (114 bits) of keystream produced by A5/1. Before the cryptanalysis itself there is a stage on which the partitioning of the original problem to a family of subproblems is carried out. Each of subproblems should be easy enough so that it could be solved in relatively small amount of time by volunteer's PC. We construct such partitioning using the special technique based on the Monte Carlo method and discrete optimization algorithms for special predictive functions. Besides this in the paper we describe the technique for reducing inversion problems of cryptographic functions to SAT.",
	 'authors': u'Alexander Semenov, Oleg Zaikin, Ilya Otpuschennikov,',
	 'category': u'Computer Science ',
	 'date': '2014-11-20',
	 'pdflink': u'http://arxiv.org/pdf/1411.5433',
	 'subjects': u'Distributed, Parallel, and Cluster Computing (cs.DC)',
	 'title': u'\nUsing Volunteer Computing for Mounting SAT-based Cryptographic Attacks',
	 'urllink': u'http://arxiv.org/abs/1411.5433'}
2015-04-10 17:17:03+0000 [xxu46_10] INFO: Crawled 820 pages (at 1 pages/min), scraped 813 items (at 1 items/min)
2015-04-10 17:17:21+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1411.5428> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 17:17:21+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1411.5428>
	{'abstract': u'An important use of private data is to build machine learning classifiers. While there is a burgeoning literature on differentially private classification algorithms, we find that they are not practical in real applications due to two reasons. First, existing differentially private classifiers provide poor accuracy on real world datasets. Second, there is no known differentially private algorithm for empirically evaluating the private classifier on a private test dataset. In this paper, we develop differentially private algorithms that mirror real world empirical machine learning workflows. We consider the private classifier training algorithm as a blackbox. We present private algorithms for selecting features that are input to the classifier. Though adding a preprocessing step takes away some of the privacy budget from the actual classification process (thus potentially making it noisier and less accurate), we show that our novel preprocessing techniques significantly increase classifier accuracy on three real-world datasets. We also present the first private algorithms for empirically constructing receiver operating characteristic (ROC) curves on a private test set.',
	 'authors': u'Ben Stoddard, Yan Chen, Ashwin Machanavajjhala,',
	 'category': u'Computer Science ',
	 'date': '2014-11-20',
	 'pdflink': u'http://arxiv.org/pdf/1411.5428',
	 'subjects': u'Learning (cs.LG)',
	 'title': u'\nDifferentially Private Algorithms for Empirical Machine Learning',
	 'urllink': u'http://arxiv.org/abs/1411.5428'}
2015-04-10 17:18:03+0000 [xxu46_10] INFO: Crawled 821 pages (at 1 pages/min), scraped 814 items (at 1 items/min)
2015-04-10 17:18:27+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1411.5417> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 17:18:27+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1411.5417>
	{'abstract': u'Empirical Risk Minimization (ERM) is a standard technique in machine learning, where a model is selected by minimizing a loss function over constraint set. When the training dataset consists of private information, it is natural to use a differentially private ERM algorithm, and this problem has been the subject of a long line of work started with Chaudhuri and Monteleoni 2008. A private ERM algorithm outputs an approximate minimizer of the loss function and its error can be measured as the difference from the optimal value of the loss function. When the constraint set is arbitrary, the required error bounds are fairly well understood~ cite. In this work, we show that the geometric properties of the constraint set can be used to derive significantly better results. Specifically, we show that a differentially private version of Mirror Descent leads to error bounds of the form for a lipschitz loss function, improving on the bounds in Bassily, Smith and Thakurta 2014. Here is the dimensionality of the problem, is the number of data points in the training set, and denotes the Gaussian width of the constraint set that we optimize over. We show similar improvements for strongly convex functions, and for smooth functions. In addition, we show that when the loss function is Lipschitz with respect to the norm and is -bounded, a differentially private version of the Frank-Wolfe algorithm gives error bounds of the form . This captures the important and common case of sparse linear regression (LASSO), when the data satisfies and we optimize over the ball. We show new lower bounds for this setting, that together with known bounds, imply that all our upper bounds are tight.',
	 'authors': u'Kunal Talwar, Abhradeep Thakurta, Li Zhang,',
	 'category': u'Computer Science ',
	 'date': '2014-11-20',
	 'pdflink': u'http://arxiv.org/pdf/1411.5417',
	 'subjects': u'Learning (cs.LG)',
	 'title': u'\nPrivate Empirical Risk Minimization Beyond the Worst Case: The Effect of  the Constraint Set Geometry',
	 'urllink': u'http://arxiv.org/abs/1411.5417'}
2015-04-10 17:19:03+0000 [xxu46_10] INFO: Crawled 822 pages (at 1 pages/min), scraped 815 items (at 1 items/min)
2015-04-10 17:19:37+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1411.5416> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 17:19:37+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1411.5416>
	{'abstract': u'Arguments are essential objects in DirectDemocracyP2P, where they can occur both in association with signatures for petitions, or in association with other debated decisions, such as bug sorting by importance. The arguments of a signer on a given issue are grouped into one single justification, are classified by the type of signature (e.g., supporting or opposing), and can be subject to various types of threading. Given the available inputs, the two addressed problems are: (i) how to recommend the best justification, of a given type, to a new voter, (ii) how to recommend a compact list of justifications subsuming the majority of known arguments for (or against) an issue. We investigate solutions based on weighted bipartite graphs.',
	 'authors': u'Marius C. Silaghi, Roussi Roussev,',
	 'category': u'Computer Science ',
	 'date': '2014-11-20',
	 'pdflink': u'http://arxiv.org/pdf/1411.5416',
	 'subjects': u'Artificial Intelligence (cs.AI)',
	 'title': u'\nRecommending the Most Encompassing Opposing and Endorsing Arguments in  Debates',
	 'urllink': u'http://arxiv.org/abs/1411.5416'}
2015-04-10 17:20:03+0000 [xxu46_10] INFO: Crawled 823 pages (at 1 pages/min), scraped 816 items (at 1 items/min)
2015-04-10 17:20:42+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1411.5415> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 17:20:42+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1411.5415>
	{'abstract': u'Neighbor discovery plays a crucial role in the formation of wireless sensor networks and mobile networks where the power of sensors (or mobile devices) is constrained. Due to the difficulty of clock synchronization, many asynchronous protocols based on wake-up scheduling have been developed over the years in order to enable timely neighbor discovery between neighboring sensors while saving energy. However, existing protocols are not fine-grained enough to support all heterogeneous battery duty cycles, which can lead to a more rapid deterioration of long-term battery health for those without support. Existing research can be broadly divided into two categories according to their neighbor-discovery techniques---the quorum based protocols and the co-primality based protocols.In this paper, we propose two neighbor discovery protocols, called Hedis and Todis, that optimize the duty cycle granularity of quorum and co-primality based protocols respectively, by enabling the finest-grained control of heterogeneous duty cycles. We compare the two optimal protocols via analytical and simulation results, which show that although the optimal co-primality based protocol (Todis) is simpler in its design, the optimal quorum based protocol (Hedis) has a better performance since it has a lower relative error rate and smaller discovery delay, while still allowing the sensor nodes to wake up at a more infrequent rate.',
	 'authors': u'Lin Chen, Ruolin Fan, Kaigui Bian, Lin Chen, Mario Gerla, Tao Wang, Xiaoming Li,',
	 'category': u'Computer Science ',
	 'date': '2014-11-20',
	 'pdflink': u'http://arxiv.org/pdf/1411.5415',
	 'subjects': u'Networking and Internet Architecture (cs.NI)',
	 'title': u'\nOn Heterogeneous Neighbor Discovery in Wireless Sensor Networks',
	 'urllink': u'http://arxiv.org/abs/1411.5415'}
2015-04-10 17:21:03+0000 [xxu46_10] INFO: Crawled 824 pages (at 1 pages/min), scraped 817 items (at 1 items/min)
2015-04-10 17:21:56+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1411.5414> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 17:21:56+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1411.5414>
	{'abstract': u'Until a few years ago, the fastest known matrix multiplication algorithm, due to Coppersmith and Winograd (1990), ran in time . Recently, a surge of activity by Stothers, Vassilevska-Williams, and Le Gall has led to an improved algorithm running in time . These algorithms are obtained by analyzing higher and higher tensor powers of a certain identity of Coppersmith and Winograd. We show that this exact approach cannot result in an algorithm with running time , and identify a wide class of variants of this approach which cannot result in an algorithm with running time ; in particular, this approach cannot prove the conjecture that for every , two matrices can be multiplied in time . We describe a new framework extending the original laser method, which is the method underlying the previously mentioned algorithms. Our framework accommodates the algorithms by Coppersmith and Winograd, Stothers, Vassilevska-Williams and Le Gall. We obtain our main result by analyzing this framework. The framework is also the first to explain why taking tensor powers of the Coppersmith-Winograd identity results in faster algorithms.',
	 'authors': u'Andris Ambainis, Yuval Filmus, Fran\xe7ois Le Gall,',
	 'category': u'Computer Science ',
	 'date': '2014-11-20',
	 'pdflink': u'http://arxiv.org/pdf/1411.5414',
	 'subjects': u'Computational Complexity (cs.CC)',
	 'title': u'\nFast Matrix Multiplication: Limitations of the Laser Method',
	 'urllink': u'http://arxiv.org/abs/1411.5414'}
2015-04-10 17:22:03+0000 [xxu46_10] INFO: Crawled 825 pages (at 1 pages/min), scraped 818 items (at 1 items/min)
2015-04-10 17:23:03+0000 [xxu46_10] INFO: Crawled 825 pages (at 0 pages/min), scraped 818 items (at 0 items/min)
2015-04-10 17:23:09+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1411.5412> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 17:23:09+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1411.5412>
	{'abstract': u'Network motifs are overrepresented interconnection patterns found in real-world networks. What functional advantages may they offer for building complex systems? We show that most network motifs emerge from interconnections patterns that best exploit the intrinsic stability characteristics of individual nodes. This feature is observed at different scales in a network, from nodes to modules, suggesting an efficient mechanism to stably build complex systems.',
	 'authors': u'Marco Tulio Angulo, Yang-Yu Liu, Jean-Jacques Slotine,',
	 'category': u'Computer Science ',
	 'date': '2014-11-20',
	 'pdflink': u'http://arxiv.org/pdf/1411.5412',
	 'subjects': u'Systems and Control (cs.SY)',
	 'title': u'\nNetwork motifs emerge from interconnections that favor stability',
	 'urllink': u'http://arxiv.org/abs/1411.5412'}
2015-04-10 17:24:03+0000 [xxu46_10] INFO: Crawled 826 pages (at 1 pages/min), scraped 819 items (at 1 items/min)
2015-04-10 17:24:28+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1411.5410> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 17:24:28+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1411.5410>
	{'abstract': u'Model counting is the problem of computing the number of models that satisfy a given propositional theory. It has recently been applied to solving inference tasks in probabilistic logic programming, where the goal is to compute the probability of given queries being true provided a set of mutually independent random variables, a model (a logic program) and some evidence. The core of solving this inference task involves translating the logic program to a propositional theory and using a model counter. In this paper, we show that for some problems that involve inductive definitions like reachability in a graph, the translation of logic programs to SAT can be expensive for the purpose of solving inference tasks. For such problems, direct implementation of stable model semantics allows for more efficient solving. We present two implementation techniques, based on unfounded set detection, that extend a propositional model counter to a stable model counter. Our experiments show that for particular problems, our approach can outperform a state-of-the-art probabilistic logic programming solver by several orders of magnitude in terms of running time and space requirements, and can solve instances of significantly larger sizes on which the current solver runs out of time or memory.',
	 'authors': u'Rehan Abdul Aziz, Geoffrey Chu, Christian Muise, Peter Stuckey,',
	 'category': u'Computer Science ',
	 'date': '2014-11-20',
	 'pdflink': u'http://arxiv.org/pdf/1411.5410',
	 'subjects': u'Artificial Intelligence (cs.AI)',
	 'title': u'\nStable Model Counting and Its Application in Probabilistic Logic  Programming',
	 'urllink': u'http://arxiv.org/abs/1411.5410'}
2015-04-10 17:25:03+0000 [xxu46_10] INFO: Crawled 827 pages (at 1 pages/min), scraped 820 items (at 1 items/min)
2015-04-10 17:25:32+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1411.5404> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 17:25:32+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1411.5404>
	{'abstract': u'There has been great interest in recent years on statistical models for dynamic networks. In this paper, I propose a stochastic block transition model (SBTM) for dynamic networks that is inspired by the well-known stochastic block model (SBM) for static networks and previous dynamic extensions of the SBM. Unlike most existing dynamic network models, it does not make a hidden Markov assumption on the edge-level dynamics, allowing the presence or absence of edges to directly influence future edge probabilities while retaining the interpretability of the SBM. I derive an approximate inference procedure for the SBTM and demonstrate that it is significantly better at reproducing durations of edges in real social network data.',
	 'authors': u'Kevin S. Xu,',
	 'category': u'Computer Science ',
	 'date': '2014-11-19',
	 'pdflink': u'http://arxiv.org/pdf/1411.5404',
	 'subjects': u'Social and Information Networks (cs.SI)',
	 'title': u'\nStochastic Block Transition Models for Dynamic Networks',
	 'urllink': u'http://arxiv.org/abs/1411.5404'}
2015-04-10 17:26:03+0000 [xxu46_10] INFO: Crawled 828 pages (at 1 pages/min), scraped 821 items (at 1 items/min)
2015-04-10 17:26:26+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1411.5394> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 17:26:26+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1411.5394>
	{'abstract': u'This paper introduces the first wireless gesture recognition system that operates using existingWi-Fi signals and devices. To achieve this, we first identify limitations of existing wireless gesture recognition approaches that limit their applicability to Wi-Fi. We then introduce algorithms that can classify gestures using information that is readily available on Wi-Fi devices. We demonstrate the feasibility of our design using a prototype implementation on off-the-shelf Wi-Fi devices. Our results show that we can achieve a classification accuracy of 91% while classifying four gestures across six participants, without the need for per-participant training. Finally, we show the feasibility of gesture recognition in non-line-ofsight situations with the participants interacting with a Wi-Fi device placed in a backpack.',
	 'authors': u'Rajalakshmi Nandakumar, Bryce Kellogg, Shyamnath Gollakota,',
	 'category': u'Computer Science ',
	 'date': '2014-11-19',
	 'pdflink': u'http://arxiv.org/pdf/1411.5394',
	 'subjects': u'Human-Computer Interaction (cs.HC)',
	 'title': u'\nWi-Fi Gesture Recognition on Existing Devices',
	 'urllink': u'http://arxiv.org/abs/1411.5394'}
2015-04-10 17:27:03+0000 [xxu46_10] INFO: Crawled 829 pages (at 1 pages/min), scraped 822 items (at 1 items/min)
2015-04-10 17:27:40+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1411.5379> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 17:27:40+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1411.5379>
	{'abstract': u'Semantic parsing has made significant progress, but most current semantic parsers are extremely slow (CKY-based) and rather primitive in representation. We introduce three new techniques to tackle these problems. First, we design the first linear-time incremental shift-reduce-style semantic parsing algorithm which is more efficient than conventional cubic-time bottom-up semantic parsers. Second, our parser, being type-driven instead of syntax-driven, uses type-checking to decide the direction of reduction, which eliminates the need for a syntactic grammar such as CCG. Third, to fully exploit the power of type-driven semantic parsing beyond simple types (such as entities and truth values), we borrow from programming language theory the concepts of subtype polymorphism and parametric polymorphism to enrich the type system in order to better guide the parsing. Our system learns very accurate parses in GeoQuery, Jobs and Atis domains.',
	 'authors': u'Kai Zhao, Liang Huang,',
	 'category': u'Computer Science ',
	 'date': '2014-11-19',
	 'pdflink': u'http://arxiv.org/pdf/1411.5379',
	 'subjects': u'Computation and Language (cs.CL)',
	 'title': u'\nType-Driven Incremental Semantic Parsing with Polymorphism',
	 'urllink': u'http://arxiv.org/abs/1411.5379'}
2015-04-10 17:28:03+0000 [xxu46_10] INFO: Crawled 830 pages (at 1 pages/min), scraped 823 items (at 1 items/min)
2015-04-10 17:28:43+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1411.5343> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 17:28:43+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1411.5343>
	{'abstract': u'The rapid depletion of fossil fuel resources and environmental concerns has given awareness on generation of renewable energy resources. Among the various renewable resources, hybrid solar and wind energy seems to be promising solutions to provide reliable power supply with improved system efficiency and reduced storage requirements for stand-alone applications. This paper presents a feasibility assessment and optimum size of photovoltaic (PV) array, wind turbine and battery bank for a standalone hybrid Solar/Wind Power system (HSWPS) at remote telecom station of Nepal at Latitude (2723\'50") and Longitude (8644\'23") consisting a telecommunication load of Very Small Aperture Terminal (VSAT), Repeater station and Code Division Multiple Access Base Transceiver Station (CDMA 2C10 BTS). In any RES based system, the feasibility assessment is considered as the first step analysis. In this work, feasibility analysis is carried through hybrid optimization model for electric renewables (HOMER) and mathematical models were implemented in the MATLAB environment to perform the optimal configuration for a given load and a desired loss of power supply probability (LPSP) from a set of systems components with the lowest value of cost function defined in terms of reliability and levelized unit electricity cost (LUCE). The simulation results for the existing and the proposed models are compared. The simulation results shows that existing architecture consisting of 6.12 kW KC85T photovoltaic modules, 1kW H3.1 wind turbine and 1600 Ah GFM-800 battery bank have a 36.6% of unmet load during a year. On the other hand, the proposed system includes 1kW *2 H3.1 Wind turbine, 8.05 kW TSM-175DA01 photovoltaic modules and 1125 Ah T-105 battery bank with system reliability of 99.99% with a significant cost reduction as well as reliable energy production.',
	 'authors': u'Subodh Paudel, Jagan Nath Shrestha, Fernando J Neto, Jorge AF Ferreira, Muna Adhikari,',
	 'category': u'Computer Science ',
	 'date': '2014-11-19',
	 'pdflink': u'http://arxiv.org/pdf/1411.5343',
	 'subjects': u'Systems and Control (cs.SY)',
	 'title': u'\nOptimization of Hybrid PV/Wind Power System for Remote Telecom Station',
	 'urllink': u'http://arxiv.org/abs/1411.5343'}
2015-04-10 17:29:03+0000 [xxu46_10] INFO: Crawled 831 pages (at 1 pages/min), scraped 824 items (at 1 items/min)
2015-04-10 17:30:01+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1411.5336> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 17:30:01+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1411.5336>
	{'abstract': u'This paper develops a dynamic agent-based model for rural-urban migration, based on the previous relevant works. The model conforms to the typical dynamic linear multi-agent systems model concerned extensively in systems science, in which the communication network is formulated as a digraph. Simulations reveal that consensus of certain variable could be harmful to the overall stability and should be avoided.',
	 'authors': u'Ning Cai, Hai-Ying Ma, M. Junaid Khan,',
	 'category': u'Computer Science ',
	 'date': '2014-11-18',
	 'pdflink': u'http://arxiv.org/pdf/1411.5336',
	 'subjects': u'Multiagent Systems (cs.MA)',
	 'title': u'\nAgent-Based Model for Rural-Urban Migration: A Dynamic Consideration',
	 'urllink': u'http://arxiv.org/abs/1411.5336'}
2015-04-10 17:30:03+0000 [xxu46_10] INFO: Crawled 832 pages (at 1 pages/min), scraped 825 items (at 1 items/min)
2015-04-10 17:31:03+0000 [xxu46_10] INFO: Crawled 832 pages (at 0 pages/min), scraped 825 items (at 0 items/min)
2015-04-10 17:31:16+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1411.5331> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 17:31:16+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1411.5331>
	{'abstract': u"Our perceptions are guided both by the bottom-up information entering our eyes, as well as our top-down expectations of what we will see. Although bottom-up visual processing has been extensively studied, comparatively little is known about top-down signals. Here, we describe REVEAL (Representations Envisioned Via Evolutionary ALgorithm), a method for visualizing an observer's internal representation of a complex, real-world scene, allowing us to, for the first time, visualize the top-down information in an observer's mind. REVEAL rests on two innovations for solving this high dimensional problem: visual noise that samples from natural image statistics, and a computer algorithm that collaborates with human observers to efficiently obtain a solution. In this work, we visualize observers' internal representations of a visual scene category (street) using an experiment in which the observer views the naturalistic visual noise and collaborates with the algorithm to externalize his internal representation. As no scene information was presented, observers had to use their internal knowledge of the target, matching it with the visual features in the noise. We matched reconstructed images with images of real-world street scenes to enhance visualization. Critically, we show that the visualized mental images can be used to predict rapid scene detection performance, as each observer had faster and more accurate responses to detecting real-world images that were the most similar to his reconstructed street templates. These results show that it is possible to visualize previously unobservable mental representations of real world stimuli. More broadly, REVEAL provides a general method for objectively examining the content of previously private, subjective mental experiences.",
	 'authors': u'Michelle R. Greene, Abraham P. Botros, Diane M. Beck, Li Fei-Fei,',
	 'category': u'Computer Science ',
	 'date': '2014-11-19',
	 'pdflink': u'http://arxiv.org/pdf/1411.5331',
	 'subjects': u'Computer Vision and Pattern Recognition (cs.CV)',
	 'title': u'\nVisual Noise from Natural Scene Statistics Reveals Human Scene Category  Representations',
	 'urllink': u'http://arxiv.org/abs/1411.5331'}
2015-04-10 17:32:03+0000 [xxu46_10] INFO: Crawled 833 pages (at 1 pages/min), scraped 826 items (at 1 items/min)
2015-04-10 17:32:26+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1411.5328> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 17:32:26+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1411.5328>
	{'abstract': u'Discovering visual knowledge from weakly labeled data is crucial to scale up computer vision recognition system, since it is expensive to obtain fully labeled data for a large number of concept categories. In this paper, we propose ConceptLearner, which is a scalable approach to discover visual concepts from weakly labeled image collections. Thousands of visual concept detectors are learned automatically, without human in the loop for additional annotation. We show that these learned detectors could be applied to recognize concepts at image-level and to detect concepts at image region-level accurately. Under domain-specific supervision, we further evaluate the learned concepts for scene recognition on SUN database and for object detection on Pascal VOC 2007. ConceptLearner shows promising performance compared to fully supervised and weakly supervised methods.',
	 'authors': u'Bolei Zhou, Vignesh Jagadeesh, Robinson Piramuthu,',
	 'category': u'Computer Science ',
	 'date': '2014-11-19',
	 'pdflink': u'http://arxiv.org/pdf/1411.5328',
	 'subjects': u'Computer Vision and Pattern Recognition (cs.CV)',
	 'title': u'\nConceptLearner: Discovering Visual Concepts from Weakly Labeled Image  Collections',
	 'urllink': u'http://arxiv.org/abs/1411.5328'}
2015-04-10 17:33:03+0000 [xxu46_10] INFO: Crawled 834 pages (at 1 pages/min), scraped 827 items (at 1 items/min)
2015-04-10 17:33:34+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1411.5326> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 17:33:34+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1411.5326>
	{'abstract': u'This paper describes a new information-theoretic policy evaluation technique for reinforcement learning. This technique converts any compression or density model into a corresponding estimate of value. Under appropriate stationarity and ergodicity conditions, we show that the use of a sufficiently powerful model gives rise to a consistent value function estimator. We also study the behavior of this technique when applied to various Atari 2600 video games, where the use of suboptimal modeling techniques is unavoidable. We consider three fundamentally different models, all too limited to perfectly model the dynamics of the system. Remarkably, we find that our technique provides sufficiently accurate value estimates for effective on-policy control. We conclude with a suggestive study highlighting the potential of our technique to scale to large problems.',
	 'authors': u'Joel Veness, Marc G. Bellemare, Marcus Hutter, Alvin Chua, Guillaume Desjardins,',
	 'category': u'Computer Science ',
	 'date': '2014-11-19',
	 'pdflink': u'http://arxiv.org/pdf/1411.5326',
	 'subjects': u'Artificial Intelligence (cs.AI)',
	 'title': u'\nCompress and Control',
	 'urllink': u'http://arxiv.org/abs/1411.5326'}
2015-04-10 17:34:03+0000 [xxu46_10] INFO: Crawled 835 pages (at 1 pages/min), scraped 828 items (at 1 items/min)
2015-04-10 17:34:35+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1411.5323> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 17:34:35+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1411.5323>
	{'abstract': u'In recent times, wireless access technology is becoming increasingly commonplace due to the ease of operation and installation of untethered wireless media. The design of wireless networking is challenging due to the highly dynamic environmental condition that makes parameter optimization a complex task. Due to the dynamic, and often unknown, operating conditions, modern wireless networking standards increasingly rely on machine learning and artificial intelligence algorithms. Genetic algorithms (GAs) provide a well-established framework for implementing artificial intelligence tasks such as classification, learning, and optimization. GAs are well-known for their remarkable generality and versatility, and have been applied in a wide variety of settings in wireless networks. In this paper, we provide a comprehensive survey of the applications of GAs in wireless networks. We provide both an exposition of common GA models and configuration and provide a broad ranging survey of GA techniques in wireless networks. We also point out open research issues and define potential future work. While various surveys on GAs exist in literature, our paper is the first paper, to the best of our knowledge, which focuses on their application in wireless networks.',
	 'authors': u'Usama Mehboob, Junaid Qadir, Salman Ali, Athanasios Vasilakos,',
	 'category': u'Computer Science ',
	 'date': '2014-11-19',
	 'pdflink': u'http://arxiv.org/pdf/1411.5323',
	 'subjects': u'Networking and Internet Architecture (cs.NI)',
	 'title': u'\nGenetic Algorithms in Wireless Networking: Techniques, Applications, and  Issues',
	 'urllink': u'http://arxiv.org/abs/1411.5323'}
2015-04-10 17:35:03+0000 [xxu46_10] INFO: Crawled 836 pages (at 1 pages/min), scraped 829 items (at 1 items/min)
2015-04-10 17:35:41+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1411.5319> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 17:35:41+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1411.5319>
	{'abstract': u'In this work, we propose and address a new computer vision task, which we call fashion item detection, where the aim is to detect various fashion items a person in the image is wearing or carrying. The types of fashion items we consider in this work include hat, glasses, bag, pants, shoes and so on. The detection of fashion items can be an important first step of various e-commerce applications for fashion industry. Our method is based on state-of-the-art object detection method which combines object proposal methods with a Deep Convolutional Neural Network. Since the locations of fashion items are in strong correlation with the locations of body joints positions, we propose a hybrid discriminative-generative model to incorporate contextual information from body poses in order to improve the detection performance. Through the experiments, we demonstrate that our algorithm outperforms baseline methods with a large margin.',
	 'authors': u'Kota Hara, Vignesh Jagadeesh, Robinson Piramuthu,',
	 'category': u'Computer Science ',
	 'date': '2014-11-19',
	 'pdflink': u'http://arxiv.org/pdf/1411.5319',
	 'subjects': u'Computer Vision and Pattern Recognition (cs.CV)',
	 'title': u'\nFashion Apparel Detection: The Role of Deep Convolutional Neural Network  and Pose-dependent Priors',
	 'urllink': u'http://arxiv.org/abs/1411.5319'}
2015-04-10 17:36:03+0000 [xxu46_10] INFO: Crawled 837 pages (at 1 pages/min), scraped 830 items (at 1 items/min)
2015-04-10 17:37:00+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1411.5313> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 17:37:00+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1411.5313>
	{'abstract': u'Module extraction - the task of computing a (preferably small) fragment M of an ontology T that preserves entailments over a signature S - has found many applications in recent years. Extracting modules of minimal size is, however, computationally hard, and often algorithmically infeasible. Thus, practical techniques are based on approximations, where M provably captures the relevant entailments, but is not guaranteed to be minimal. Existing approximations, however, ensure that M preserves all second-order entailments of T w.r.t. S, which is stronger than is required in many applications, and may lead to large modules in practice. In this paper we propose a novel approach in which module extraction is reduced to a reasoning problem in datalog. Our approach not only generalises existing approximations in an elegant way, but it can also be tailored to preserve only specific kinds of entailments, which allows us to extract significantly smaller modules. An evaluation on widely-used ontologies has shown very encouraging results.',
	 'authors': u'Ana Armas Romero, Mark Kaminski, Bernardo Cuenca Grau, Ian Horrocks,',
	 'category': u'Computer Science ',
	 'date': '2014-11-19',
	 'pdflink': u'http://arxiv.org/pdf/1411.5313',
	 'subjects': u'Artificial Intelligence (cs.AI)',
	 'title': u'\nOntology Module Extraction via Datalog Reasoning',
	 'urllink': u'http://arxiv.org/abs/1411.5313'}
2015-04-10 17:37:03+0000 [xxu46_10] INFO: Crawled 838 pages (at 1 pages/min), scraped 831 items (at 1 items/min)
2015-04-10 17:38:03+0000 [xxu46_10] INFO: Crawled 838 pages (at 0 pages/min), scraped 831 items (at 0 items/min)
2015-04-10 17:38:05+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1411.5309> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 17:38:05+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1411.5309>
	{'abstract': u'Deformable Parts Models and Convolutional Networks each have achieved notable performance in object detection. Yet these two approaches find their strengths in complementary areas: DPMs are well-versed in object composition, modeling fine-grained spatial relationships between parts; likewise, ConvNets are adept at producing powerful image features, having been discriminatively trained directly on the pixels. In this paper, we propose a new model that combines these two approaches, obtaining the advantages of each. We train this model using a new structured loss function that considers all bounding boxes within an image, rather than isolated object instances. This enables the non-maximal suppression (NMS) operation, previously treated as a separate post-processing stage, to be integrated into the model. This allows for discriminative training of our combined Convnet + DPM + NMS model in end-to-end fashion. We evaluate our system on PASCAL VOC 2007 and 2011 datasets, achieving competitive results on both benchmarks.',
	 'authors': u'Li Wan, David Eigen, Rob Fergus,',
	 'category': u'Computer Science ',
	 'date': '2014-11-19',
	 'pdflink': u'http://arxiv.org/pdf/1411.5309',
	 'subjects': u'Computer Vision and Pattern Recognition (cs.CV)',
	 'title': u'\nEnd-to-End Integration of a Convolutional Network, Deformable Parts  Model and Non-Maximum Suppression',
	 'urllink': u'http://arxiv.org/abs/1411.5309'}
2015-04-10 17:39:03+0000 [xxu46_10] INFO: Crawled 839 pages (at 1 pages/min), scraped 832 items (at 1 items/min)
2015-04-10 17:39:12+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1411.5307> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 17:39:12+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1411.5307>
	{'abstract': u'Text is ubiquitous in the artificial world and easily attainable when it comes to book title and author names. Using the images from the book cover set from the Stanford Mobile Visual Search dataset and additional book covers and metadata from openlibrary.org, we construct a large scale book cover retrieval dataset, complete with 100K distractor covers and title and author strings for each. Because our query images are poorly conditioned for clean text extraction, we propose a method for extracting a matching noisy and erroneous OCR readings and matching it against clean author and book title strings in a standard document look-up problem setup. Finally, we demonstrate how to use this text-matching as a feature in conjunction with popular retrieval features such as VLAD using a simple learning setup to achieve significant improvements in retrieval accuracy over that of either VLAD or the text alone.',
	 'authors': u'Kevin Shih, Wei Di, Vignesh Jagadeesh, Robinson Piramuthu,',
	 'category': u'Computer Science ',
	 'date': '2014-11-19',
	 'pdflink': u'http://arxiv.org/pdf/1411.5307',
	 'subjects': u'Information Retrieval (cs.IR)',
	 'title': u'\nEfficient Media Retrieval from Non-Cooperative Queries',
	 'urllink': u'http://arxiv.org/abs/1411.5307'}
2015-04-10 17:40:03+0000 [xxu46_10] INFO: Crawled 840 pages (at 1 pages/min), scraped 833 items (at 1 items/min)
2015-04-10 17:40:19+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1411.5302> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 17:40:19+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1411.5302>
	{'abstract': u"Traditional regulatory methods for spectrum licensing have been recently identified as one of the causes for the under-utilization of the valuable radio spectrum. Governmental agencies such as the Federal Communications Commission (FCC) are seeking ways to remove stringent regulatory barriers and facilitate broader access to the spectrum resources. The goal is to allow for an improved and ubiquitous sharing of the precious radio spectrum between commercial service providers. In this paper, we propose a novel noncooperative game theoretic approach, to show how to foster more sharing of the radio spectrum via the use of regulatory power. We define a two stage game in which the government regulators move first, followed by the providers. The providers are incentivized by lower spectrum allocation fees from the regulators in return for proof-of-sharing. The providers are offered discounted spectrum bands, potentially at different locations, but will be asked to provide coverage to users that are not subscribed to them so as to maintain their subsidy incentives from the government. In a simplification of the model, analytical expressions for the providers' perfect equilibrium strategies are derived, and we argue for the existence of the government's part of a perfect equilibrium. Our analysis shows that through subsidization, the government can provide small service providers a fair chance to compete with the large providers, thereby avoiding monopolization in the market.",
	 'authors': u'Arvind Merwaday, Murat Yuksel, Thomas Quint, Ismail Guvenc, Walid Saad, Naim Kapucu,',
	 'category': u'Computer Science ',
	 'date': '2014-11-19',
	 'pdflink': u'http://arxiv.org/pdf/1411.5302',
	 'subjects': u'Networking and Internet Architecture (cs.NI)',
	 'title': u'\nIncentivizing Spectrum Sharing via Subsidy Regulations',
	 'urllink': u'http://arxiv.org/abs/1411.5302'}
2015-04-10 17:41:03+0000 [xxu46_10] INFO: Crawled 841 pages (at 1 pages/min), scraped 834 items (at 1 items/min)
2015-04-10 17:41:36+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1411.5299> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 17:41:36+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1411.5299>
	{'abstract': u"We derive the capacity of the two-hop half-duplex (HD) relay channel. This channel is comprised of a source, a HD relay, and a destination, and there is no direct link between the source and the destination. Hence, the source is forced to transmit its message to the destination via the HD relay. Since the relay is HD, it cannot transmit and receive at the same time. For the considered relay channel, we show that achieving the capacity requires the relay to switch between reception and transmission in a symbol-by-symbol manner. Moreover, to achieve the capacity, the relay sends information to the destination by transmitting information-carrying symbols and with the zero symbol resulting from the relay's silence during reception. We derive simplified capacity expressions for the following two special cases: 1) The source-relay and relay-destination links are both binary-symmetric channels (BSCs); 2) The source-relay and relay-destination links are both additive white Gaussian noise (AWGN) channels. For these two cases, we numerically compare the capacity rate with the rate achieved by conventional relaying where the relay receives and transmits in a codeword-by-codeword fashion and switches between reception and transmission in a strictly alternating manner. Our numerical results show that the capacity rate is significantly larger than the rate achieved with conventional relaying, for both the BSC and the AWGN channel.",
	 'authors': u'Nikola Zlatanov, Vahid Jamali, Robert Schober,',
	 'category': u'Computer Science ',
	 'date': '2014-11-19',
	 'pdflink': u'http://arxiv.org/pdf/1411.5299',
	 'subjects': u'Information Theory (cs.IT)',
	 'title': u'\nCapacity of the Two-Hop Half-Duplex Relay Channel',
	 'urllink': u'http://arxiv.org/abs/1411.5299'}
2015-04-10 17:42:03+0000 [xxu46_10] INFO: Crawled 842 pages (at 1 pages/min), scraped 835 items (at 1 items/min)
2015-04-10 17:42:30+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1411.5289> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 17:42:30+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1411.5289>
	{'abstract': u'The original liveness based flow and context sensitive points-to analysis (LFCPA) is restricted to scalar pointer variables and scalar pointees on stack and static memory. In this paper, we extend it to support heap memory and pointer expressions involving structures, unions, arrays, and pointer arithmetic. The key idea behind these extensions involves constructing bounded names for locations in terms of compile time constants (names and fixed offsets), and introducing sound approximations when it is not possible to do so. We achieve this by defining a grammar for pointer expressions, suitable memory models and location naming conventions, and some key evaluations of pointer expressions that compute the named locations. These extensions preserve the spirit of the original LFCPA which is evidenced by the fact that although the lattices and flow functions change, the overall data flow equations remain unchanged.',
	 'authors': u'Uday P. Khedker, Vini Kanvar,',
	 'category': u'Computer Science ',
	 'date': '2014-11-19',
	 'pdflink': u'http://arxiv.org/pdf/1411.5289',
	 'subjects': u'Programming Languages (cs.PL)',
	 'title': u'\nGeneralizing the Liveness Based Points-to Analysis',
	 'urllink': u'http://arxiv.org/abs/1411.5289'}
2015-04-10 17:43:03+0000 [xxu46_10] INFO: Crawled 843 pages (at 1 pages/min), scraped 836 items (at 1 items/min)
2015-04-10 17:43:31+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1411.5283> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 17:43:31+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1411.5283>
	{'abstract': u'Sorting has been a profound area for the algorithmic researchers and many resources are invested to suggest more works for sorting algorithms. For this purpose, many existing sorting algorithms were observed in terms of the efficiency of the algorithmic complexity. In this paper we implemented the bubble and merge sort algorithms using Message Passing Interface (MPI) approach. The proposed work tested on two standard datasets (text file) with different size. The main idea of the proposed algorithm is distributing the elements of the input datasets into many additional temporary sub-arrays according to a number of characters in each word. The sizes of each of these sub-arrays are decided depending on a number of elements with the same number of characters in the input array. We implemented MPI using Intel core i7-3610QM ,(8 CPUs),using two approaches (vectors of string and array 3D) . Finally, we get the data structure effects on the performance of the algorithm for that we choice the second approach.',
	 'authors': u'Zaid Abdi Alkareem Alyasseri, Kadhim Al-Attar, Mazin Nasser,',
	 'category': u'Computer Science ',
	 'date': '2014-11-19',
	 'pdflink': u'http://arxiv.org/pdf/1411.5283',
	 'subjects': u'Distributed, Parallel, and Cluster Computing (cs.DC)',
	 'title': u'\nParallelize Bubble and Merge Sort Algorithms Using Message Passing  Interface (MPI)',
	 'urllink': u'http://arxiv.org/abs/1411.5283'}
2015-04-10 17:44:03+0000 [xxu46_10] INFO: Crawled 844 pages (at 1 pages/min), scraped 837 items (at 1 items/min)
2015-04-10 17:44:31+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1411.5282> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 17:44:31+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1411.5282>
	{'abstract': u'We address the problem of reaching consensus in the presence of Byzantine faults. Fault-tolerant consensus algorithms typically assume knowledge of nonlocal information and multi-hop communication; however, this assumption is not suitable for large-scale static/dynamic networks. A handful of iterative algorithms have been proposed recently under the assumption that each node (faulty or fault-free) can only access local information, thus is only capable of sending messages via one-hop communication. In this paper, we unify these two streams of work by assuming that each node knows the topology of up to hop neighborhood and can send messages to other nodes via up to -hop transmission, where and is the number of nodes. We prove a family of necessary and sufficient conditions for the existence of iterative algorithms that achieve approximate Byzantine consensus in arbitrary directed graphs. The class of iterative algorithms considered in this paper ensures that, after each iteration of the algorithm, the state of each fault-free node remains in the convex hull of the initial states of the fault-free nodes. The following convergence requirement is imposed: for any , after a sufficiently large number of iterations, the states of the fault-free nodes are guaranteed to be within of each other.',
	 'authors': u'Lili Su, Nitin Vaidya,',
	 'category': u'Computer Science ',
	 'date': '2014-11-19',
	 'pdflink': u'http://arxiv.org/pdf/1411.5282',
	 'subjects': u'Distributed, Parallel, and Cluster Computing (cs.DC)',
	 'title': u'\nReaching Approximate Byzantine Consensus with Multi-hop Communication',
	 'urllink': u'http://arxiv.org/abs/1411.5282'}
2015-04-10 17:45:03+0000 [xxu46_10] INFO: Crawled 845 pages (at 1 pages/min), scraped 838 items (at 1 items/min)
2015-04-10 17:45:40+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1411.5281> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 17:45:40+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1411.5281>
	{'abstract': u"Interest-based Behavioral Targeted (IBT) advertising has risen in prominence as a method to increase the effectiveness of online advertising. IBT operates by associating tags or labels to users based on their online activity and then using these labels to target them. It's rise has been accompanied by privacy concerns from researchers, regulators and the press. In this paper, we present a novel methodology for measuring and understanding IBT in the online advertising market. We rely on training artificial online personas representing behavioral traits like football enthusiast, affluent, recent parents etc. and build a measurement system that is automated, scalable and supports testing of multiple configurations. We observe that IBT advertising is a frequent practice and notice that some personas like Recent Parent are clearly more targeted than others such as Football Enthusiast. Furthermore, we compare the volume of IBT advertising for our personas in two different geographical locations (US and Spain) without observing any significant geographical bias in the utilization of IBT. Finally, we check for targeting with do-not-track (DNT) enabled and discovered that DNT is not yet enforced in the web.",
	 'authors': u'J. M. Carrascosa, J. Mikians, R. Cuevas, V. Erramilli, N. Laoutaris,',
	 'category': u'Computer Science ',
	 'date': '2014-11-19',
	 'pdflink': u'http://arxiv.org/pdf/1411.5281',
	 'subjects': u'Computers and Society (cs.CY)',
	 'title': u'\nUnderstanding Interest-based Behavioural Targeted Advertising',
	 'urllink': u'http://arxiv.org/abs/1411.5281'}
2015-04-10 17:46:03+0000 [xxu46_10] INFO: Crawled 846 pages (at 1 pages/min), scraped 839 items (at 1 items/min)
2015-04-10 17:46:52+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1411.5268> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 17:46:52+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1411.5268>
	{'abstract': u'The sparse, hierarchical, and modular processing of natural signals is related to the ability of humans to recognize objects with high accuracy. In this study, we report a sparse feature processing and encoding method, which improved the recognition performance of an automated object recognition system. Randomly distributed localized gradient enhanced features were selected before employing aggregate functions for representation, where we used a modular and hierarchical approach to detect the object features. These object features were combined with a minimum distance classifier, thereby obtaining object recognition system accuracies of 93% using the Amsterdam library of object images (ALOI) database, 92% using the Columbia object image library (COIL)-100 database, and 69% using the PASCAL visual object challenge 2007 database. The object recognition performance was shown to be robust to variations in noise, object scaling, and object shifts. Finally, a comparison with eight existing object recognition methods indicated that our new method improved the recognition accuracy by 10% with ALOI, 8% with the COIL-100 database, and 10% with the PASCAL visual object challenge 2007 database.',
	 'authors': u'Swathikiran Sudhakarana, Alex Pappachen James,',
	 'category': u'Computer Science ',
	 'date': '2014-11-19',
	 'pdflink': u'http://arxiv.org/pdf/1411.5268',
	 'subjects': u'Computer Vision and Pattern Recognition (cs.CV)',
	 'title': u'\nSparse distributed localized gradient fused features of objects',
	 'urllink': u'http://arxiv.org/abs/1411.5268'}
2015-04-10 17:47:03+0000 [xxu46_10] INFO: Crawled 847 pages (at 1 pages/min), scraped 840 items (at 1 items/min)
2015-04-10 17:48:03+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1411.5255> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 17:48:03+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1411.5255>
	{'abstract': u'Brain inspired circuits can provide an alternative solution to implement computing architectures taking advantage of fault tolerance and generalisation ability of logic gates. In this brief, we advance over the memristive threshold circuit configuration consisting of memristive averaging circuit in combination with operational amplifier and/or CMOS inverters in application to realizing complex computing circuits. The developed memristive threshold logic gates are used for designing FFT and multiplication circuits useful for modern microprocessors. Overall, the proposed threshold logic outperforms previous memristive-CMOS logic cells on every aspect, however, indicate a lower chip area, lower THD, and controllable leakage power, but a higher power dissipation with respect to CMOS logic.',
	 'authors': u'Alex Pappachen James, Dinesh S. Kumar, Arun Ajayan,',
	 'category': u'Computer Science ',
	 'date': '2014-11-19',
	 'pdflink': u'http://arxiv.org/pdf/1411.5255',
	 'subjects': u'Emerging Technologies (cs.ET)',
	 'title': u'\nThreshold Logic Computing: Memristive-CMOS Circuits for Fast Fourier  Transform and Vedic Multiplication',
	 'urllink': u'http://arxiv.org/abs/1411.5255'}
2015-04-10 17:48:03+0000 [xxu46_10] INFO: Crawled 848 pages (at 1 pages/min), scraped 841 items (at 1 items/min)
2015-04-10 17:49:03+0000 [xxu46_10] INFO: Crawled 848 pages (at 0 pages/min), scraped 841 items (at 0 items/min)
2015-04-10 17:49:20+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1411.5245> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 17:49:20+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1411.5245>
	{'abstract': u'In previous studies, much attention from multidisciplinary fields has been devoted to understand the mechanism of underlying scholarly networks including bibliographic networks, citation networks and co-citation networks. Particularly focusing on networks constructed by means of either authors affinities or the mutual content. Missing a valuable dimension of network, which is an audience scholarly paper. We aim at this paper to assess the impact that social networks and media can have on scholarly papers. We also examine the process of information flow in such networks. We also mention some observa- tions of attractive incidents that our proposed network model revealed.',
	 'authors': u'Amir Razmjou,',
	 'category': u'Computer Science ',
	 'date': '2014-11-19',
	 'pdflink': u'http://arxiv.org/pdf/1411.5245',
	 'subjects': u'Social and Information Networks (cs.SI)',
	 'title': u'\nCorrelation of Scholarly Networks and Social Networks',
	 'urllink': u'http://arxiv.org/abs/1411.5245'}
2015-04-10 17:50:03+0000 [xxu46_10] INFO: Crawled 849 pages (at 1 pages/min), scraped 842 items (at 1 items/min)
2015-04-10 17:50:31+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1411.5240> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 17:50:31+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1411.5240>
	{'abstract': u'A -edge-colored multigraph has each edge colored with one of the available colors and no two parallel edges have the same color. A proper Hamiltonian cycle is a cycle containing all the vertices of the multigraph such that no two adjacent edges have the same color. In this work we establish sufficient conditions for a multigraph to have a proper Hamiltonian cycle, depending on several parameters such as the number of edges and the rainbow degree.',
	 'authors': u'Raquel \xc1gueda, Valentin Borozan, Raquel D\xedaz, Yannis Manoussakis, Leandro Montero,',
	 'category': u'Computer Science ',
	 'date': '2014-11-19',
	 'pdflink': u'http://arxiv.org/pdf/1411.5240',
	 'subjects': u'Discrete Mathematics (cs.DM)',
	 'title': u'\nProper Hamiltonian Cycles in Edge-Colored Multigraphs',
	 'urllink': u'http://arxiv.org/abs/1411.5240'}
2015-04-10 17:51:03+0000 [xxu46_10] INFO: Crawled 850 pages (at 1 pages/min), scraped 843 items (at 1 items/min)
2015-04-10 17:51:42+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1411.5228> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 17:51:42+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1411.5228>
	{'abstract': u'In any tactical scenario, the successful quantification and triangulation of potential hostile elements is instrumental to minimize any casualties which might be incurred. The most commonly deployed infrastructures to cater to this have mostly been surveillance systems which only extract some data pertaining to the targets of interest in the area of observation and convey the information to the human operators. Accordingly, with the ever increasing rate at which warfare tactics are evolving, there has been a growing need for smarter solutions to this problem of hostile intent enumeration. Recently, a number of developments have been made to ameliorate the efficacy and the certitude with which this task is performed. This paper discusses two of the most prominent approaches which address this problem and posits the outline of a novel solution which seeks to address the shortcomings faced by the existing approaches.',
	 'authors': u'Souham Biswas, Manisha J. Nene,',
	 'category': u'Computer Science ',
	 'date': '2014-11-19',
	 'pdflink': u'http://arxiv.org/pdf/1411.5228',
	 'subjects': u'Other Computer Science (cs.OH)',
	 'title': u'\nHostile Intent Enumeration using Soft Computing Techniques',
	 'urllink': u'http://arxiv.org/abs/1411.5228'}
2015-04-10 17:52:03+0000 [xxu46_10] INFO: Crawled 851 pages (at 1 pages/min), scraped 844 items (at 1 items/min)
2015-04-10 17:52:26+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1411.5225> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 17:52:26+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1411.5225>
	{'abstract': u'This paper aims to present an online placement test. It is based on the Item Response Theory to provide relevant estimates of learner competences. The proposed test is the entry point of our e-Learning system. It gathers the learner response to a set of questions and uses a specific developed algorithm to estimate its level. This algorithm identifies learning gaps, which allows tutors to conceive sequence of courses and remediation adapted to each case of learner, in order to achieve a competence.',
	 'authors': u'Farid Merrouch, Meriem Hnida, Mohammed Khalidi Idrissi, Samir Bennani,',
	 'category': u'Computer Science ',
	 'date': '2014-11-19',
	 'pdflink': u'http://arxiv.org/pdf/1411.5225',
	 'subjects': u'Computers and Society (cs.CY)',
	 'title': u'\nOnline placement test based on Item Response Theory and IMS Global  standards',
	 'urllink': u'http://arxiv.org/abs/1411.5225'}
2015-04-10 17:53:03+0000 [xxu46_10] INFO: Crawled 852 pages (at 1 pages/min), scraped 845 items (at 1 items/min)
2015-04-10 17:53:31+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1411.5224> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 17:53:31+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1411.5224>
	{'abstract': u'The underlying physiological mechanisms of generating conscious states are still unknown. To make progress on the problem of consciousness, we will need to experimentally design a system that evolves in a similar way our brains do. Recent experimental data show that the multiscale nature of the evolving human brain can be implemented by reprogramming human cells. A hybrid system can be designed to include an evolving brain equipped with digital computers that maintain homeostasis and provide the right amount of nutrients and oxygen for the brain growth. Shaping the structure of the evolving brain will be progressively achieved by controlling spatial organization of various types of cells. Following a specific program, the evolving brain can be trained using substitutional reality to learn and experience live scenes. We already know from neuroelectrodynamics that meaningful information in the brain is electrically (wirelessly) read out and written fast in neurons and synapses at the molecular (protein) level during the generation of action potentials and synaptic activities. Since with training, meaningful information accumulates and is electrically integrated in the brain, one can predict, that this gradual process of training will trigger a tipping point for conscious experience to emerge in the hybrid system.',
	 'authors': u'Dorian Aur,',
	 'category': u'Computer Science ',
	 'date': '2014-11-17',
	 'pdflink': u'http://arxiv.org/pdf/1411.5224',
	 'subjects': u'Emerging Technologies (cs.ET)',
	 'title': u'\nCan we build a conscious machine?',
	 'urllink': u'http://arxiv.org/abs/1411.5224'}
2015-04-10 17:54:03+0000 [xxu46_10] INFO: Crawled 853 pages (at 1 pages/min), scraped 846 items (at 1 items/min)
2015-04-10 17:54:45+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1411.5220> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 17:54:45+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1411.5220>
	{'abstract': u'Finite chase, or alternatively chase termination, is an important condition to ensure the decidability of existential rule languages. In the past few years, a number of rule languages with finite chase have been studied. In this work, we propose a novel approach for classifying the rule languages with finite chase. Using this approach, a family of decidable rule languages, which extend the existing languages with the finite chase property, are naturally defined. We then study the complexity of these languages. Although all of them are tractable for data complexity, we show that their combined complexity can be arbitrarily high. Furthermore, we prove that all the rule languages with finite chase that extend the weakly acyclic language are of the same expressiveness as the weakly acyclic one, while rule languages with higher combined complexity are in general more succinct than those with lower combined complexity.',
	 'authors': u'Heng Zhang, Yan Zhang, Jia-Huai You,',
	 'category': u'Computer Science ',
	 'date': '2014-11-19',
	 'pdflink': u'http://arxiv.org/pdf/1411.5220',
	 'subjects': u'Artificial Intelligence (cs.AI)',
	 'title': u'\nExistential Rule Languages with Finite Chase: Complexity and  Expressiveness',
	 'urllink': u'http://arxiv.org/abs/1411.5220'}
2015-04-10 17:55:03+0000 [xxu46_10] INFO: Crawled 854 pages (at 1 pages/min), scraped 847 items (at 1 items/min)
2015-04-10 17:55:42+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1411.5213> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 17:55:42+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1411.5213>
	{'abstract': u'With our growing reliability on distributed networks, the security aspect of such networks becomes of prime importance. In large scale distributed networks it becomes cardinal to have an efficient and effective monitoring scheme. The monitoring schemes supervise the node behaviour in the network and look out for any discrepancy. Monitoring schemes comprise of monitoring components that work together to help schemes in meeting various security requirement parameters for the networks. These security parameters are breached via various attacks by manipulation of monitoring components of particular monitoring schemes to produce faulty results and thereby reducing efficiency of networks, reliability and security. In this paper we have discussed these components of monitoring, multiple monitoring schemes, their security parameters and various types of attacks possible on these monitoring components by manipulating assumptions of monitoring schemes.',
	 'authors': u'Atul Vaibhav,',
	 'category': u'Computer Science ',
	 'date': '2014-11-19',
	 'pdflink': u'http://arxiv.org/pdf/1411.5213',
	 'subjects': u'Distributed, Parallel, and Cluster Computing (cs.DC)',
	 'title': u'\nSecurity in Monitoring Schemes: A Survey',
	 'urllink': u'http://arxiv.org/abs/1411.5213'}
2015-04-10 17:56:03+0000 [xxu46_10] INFO: Crawled 855 pages (at 1 pages/min), scraped 848 items (at 1 items/min)
2015-04-10 17:56:58+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1411.5210> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 17:56:58+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1411.5210>
	{'abstract': u'The performance of a wireless sensor network (WSN) depends fundamentally on how its various parameters are configured under different link quality conditions. Surprisingly, even though WSNs have been extensively researched, there still lacks an in-depth understanding on how parameter configurations affect, in particular jointly, the performance under different link quality conditions. To fill the gap, this paper presents an extensive experimental study on the performance of a wireless sensor network link, where measurement data of more than 200 million packets were collected. Different from existing work, we consider major parameters from different layers together and measure their joint effects on key performance metrics under an extensive set of parameter configurations. Based on the large amount of measurement data, we investigate the impacts of these parameters and their joint configurations on the performance, introduce empirical models to theoretically reason the impacts, discuss implications of the obtained results, and suggest practical guidelines for parameter configurations. Through these, a comprehensive overview of parameter configuration impacts on the erformance is provided, which provides new insights on wireless link performance in WSNs.',
	 'authors': u'Songwei Fu, Yan Zhang, Yuming Jiang, Chia-Yen Shih, Pedro Jose Marron,',
	 'category': u'Computer Science ',
	 'date': '2014-11-19',
	 'pdflink': u'http://arxiv.org/pdf/1411.5210',
	 'subjects': u'Networking and Internet Architecture (cs.NI)',
	 'title': u'\nAn Experimental Study Towards Understanding Data Delivery Performance  Over a WSN Link',
	 'urllink': u'http://arxiv.org/abs/1411.5210'}
2015-04-10 17:57:03+0000 [xxu46_10] INFO: Crawled 856 pages (at 1 pages/min), scraped 849 items (at 1 items/min)
2015-04-10 17:57:58+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1411.5204> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 17:57:58+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1411.5204>
	{'abstract': u'Measuring socioeconomic deprivation of cities in an accurate and timely fashion has become a priority for governments around the world, as the massive urbanization process we are witnessing is causing high levels of inequalities which require intervention. Traditionally, deprivation indexes have been derived from census data, which is however very expensive to obtain, and thus acquired only every few years. Alternative computational methods have been proposed in recent years to automatically extract proxies of deprivation at a fine spatio-temporal level of granularity; however, they usually require access to datasets (e.g., call details records) that are not publicly available to governments and agencies. To remedy this, we propose a new method to automatically mine deprivation at a fine level of spatio-temporal granularity that only requires access to freely available user-generated content. More precisely, the method needs access to datasets describing what urban elements are present in the physical environment; examples of such datasets are Foursquare and OpenStreetMap. Using these datasets, we quantitatively describe neighborhoods by means of a metric, called , that reflects which urban elements are distinctive features of each neighborhood. We then use that metric to build accurate classifiers of urban deprivation and interpret the outcomes through thematic analysis. We apply the method to three UK urban areas of different scale and elaborate on the results in terms of precision and recall.',
	 'authors': u'Alessandro Venerandi, Giovanni Quattrone, Licia Capra, Daniele Quercia, Diego Saez-Trumper,',
	 'category': u'Computer Science ',
	 'date': '2014-11-19',
	 'pdflink': u'http://arxiv.org/pdf/1411.5204',
	 'subjects': u'Social and Information Networks (cs.SI)',
	 'title': u'\nMeasuring Urban Deprivation from User Generated Content',
	 'urllink': u'http://arxiv.org/abs/1411.5204'}
2015-04-10 17:58:03+0000 [xxu46_10] INFO: Crawled 857 pages (at 1 pages/min), scraped 850 items (at 1 items/min)
2015-04-10 17:58:58+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1411.5197> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 17:58:58+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1411.5197>
	{'abstract': u'In 1946 Emil Leon Post (Bulletin of Amer. Math. Soc. 52 (1946), 264 - 268) defined a famous correspondence decision problem which is nowadays called the Post Correspondence Problem, and he proved that the problem is undecidable. In this article we follow the steps of Post, and give another, simpler and more straightforward proof of the undecidability of the problem using the same source of reduction as Post original did, namely, the Post Normal Systems.',
	 'authors': u'Vesa Halava,',
	 'category': u'Computer Science ',
	 'date': '2014-11-19',
	 'pdflink': u'http://arxiv.org/pdf/1411.5197',
	 'subjects': u'Logic in Computer Science (cs.LO)',
	 'title': u'\nAnother proof of undecidability for the correspondence decision problem  - Had I been Emil Post',
	 'urllink': u'http://arxiv.org/abs/1411.5197'}
2015-04-10 17:59:03+0000 [xxu46_10] INFO: Crawled 858 pages (at 1 pages/min), scraped 851 items (at 1 items/min)
2015-04-10 18:00:03+0000 [xxu46_10] INFO: Crawled 858 pages (at 0 pages/min), scraped 851 items (at 0 items/min)
2015-04-10 18:00:06+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1411.5196> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 18:00:06+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1411.5196>
	{'abstract': u"In view of the paradigm shift that makes science ever more data-driven, in this paper we consider deterministic scientific hypotheses as uncertain data. In the form of mathematical equations, hypotheses symmetrically relate aspects of the studied phenomena. For computing predictions, however, deterministic hypotheses are used asymmetrically as functions. We refer to Simon's notion of structural equations in order to extract the (so-called) causal ordering embedded in a hypothesis. Then we encode it into a set of functional dependencies (fd's) that is basic input to a design-theoretic method for the synthesis of U-relational databases (DB's). The causal ordering captured from a formally-specified system of mathematical equations into fd's determines not only the constraints (structure), but also the correlations (uncertainty chaining) hidden in the hypothesis predictive data. We show how to process it effectively through original algorithms for encoding and reasoning on the given hypotheses as constraints and correlations into U-relational DB's. The method is applicable to both quantitative and qualitative hypotheses and has underwent initial tests in a realistic use case from computational science.",
	 'authors': u'Bernardo Gon\xe7alves, Fabio Porto,',
	 'category': u'Computer Science ',
	 'date': '2014-11-19',
	 'pdflink': u'http://arxiv.org/pdf/1411.5196',
	 'subjects': u'Databases (cs.DB)',
	 'title': u'\nDesign-theoretic encoding of deterministic hypotheses as constraints and  correlations into U-relational databases',
	 'urllink': u'http://arxiv.org/abs/1411.5196'}
2015-04-10 18:01:03+0000 [xxu46_10] INFO: Crawled 859 pages (at 1 pages/min), scraped 852 items (at 1 items/min)
2015-04-10 18:01:10+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1411.5190> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 18:01:10+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1411.5190>
	{'abstract': u'Over the last two decades we have witnessed strong progress on modeling visual object classes, scenes and attributes that have significantly contributed to automated image understanding. On the other hand, surprisingly little progress has been made on incorporating a spatial representation and reasoning in the inference process. In this work, we propose a pooling interpretation of spatial relations and show how it improves image retrieval and annotations tasks involving spatial language. Due to the complexity of the spatial language, we argue for a learning-based approach that acquires a representation of spatial relations by learning parameters of the pooling operator. We show improvements on previous work on two datasets and two different tasks as well as provide additional insights on a new dataset with an explicit focus on spatial relations.',
	 'authors': u'Mateusz Malinowski, Mario Fritz,',
	 'category': u'Computer Science ',
	 'date': '2014-11-19',
	 'pdflink': u'http://arxiv.org/pdf/1411.5190',
	 'subjects': u'Computer Vision and Pattern Recognition (cs.CV)',
	 'title': u'\nA Pooling Approach to Modelling Spatial Relations for Image Retrieval  and Annotation',
	 'urllink': u'http://arxiv.org/abs/1411.5190'}
2015-04-10 18:02:03+0000 [xxu46_10] INFO: Crawled 860 pages (at 1 pages/min), scraped 853 items (at 1 items/min)
2015-04-10 18:02:18+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1411.5187> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 18:02:18+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1411.5187>
	{'abstract': u'In this paper, we introduce a new sparse signal recovery algorithm, referred to as sparse Kalman tree search (sKTS), that provides a robust reconstruction of the sparse vector from the sequence of correlated observation vectors. The proposed sKTS algorithm builds on expectation-maximization algorithm and consists of two main operations: 1) Kalman smoothing for obtaining the a posteriori statistics of the source signal vectors and 2) identification of the support of the signal vectors via greedy tree search algorithm. The performance of the sKTS algorithm is evaluated in the context of sparse channel estimation for wireless communications. Through computer simulations, we demonstrate that sKTS outperforms conventional sparse recovery algorithms and also performs close to the Oracle (genie-based) Kalman estimator.',
	 'authors': u'Jun Won Choi, Byonghyo Shim,',
	 'category': u'Computer Science ',
	 'date': '2014-11-19',
	 'pdflink': u'http://arxiv.org/pdf/1411.5187',
	 'subjects': u'Information Theory (cs.IT)',
	 'title': u'\nStatistical Estimation of Block-Sparse Time-Varying Signals from  Multiple Measurement Vectors',
	 'urllink': u'http://arxiv.org/abs/1411.5187'}
2015-04-10 18:03:03+0000 [xxu46_10] INFO: Crawled 861 pages (at 1 pages/min), scraped 854 items (at 1 items/min)
2015-04-10 18:03:26+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1411.5178> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 18:03:26+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1411.5178>
	{'abstract': u'This paper gives performance limits of the segmented compressive sampling (CS) which collects correlated samples. It is shown that the effect of correlation among samples for the segmented CS can be characterized by a penalty term in the corresponding bounds on the sampling rate. Moreover, this penalty term is vanishing as the signal dimension increases. It means that the performance degradation due to the fixed correlation among samples obtained by the segmented CS (as compared to the standard CS with equivalent size sampling matrix) is negligible for a high-dimensional signal. In combination with the fact that the signal reconstruction quality improves with additional samples obtained by the segmented CS (as compared to the standard CS with sampling matrix of the size given by the number of original uncorrelated samples), the fact that the additional correlated samples also provide new information about a signal is a strong argument for the segmented CS.',
	 'authors': u'Hao Fang, Sergiy A. Vorobyov, Hai Jiang,',
	 'category': u'Computer Science ',
	 'date': '2014-11-19',
	 'pdflink': u'http://arxiv.org/pdf/1411.5178',
	 'subjects': u'Information Theory (cs.IT)',
	 'title': u'\nPerformance Limits of Segmented Compressive Sampling: Correlated Samples  versus Bits',
	 'urllink': u'http://arxiv.org/abs/1411.5178'}
2015-04-10 18:04:03+0000 [xxu46_10] INFO: Crawled 862 pages (at 1 pages/min), scraped 855 items (at 1 items/min)
2015-04-10 18:04:15+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1411.5173> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 18:04:15+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1411.5173>
	{'abstract': u'The SINR (signal to interference plus noise ratio) is a key factor for wireless networks analysis. Indeed, the SINR distribution allows the derivation of performance and quality of service (QoS) evaluation. Moreover, it also enables the analysis of radio resources allocation and scheduling policies, since they depend on the SINR reached by a UE (User Equipment). Therefore, it is particularly interesting to develop an analytical method which allows to evaluate the SINR, in a simple and quick way, for a realistic environment. Considering a stochastic Poisson network model, we establish the CDF (cumulative distributed function) of the SINR. We show that the shadowing can be neglected, in many cases, as long as mobiles are connected to their best serving base station (BS), i.e. the BS which offers them the most powerful useful signal. As a consequence, the analysis of performance and quality of service, directly derived from the CDF of SINR, can be established by using a propagation model which takes into account only the pathloss. Moreover, we establish that the Fluid network model we have proposed can be used to analyze stochastic Poisson distributed network. Therefore, the analysis of stochastic Poisson network can be done in an easy and quick way, by using the analytical expression of the SINR established thanks to the Fluid network model.',
	 'authors': u'Jean-Marc Kelif, Stephane Senecal, Marceau Coupechoux, Constant Bridon,',
	 'category': u'Computer Science ',
	 'date': '2014-11-19',
	 'pdflink': u'http://arxiv.org/pdf/1411.5173',
	 'subjects': u'Networking and Internet Architecture (cs.NI)',
	 'title': u'\nAnalytical Performance Model for Poisson Wireless Networks with Pathloss  and Shadowing Propagation',
	 'urllink': u'http://arxiv.org/abs/1411.5173'}
2015-04-10 18:05:03+0000 [xxu46_10] INFO: Crawled 863 pages (at 1 pages/min), scraped 856 items (at 1 items/min)
2015-04-10 18:05:20+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1411.5172> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 18:05:20+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1411.5172>
	{'abstract': u'Modeling dynamical systems with ordinary differential equations implies a mechanistic view of the process underlying the dynamics. However in many cases, this knowledge is not available. To overcome this issue, we introduce a general framework for nonparametric ODE models using penalized regression in Reproducing Kernel Hilbert Spaces (RKHS) based on operator-valued kernels. Moreover, we extend the scope of gradient matching approaches to nonparametric ODE. A smooth estimate of the solution ODE is built to provide an approximation of the derivative of the ODE solution which is in turn used to learn the nonparametric ODE model. This approach benefits from the flexibility of penalized regression in RKHS allowing for ridge or (structured) sparse regression as well. Very good results are shown on 3 different ODE systems.',
	 'authors': u"Markus Heinonen, Florence d'Alch\xe9-Buc,",
	 'category': u'Computer Science ',
	 'date': '2014-11-19',
	 'pdflink': u'http://arxiv.org/pdf/1411.5172',
	 'subjects': u'Learning (cs.LG)',
	 'title': u'\nLearning nonparametric differential equations with operator-valued  kernels and gradient matching',
	 'urllink': u'http://arxiv.org/abs/1411.5172'}
2015-04-10 18:06:03+0000 [xxu46_10] INFO: Crawled 864 pages (at 1 pages/min), scraped 857 items (at 1 items/min)
2015-04-10 18:06:32+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1411.5166> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 18:06:32+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1411.5166>
	{'abstract': u"For helping themselves in writing, debugging and maintaining their software, professional OO software developers keep in their minds an image of the subtyping relation between types in their software while they are developing their software. In pre-generics Java, the structure of the subtyping mental image was simple: the graph of the subtyping relation between classes and interfaces was a directed-acyclic graph, and the graph of the subtyping relation between classes alone was simply a tree. This fact about the graph of the subtyping relation applied not only to Java but, more generally, also to the non-generic sublanguage of nominally-typed OO languages similar to Java, such as C#, C++, and Scala. The goal of this casual essay is to present and defend, even if incompletely and not quite rigorously, a hunch and intuition the author had years ago about the graph of the subtyping relation in Java after generics were added to it. The author observed that: after the addition of generics---and of wildcards in particular---to Java, the graph of the subtyping relation is still a DAG, but no longer a simple DAG but rather one whose structure can be better understood as a /fractal/. Today, generics and wildcards (or some other form of `variance annotations') are a standard feature of mainstream nominally-typed OO languages. Accordingly, the shape of the subtyping relation in nominally-typed OO languages is more complex than a tree or a simple DAG. Given the popularity of fractals, the fractals observation may help OO software developers keep a useful and intuitive mental image of their software's subtyping relation, even if it is one a little more frightening, and amazing, than before.",
	 'authors': u'Moez A. AbdelGawad,',
	 'category': u'Computer Science ',
	 'date': '2014-11-19',
	 'pdflink': u'http://arxiv.org/pdf/1411.5166',
	 'subjects': u'Programming Languages (cs.PL)',
	 'title': u'\nSubtyping in Java with Generics and Wildcards is a Fractal',
	 'urllink': u'http://arxiv.org/abs/1411.5166'}
2015-04-10 18:07:03+0000 [xxu46_10] INFO: Crawled 865 pages (at 1 pages/min), scraped 858 items (at 1 items/min)
2015-04-10 18:07:44+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1411.5161> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 18:07:44+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1411.5161>
	{'abstract': u"Cloud era brought revolution of computerization world. People could access their data from anywhere and anytime with different devices. One of the cloud's model is Software as a Service, which capable to provide applications that run on a cloud infrastructure.An IDE (Integrated Development Environment) is the most popular tool to develop application in the network or single computer development. By installing IDE in each computer of the network could causes the lot of time and budget spending. The objective of the research is developing an efficient cloud based IDE. The IDE could compile the code which sent from client browser through SaaS IDE to the server and send it back to the client. The method that used in the research is the System Development Life-Cycle: Waterfall and Unified Model Language as system designing tool. The research successfully produced the cloud-based SaaS IDE with excellent result from several testing in local network and internet.",
	 'authors': u'A.B. Mutiara, R. Refianti, B.A. Witono,',
	 'category': u'Computer Science ',
	 'date': '2014-11-19',
	 'pdflink': u'http://arxiv.org/pdf/1411.5161',
	 'subjects': u'Software Engineering (cs.SE)',
	 'title': u'\nDeveloping a SAAS-Cloud Integrated Development Environment (IDE) for C,  C++, and Java',
	 'urllink': u'http://arxiv.org/abs/1411.5161'}
2015-04-10 18:08:03+0000 [xxu46_10] INFO: Crawled 866 pages (at 1 pages/min), scraped 859 items (at 1 items/min)
2015-04-10 18:08:58+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1411.5153> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 18:08:58+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1411.5153>
	{'abstract': u'Nowadays, Web Services (WS) remain a main actor in the implementation of distributed applications. They represent a new promising paradigm for the development, deployment and integration of Internet applications. These services are in most cases unable to provide the required functionality; they must be composed to provide appropriate services, richer and more interesting for other applications as well as for human users. The composition of Web services is considered as a strong point, which allows answering complex queries by combining the functionality of multiple services within a same composition. In this work we showed how the formalism of graphs can be used to improve the composition of web services and make it automatic. We have proposed the rewriting logic and its language Maude as a support for a graph-based approach to automatic composition of web services. The proposed model has made possible the exploration of different composition schemas as well as the formal analysis of service compositions. The paper introduces a case study showing how to apply our formalization.',
	 'authors': u'Walid Berrouk, Ouanes Aissaoui,',
	 'category': u'Computer Science ',
	 'date': '2014-11-19',
	 'pdflink': u'http://arxiv.org/pdf/1411.5153',
	 'subjects': u'Software Engineering (cs.SE)',
	 'title': u'\nA Rewriting Logic Approach for Automatic Composition of Web Services',
	 'urllink': u'http://arxiv.org/abs/1411.5153'}
2015-04-10 18:09:03+0000 [xxu46_10] INFO: Crawled 867 pages (at 1 pages/min), scraped 860 items (at 1 items/min)
2015-04-10 18:10:03+0000 [xxu46_10] INFO: Crawled 867 pages (at 0 pages/min), scraped 860 items (at 0 items/min)
2015-04-10 18:10:09+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1411.5140> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 18:10:09+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1411.5140>
	{'abstract': u'Attentional Neural Network is a new framework that integrates top-down cognitive bias and bottom-up feature extraction in one coherent architecture. The top-down influence is especially effective when dealing with high noise or difficult segmentation problems. Our system is modular and extensible. It is also easy to train and cheap to run, and yet can accommodate complex behaviors. We obtain classification accuracy better than or competitive with state of art results on the MNIST variation dataset, and successfully disentangle overlaid digits with high success rates. We view such a general purpose framework as an essential foundation for a larger system emulating the cognitive abilities of the whole brain.',
	 'authors': u'Qian Wang, Jiaxing Zhang, Sen Song, Zheng Zhang,',
	 'category': u'Computer Science ',
	 'date': '2014-11-19',
	 'pdflink': u'http://arxiv.org/pdf/1411.5140',
	 'subjects': u'Computer Vision and Pattern Recognition (cs.CV)',
	 'title': u'\nAttentional Neural Network: Feature Selection Using Cognitive Feedback',
	 'urllink': u'http://arxiv.org/abs/1411.5140'}
2015-04-10 18:11:03+0000 [xxu46_10] INFO: Crawled 868 pages (at 1 pages/min), scraped 861 items (at 1 items/min)
2015-04-10 18:11:26+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1411.5137> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 18:11:26+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1411.5137>
	{'abstract': u'With the growing technology, we humans always need something that stands out from the other thing. Gestures are most desirable source to Communicate with the Machines. Human Computer Interaction finds its importance when it comes to working with the Human gestures to control the computer applications. Usually we control the applications using mouse, keyboard, laser pointers etc. but, with the recent advent in the technology it has even left behind their usage by introducing more efficient techniques to control applications. There are many Gesture Recognition techniques that have been implemented using image processing in the past. However recognizing the gestures in the noisy background has always been a difficult task to achieve. In the proposed system, we are going to use one such technique called Augmentation in Image processing to control Media Player. We will recognize Gestures using which we are going to control the operations on Media player. Augmentation usually is one step ahead when it comes to virtual reality. It has no restrictions on the background. Moreover it also does not rely on certain things like gloves, color pointers etc. for recognizing the gesture. This system mainly appeals to those users who always looks out for a better option that makes their interaction with computer more simpler or easier.',
	 'authors': u'Sandeep Vasave, Amol Plave,',
	 'category': u'Computer Science ',
	 'date': '2014-11-19',
	 'pdflink': u'http://arxiv.org/pdf/1411.5137',
	 'subjects': u'Human-Computer Interaction (cs.HC)',
	 'title': u'\nStudy of Gesture Recognition methods and augmented reality',
	 'urllink': u'http://arxiv.org/abs/1411.5137'}
2015-04-10 18:12:03+0000 [xxu46_10] INFO: Crawled 869 pages (at 1 pages/min), scraped 862 items (at 1 items/min)
2015-04-10 18:12:43+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1411.5132> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 18:12:43+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1411.5132>
	{'abstract': u'In this paper, the energy efficiency of multihop relaying over Nakagami- fading channels is investigated. The "consumption factor" is used as a metric to evaluate the energy efficiency, and it is derived in closed-form for both amplify-and-forward and decode-and-forward relaying. Then, based on the obtained expressions, we propose a power allocation strategy maximizing the consumption factor. In addition, two sub-optimal, low complexity, power allocation algorithms are proposed and analyzed, and the obtained power allocation schemes are compared, in terms of energy efficiency as well as other common performance metrics, to other power allocation schemes from the literature. Analytical and simulation results confirm the accuracy of our derivations, and assess the performance gains of the proposed approach.',
	 'authors': u'Itsikiantsoa Randrianantenaina, Mustapha Benjillali, Mohamed-Slim Alouini,',
	 'category': u'Computer Science ',
	 'date': '2014-11-19',
	 'pdflink': u'http://arxiv.org/pdf/1411.5132',
	 'subjects': u'Information Theory (cs.IT)',
	 'title': u'\nConsumption Factor Optimization for Multihop Relaying over Nakagami-m  Fading channels',
	 'urllink': u'http://arxiv.org/abs/1411.5132'}
2015-04-10 18:13:03+0000 [xxu46_10] INFO: Crawled 870 pages (at 1 pages/min), scraped 863 items (at 1 items/min)
2015-04-10 18:13:36+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1411.5131> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 18:13:36+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1411.5131>
	{'abstract': u'Often, when analyzing the behaviour of systems modelled as context-free languages, we wish to know if two languages overlap. To this end, we present an effective semi-decision procedure for regular separability of context-free languages, based on counter-example guided abstraction refinement. We propose two refinement methods, one inexpensive but incomplete, and the other complete but more expensive. We provide an experimental evaluation of this procedure, and demonstrate its practicality on a range of verification and language-theoretic instances.',
	 'authors': u'Graeme Gange, Jorge A. Navas, Peter Schachte, Harald Sondergaard, Peter J. Stuckey,',
	 'category': u'Computer Science ',
	 'date': '2014-11-19',
	 'pdflink': u'http://arxiv.org/pdf/1411.5131',
	 'subjects': u'Formal Languages and Automata Theory (cs.FL)',
	 'title': u'\nA Complete Refinement Procedure for Regular Separability of Context-Free  Languages',
	 'urllink': u'http://arxiv.org/abs/1411.5131'}
2015-04-10 18:14:03+0000 [xxu46_10] INFO: Crawled 871 pages (at 1 pages/min), scraped 864 items (at 1 items/min)
2015-04-10 18:14:43+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1411.5127> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 18:14:43+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1411.5127>
	{'abstract': u'Sorted data is usually easier to compress than unsorted permutations of the same data. This motivates a simple compression scheme: specify the sorted permutation of the data along with a representation of the sorted data compressed recursively. The sorted permutation can be specified by recording the decisions made by quicksort. If the size of the data is known, then the quicksort decisions describe the data at a rate that is nearly as efficient as the minimal prefix-free code for the distribution, which is bounded by the entropy of the distribution. This is possible even though the distribution is unknown ahead of time. Used in this way, quicksort acts as a universal code in that it is asymptotically optimal for any stationary source. The Shannon entropy is a lower bound when describing stochastic, independent symbols. However, it is possible to encode non-uniform, finite strings below the entropy of the sample distribution by also encoding symbol counts because the values in the sequence are no longer independent once the counts are known. The key insight is that sparse quicksort comparison vectors can also be compressed to achieve an even lower rate when data is highly non-uniform while incurring only a modest penalty when data is random.',
	 'authors': u'Oscar Stiffelman,',
	 'category': u'Computer Science ',
	 'date': '2014-11-19',
	 'pdflink': u'http://arxiv.org/pdf/1411.5127',
	 'subjects': u'Data Structures and Algorithms (cs.DS)',
	 'title': u'\nPivotCompress: Compression by Sorting',
	 'urllink': u'http://arxiv.org/abs/1411.5127'}
2015-04-10 18:15:03+0000 [xxu46_10] INFO: Crawled 872 pages (at 1 pages/min), scraped 865 items (at 1 items/min)
2015-04-10 18:15:14+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1411.5123> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 18:15:14+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1411.5123>
	{'abstract': u"We present a deterministic near-linear time algorithm that computes the edge-connectivity and finds a minimum cut for a simple undirected unweighted graph G with n vertices and m edges. This is the first o(mn) time deterministic algorithm for the problem. In near-linear time we can also construct the classic cactus representation of all minimum cuts. The previous fastest deterministic algorithm by Gabow from STOC'91 took ~O(m+k^2 n), where k is the edge connectivity, but k could be Omega(n). At STOC'96 Karger presented a randomized near linear time Monte Carlo algorithm for the minimum cut problem. As he points out, there is no better way of certifying the minimality of the returned cut than to use Gabow's slower deterministic algorithm and compare sizes. Our main technical contribution is a near-linear time algorithm that contract vertex sets of a simple input graph G with minimum degree d, producing a multigraph with ~O(m/d) edges which preserves all minimum cuts of G with at least 2 vertices on each side. In our deterministic near-linear time algorithm, we will decompose the problem via low-conductance cuts found using PageRank a la Brin and Page (1998), as analyzed by Andersson, Chung, and Lang at FOCS'06. Normally such algorithms for low-conductance cuts are randomized Monte Carlo algorithms, because they rely on guessing a good start vertex. However, in our case, we have so much structure that no guessing is needed.",
	 'authors': u'Ken-ichi Kawarabayashi, Mikkel Thorup,',
	 'category': u'Computer Science ',
	 'date': '2014-11-19',
	 'pdflink': u'http://arxiv.org/pdf/1411.5123',
	 'subjects': u'Data Structures and Algorithms (cs.DS)',
	 'title': u'\nDeterministic Global Minimum Cut of a Simple Graph in Near-Linear Time',
	 'urllink': u'http://arxiv.org/abs/1411.5123'}
2015-04-10 18:16:03+0000 [xxu46_10] INFO: Crawled 873 pages (at 1 pages/min), scraped 866 items (at 1 items/min)
2015-04-10 18:16:34+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1411.5118> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 18:16:34+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1411.5118>
	{'abstract': u'In social networks, link prediction predicts missing links in current networks and new or dissolution links in future networks, is important for mining and analyzing the evolution of social networks. In the past decade, many works have been done about the link prediction in social networks. The goal of this paper is to comprehensively review, analyze and discuss the state-of-the-art of the link prediction in social networks. A systematical category for link prediction techniques and problems is presented. Then link prediction techniques and problems are analyzed and discussed. Typical applications of link prediction are also addressed. Achievements and roadmaps of some active research groups are introduced. Finally, some future challenges of the link prediction in social networks are discussed.',
	 'authors': u'Peng Wang, Baowen Xu, Yurong Wu, Xiaoyu Zhou,',
	 'category': u'Computer Science ',
	 'date': '2014-11-19',
	 'pdflink': u'http://arxiv.org/pdf/1411.5118',
	 'subjects': u'Social and Information Networks (cs.SI)',
	 'title': u'\nLink Prediction in Social Networks: the State-of-the-Art',
	 'urllink': u'http://arxiv.org/abs/1411.5118'}
2015-04-10 18:17:03+0000 [xxu46_10] INFO: Crawled 874 pages (at 1 pages/min), scraped 867 items (at 1 items/min)
2015-04-10 18:17:50+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1411.5110> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 18:17:50+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1411.5110>
	{'abstract': u'To represent mutually exclusive procedures, we propose a choice-conjunctive declaration statement of the form where are the procedure declaration statements within a module. This statement has the following semantics: request the machine to choose a successful one between and . This statement is useful for representing objects with mutually exclusive procedures. We illustrate our idea via C^uchoo, an extension of the core C with a new statement.',
	 'authors': u'Keehang Kwon,',
	 'category': u'Computer Science ',
	 'date': '2014-11-19',
	 'pdflink': u'http://arxiv.org/pdf/1411.5110',
	 'subjects': u'Programming Languages (cs.PL)',
	 'title': u'\nMutually Exclusive Procedures in Imperative Languages',
	 'urllink': u'http://arxiv.org/abs/1411.5110'}
2015-04-10 18:18:03+0000 [xxu46_10] INFO: Crawled 875 pages (at 1 pages/min), scraped 868 items (at 1 items/min)
2015-04-10 18:18:57+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1411.5107> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 18:18:57+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1411.5107>
	{'abstract': u"Substantial empirical research has shown that the level of individualism vs. collectivism is one of the most critical and important determinants of societal traits, such as economic growth, economic institutions and health conditions. But the exact nature of this impact has thus far not been well understood in an analytical setting. In this work, we develop one of the first theoretical models that analytically studies the impact of individualism-collectivism on the society. We model the growth of an individual's welfare (wealth, resources and health) as depending not only on himself, but also on the level of collectivism, i.e. the level of dependence on the rest of the individuals in the society, which leads to a co-evolutionary setting. Based on our model, we are able to predict the impact of individualism-collectivism on various societal metrics, such as average welfare, average life-time, total population, cumulative welfare and average inequality. We analytically show that individualism has a positive impact on average welfare and cumulative welfare, but comes with the drawbacks of lower average life-time, lower total population and higher average inequality.",
	 'authors': u'Kartik Ahuja, Simpson Zhang, Mihaela van der Schaar,',
	 'category': u'Computer Science ',
	 'date': '2014-11-19',
	 'pdflink': u'http://arxiv.org/pdf/1411.5107',
	 'subjects': u'Social and Information Networks (cs.SI)',
	 'title': u'\nTowards a Theory of Societal Co-Evolution: Individualism versus  Collectivism',
	 'urllink': u'http://arxiv.org/abs/1411.5107'}
2015-04-10 18:19:03+0000 [xxu46_10] INFO: Crawled 876 pages (at 1 pages/min), scraped 869 items (at 1 items/min)
2015-04-10 18:20:03+0000 [xxu46_10] INFO: Crawled 876 pages (at 0 pages/min), scraped 869 items (at 0 items/min)
2015-04-10 18:20:05+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1411.5102> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 18:20:05+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1411.5102>
	{'abstract': u'We study the problem of interference management in large-scale small cell networks, where each user equipment (UE) needs to determine in a distributed manner when and at what power level it should transmit to its serving small cell base station (SBS) such that a given network performance criterion is maximized subject to minimum quality of service (QoS) requirements by the UEs. We first propose a distributed algorithm for the UE-SBS pairs to find a subset of weakly interfering UE-SBS pairs, namely the maximal independent sets (MISs) of the interference graph in logarithmic time (with respect to the number of UEs). Then we propose a novel problem formulation which enables UE-SBS pairs to determine the optimal fractions of time occupied by each MIS in a distributed manner. We analytically bound the performance of our distributed policy in terms of the competitive ratio with respect to the optimal network performance, which is obtained in a centralized manner with NP (non-deterministic polynomial time) complexity. Remarkably, the competitive ratio is independent of the network size, which guarantees scalability in terms of performance for arbitrarily large networks. Through simulations, we show that our proposed policies achieve significant performance improvements (from 150% to 700%) over the existing policies.',
	 'authors': u'Kartik Ahuja, Yuanzhang Xiao, Mihaela van der Schaar,',
	 'category': u'Computer Science ',
	 'date': '2014-11-19',
	 'pdflink': u'http://arxiv.org/pdf/1411.5102',
	 'subjects': u'Information Theory (cs.IT)',
	 'title': u'\nDistributed Interference Management Policies for Heterogeneous Small  Cell Networks',
	 'urllink': u'http://arxiv.org/abs/1411.5102'}
2015-04-10 18:21:03+0000 [xxu46_10] INFO: Crawled 877 pages (at 1 pages/min), scraped 870 items (at 1 items/min)
2015-04-10 18:21:13+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1411.5098> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 18:21:13+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1411.5098>
	{'abstract': u'We study the design of a DVB-S2 system in order to maximise spectral efficiency. This task is usually challenging due to channel variability. The solution adopted in modern satellite communications systems such as DVB-SH and DVB-S2 relies mainly on a time sharing strategy. Recently, we proposed to combine time sharing with hierarchical modulation to increase the transmission rate of broadcast systems. However, the optimal spectral efficiency remained an open question. In this paper, we show that the optimal transmission rate is the solution of a linear programming problem. We also study the performance of the optimal scheme for a DVB-S2 use case.',
	 'authors': u'Hugo Meric,',
	 'category': u'Computer Science ',
	 'date': '2014-11-19',
	 'pdflink': u'http://arxiv.org/pdf/1411.5098',
	 'subjects': u'Information Theory (cs.IT)',
	 'title': u'\nOptimal DVB-S2 spectral efficiency with hierarchical modulation',
	 'urllink': u'http://arxiv.org/abs/1411.5098'}
2015-04-10 18:22:03+0000 [xxu46_10] INFO: Crawled 878 pages (at 1 pages/min), scraped 871 items (at 1 items/min)
2015-04-10 18:22:13+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1411.5082> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 18:22:13+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1411.5082>
	{'abstract': u'Thanks to the property of provably capacity-achieving, the recently-discovered polar codes have been taken many attentions. Although the Successive Cancellation (SC) is the first and widely known decoding algorithm for polar codes, the theoretical study of SC is not complete yet. In the paper, the SC is studied thoroughly starting from its root, i.e. recursive formula, and some meaningful results are drawn. Firstly, the general forms of likelihood ratios (LR) and partial sums are deduced, and a new-found indicator, i.e. sharing factor, is demonstrated to play many important roles in SC decoding. Secondly, based on the first contributions, not only some new properties are discovered, but also the theoretical deducing of some existing empirical properties are provided. These properties are very important to implement the SC decoding efficiently. Thirdly, based on the properties a non-recursive tree SC decoding algorithm is proposed which can reduce the latency and implementation complexity to some extend.',
	 'authors': u'Dan Le, Qiong Li, Xiamu Niu,',
	 'category': u'Computer Science ',
	 'date': '2014-11-19',
	 'pdflink': u'http://arxiv.org/pdf/1411.5082',
	 'subjects': u'Information Theory (cs.IT)',
	 'title': u'\nStudy on Successive Cancellation Decoding of Polar Codes',
	 'urllink': u'http://arxiv.org/abs/1411.5082'}
2015-04-10 18:23:03+0000 [xxu46_10] INFO: Crawled 879 pages (at 1 pages/min), scraped 872 items (at 1 items/min)
2015-04-10 18:23:24+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1411.5078> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 18:23:24+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1411.5078>
	{'abstract': u'Exposing the rate information of wireless transmission enables highly efficient attacks that can severely degrade the performance of a network at very low cost. In this paper, we introduce an integrated solution to conceal the rate information of wireless transmissions while simultaneously boosting the resiliency against interference. The proposed solution is based on a generalization of Trellis Coded Modulation combined with Cryptographic Interleaving. We develop algorithms for discovering explicit codes for concealing any modulation in . We demonstrate that in most cases this modulation hiding scheme has the side effect of boosting resiliency by up to 8.5dB.',
	 'authors': u'Triet D. Vo-Huu, Guevara Noubir,',
	 'category': u'Computer Science ',
	 'date': '2014-11-19',
	 'pdflink': u'http://arxiv.org/pdf/1411.5078',
	 'subjects': u'Cryptography and Security (cs.CR)',
	 'title': u'\nCBM: A Crypto-Coded Modulation Scheme for Rate Information Concealing  and Robustness Boosting',
	 'urllink': u'http://arxiv.org/abs/1411.5078'}
2015-04-10 18:24:03+0000 [xxu46_10] INFO: Crawled 880 pages (at 1 pages/min), scraped 873 items (at 1 items/min)
2015-04-10 18:24:15+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1411.5077> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 18:24:15+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1411.5077>
	{'abstract': u'Cloud computing is being viewed as the technology of today and the future. Through this paradigm, the customers gain access to shared computing resources located in remote data centers that are hosted by cloud providers (CP). This technology allows for provisioning of various resources such as virtual machines (VM), physical machines, processors, memory, network, storage and software as per the needs of customers. Application providers (AP), who are customers of the CP, deploy applications on the cloud infrastructure and then these applications are used by the end-users. To meet the fluctuating application workload demands, dynamic provisioning is essential and this article provides a detailed literature survey of dynamic provisioning within cloud systems with focus on application performance. The well-known types of provisioning and the associated problems are clearly and pictorially explained and the provisioning terminology is clarified. A very detailed and general cloud provisioning classification is presented, which views provisioning from different perspectives, aiding in understanding the process inside-out. Cloud dynamic provisioning is explained by considering resources, stakeholders, techniques, technologies, algorithms, problems, goals and more.',
	 'authors': u'Yasir Shoaib, Olivia Das,',
	 'category': u'Computer Science ',
	 'date': '2014-11-19',
	 'pdflink': u'http://arxiv.org/pdf/1411.5077',
	 'subjects': u'Distributed, Parallel, and Cluster Computing (cs.DC)',
	 'title': u'\nPerformance-oriented Cloud Provisioning: Taxonomy and Survey',
	 'urllink': u'http://arxiv.org/abs/1411.5077'}
2015-04-10 18:24:54+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1411.5065> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 18:24:54+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1411.5065>
	{'abstract': u'In this paper, we propose a novel method for image fusion with a high-resolution panchromatic image and a low-resolution multispectral image at the same geographical location. The fusion is formulated as a convex optimization problem which minimizes a linear combination of a least-squares fitting term and a dynamic gradient sparsity regularizer. The former is to preserve accurate spectral information of the multispectral image, while the latter is to keep sharp edges of the high-resolution panchromatic image. We further propose to simultaneously register the two images during the fusing process, which is naturally achieved by virtue of the dynamic gradient sparsity property. An efficient algorithm is then devised to solve the optimization problem, accomplishing a linear computational complexity in the size of the output image in each iteration. We compare our method against seven state-of-the-art image fusion methods on multispectral image datasets from four satellites. Extensive experimental results demonstrate that the proposed method substantially outperforms the others in terms of both spatial and spectral qualities. We also show that our method can provide high-quality products from coarsely registered real-world datasets. Finally, a MATLAB implementation is provided to facilitate future research.',
	 'authors': u'Chen Chen, Yeqing Li, Wei Liu, Junzhou Huang,',
	 'category': u'Computer Science ',
	 'date': '2014-11-18',
	 'pdflink': u'http://arxiv.org/pdf/1411.5065',
	 'subjects': u'Computer Vision and Pattern Recognition (cs.CV)',
	 'title': u'\nSIRF: Simultaneous Image Registration and Fusion in A Unified Framework',
	 'urllink': u'http://arxiv.org/abs/1411.5065'}
2015-04-10 18:25:03+0000 [xxu46_10] INFO: Crawled 882 pages (at 2 pages/min), scraped 875 items (at 2 items/min)
2015-04-10 18:26:03+0000 [xxu46_10] INFO: Crawled 882 pages (at 0 pages/min), scraped 875 items (at 0 items/min)
2015-04-10 18:26:10+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1411.5060> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 18:26:10+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1411.5060>
	{'abstract': u'We show FIXP-hardness of computing equilibria in Arrow-Debreu exchange markets under Leontief utility functions, and Arrow-Debreu markets under linear utility functions and Leontief production sets, thereby settling these open questions (Vazirani and Yannakakis, 2009). As corollaries, we obtain FIXP-hardness for piecewise-linear concave (PLC) utilities and for Arrow-Debreu markets under linear utility functions and polyhedral production sets. In all cases, as required under FIXP, the set of instances mapped onto will admit equilibria, i.e., will be "yes" instances. If all instances are under consideration, then in all cases we prove that the problem of deciding if a given instance admits an equilibrium is ETR-complete, where ETR is the class Existential Theory of Reals. As a consequence of the results stated above, and the fact that membership in FIXP has been established for PLC utilities, the entire computational difficulty of Arrow-Debreu markets under PLC utility functions lies in the Leontief utility subcase. This is perhaps the most unexpected aspect of our result, since Leontief utilities are meant for the case that goods are perfect complements, whereas PLC utilities are very general, capturing not only the cases when goods are complements and substitutes, but also arbitrary combinations of these and much more. The main technical part of our result is the following reduction: Given a set \'S\' of simultaneous multivariate polynomial equations in which the variables are constrained to be in a closed bounded region in the positive orthant, we construct a Leontief exchange market \'M\' which has one good corresponding to each variable in \'S\'. We prove that the equilibria of \'M\', when projected onto prices of these latter goods, are in one-to-one correspondence with the set of solutions of the polynomials. This reduction is related to a classic result of Sonnenschein (1972-73).',
	 'authors': u'Jugal Garg, Ruta Mehta, Vijay V. Vazirani, Sadra Yazdanbod,',
	 'category': u'Computer Science ',
	 'date': '2014-11-18',
	 'pdflink': u'http://arxiv.org/pdf/1411.5060',
	 'subjects': u'Computational Complexity (cs.CC)',
	 'title': u'\nLeontief Exchange Markets Can Solve Multivariate Polynomial Equations,  Yielding FIXP and ETR Hardness',
	 'urllink': u'http://arxiv.org/abs/1411.5060'}
2015-04-10 18:27:03+0000 [xxu46_10] INFO: Crawled 883 pages (at 1 pages/min), scraped 876 items (at 1 items/min)
2015-04-10 18:27:27+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1411.5057> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 18:27:27+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1411.5057>
	{'abstract': u'In this paper, we propose a novel algorithm for analysis-based sparsity reconstruction. It can solve the generalized problem by structured sparsity regularization with an orthogonal basis and total variation regularization. The proposed algorithm is based on the iterative reweighted least squares (IRLS) model, which is further accelerated by the preconditioned conjugate gradient method. The convergence rate of the proposed algorithm is almost the same as that of the traditional IRLS algorithms, that is, exponentially fast. Moreover, with the specifically devised preconditioner, the computational cost for each iteration is significantly less than that of traditional IRLS algorithms, which enables our approach to handle large scale problems. In addition to the fast convergence, it is straightforward to apply our method to standard sparsity, group sparsity, overlapping group sparsity and TV based problems. Experiments are conducted on a practical application: compressive sensing magnetic resonance imaging. Extensive results demonstrate that the proposed algorithm achieves superior performance over 14 state-of-the-art algorithms in terms of both accuracy and computational cost.',
	 'authors': u'Chen Chen, Junzhou Huang, Lei He, Hongsheng Li,',
	 'category': u'Computer Science ',
	 'date': '2014-11-18',
	 'pdflink': u'http://arxiv.org/pdf/1411.5057',
	 'subjects': u'Computer Vision and Pattern Recognition (cs.CV)',
	 'title': u'\nFast Iteratively Reweighted Least Squares Algorithms for Analysis-Based  Sparsity Reconstruction',
	 'urllink': u'http://arxiv.org/abs/1411.5057'}
2015-04-10 18:28:03+0000 [xxu46_10] INFO: Crawled 884 pages (at 1 pages/min), scraped 877 items (at 1 items/min)
2015-04-10 18:28:31+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1411.5053> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 18:28:31+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1411.5053>
	{'abstract': u'The model of interaction between learning and evolutionary optimization is designed and investigated. The evolving population of modeled organisms is considered. The mechanism of the genetic assimilation of the acquired features during a number of generations of Darwinian evolution is studied. It is shown that the genetic assimilation takes place as follows: phenotypes of modeled organisms move towards the optimum at learning; then the selection takes place; genotypes of selected organisms also move towards the optimum. The hiding effect is also studied; this effect means that strong learning can inhibit the evolutionary search for the optimal genotype. The mechanism of influence of the learning load on the interaction between learning and evolution is analyzed. It is shown that the learning load can lead to a significant acceleration of evolution.',
	 'authors': u"Vladimir G. Red'ko,",
	 'category': u'Computer Science ',
	 'date': '2014-11-18',
	 'pdflink': u'http://arxiv.org/pdf/1411.5053',
	 'subjects': u'Neural and Evolutionary Computing (cs.NE)',
	 'title': u'\nModel of Interaction between Learning and Evolution',
	 'urllink': u'http://arxiv.org/abs/1411.5053'}
2015-04-10 18:29:03+0000 [xxu46_10] INFO: Crawled 885 pages (at 1 pages/min), scraped 878 items (at 1 items/min)
2015-04-10 18:29:18+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1411.5050> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 18:29:18+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1411.5050>
	{'abstract': u'Binary quadratic programming problems have attracted much attention in the last few decades due to their potential applications. This type of problems are NP-hard in general, and still considered a challenge in the design of efficient approximation algorithms for their solutions. The purpose of this paper is to investigate the approximability for a class of such problems where the constraint matrices are and have low . In the first part of the paper, we show that a completely positive rational factorization of such matrices can be computed in polynomial time, within any desired accuracy. We next consider binary quadratic programming problems of the following form: Given matrices , and a system of constrains (), , we seek to find a vector that maximizes (minimizes) a given function . This class of problems generalizes many fundamental problems in discrete optimization such as packing and covering integer programs/knapsack problems, quadratic knapsack problems, submodular maximization, etc. We consider the case when and the cp-ranks of the matrices are bounded by a constant. Our approximation results for the maximization problem are as follows. For the case when the objective function is nonnegative submodular, we give an -approximation algorithm, for any ; when the function is linear, we present a PTAS. We next extend our PTAS result to a wider class of non-linear objective functions including quadratic functions, multiplicative functions, and sum-of-ratio functions. The minimization problem seems to be much harder due to the fact that the relaxation is convex. For this case, we give a QPTAS for .',
	 'authors': u'Khaled Elbassioni, Trung Thanh Nguyen,',
	 'category': u'Computer Science ',
	 'date': '2014-11-18',
	 'pdflink': u'http://arxiv.org/pdf/1411.5050',
	 'subjects': u'Data Structures and Algorithms (cs.DS)',
	 'title': u'\nApproximation Schemes for Binary Quadratic Programming Problems with Low  cp-Rank Decompositions',
	 'urllink': u'http://arxiv.org/abs/1411.5050'}
2015-04-10 18:30:03+0000 [xxu46_10] INFO: Crawled 886 pages (at 1 pages/min), scraped 879 items (at 1 items/min)
2015-04-10 18:30:33+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1411.5014> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 18:30:33+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1411.5014>
	{'abstract': u'Music accounts for a significant chunk of interest among various online activities. This is reflected by wide array of alternatives offered in music related web/mobile apps, information portals, featuring millions of artists, songs and events attracting user activity at similar scale. Availability of large scale structured and unstructured data has attracted similar level of attention by data science community. This paper attempts to offer current state-of-the-art in music related analysis. Various approaches involving machine learning, information theory, social network analysis, semantic web and linked open data are represented in the form of taxonomy along with data sources and use cases addressed by the research community.',
	 'authors': u'Shubhanshu Gupta,',
	 'category': u'Computer Science ',
	 'date': '2014-11-18',
	 'pdflink': u'http://arxiv.org/pdf/1411.5014',
	 'subjects': u'Databases (cs.DB)',
	 'title': u'\nMusic Data Analysis: A State-of-the-art Survey',
	 'urllink': u'http://arxiv.org/abs/1411.5014'}
2015-04-10 18:31:03+0000 [xxu46_10] INFO: Crawled 887 pages (at 1 pages/min), scraped 880 items (at 1 items/min)
2015-04-10 18:31:44+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1411.5007> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 18:31:44+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1411.5007>
	{'abstract': u'The task of computing approximate Nash equilibria in large zero-sum extensive-form games has received a tremendous amount of attention due mainly to the Annual Computer Poker Competition. Immediately after its inception, two competing and seemingly different approaches emerged---one an application of no-regret online learning, the other a sophisticated gradient method applied to a convex-concave saddle-point formulation. Since then, both approaches have grown in relative isolation with advancements on one side not effecting the other. In this paper, we rectify this by dissecting and, in a sense, unify the two views.',
	 'authors': u'Kevin Waugh, J. Andrew Bagnell,',
	 'category': u'Computer Science ',
	 'date': '2014-11-18',
	 'pdflink': u'http://arxiv.org/pdf/1411.5007',
	 'subjects': u'Artificial Intelligence (cs.AI)',
	 'title': u'\nA Unified View of Large-scale Zero-sum Equilibrium Computation',
	 'urllink': u'http://arxiv.org/abs/1411.5007'}
2015-04-10 18:32:03+0000 [xxu46_10] INFO: Crawled 888 pages (at 1 pages/min), scraped 881 items (at 1 items/min)
2015-04-10 18:32:52+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1411.5005> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 18:32:52+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1411.5005>
	{'abstract': u'Recent years have seen the rise of more sophisticated attacks including advanced persistent threats (APTs) which pose severe risks to organizations and governments by targeting confidential proprietary information. Additionally, new malware strains are appearing at a higher rate than ever before. Since many of these malware are designed to evade existing security products, traditional defenses deployed by most enterprises today, e.g., anti-virus, firewalls, intrusion detection systems, often fail at detecting infections at an early stage. We address the problem of detecting early-stage infection in an enterprise setting by proposing a new framework based on belief propagation inspired from graph theory. Belief propagation can be used either with "seeds" of compromised hosts or malicious domains (provided by the enterprise security operation center -- SOC) or without any seeds. In the latter case we develop a detector of C&amp;C communication particularly tailored to enterprises which can detect a stealthy compromise of only a single host communicating with the C&amp;C server. We demonstrate that our techniques perform well on detecting enterprise infections. We achieve high accuracy with low false detection and false negative rates on two months of anonymized DNS logs released by Los Alamos National Lab (LANL), which include APT infection attacks simulated by LANL domain experts. We also apply our algorithms to 38TB of real-world web proxy logs collected at the border of a large enterprise. Through careful manual investigation in collaboration with the enterprise SOC, we show that our techniques identified hundreds of malicious domains overlooked by state-of-the-art security products.',
	 'authors': u'Alina Oprea, Zhou Li, Ting-Fang Yen, Sang Chin, Sumayah Alrwais,',
	 'category': u'Computer Science ',
	 'date': '2014-11-18',
	 'pdflink': u'http://arxiv.org/pdf/1411.5005',
	 'subjects': u'Cryptography and Security (cs.CR)',
	 'title': u'\nDetection of Early-Stage Enterprise Infection by Mining Large-Scale Log  Data',
	 'urllink': u'http://arxiv.org/abs/1411.5005'}
2015-04-10 18:33:03+0000 [xxu46_10] INFO: Crawled 889 pages (at 1 pages/min), scraped 882 items (at 1 items/min)
2015-04-10 18:33:29+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1411.5003> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 18:33:29+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1411.5003>
	{'abstract': u'Mobile (cellular) networks enable innovation, but can also stifle it and lead to user frustration when network performance falls below expectations. As mobile networks become the predominant method of Internet access, research, development, and regulatory communities have taken an increased interest in measuring mobile network performance and its impact on user experience. In this survey we examine current approaches to end-to-end mobile network performance measurement, diagnosis, and application prototyping. We compare available tools and their shortcomings with respect to the needs of researchers, developers, regulators, and the public. We intend for this survey to provide a comprehensive view of currently active efforts and some auspicious directions for future work in mobile network measurement and mobile application performance evaluation.',
	 'authors': u'Utkarsh Goel, Mike P. Wittie, Kimberly C. Claffy, Andrew Le,',
	 'category': u'Computer Science ',
	 'date': '2014-11-18',
	 'pdflink': u'http://arxiv.org/pdf/1411.5003',
	 'subjects': u'Networking and Internet Architecture (cs.NI)',
	 'title': u'\nSurvey of End-to-End Mobile Network Measurement Testbeds',
	 'urllink': u'http://arxiv.org/abs/1411.5003'}
2015-04-10 18:34:03+0000 [xxu46_10] INFO: Crawled 890 pages (at 1 pages/min), scraped 883 items (at 1 items/min)
2015-04-10 18:34:15+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1411.4982> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 18:34:15+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1411.4982>
	{'abstract': u"A random sampling function Sample:U-&gt; for a key universe U is a distinguisher with probability p if for any given assignment of values v(x) to the keys x in U, including at least one non-zero v(x)!=0, the sampled sum sum is non-zero with probability at least p. Here the key values may come from any commutative monoid (addition is commutative and associative and zero is neutral). Such distinguishers were introduced by Vazirani [PhD thesis 1986], and Naor and Naor used them for their small bias probability spaces [STOC'90]. Constant probability distinguishers are used for testing in contexts where the key values are not computed directly, yet where the sum is easily computed. A simple example is when we get a stream of key value pairs (x_1,v_1),(x_2,v_2),...,(x_n,v_n) where the same key may appear many times. The accumulated value of key x is v(x)=sum. For space reasons, we may not be able to maintain v(x) for every key x, but the sampled sum is easily maintained as the single value sum. Here we show that when dealing with w-bit integers, if a is a uniform odd w-bit integer and t is a uniform w-bit integer, then Sample(x)=[ax mod 2^w &lt;= t] is a distinguisher with probability 1/8. Working with standard units, that is, w=8, 16, 32, 64, we exploit that w-bit multiplication works modulo 2^w, discarding overflow automatically, and then the sampling decision is implemented by the C-code a*x&lt;=t. Previous such samplers were much less computer friendly, e.g., the distinguisher of Naor and Naor [STOC'90] was more complicated and involved a 7-independent hash function.",
	 'authors': u'Mikkel Thorup,',
	 'category': u'Computer Science ',
	 'date': '2014-11-18',
	 'pdflink': u'http://arxiv.org/pdf/1411.4982',
	 'subjects': u'Data Structures and Algorithms (cs.DS)',
	 'title': u'\nSample(x)=(a*x<=t) is a distinguisher with probability 1/8',
	 'urllink': u'http://arxiv.org/abs/1411.4982'}
2015-04-10 18:35:03+0000 [xxu46_10] INFO: Crawled 891 pages (at 1 pages/min), scraped 884 items (at 1 items/min)
2015-04-10 18:35:18+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1411.4972> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 18:35:18+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1411.4972>
	{'abstract': u"Online reputation systems are commonly used by e-commerce providers nowadays. In order to generate an objective ranking of online items' quality according to users' ratings, many sophisticated algorithms have been proposed in the literature. In this paper, instead of proposing new algorithms we focus on a more fundamental problem: the rating projection. The basic idea is that even though the rating values given by users are linearly separated, the real preference of users to items between different values gave is nonlinear. We thus design an approach to project the original ratings of users to more representative values. This approach can be regarded as a data pretreatment method. Simulation in both artificial and real networks shows that the performance of the ranking algorithms can be improved when the projected ratings are used.",
	 'authors': u'Hao Liao, An Zeng, Yi-Cheng Zhang,',
	 'category': u'Computer Science ',
	 'date': '2014-11-15',
	 'pdflink': u'http://arxiv.org/pdf/1411.4972',
	 'subjects': u'Information Retrieval (cs.IR)',
	 'title': u'\nTowards an objective ranking in online reputation systems: the effect of  the rating projection',
	 'urllink': u'http://arxiv.org/abs/1411.4972'}
2015-04-10 18:36:03+0000 [xxu46_10] INFO: Crawled 892 pages (at 1 pages/min), scraped 885 items (at 1 items/min)
2015-04-10 18:36:21+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1411.4960> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 18:36:21+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1411.4960>
	{'abstract': u'In this paper we analyse network motifs in the co-occurrence directed networks constructed from five different texts (four books and one portal) in the Croatian language. After preparing the data and network construction, we perform the network motif analysis. We analyse the motif frequencies and Z-scores in the five networks. We present the triad significance profile for five datasets. Furthermore, we compare our results with the existing results for the linguistic networks. Firstly, we show that the triad significance profile for the Croatian language is very similar with the other languages and all the networks belong to the same family of networks. However, there are certain differences between the Croatian language and other analysed languages. We conclude that this is due to the free word-order of the Croatian language.',
	 'authors': u'Hana Rizvi\u0107, Sanda Martin\u010di\u0107-Ip\u0161i\u0107, Ana Me\u0161trovi\u0107,',
	 'category': u'Computer Science ',
	 'date': '2014-11-18',
	 'pdflink': u'http://arxiv.org/pdf/1411.4960',
	 'subjects': u'Computation and Language (cs.CL)',
	 'title': u'\nNetwork Motifs Analysis of Croatian Literature',
	 'urllink': u'http://arxiv.org/abs/1411.4960'}
2015-04-10 18:37:03+0000 [xxu46_10] INFO: Crawled 893 pages (at 1 pages/min), scraped 886 items (at 1 items/min)
2015-04-10 18:37:39+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1411.4958> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 18:37:39+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1411.4958>
	{'abstract': u'In the past few years, convolutional neural nets (CNN) have shown incredible promise for learning visual representations. In this paper, we use CNNs for the task of predicting surface normals from a single image. But what is the right architecture we should use? We propose to build upon the decades of hard work in 3D scene understanding, to design new CNN architecture for the task of surface normal estimation. We show by incorporating several constraints (man-made, manhattan world) and meaningful intermediate representations (room layout, edge labels) in the architecture leads to state of the art performance on surface normal estimation. We also show that our network is quite robust and show state of the art results on other datasets as well without any fine-tuning.',
	 'authors': u'Xiaolong Wang, David F. Fouhey, Abhinav Gupta,',
	 'category': u'Computer Science ',
	 'date': '2014-11-18',
	 'pdflink': u'http://arxiv.org/pdf/1411.4958',
	 'subjects': u'Computer Vision and Pattern Recognition (cs.CV)',
	 'title': u'\nDesigning Deep Networks for Surface Normal Estimation',
	 'urllink': u'http://arxiv.org/abs/1411.4958'}
2015-04-10 18:38:03+0000 [xxu46_10] INFO: Crawled 894 pages (at 1 pages/min), scraped 887 items (at 1 items/min)
2015-04-10 18:38:49+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1411.4956> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 18:38:49+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1411.4956>
	{'abstract': u'We present an integrated, three dimensional, model of urban canopy, building energy and radiosity, for early stage urban designs and test it on four urban morphologies. All sub-models share a common descriptions of the urban morphology, similar to 3D urban design master plans and have simple parameters. The canopy model is a multilayer model, with a new discrete layer approach that does not rely on simplified geometry such as canyon or regular arrays. The building energy model is a simplified RC equivalent model, with no hypotheses on internal zoning or wall composition. We use the CitySim software for the radiosity model. We study the effects of convexity, the number of buildings and building height, at constant density and thermal characteristics. Our results suggest that careful three dimensional morphology design can reduce heat demand by a factor of 2, especially by improving insolation of lower levels. The most energy efficient morphology in our simulations has both the highest surface/volume ratio and the biggest impact on the urban climate.',
	 'authors': u'Etienne Burdet, Morgane Colombert, Denis Morand, Youssef Diab,',
	 'category': u'Computer Science ',
	 'date': '2014-11-17',
	 'pdflink': u'http://arxiv.org/pdf/1411.4956',
	 'subjects': u'Computers and Society (cs.CY)',
	 'title': u'\nIntegrated canopy, building energy and radiosity model for 3D urban  design',
	 'urllink': u'http://arxiv.org/abs/1411.4956'}
2015-04-10 18:39:03+0000 [xxu46_10] INFO: Crawled 895 pages (at 1 pages/min), scraped 888 items (at 1 items/min)
2015-04-10 18:39:58+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1411.4952> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 18:39:58+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1411.4952>
	{'abstract': u'This paper presents a novel approach for automatically generating image descriptions: visual detectors and language models learn directly from a dataset of image captions. We use Multiple Instance Learning to train visual detectors for words that commonly occur in captions, including many different parts of speech such as nouns, verbs, and adjectives. The word detector outputs serve as conditional inputs to a maximum-entropy language model. The language model learns from a set of over 400,000 image descriptions to capture the statistics of word usage. We capture global semantics by re-ranking caption candidates using sentence-level features and a deep multimodal similarity model. When human judges compare the system captions to ones written by other people, the system captions have equal or better quality over 23% of the time.',
	 'authors': u'Hao Fang, Saurabh Gupta, Forrest Iandola, Rupesh Srivastava, Li Deng, Piotr Doll\xe1r, Jianfeng Gao, Xiaodong He, Margaret Mitchell, John C. Platt, C. Lawrence Zitnick, Geoffrey Zweig,',
	 'category': u'Computer Science ',
	 'date': '2014-11-18',
	 'pdflink': u'http://arxiv.org/pdf/1411.4952',
	 'subjects': u'Computer Vision and Pattern Recognition (cs.CV)',
	 'title': u'\nFrom Captions to Visual Concepts and Back',
	 'urllink': u'http://arxiv.org/abs/1411.4952'}
2015-04-10 18:40:03+0000 [xxu46_10] INFO: Crawled 896 pages (at 1 pages/min), scraped 889 items (at 1 items/min)
2015-04-10 18:41:03+0000 [xxu46_10] INFO: Crawled 896 pages (at 0 pages/min), scraped 889 items (at 0 items/min)
2015-04-10 18:41:13+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1411.4949> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 18:41:13+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1411.4949>
	{'abstract': u"Understanding the nature of strategic voting is the holy grail of social choice theory, where game-theory, social science and recently computational approaches are all applied in order to model the incentives and behavior of voters. In a recent paper, Meir et al.[EC'14] made another step in this direction, by suggesting a behavioral game-theoretic model for voters under uncertainty. For a specific variation of best-response heuristics, they proved initial existence and convergence results in the Plurality voting system. In this paper, we extend the model in multiple directions, considering voters with different uncertainty levels, simultaneous strategic decisions, and a more permissive notion of best-response. We prove that a voting equilibrium exists even in the most general case. Further, any society voting in an iterative setting is guaranteed to converge. We also analyze an alternative behavior where voters try to minimize their worst-case regret. We show that the two behaviors coincide in the simple setting of Meir et al., but not in the general case.",
	 'authors': u'Reshef Meir,',
	 'category': u'Computer Science ',
	 'date': '2014-11-18',
	 'pdflink': u'http://arxiv.org/pdf/1411.4949',
	 'subjects': u'Multiagent Systems (cs.MA)',
	 'title': u'\nPlurality Voting under Uncertainty',
	 'urllink': u'http://arxiv.org/abs/1411.4949'}
2015-04-10 18:42:03+0000 [xxu46_10] INFO: Crawled 897 pages (at 1 pages/min), scraped 890 items (at 1 items/min)
2015-04-10 18:42:07+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1411.4943> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 18:42:07+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1411.4943>
	{'abstract': u'We put forward a new model of congestion games where agents have uncertainty over the routes used by other agents. We take a non-probabilistic approach, assuming that each agent knows that the number of agents using an edge is within a certain range. Given this uncertainty, we model agents who either minimize their worst-case cost (WCC) or their worst-case regret (WCR), and study implications on equilibrium existence, convergence through adaptive play, and efficiency. Under the WCC behavior the game reduces to a modified congestion game, and welfare improves when agents have moderate uncertainty. Under WCR behavior the game is not, in general, a congestion game, but we show convergence and efficiency bounds for a simple class of games.',
	 'authors': u'Reshef Meir, David Parkes,',
	 'category': u'Computer Science ',
	 'date': '2014-11-18',
	 'pdflink': u'http://arxiv.org/pdf/1411.4943',
	 'subjects': u'Computer Science and Game Theory (cs.GT)',
	 'title': u'\nCongestion Games with Distance-Based Strict Uncertainty',
	 'urllink': u'http://arxiv.org/abs/1411.4943'}
2015-04-10 18:43:03+0000 [xxu46_10] INFO: Crawled 898 pages (at 1 pages/min), scraped 891 items (at 1 items/min)
2015-04-10 18:43:16+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1411.4942> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 18:43:16+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1411.4942>
	{'abstract': u'Counting the frequency of small subgraphs is a fundamental technique in network analysis across various domains, most notably in bioinformatics and social networks. The special case of triangle counting has received much attention. Getting results for 4-vertex patterns is highly challenging, and there are few practical results known that can scale to massive sizes. Indeed, even a highly tuned enumeration code takes more than a day on a graph with millions of edges. Most previous work that runs for truly massive graphs employ clusters and massive parallelization. We provide a sampling algorithm that provably and accurately approximates the frequencies of all 4-vertex pattern subgraphs. Our algorithm is based on a novel technique of 3-path sampling and a special pruning scheme to decrease the variance in estimates. We provide theoretical proofs for the accuracy of our algorithm, and give formal bounds for the error and confidence of our estimates. We perform a detailed empirical study and show that our algorithm provides estimates within 1% relative error for all subpatterns (over a large class of test graphs), while being orders of magnitude faster than enumeration and other sampling based algorithms. Our algorithm takes less than a minute (on a single commodity machine) to process an Orkut social network with 300 million edges.',
	 'authors': u'Madhav Jha, C. Seshadhri, Ali Pinar,',
	 'category': u'Computer Science ',
	 'date': '2014-11-18',
	 'pdflink': u'http://arxiv.org/pdf/1411.4942',
	 'subjects': u'Data Structures and Algorithms (cs.DS)',
	 'title': u'\nPath Sampling: A Fast and Provable Method for Estimating 4-Vertex  Subgraph Counts',
	 'urllink': u'http://arxiv.org/abs/1411.4942'}
2015-04-10 18:43:49+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1411.4940> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 18:43:49+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1411.4940>
	{'abstract': u'Indexing moving objects has been extensively studied in the past decades. However, none of the existing work considers the distribution of the speed values of the moving objects. Actually, in most applications, moving objects, such as pedestrians, vehicles, and airplanes, have their typical speed ranges. In this paper, we propose a novel index partitioning technique based on speed values of the moving objects. We first show that speed partitioning will significantly reduce the search space expansion which has direct impacts on the query performance of the indexing structure. Next we formulate the optimal speed partitioning problem based on the search space expansion analysis and then compute optimal solution using dynamic programming. We build the partitioned indexing system where queries are duplicated and processed in each index partition either concurrently or sequentially. Extensive experiments demonstrate that our methods dramatically improve the query performance of the indexing structures and outperforms other state-of-the-art velocity-based partitioning methods.',
	 'authors': u'Xiaofeng Xu, Li Xiong, Vaidy Sunderam, Jinfei Liu, Jun Luo,',
	 'category': u'Computer Science ',
	 'date': '2014-11-18',
	 'pdflink': u'http://arxiv.org/pdf/1411.4940',
	 'subjects': u'Databases (cs.DB)',
	 'title': u'\nSpeed Partitioning for Indexing Moving Objects',
	 'urllink': u'http://arxiv.org/abs/1411.4940'}
2015-04-10 18:44:03+0000 [xxu46_10] INFO: Crawled 900 pages (at 2 pages/min), scraped 893 items (at 2 items/min)
2015-04-10 18:45:03+0000 [xxu46_10] INFO: Crawled 900 pages (at 0 pages/min), scraped 893 items (at 0 items/min)
2015-04-10 18:45:06+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1411.4925> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 18:45:06+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1411.4925>
	{'abstract': u'We present in this paper an application which automatically generates textual short-term weather forecasts for every municipality in Galicia (NW Spain), using the real data provided by the Galician Meteorology Agency (MeteoGalicia). This solution combines in an innovative way computing with perceptions techniques and strategies for linguistic description of data together with a natural language generation (NLG) system. The application, named GALiWeather, extracts relevant information from weather forecast input data and encodes it into intermediate descriptions using linguistic variables and temporal references. These descriptions are later translated into natural language texts by the natural language generation system. The obtained forecast results have been thoroughly validated by an expert meteorologist from MeteoGalicia using a quality assessment methodology which covers two key dimensions of a text: the accuracy of its content and the correctness of its form. Following this validation GALiWeather will be released as a real service offering custom forecasts for a wide public.',
	 'authors': u'A. Ramos-Soto, A. Bugar\xedn, S. Barro, J. Taboada,',
	 'category': u'Computer Science ',
	 'date': '2014-11-18',
	 'pdflink': u'http://arxiv.org/pdf/1411.4925',
	 'subjects': u'Artificial Intelligence (cs.AI)',
	 'title': u'\nLinguistic Descriptions for Automatic Generation of Textual Short-Term  Weather Forecasts on Real Prediction Data',
	 'urllink': u'http://arxiv.org/abs/1411.4925'}
2015-04-10 18:46:03+0000 [xxu46_10] INFO: Crawled 901 pages (at 1 pages/min), scraped 894 items (at 1 items/min)
2015-04-10 18:46:23+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1411.4916> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 18:46:23+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1411.4916>
	{'abstract': u'We study anonymous posted price mechanisms for combinatorial auctions in a Bayesian framework. In a posted price mechanism, item prices are posted, then the consumers approach the seller sequentially in an arbitrary order, each purchasing her favorite bundle from among the unsold items at the posted prices. These mechanisms are simple, transparent and trivially dominant strategy incentive compatible (DSIC). We show that when agent preferences are fractionally subadditive (which includes all submodular functions), there always exist prices that, in expectation, obtain at least half of the optimal welfare. Our result is constructive: given black-box access to a combinatorial auction algorithm A, sample access to the prior distribution, and appropriate query access to the sampled valuations, one can compute, in polytime, prices that guarantee at least half of the expected welfare of A. As a corollary, we obtain the first polytime (in n and m) constant-factor DSIC mechanism for Bayesian submodular combinatorial auctions, given access to demand query oracles. Our results also extend to valuations with complements, where the approximation factor degrades linearly with the level of complementarity.',
	 'authors': u'Michal Feldman, Nick Gravin, Brendan Lucier,',
	 'category': u'Computer Science ',
	 'date': '2014-11-18',
	 'pdflink': u'http://arxiv.org/pdf/1411.4916',
	 'subjects': u'Computer Science and Game Theory (cs.GT)',
	 'title': u'\nCombinatorial Auctions via Posted Prices',
	 'urllink': u'http://arxiv.org/abs/1411.4916'}
2015-04-10 18:47:03+0000 [xxu46_10] INFO: Crawled 902 pages (at 1 pages/min), scraped 895 items (at 1 items/min)
2015-04-10 18:47:12+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1411.4894> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 18:47:12+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1411.4894>
	{'abstract': u'To reliably recover depth, motion, and other physical scene properties from images, low-level vision algorithms need to be able to reason with a local scene model (e.g., a locally planar model for stereo) to address the ambiguity of point-wise estimation. However, since these local models only hold piecewise in a real scene, inference algorithms must also reason about where the model is valid. We introduce a new framework for this reasoning that is defined over a dense set of overlapping regions at multiple scales. Inference in this framework requires finding the right subset of "inlier" regions where the model holds, and the model-based estimates for scene properties in each, through the optimization of an objective that requires these per-region estimates to also be globally consistent. We describe an efficient and parallel architecture to perform this optimization that exploits spatial redundancies in the overlapping set of regions. Specifically, we show that when these regions are organized into a multi-scale hierarchy, inference only requires sharing information across a very sparse set of links between parent and child regions. We demonstrate the benefits of this framework by applying it to stereo estimation, and evaluating performance on the KITTI benchmark.',
	 'authors': u'Ayan Chakrabarti, Ying Xiong, Steven J. Gortler, Todd Zickler,',
	 'category': u'Computer Science ',
	 'date': '2014-11-18',
	 'pdflink': u'http://arxiv.org/pdf/1411.4894',
	 'subjects': u'Computer Vision and Pattern Recognition (cs.CV)',
	 'title': u'\nLow-level Vision by Consensus in a Spatial Hierarchy of Regions',
	 'urllink': u'http://arxiv.org/abs/1411.4894'}
2015-04-10 18:48:03+0000 [xxu46_10] INFO: Crawled 903 pages (at 1 pages/min), scraped 896 items (at 1 items/min)
2015-04-10 18:48:12+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1411.4890> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 18:48:12+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1411.4890>
	{'abstract': u'Automatic image tagging has been a long standing problem, it mainly relies on image recognition techniques of which the accuracy is still not satisfying. This paper attempts to explore out-of-band sensing base on the mobile phone to sense the people in a picture while the picture is being taken and create name tags on-the-fly. The major challenges pertain to two aspects - "Who" and "Which". (1) "Who": discriminating people who are in the picture from those that are not; (2) "Which": correlating each name tag with its corresponding people in the picture. We propose an accurate acoustic scheme applying on the mobile phones, which leverages the Doppler effect of sound wave to address these two challenges. As a proof of concept, we implement the scheme on 7 android phones and take pictures in various real-life scenarios with people positioning in different ways. Extensive experiments show that the accuracy of tag correlation is above 85% within 3m for picturing.',
	 'authors': u'Xing Zhang,',
	 'category': u'Computer Science ',
	 'date': '2014-11-17',
	 'pdflink': u'http://arxiv.org/pdf/1411.4890',
	 'subjects': u'Sound (cs.SD)',
	 'title': u'\nWhich Are You In A Photo?',
	 'urllink': u'http://arxiv.org/abs/1411.4890'}
2015-04-10 18:49:03+0000 [xxu46_10] INFO: Crawled 904 pages (at 1 pages/min), scraped 897 items (at 1 items/min)
2015-04-10 18:49:29+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1411.4862> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 18:49:29+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1411.4862>
	{'abstract': u'Storage requirements for visual data have been increasing in recent years, following the emergence of many new highly interactive, multimedia services and applications for both personal and corporate use. This has been a key driving factor for the adoption of cloud-based data outsourcing solutions. However, outsourcing data storage to the Cloud also leads to new challenges that must be carefully addressed, especially regarding privacy. In this paper we propose a secure framework for outsourced privacy-preserving storage and retrieval in large image repositories. Our proposal is based on a novel cryptographic scheme, named IES-CBIR, specifically designed for media image data. Our solution enables both encrypted storage and querying using Content Based Image Retrieval (CBIR) while preserving privacy. We have built a prototype of the proposed framework, formally analyzed and proven its security properties, and experimentally evaluated its performance and precision. Our results show that IES-CBIR is provably secure, allows more efficient operations than existing proposals, both in terms of time and space complexity, and enables more realistic, interesting and practical application scenarios.',
	 'authors': u'Bernardo Ferreira, Jo\xe3o Rodrigues, Jo\xe3o Leit\xe3o, Henrique Domingos,',
	 'category': u'Computer Science ',
	 'date': '2014-11-18',
	 'pdflink': u'http://arxiv.org/pdf/1411.4862',
	 'subjects': u'Cryptography and Security (cs.CR)',
	 'title': u'\nPrivacy-Preserving Content-Based Image Retrieval in the Cloud',
	 'urllink': u'http://arxiv.org/abs/1411.4862'}
2015-04-10 18:50:03+0000 [xxu46_10] INFO: Crawled 905 pages (at 1 pages/min), scraped 898 items (at 1 items/min)
2015-04-10 18:50:19+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1411.4848> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 18:50:19+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1411.4848>
	{'abstract': u'Full-duplex (FD) radio has been introduced for bidirectional communications on the same temporal and spectral resources so as to maximize spectral efficiency. In this paper, motivated by the recent advances in FD radios, we provide a foundation for hybrid-duplex heterogeneous networks (HDHNs), composed of multi-tier networks with a mixture of access points (APs), operating either in bidirectional FD mode or downlink half-duplex (HD) mode. Specifically, we characterize the net- work interference from FD-mode cells, and derive the HDHN throughput by accounting for AP spatial density, self-interference cancellation (IC) capability, and transmission power of APs and users. By quantifying the HDHN throughput, we present the effect of network parameters and the self-IC capability on the HDHN throughput, and show the superiority of FD mode for larger AP densities (i.e., larger network interference and shorter communication distance) or higher self-IC capability. Furthermore, our results show operating all APs in FD or HD achieves higher throughput compared to the mixture of two mode APs in each tier network, and introducing hybrid-duplex for different tier networks improves the heterogenous network throughput.',
	 'authors': u'Jemin Lee, Tony Q. S. Quek,',
	 'category': u'Computer Science ',
	 'date': '2014-11-18',
	 'pdflink': u'http://arxiv.org/pdf/1411.4848',
	 'subjects': u'Information Theory (cs.IT)',
	 'title': u'\nHybrid Full-/Half-Duplex System Analysis in Heterogeneous Wireless  Networks',
	 'urllink': u'http://arxiv.org/abs/1411.4848'}
2015-04-10 18:51:03+0000 [xxu46_10] INFO: Crawled 906 pages (at 1 pages/min), scraped 899 items (at 1 items/min)
2015-04-10 18:51:37+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1411.4836> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 18:51:37+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1411.4836>
	{'abstract': u'Content fingerprinting and digital watermarking are techniques that are used for content protection and distribution monitoring. Over the past few years, both techniques have been well studied and their shortcomings understood. Recently, a new content fingerprinting scheme called was introduced to overcome these shortcomings. Active content fingerprinting aims to modify a content to extract robuster fingerprints than the conventional content fingerprinting. Moreover, contrary to digital watermarking, active content fingerprinting does not embed any message independent of contents thus does not face host interference. The main goal of this paper is to analyze fundamental limits of active content fingerprinting in an information theoretical framework.',
	 'authors': u'Farzad Farhadzadeh, Frans M.J. Willems, Sviatoslav Voloshinovskiy,',
	 'category': u'Computer Science ',
	 'date': '2014-11-18',
	 'pdflink': u'http://arxiv.org/pdf/1411.4836',
	 'subjects': u'Information Theory (cs.IT)',
	 'title': u'\nInformation Theoretical Analysis of Identification based on Active  Content Fingerprinting',
	 'urllink': u'http://arxiv.org/abs/1411.4836'}
2015-04-10 18:52:03+0000 [xxu46_10] INFO: Crawled 907 pages (at 1 pages/min), scraped 900 items (at 1 items/min)
2015-04-10 18:52:29+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1411.4825> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 18:52:29+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1411.4825>
	{'abstract': u'This paper briefly characterizes the field of cognitive computing. As an exemplification, the field of natural language question answering is introduced together with its specific challenges. A possibility to master these challenges is illustrated by a detailed presentation of the LogAnswer system, which is a successful representative of the field of natural language question answering.',
	 'authors': u'Ulrich Furbach, Claudia Schon, Frieder Stolzenburg,',
	 'category': u'Computer Science ',
	 'date': '2014-11-18',
	 'pdflink': u'http://arxiv.org/pdf/1411.4825',
	 'subjects': u'Artificial Intelligence (cs.AI)',
	 'title': u'\nCognitive Systems and Question Answering',
	 'urllink': u'http://arxiv.org/abs/1411.4825'}
2015-04-10 18:53:03+0000 [xxu46_10] INFO: Crawled 908 pages (at 1 pages/min), scraped 901 items (at 1 items/min)
2015-04-10 18:53:39+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1411.4823> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 18:53:39+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1411.4823>
	{'abstract': u'Deontic logic is a very well researched branch of mathematical logic and philosophy. Various kinds of deontic logics are discussed for different application domains like argumentation theory, legal reasoning, and acts in multi-agent systems. In this paper, we show how standard deontic logic can be stepwise transformed into description logic and DL- clauses, such that it can be processed by Hyper, a high performance theorem prover which uses a hypertableau calculus. Two use cases, one from multi-agent research and one from the development of normative system are investigated.',
	 'authors': u'Ulrich Furbach, Claudia Schon, Frieder Stolzenburg,',
	 'category': u'Computer Science ',
	 'date': '2014-11-18',
	 'pdflink': u'http://arxiv.org/pdf/1411.4823',
	 'subjects': u'Artificial Intelligence (cs.AI)',
	 'title': u'\nAutomated Reasoning in Deontic Logic',
	 'urllink': u'http://arxiv.org/abs/1411.4823'}
2015-04-10 18:54:03+0000 [xxu46_10] INFO: Crawled 909 pages (at 1 pages/min), scraped 902 items (at 1 items/min)
2015-04-10 18:54:45+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1411.4819> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 18:54:45+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1411.4819>
	{'abstract': u'A fundamental theorem in graph theory states that any 3-connected graph contains a subdivision of . As a generalization, we ask for the minimum number of -subdivisions that are contained in every -connected graph on vertices. We prove that there are such -subdivisions and show that the order of this bound is tight for infinitely many graphs. We further prove that the computational complexity of the problem of counting the exact number of -subdivisions is -hard.',
	 'authors': u'Tillmann Miltzow, Jens M. Schmidt, Mingji Xia,',
	 'category': u'Computer Science ',
	 'date': '2014-11-18',
	 'pdflink': u'http://arxiv.org/pdf/1411.4819',
	 'subjects': u'Discrete Mathematics (cs.DM)',
	 'title': u'\nCounting K_4-Subdivisions',
	 'urllink': u'http://arxiv.org/abs/1411.4819'}
2015-04-10 18:55:03+0000 [xxu46_10] INFO: Crawled 910 pages (at 1 pages/min), scraped 903 items (at 1 items/min)
2015-04-10 18:55:56+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1411.4813> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 18:55:56+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1411.4813>
	{'abstract': u"Since fully homomorphic encryption and homomorphically encrypted computing preserve algebraic identities such as 2*2=2+2, a natural question is whether this extremely utilitarian feature also sets up cryptographic attacks that use the encrypted arithmetic operators to generate or identify the encryptions of known constants. In particular, software or hardware might use encrypted addition and multiplication to do encrypted division and deliver the encryption of x/x=1. That can then be used to generate 1+1=2, etc, until a complete codebook is obtained. This paper shows that there is no formula or computation using 32-bit multiplication x*y and three-input addition x+y+z that yields a known constant from unknown inputs. We characterise what operations are similarly `safe' alone or in company, and show that 32-bit division is not safe in this sense, but there are trivial modifications that make it so.",
	 'authors': u'Peter T. Breuer, Jonathan P. Bowen,',
	 'category': u'Computer Science ',
	 'date': '2014-11-18',
	 'pdflink': u'http://arxiv.org/pdf/1411.4813',
	 'subjects': u'Cryptography and Security (cs.CR)',
	 'title': u'\nOn the Security of Fully Homomorphic Encryption and Encrypted Computing:  Is Division safe?',
	 'urllink': u'http://arxiv.org/abs/1411.4813'}
2015-04-10 18:56:03+0000 [xxu46_10] INFO: Crawled 911 pages (at 1 pages/min), scraped 904 items (at 1 items/min)
2015-04-10 18:57:03+0000 [xxu46_10] INFO: Crawled 911 pages (at 0 pages/min), scraped 904 items (at 0 items/min)
2015-04-10 18:57:05+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1411.4798> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 18:57:05+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1411.4798>
	{'abstract': u'Memcomputing is a novel non-Turing paradigm of computation that uses interacting memory cells (memprocessors for short) to store and process information on the same physical platform. It was recently proved mathematically that memcomputing machines have the same computational power of non-deterministic Turing machines. Therefore they can solve NP-complete problems in polynomial time and, using the appropriate architecture, with resources that only grow polynomially with the input size. The reason for this computational power stems from three main properties inspired by the brain and shared by any universal memcomputing machine: intrinsic parallelism, functional polymorphism and information overhead, namely the capability of storing more information than the number of memory elements by using the collective state of the memprocessor network. Here, we show an experimental demonstration of an actual memcomputing architecture that solves the NP-complete version of the subset-sum problem in only one step and is composed of a number of memprocessors that scales linearly with the size of the problem. We have fabricated this architecture using standard microelectronic technology so that it can be easily realized in any laboratory setting, whether academic or industrial. Even though the particular machine presented here is eventually limited by noise, it represents the first proof-of-concept of a machine capable of working with the collective state of interacting memory cells, unlike the present-day single-state machines built using the von Neumann architecture.',
	 'authors': u'Fabio L. Traversa, Chiara Ramella, Fabrizio Bonani, Massimiliano Di Ventra,',
	 'category': u'Computer Science ',
	 'date': '2014-11-18',
	 'pdflink': u'http://arxiv.org/pdf/1411.4798',
	 'subjects': u'Emerging Technologies (cs.ET)',
	 'title': u'\nMemcomputing NP-complete problems in polynomial time using polynomial  resources and collective states',
	 'urllink': u'http://arxiv.org/abs/1411.4798'}
2015-04-10 18:58:03+0000 [xxu46_10] INFO: Crawled 912 pages (at 1 pages/min), scraped 905 items (at 1 items/min)
2015-04-10 18:58:23+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1411.4796> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 18:58:23+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1411.4796>
	{'abstract': u'We consider infinite-state Attacker-Defender games with reachability objectives. The results of the paper are twofold. Firstly we prove a new language-theoretic result for weighted automata on infinite words and show its encoding into the framework of Attacker-Defender games. Secondly we use this novel concept to prove undecidability for checking existence of a winning strategy in several low-dimensional mathematical games including vector reachability games, word games and braid games.',
	 'authors': u'Vesa Halava, Tero Harju, Reino Niskanen, Igor Potapov,',
	 'category': u'Computer Science ',
	 'date': '2014-11-18',
	 'pdflink': u'http://arxiv.org/pdf/1411.4796',
	 'subjects': u'Formal Languages and Automata Theory (cs.FL)',
	 'title': u'\nWeighted automata on infinite words in the context of Attacker-Defender  games',
	 'urllink': u'http://arxiv.org/abs/1411.4796'}
2015-04-10 18:59:03+0000 [xxu46_10] INFO: Crawled 913 pages (at 1 pages/min), scraped 906 items (at 1 items/min)
2015-04-10 18:59:42+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1411.4790> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 18:59:42+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1411.4790>
	{'abstract': u'With the tremendous advancements in technology and the Internet, data security has become a major issue around the globe. To guarantee that data is protected and does not go to an unintended endpoint, the art of data hiding (steganography) emerged. Steganography is the art of hiding information such that it is not detectable to the naked eye. Various techniques have been proposed for hiding a secret message in a carrier document. In this paper, we present a novel design that applies Reed-Solomon (RS) error correcting codes in steganographic applications. The model works by substituting the redundant RS codes with the steganographic message. The experimental results show that the proposed design is satisfactory with the percentage of decoded information 100% and percentage of decoded secret message 97. 36%. The proposed model proved that it could be applied in various steganographic applications.',
	 'authors': u'Fredrick R. Ishengoma,',
	 'category': u'Computer Science ',
	 'date': '2014-11-18',
	 'pdflink': u'http://arxiv.org/pdf/1411.4790',
	 'subjects': u'Multimedia (cs.MM)',
	 'title': u'\nThe Art of Data Hiding with Reed-Solomon Error Correcting Codes',
	 'urllink': u'http://arxiv.org/abs/1411.4790'}
2015-04-10 19:00:03+0000 [xxu46_10] INFO: Crawled 914 pages (at 1 pages/min), scraped 907 items (at 1 items/min)
2015-04-10 19:00:49+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1411.4781> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 19:00:49+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1411.4781>
	{'abstract': u'In heterogeneous cellular networks (HCNs), the interference received at a user is correlated over time slots since it comes from the same set of randomly located BSs. This results in the correlations of link successes, thus affecting network performance. Under the assumptions of a K-tier Poisson network, strongest-candidate based BS association, and independent Rayleigh fading, we first quantify the correlation coefficients of interference. We observe that the interference correlation is independent of the number of tiers, BS density, SIR threshold, and transmit power. Then, we study the correlations of link successes in terms of the joint success probability over multiple time slots. We show that the joint success probability is decided by the success probability in a single time slot and a diversity polynomial, which represents the temporal interference correlation. Moreover, the parameters of HCNs have an important influence on the joint success probability by affecting the success probability in a single time slot. Particularly, we obtain the condition under which the joint success probability increases with the BS density and transmit power. We further show that the conditional success probability given prior successes only depends on the path loss exponent and the number of time slots.',
	 'authors': u'Min Sheng, Juan Wen, Jiandong Li, Ben Liang,',
	 'category': u'Computer Science ',
	 'date': '2014-11-18',
	 'pdflink': u'http://arxiv.org/pdf/1411.4781',
	 'subjects': u'Information Theory (cs.IT)',
	 'title': u'\nCorrelations of Interference and Link Successes in Heterogeneous  Cellular Networks',
	 'urllink': u'http://arxiv.org/abs/1411.4781'}
2015-04-10 19:01:03+0000 [xxu46_10] INFO: Crawled 915 pages (at 1 pages/min), scraped 908 items (at 1 items/min)
2015-04-10 19:01:59+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1411.4762> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 19:01:59+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1411.4762>
	{'abstract': u"In this paper we study the problem of storing reliably an archive of versioned data. Specifically, we focus on systems where the differences (deltas) between subsequent versions rather than the whole objects are stored - a typical model for storing versioned data. For reliability, we propose erasure encoding techniques that exploit the sparsity of information in the deltas while storing them reliably in a distributed back-end storage system, resulting in improved I/O read performance to retrieve the whole versioned archive. Along with the basic techniques, we propose a few optimization heuristics, and evaluate the techniques' efficacy analytically and with numerical simulations.",
	 'authors': u'J. Harshan, Fr\xe9d\xe9rique Oggier, Anwitaman Datta,',
	 'category': u'Computer Science ',
	 'date': '2014-11-18',
	 'pdflink': u'http://arxiv.org/pdf/1411.4762',
	 'subjects': u'Information Theory (cs.IT)',
	 'title': u'\nSparsity Exploiting Erasure Coding for Resilient Storage and Efficient  I/O Access in Delta based Versioning Systems',
	 'urllink': u'http://arxiv.org/abs/1411.4762'}
2015-04-10 19:02:03+0000 [xxu46_10] INFO: Crawled 916 pages (at 1 pages/min), scraped 909 items (at 1 items/min)
2015-04-10 19:02:57+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1411.4759> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 19:02:57+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1411.4759>
	{'abstract': u"In this paper we develop an analytical framework, based on the Che approximation cite, for the analysis of Least Recently Used (LRU) caches operating under the Shot Noise requests Model (SNM). The SNM was recently proposed in cite to better capture the main characteristics of today Video on Demand (Vod) traffic. In this context, the Che approximation is derived as the application of a mean field principle to the cache eviction time. We investigate the validity of this approximation through an asymptotic analysis of the cache eviction time. Particularly, we provide a large deviation principle and a central limit theorem for the cache eviction time, as the cache size grows large. Furthermore, we obtain a non-asymptotic analytical upper bound on the error entailed by Che's approximation of the hit probability.",
	 'authors': u'Emilio Leonardi, Giovanni Luca Torrisi,',
	 'category': u'Computer Science ',
	 'date': '2014-11-18',
	 'pdflink': u'http://arxiv.org/pdf/1411.4759',
	 'subjects': u'Performance (cs.PF)',
	 'title': u'\nLeast Recently Used caches under the Shot Noise Model',
	 'urllink': u'http://arxiv.org/abs/1411.4759'}
2015-04-10 19:03:03+0000 [xxu46_10] INFO: Crawled 917 pages (at 1 pages/min), scraped 910 items (at 1 items/min)
2015-04-10 19:04:03+0000 [xxu46_10] INFO: Crawled 917 pages (at 0 pages/min), scraped 910 items (at 0 items/min)
2015-04-10 19:04:15+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1411.4738> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 19:04:15+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1411.4738>
	{'abstract': u'The cross-media retrieval problem has received much attention in recent years due to the rapidly increasing multimedia data on the Internet. A new approach to the problem was raised which intends to match the different modal features directly. Thus, how to get rid of the heterogeneity between the different modalities and match the different modal features with different dimensions become the key points in this research. On the other hand, the metric learning shows great power to learn a distance metric to explore the relationship between data points. However, the traditional metric learning algorithms are only focusing on one modality features, which suffers difficulties to handle the heterogeneous features with different dimensions. Thus, in this paper, we proposed a heterogeneous similarity learning algorithm based on the metric learning for the cross-modal feature matching. With the nuclear penalization, an accelerated proximal gradient algorithm is successfully imported to find the optimal solution with the fast convergence rate of O(1/t^2). We applied it to the image-text cross-media retrieval problem, and compared it with several popular and the state-of-the-art algorithms. Experiments on two well known databases show that the proposed method achieves the best performance compared to the state-of-the-art algorithms.',
	 'authors': u'Cuicui Kang, Shengcai Liao, Yonghao He, Jian Wang, Shiming Xiang, Chunhong Pan,',
	 'category': u'Computer Science ',
	 'date': '2014-11-18',
	 'pdflink': u'http://arxiv.org/pdf/1411.4738',
	 'subjects': u'Multimedia (cs.MM)',
	 'title': u'\nCross-Modal Similarity Learning : A Low Rank Bilinear Formulation',
	 'urllink': u'http://arxiv.org/abs/1411.4738'}
2015-04-10 19:05:03+0000 [xxu46_10] INFO: Crawled 918 pages (at 1 pages/min), scraped 911 items (at 1 items/min)
2015-04-10 19:05:23+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1411.4734> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 19:05:23+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1411.4734>
	{'abstract': u'In this paper we address three different computer vision tasks using a single basic architecture: depth prediction, surface normal estimation, and semantic labeling. We use a multiscale convolutional network that is able to adapt easily to each task using only small modifications, regressing from the input image to the output map directly. Our method progressively refines predictions using a sequence of scales, and captures many image details without any superpixels or low-level segmentation. We achieve state-of-the-art performance on benchmarks for all three tasks.',
	 'authors': u'David Eigen, Rob Fergus,',
	 'category': u'Computer Science ',
	 'date': '2014-11-18',
	 'pdflink': u'http://arxiv.org/pdf/1411.4734',
	 'subjects': u'Computer Vision and Pattern Recognition (cs.CV)',
	 'title': u'\nPredicting Depth, Surface Normals and Semantic Labels with a Common  Multi-Scale Convolutional Architecture',
	 'urllink': u'http://arxiv.org/abs/1411.4734'}
2015-04-10 19:06:03+0000 [xxu46_10] INFO: Crawled 919 pages (at 1 pages/min), scraped 912 items (at 1 items/min)
2015-04-10 19:06:32+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1411.4733> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 19:06:32+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1411.4733>
	{'abstract': u'OpenFlow is one of the most commonly used protocols for communication between the controller and the forwarding element in a software defined network (SDN). A model based on M/M/1 queues is proposed in [1] to capture the communication between the forwarding element and the controller. Albeit the model provides useful insight, it is accurate only for the case when the probability of expecting a new flow is small. Secondly, it is not straight forward to extend the model in [1] to more than one forwarding element in the data plane. In this work we propose a model which addresses both these challenges. The model is based on Jackson assumption but with corrections tailored to the OpenFlow based SDN network. Performance analysis using the proposed model indicates that the model is accurate even for the case when the probability of new flow is quite large. Further we show by a toy example that the model can be extended to more than one node in the data plane.',
	 'authors': u'Kashif Mahmood, Ameen Chilwan, Olav N. \xd8sterb\xf8, Michael Jarschel,',
	 'category': u'Computer Science ',
	 'date': '2014-11-18',
	 'pdflink': u'http://arxiv.org/pdf/1411.4733',
	 'subjects': u'Performance (cs.PF)',
	 'title': u'\nOn The Modeling of OpenFlow-based SDNs: The Single Node Case',
	 'urllink': u'http://arxiv.org/abs/1411.4733'}
2015-04-10 19:07:03+0000 [xxu46_10] INFO: Crawled 920 pages (at 1 pages/min), scraped 913 items (at 1 items/min)
2015-04-10 19:07:10+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1411.4732> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 19:07:10+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1411.4732>
	{'abstract': u'This paper considers the problem of defining a measure of redundant information that quantifies how much common information two or more random variables specify about a target random variable. We discussed desired properties of such a measure, and propose new measures with some desirable properties.',
	 'authors': u'Virgil Griffith, Tracey Ho,',
	 'category': u'Computer Science ',
	 'date': '2014-11-18',
	 'pdflink': u'http://arxiv.org/pdf/1411.4732',
	 'subjects': u'Information Theory (cs.IT)',
	 'title': u'\nQuantifying Redundant Information in Predicting a Target Random Variable',
	 'urllink': u'http://arxiv.org/abs/1411.4732'}
2015-04-10 19:08:03+0000 [xxu46_10] INFO: Crawled 921 pages (at 1 pages/min), scraped 914 items (at 1 items/min)
2015-04-10 19:08:12+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1411.4726> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 19:08:12+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1411.4726>
	{'abstract': u'Despite the advent of wearable devices and the proliferation of smartphones, there still is no ideal platform that can continuously sense and precisely collect all available contextual information. Ideally, mobile sensing data collection approaches should deal with uncertainty and data loss originating from software and hardware restrictions. We have conducted life logging data collection experiments from 35 users and created a rich dataset (9.26 million records) to represent the real-world deployment issues of mobile sensing systems. We create a novel set of algorithms to identify human behavioral motifs while considering the uncertainty of collected data objects. Our work benefits from combinations of sensors available on a device and identifies behavioral patterns with a temporal granularity similar to human time perception. Employing a combination of sensors rather than focusing on only one sensor can handle uncertainty by neglecting sensor data that is not available and focusing instead on available data. Moreover, by experimenting on two real, large datasets, we demonstrate that using a sliding window significantly improves the scalability of our algorithms, which can be used by applications for small devices, such as smartphones and wearables.',
	 'authors': u'Reza Rawassizadeh, Elaheh Momeni, Prajna Shetty,',
	 'category': u'Computer Science ',
	 'date': '2014-11-18',
	 'pdflink': u'http://arxiv.org/pdf/1411.4726',
	 'subjects': u'Human-Computer Interaction (cs.HC)',
	 'title': u'\nScalable Mining of Daily Behavioral Patterns in Context Sensing Life-Log  Data',
	 'urllink': u'http://arxiv.org/abs/1411.4726'}
2015-04-10 19:08:44+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1411.4701> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 19:08:44+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1411.4701>
	{'abstract': u'We propose a vision-based highway border detection algorithm using structured Hough voting. Our approach takes advantage of the geometric relationship between highway road borders and highway lane markings. It uses a strategy where a number of trained road border and lane marking detectors are triggered, followed by Hough voting to generate corresponding detection of the border and lane marking. Since the initially triggered detectors usually result in large number of positives, conventional frame-wise Hough voting is not able to always generate robust border and lane marking results. Therefore, we formulate this problem as a joint detection-and-tracking problem under the structured Hough voting model, where tracking refers to exploiting inter-frame structural information to stabilize the detection results. Both qualitative and quantitative evaluations show the superiority of the proposed structured Hough voting model over a number of baseline methods.',
	 'authors': u'Zhiding Yu, Wende Zhang, B. V. K. Vijaya Kumar, Dan Levi,',
	 'category': u'Computer Science ',
	 'date': '2014-11-18',
	 'pdflink': u'http://arxiv.org/pdf/1411.4701',
	 'subjects': u'Computer Vision and Pattern Recognition (cs.CV)',
	 'title': u'\nStructured Hough Voting for Vision-based Highway Border Detection',
	 'urllink': u'http://arxiv.org/abs/1411.4701'}
2015-04-10 19:09:03+0000 [xxu46_10] INFO: Crawled 923 pages (at 2 pages/min), scraped 916 items (at 2 items/min)
2015-04-10 19:10:02+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1411.4696> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 19:10:02+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1411.4696>
	{'abstract': u'Aggregate signatures allow anyone to combine different signatures signed by different signers on different messages into a single short signature. An ideal aggregate signature scheme is an identity-based aggregate signature (IBAS) scheme that supports full aggregation since it can reduce the total transmitted data by using an identity string as a public key and anyone can freely aggregate different signatures. Constructing a secure IBAS scheme that supports full aggregation in bilinear maps is an important open problem. Recently, Yuan proposed an IBAS scheme with full aggregation in bilinear maps and claimed its security in the random oracle model under the computational Diffie-Hellman assumption. In this paper, we show that there exists an efficient forgery attacker on their IBAS scheme and their security proof has a serious flaw.',
	 'authors': u'Kwangsu Lee, Dong Hoon Lee,',
	 'category': u'Computer Science ',
	 'date': '2014-11-18',
	 'pdflink': u'http://arxiv.org/pdf/1411.4696',
	 'subjects': u'Cryptography and Security (cs.CR)',
	 'title': u'\nSecurity Analysis of the Unrestricted Identity-Based Aggregate Signature  Scheme',
	 'urllink': u'http://arxiv.org/abs/1411.4696'}
2015-04-10 19:10:03+0000 [xxu46_10] INFO: Crawled 924 pages (at 1 pages/min), scraped 917 items (at 1 items/min)
2015-04-10 19:11:03+0000 [xxu46_10] INFO: Crawled 924 pages (at 0 pages/min), scraped 917 items (at 0 items/min)
2015-04-10 19:11:18+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1411.4695> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 19:11:18+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1411.4695>
	{'abstract': u'The problem of optimal switching between nonlinear autonomous subsystems is investigated in this study where the objective is not only bringing the states to close to the desired point, but also adjusting the switching pattern, in the sense of penalizing switching occurrences and assigning different preferences to utilization of different modes. The mode sequence is unspecified and a switching cost term is used in the cost function for penalizing each switching. It is shown that once a switching cost is incorporated, the optimal cost-to-go function depends on the already active subsystem, i.e., the subsystem which was engaged in the previous time step. Afterwards, an approximate dynamic programming based method is developed which provides an approximation of the optimal solution to the problem in a feedback form and for different initial conditions. Finally, the performance of the method is analyzed through numerical examples.',
	 'authors': u'Ali Heydari,',
	 'category': u'Computer Science ',
	 'date': '2014-11-17',
	 'pdflink': u'http://arxiv.org/pdf/1411.4695',
	 'subjects': u'Systems and Control (cs.SY)',
	 'title': u'\nFeedback Solution to Optimal Switching Problems with Switching Cost',
	 'urllink': u'http://arxiv.org/abs/1411.4695'}
2015-04-10 19:12:03+0000 [xxu46_10] INFO: Crawled 925 pages (at 1 pages/min), scraped 918 items (at 1 items/min)
2015-04-10 19:12:24+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1411.4692> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 19:12:24+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1411.4692>
	{'abstract': u'A function is triangle-free if there are no satisfying and . In testing triangle-freeness, the goal is to distinguish with high probability triangle-free functions from those that are -far from being triangle-free. It was shown by Green that the query complexity of the canonical tester for the problem is upper bounded by a function that depends only on (GAFA, 2005), however the best known upper bound is a tower type function of . The best known lower bound on the query complexity of the canonical tester is (Fu and Kleinberg, RANDOM, 2014). In this work we introduce a new approach to proving lower bounds on the query complexity of triangle-freeness. We relate the problem to combinatorial questions on collections of vectors in and to sunflower conjectures studied by Alon, Shpilka, and Umans (Comput. Complex., 2013). The relations yield that a refutation of the Weak Sunflower Conjecture over implies a super-polynomial lower bound on the query complexity of the canonical tester for triangle-freeness. Our results are extended to testing -cycle-freeness of functions with domain for every and a prime . In addition, we generalize the lower bound of Fu and Kleinberg to -cycle-freeness for by generalizing the construction of uniquely solvable puzzles due to Coppersmith and Winograd (J. Symbolic Comput., 1990).',
	 'authors': u'Ishay Haviv, Ning Xie,',
	 'category': u'Computer Science ',
	 'date': '2014-11-17',
	 'pdflink': u'http://arxiv.org/pdf/1411.4692',
	 'subjects': u'Data Structures and Algorithms (cs.DS)',
	 'title': u'\nSunflowers and Testing Triangle-Freeness of Functions',
	 'urllink': u'http://arxiv.org/abs/1411.4692'}
2015-04-10 19:13:03+0000 [xxu46_10] INFO: Crawled 926 pages (at 1 pages/min), scraped 919 items (at 1 items/min)
2015-04-10 19:13:37+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1411.4679> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 19:13:37+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1411.4679>
	{'abstract': u'This paper presents the building heating demand prediction model with occupancy profile and operational heating power level characteristics in short time horizon (a couple of days) using artificial neural network. In addition, novel pseudo dynamic transitional model is introduced, which consider time dependent attributes of operational power level characteristics and its effect in the overall model performance is outlined. Pseudo dynamic model is applied to a case study of French Institution building and compared its results with static and other pseudo dynamic neural network models. The results show the coefficients of correlation in static and pseudo dynamic neural network model of 0.82 and 0.89 (with energy consumption error of 0.02%) during the learning phase, and 0.61 and 0.85 during the prediction phase respectively. Further, orthogonal array design is applied to the pseudo dynamic model to check the schedule of occupancy profile and operational heating power level characteristics. The results show the new schedule and provide the robust design for pseudo dynamic model. Due to prediction in short time horizon, it finds application for Energy Services Company (ESCOs) to manage the heating load for dynamic control of heat production system.',
	 'authors': u'S. Paudel, M. Elmtiri, W.L. Kling, O. Le Corre, B. Lacarriere,',
	 'category': u'Computer Science ',
	 'date': '2014-11-17',
	 'pdflink': u'http://arxiv.org/pdf/1411.4679',
	 'subjects': u'Computational Engineering, Finance, and Science (cs.CE)',
	 'title': u'\nPseudo Dynamic Transitional Modeling of Building Heating Energy Demand  Using Artificial Neural Network',
	 'urllink': u'http://arxiv.org/abs/1411.4679'}
2015-04-10 19:14:03+0000 [xxu46_10] INFO: Crawled 927 pages (at 1 pages/min), scraped 920 items (at 1 items/min)
2015-04-10 19:14:23+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1411.4670> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 19:14:23+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1411.4670>
	{'abstract': u'In this paper, we introduce the first phase of a new dataset for offline Arabic handwriting recognition. The aim is to collect a very large dataset of isolated Arabic words that covers all letters of the alphabet in all possible shapes using a small number of simple words. The end goal is to collect a very large dataset of segmented letter images, which can be used to build and evaluate Arabic handwriting recognition systems that are based on segmented letter recognition. The current version of the dataset contains samples of unique Arabic words that cover all possible shapes of all alphabet letters. The samples were collected from writers. In its current form, the dataset can be used for the problem of closed-vocabulary word recognition. We evaluated a number of window-based descriptors and classifiers on this task and obtained an accuracy of using a SIFT-based descriptor and ANN.',
	 'authors': u'Mohamed E. Hussein, Marwan Torki, Ahmed Elsallamy, Mahmoud Fayyaz,',
	 'category': u'Computer Science ',
	 'date': '2014-11-17',
	 'pdflink': u'http://arxiv.org/pdf/1411.4670',
	 'subjects': u'Computer Vision and Pattern Recognition (cs.CV)',
	 'title': u'\nAlexU-Word: A New Dataset for Isolated-Word Closed-Vocabulary Offline  Arabic Handwriting Recognition',
	 'urllink': u'http://arxiv.org/abs/1411.4670'}
2015-04-10 19:15:03+0000 [xxu46_10] INFO: Crawled 928 pages (at 1 pages/min), scraped 921 items (at 1 items/min)
2015-04-10 19:15:37+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1411.4630> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 19:15:38+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1411.4630>
	{'abstract': u'The security evaluation for Mail Distribution Systems focuses on certification and reliability of sensitive data between mail servers. The need to certify the information conveyed is a result of known weaknesses in the simple mail transfer protocol (SMTP). The most important consequence of these weaknesses is the possibility to mislead the recipient, which is achieved via spam (especially email spoofing). Email spoofing refers to alterations in the headers and/or the content of the message. Therefore, the authenticity of the message is compromised. Unfortunately, the broken link between certification and reliability of the information is unsolicited email (spam). Unlike the current practice of estimating the cost of spam, which prompts organizations to purchase and maintain appropriate anti-spam software, our approach offers an alternative perspective of the economic and moral consequences of unsolicited mail. The financial data provided in this paper show that spam is a major contributor to the financial and production cost of an organization, necessitating further attention. Additionally, this paper highlights the importance and severity of the weaknesses of the SMTP protocol, which can be exploited even with the use of simple applications incorporated within most commonly used Operating Systems (e.g. Telnet). As a consequence of these drawbacks Mail Distribution Systems need to be appropriate configured so as to provide the necessary security services to the users.',
	 'authors': u'Antonis S. Rizopoulos, Dimitrios N. Kallergis, George N. Prezerakos,',
	 'category': u'Computer Science ',
	 'date': '2014-10-18',
	 'pdflink': u'http://arxiv.org/pdf/1411.4630',
	 'subjects': u'Cryptography and Security (cs.CR)',
	 'title': u'\nSecurity Evaluation for Mail Distribution Systems',
	 'urllink': u'http://arxiv.org/abs/1411.4630'}
2015-04-10 19:16:03+0000 [xxu46_10] INFO: Crawled 929 pages (at 1 pages/min), scraped 922 items (at 1 items/min)
2015-04-10 19:16:49+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1411.4619> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 19:16:49+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1411.4619>
	{'abstract': u'We investigate the potential of using ordinal peer grading for the evaluation of students in massive online open courses (MOOCs). According to such grading schemes, each student receives a few assignments (by other students) which she has to rank. Then, a global ranking (possibly translated into numerical scores) is produced by combining the individual ones. This is a novel application area for social choice concepts and methods where the important problem to be solved is as follows: how should the assignments be distributed so that the collected individual rankings can be easily merged into a global one that is as close as possible to the ranking that represents the relative performance of the students in the assignment? Our main theoretical result suggests that using very simple ways to distribute the assignments so that each student has to rank only of them, a Borda-like aggregation method can recover a fraction of the true ranking when each student correctly ranks the assignments she receives. Experimental results strengthen our analysis further and also demonstrate that the same method is extremely robust even when students have imperfect capabilities as graders. We believe that our results provide strong evidence that ordinal peer grading can be a highly effective and scalable solution for evaluation in MOOCs.',
	 'authors': u'Ioannis Caragiannis, George A. Krimpas, Alexandros A. Voudouris,',
	 'category': u'Computer Science ',
	 'date': '2014-11-17',
	 'pdflink': u'http://arxiv.org/pdf/1411.4619',
	 'subjects': u'Artificial Intelligence (cs.AI)',
	 'title': u'\nAggregating partial rankings with applications to peer grading in  massive online open courses',
	 'urllink': u'http://arxiv.org/abs/1411.4619'}
2015-04-10 19:17:03+0000 [xxu46_10] INFO: Crawled 930 pages (at 1 pages/min), scraped 923 items (at 1 items/min)
2015-04-10 19:18:02+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1411.4618> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 19:18:02+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1411.4618>
	{'abstract': u'We explore the idea of using a "possibilistic graphical model" as the basis for a world model that drives a dialog system. As a first step we have developed a system that uses text-based dialog to derive a model of the user\'s family relations. The system leverages its world model to infer relational triples, to learn to recover from upstream coreference resolution errors and ambiguities, and to learn context-dependent paraphrase models. We also explore some theoretical aspects of the underlying graphical model.',
	 'authors': u'Christopher J.C. Burges, Erin Renshaw, Andrzej Pastusiak,',
	 'category': u'Computer Science ',
	 'date': '2014-11-17',
	 'pdflink': u'http://arxiv.org/pdf/1411.4618',
	 'subjects': u'Computation and Language (cs.CL)',
	 'title': u'\nRelations World: A Possibilistic Graphical Model',
	 'urllink': u'http://arxiv.org/abs/1411.4618'}
2015-04-10 19:18:03+0000 [xxu46_10] INFO: Crawled 931 pages (at 1 pages/min), scraped 924 items (at 1 items/min)
2015-04-10 19:19:01+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1411.4617> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 19:19:01+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1411.4617>
	{'abstract': u'Write-Once-Memory (WOM) is a model for many modern non-volatile memories, such as flash memories. Recently, several capacity-achieving WOM coding schemes have been proposed based on polar coding. Due to the fact that practical computer memory systems always contain noises, a nature question to ask next is how may we generalize these coding schemes, such that they may also have the error-control capabilities. In this paper, we discuss a joint WOM and error-control coding scheme, which is a generalization of the capacity-achieving WOM codes based on source polarization. In this paper, we prove a sufficient and necessary condition for the noisy reading channel being less noisy than the test channel in data encoding in the polar WOM coding. Such a sufficient and necessary condition is usually satisfied in reality. As a consequence of the sufficient and necessary condition, the high entropy set related to the noisy channel is usually strictly contained in the high entropy set related to the test channel in data encoding. Therefore the low-complexity polar joint WOM and error-control codes are sufficient for most practical coding scenarios.',
	 'authors': u'Xudong Ma,',
	 'category': u'Computer Science ',
	 'date': '2014-11-17',
	 'pdflink': u'http://arxiv.org/pdf/1411.4617',
	 'subjects': u'Information Theory (cs.IT)',
	 'title': u'\nJoint Write-Once-Memory and Error-Control Codes',
	 'urllink': u'http://arxiv.org/abs/1411.4617'}
2015-04-10 19:19:03+0000 [xxu46_10] INFO: Crawled 932 pages (at 1 pages/min), scraped 925 items (at 1 items/min)
2015-04-10 19:20:03+0000 [xxu46_10] INFO: Crawled 932 pages (at 0 pages/min), scraped 925 items (at 0 items/min)
2015-04-10 19:20:05+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1411.4616> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 19:20:05+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1411.4616>
	{'abstract': u'This paper is aimed at providing a very first, more "global", systematic point of view with respect to possible conflict generation in CA-EN-like causal structures. For simplicity, only the outermost level of graphs is taken into account. Localization of the "conflict area", diagnostic preferences, and bases for systematic conflict generation are considered. A notion of () constituting a basic tool for identification of possible conflicts is proposed and its use is discussed.',
	 'authors': u'Antoni Lig\u0119za,',
	 'category': u'Computer Science ',
	 'date': '2014-11-17',
	 'pdflink': u'http://arxiv.org/pdf/1411.4616',
	 'subjects': u'Artificial Intelligence (cs.AI)',
	 'title': u'\nA Note on Systematic Conflict Generation in CA-EN-type Causal Structures',
	 'urllink': u'http://arxiv.org/abs/1411.4616'}
2015-04-10 19:20:59+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1411.4614> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 19:20:59+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1411.4614>
	{'abstract': u"A graphical language addresses the need to communicate medical information in a synthetic way. Medical concepts are expressed by icons conveying fast visual information about patients' current state or about the known effects of drugs. In order to increase the visual language's acceptance and usability, a natural language generation interface is currently developed. In this context, this paper describes the use of an informatics method ---graph transformation--- to prepare data consisting of concepts in an OWL-DL ontology for use in a natural language generation component. The OWL concept may be considered as a star-shaped graph with a central node. The method transforms it into a graph representing the deep semantic structure of a natural language phrase. This work may be of future use in other contexts where ontology concepts have to be mapped to half-formalized natural language expressions.",
	 'authors': u'Pascal Vaillant, Jean-Baptiste Lamy,',
	 'category': u'Computer Science ',
	 'date': '2014-9-26',
	 'pdflink': u'http://arxiv.org/pdf/1411.4614',
	 'subjects': u'Computation and Language (cs.CL)',
	 'title': u'\nUsing graph transformation algorithms to generate natural language  equivalents of icons expressing medical concepts',
	 'urllink': u'http://arxiv.org/abs/1411.4614'}
2015-04-10 19:21:03+0000 [xxu46_10] INFO: Crawled 934 pages (at 2 pages/min), scraped 927 items (at 2 items/min)
2015-04-10 19:22:03+0000 [xxu46_10] INFO: Crawled 934 pages (at 0 pages/min), scraped 927 items (at 0 items/min)
2015-04-10 19:22:13+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1411.4613> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 19:22:13+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1411.4613>
	{'abstract': u'We show that the integrality gap of the natural LP relaxation of the Asymmetric Traveling Salesman Problem is . In other words, there is a polynomial time algorithm that approximates the of the optimum tour within a factor of . We prove this by showing that any -edge-connected unweighted graph with has a -thin spanning tree. Our main new ingredient is a procedure, albeit an exponentially sized convex program, that "transforms" graphs that do not admit any thin trees into those that provably have spectrally thin trees. More precisely, given a -edge-connected graph where , we show that there is a matrix that "preserves" the structure of all cuts of such that for a set that induces an -connected graph, the effective resistance of every edge in w.r.t. is at most . Then, we use a recent extension of the seminal work of Marcus, Spielman and Srivastava [MSS13] by the authors [AO14b] to prove the existence of an -spectrally thin tree with respect to . Such a tree is -combinatorially thin with respect to as preserves the structure of cuts of .',
	 'authors': u'Nima Anari, Shayan Oveis Gharan,',
	 'category': u'Computer Science ',
	 'date': '2014-11-17',
	 'pdflink': u'http://arxiv.org/pdf/1411.4613',
	 'subjects': u'Data Structures and Algorithms (cs.DS)',
	 'title': u'\nEffective-Resistance-Reducing Flows, Spectrally Thin Trees, and  Asymmetric TSP',
	 'urllink': u'http://arxiv.org/abs/1411.4613'}
2015-04-10 19:23:03+0000 [xxu46_10] INFO: Crawled 935 pages (at 1 pages/min), scraped 928 items (at 1 items/min)
2015-04-10 19:23:31+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1411.4604> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 19:23:31+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1411.4604>
	{'abstract': u'Synthesis of program parts is very useful for concurrent systems. However, most synthesis approaches do not support common design tasks, like modifying a single process without having to re-synthesize or verify the whole system. Assume-guarantee synthesis (AGS) provides robustness against modifications of system parts, but thus far has been limited to the perfect information setting. This means that local variables cannot be hidden from other processes, which renders synthesis results cumbersome or even impossible to realize. We resolve this shortcoming by defining AGS in a partial information setting. We analyze the complexity and decidability in different settings, showing that the problem has a high worst-case complexity and is undecidable in many interesting cases. Based on these observations, we present a pragmatic algorithm based on bounded synthesis, and demonstrate its practical applicability on several examples.',
	 'authors': u'Roderick Bloem, Krishnendu Chatterjee, Swen Jacobs, Robert Koenighofer,',
	 'category': u'Computer Science ',
	 'date': '2014-11-17',
	 'pdflink': u'http://arxiv.org/pdf/1411.4604',
	 'subjects': u'Logic in Computer Science (cs.LO)',
	 'title': u'\nAssume-Guarantee Synthesis for Concurrent Reactive Programs with Partial  Information',
	 'urllink': u'http://arxiv.org/abs/1411.4604'}
2015-04-10 19:24:03+0000 [xxu46_10] INFO: Crawled 936 pages (at 1 pages/min), scraped 929 items (at 1 items/min)
2015-04-10 19:24:30+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1411.4597> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 19:24:30+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1411.4597>
	{'abstract': u'The several algebraic approaches to graph transformation proposed in the literature all ensure that if an item is preserved by a rule, so are its connections with the context graph where it is embedded. But there are applications in which it is desirable, for example when cloning an item, to specify different embeddings for the original and for the copy. Therefore we propose a conservative extension of these approaches where a rule can specify how the embedding of a preserved item should be changed, typically by removing certain connections.',
	 'authors': u'Anadrea Corradini, Dominique Duval, Rachid Echahed, Fr\xe9d\xe9ric Prost, Leila Ribeiro,',
	 'category': u'Computer Science ',
	 'date': '2014-11-14',
	 'pdflink': u'http://arxiv.org/pdf/1411.4597',
	 'subjects': u'Logic in Computer Science (cs.LO)',
	 'title': u'\nAGREE -- Algebraic Graph Rewriting with Controlled Embedding',
	 'urllink': u'http://arxiv.org/abs/1411.4597'}
2015-04-10 19:25:03+0000 [xxu46_10] INFO: Crawled 937 pages (at 1 pages/min), scraped 930 items (at 1 items/min)
2015-04-10 19:25:44+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1411.4591> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 19:25:44+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1411.4591>
	{'abstract': u'This paper proves that a family of number field lattice codes simultaneously achieves a constant gap to capacity in Rayleigh fast fading and Gaussian channels. The key property in the proof is the existence of infinite towers of Hilbert class fields with bounded root discriminant. The gap to capacity of the proposed families is determined by the root discriminant. The comparison between the Gaussian and fading case reveals that in Rayleigh fading channels the normalized minimum product distance plays an analogous role to the Hermite invariant in Gaussian channels.',
	 'authors': u'Roope Vehkalahti, Laura Luzzi,',
	 'category': u'Computer Science ',
	 'date': '2014-11-17',
	 'pdflink': u'http://arxiv.org/pdf/1411.4591',
	 'subjects': u'Information Theory (cs.IT)',
	 'title': u'\nNumber field lattices achieve Gaussian and Rayleigh channel capacity  within a constant gap',
	 'urllink': u'http://arxiv.org/abs/1411.4591'}
2015-04-10 19:26:03+0000 [xxu46_10] INFO: Crawled 938 pages (at 1 pages/min), scraped 931 items (at 1 items/min)
2015-04-10 19:26:48+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1411.4590> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 19:26:48+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1411.4590>
	{'abstract': u'This paper studies the parameters for which Reed-Muller (RM) codes over can correct random erasures and random errors with high probability, and in particular when can they achieve capacity for these two classical channels. Necessarily, the paper also studies properties of evaluations of multi-variate polynomials on random sets of inputs. For erasures, we prove that RM codes achieve capacity both for very high rate and very low rate regimes. For errors, we prove that RM codes achieve capacity for very low rate regimes, and for very high rates, we show that they can uniquely decode at about square root of the number of errors at capacity. The proofs of these four results are based on different techniques, which we find interesting in their own right. In particular, we study the following questions about , the matrix whose rows are truth tables of all monomials of degree in variables. What is the most (resp. least) number of random columns in that define a submatrix having full column rank (resp. full row rank) with high probability? We obtain tight bounds for very small (resp. very large) degrees , which we use to show that RM codes achieve capacity for erasures in these regimes. Our decoding from random errors follows from the following novel reduction. For every linear code of sufficiently high rate we construct a new code , also of very high rate, such that for every subset of coordinates, if can recover from erasures in , then can recover from errors in . Specializing this to RM codes and using our results for erasures imply our result on unique decoding of RM codes at high rate. Finally, two of our capacity achieving results require tight bounds on the weight distribution of RM codes. We obtain such bounds extending the recent cite bounds from constant degree to linear degree polynomials.',
	 'authors': u'Emmanuel Abbe, Amir Shpilka, Avi Wigderson,',
	 'category': u'Computer Science ',
	 'date': '2014-11-17',
	 'pdflink': u'http://arxiv.org/pdf/1411.4590',
	 'subjects': u'Information Theory (cs.IT)',
	 'title': u'\nReed-Muller codes for random erasures and errors',
	 'urllink': u'http://arxiv.org/abs/1411.4590'}
2015-04-10 19:27:03+0000 [xxu46_10] INFO: Crawled 939 pages (at 1 pages/min), scraped 932 items (at 1 items/min)
2015-04-10 19:27:50+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1411.4586> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 19:27:50+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1411.4586>
	{'abstract': u"This paper is concerned with the optimal approximation of a given multivariate Dirac mixture, i.e., a density comprising weighted Dirac distributions on a continuous domain, by an equally weighted Dirac mixture with a reduced number of components. The parameters of the approximating density are calculated by minimizing a smooth global distance measure, a generalization of the well-known Cram 'r-von Mises Distance to the multivariate case. This generalization is achieved by defining an alternative to the classical cumulative distribution, the Localized Cumulative Distribution (LCD), as a characterization of discrete random quantities (on continuous domains), which is unique and symmetric also in the multivariate case. The resulting approximation method provides the basis for various efficient nonlinear state and parameter estimation methods.",
	 'authors': u'Uwe D. Hanebeck,',
	 'category': u'Computer Science ',
	 'date': '2014-11-17',
	 'pdflink': u'http://arxiv.org/pdf/1411.4586',
	 'subjects': u'Systems and Control (cs.SY)',
	 'title': u'\nOptimal Reduction of Multivariate Dirac Mixture Densities',
	 'urllink': u'http://arxiv.org/abs/1411.4586'}
2015-04-10 19:28:03+0000 [xxu46_10] INFO: Crawled 940 pages (at 1 pages/min), scraped 933 items (at 1 items/min)
2015-04-10 19:28:41+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1411.4584> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 19:28:41+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1411.4584>
	{'abstract': u'The problem of constructing pseudorandom generators that fool halfspaces has been studied intensively in recent times. For fooling halfspaces over the hypercube with polynomially small error, the best construction known requires seed-length O(log^2 n) (MekaZ13). Getting the seed-length down to O(log(n)) is a natural challenge in its own right, which needs to be overcome in order to derandomize RL. In this work we make progress towards this goal by obtaining near-optimal generators for two important special cases: 1) We give a near optimal derandomization of the Chernoff bound for independent, uniformly random bits. Specifically, we show how to generate a x in ^n using random bits such that for any unit vector u, &lt;u,x&gt; matches the sub-Gaussian tail behaviour predicted by the Chernoff bound up to error eps. 2) We construct a generator which fools halfspaces with coefficients with error eps with a seed-length of . This includes the important special case of majorities. In both cases, the best previous results required seed-length of . Technically, our work combines new Fourier-analytic tools with the iterative dimension reduction techniques and the gradually increasing independence paradigm of previous works (KaneMN11, CelisRSW13, GopalanMRTV12).',
	 'authors': u'Parikshit Gopalan, Daniel Kane, Raghu Meka,',
	 'category': u'Computer Science ',
	 'date': '2014-11-17',
	 'pdflink': u'http://arxiv.org/pdf/1411.4584',
	 'subjects': u'Computational Complexity (cs.CC)',
	 'title': u'\nPseudorandomness for concentration bounds and signed majorities',
	 'urllink': u'http://arxiv.org/abs/1411.4584'}
2015-04-10 19:29:03+0000 [xxu46_10] INFO: Crawled 941 pages (at 1 pages/min), scraped 934 items (at 1 items/min)
2015-04-10 19:29:36+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1411.4575> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 19:29:36+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1411.4575>
	{'abstract': u'The search for linear kernels for the Dominating Set problem on classes of graphs of a topological nature has been one of the leading trends in kernelization in recent years. Following the fundamental work of Alber et al. that established a linear kernel for the problem on planar graphs, linear kernels have been given for bounded-genus graphs, apex-minor-free graphs, -minor-free graphs, and -topological-minor-free graphs. These generalizations are based on bidimensionality and powerful decomposition theorems for -minor-free graphs and -topological-minor-free graphs of Robertson and Seymour and of Grohe and Marx. In this work we investigate a new approach to kernelization for Dominating Set on sparse graph classes. The approach is based on the theory of bounded expansion and nowhere dense graph classes, developed in the recent years by Ne vet vil and Ossona de Mendez, among others. More precisely, we prove that Dominating Set admits a linear kernel on any hereditary graph class of bounded expansion and an almost linear kernel on any hereditary nowhere dense graph class. Since the class of -topological-minor-free graphs has bounded expansion, our results strongly generalize all the above mentioned works on kernelization of Dominating Set. At the same time, our algorithms are based on relatively short and self-contained combinatorial arguments, and do not depend on bidimensionality or decomposition theorems. Finally, we prove that for the closely related Connected Dominating Set problem, the existence of such kernelization algorithms is unlikely, even though the problem is known to admit a linear kernel on -topological-minor-free graphs. Thus, it seems that whereas for Dominating Set sparsity is enough to guarantee the existence of an efficient kernelization algorithm, for Connected Dominating Set stronger constraints of topological nature become necessary.',
	 'authors': u'P\xe5l Gr\xf8n\xe5s Drange, Markus S. Dregi, Fedor V. Fomin, Stephan Kreutzer, Daniel Lokshtanov, Marcin Pilipczuk, Micha\u0142 Pilipczuk, Felix Reidl, Saket Saurabh, Fernando S\xe1nchez Villaamil, Somnath Sikdar,',
	 'category': u'Computer Science ',
	 'date': '2014-11-17',
	 'pdflink': u'http://arxiv.org/pdf/1411.4575',
	 'subjects': u'Data Structures and Algorithms (cs.DS)',
	 'title': u'\nKernelization and Sparseness: the case of Dominating Set',
	 'urllink': u'http://arxiv.org/abs/1411.4575'}
2015-04-10 19:30:03+0000 [xxu46_10] INFO: Crawled 942 pages (at 1 pages/min), scraped 935 items (at 1 items/min)
2015-04-10 19:30:40+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1411.4573> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 19:30:40+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1411.4573>
	{'abstract': u'We consider various . There is a fleet of vehicles located at one or more depot nodes, and we seek a collection of routes for these vehicles that visit all nodes so as to minimize the total latency incurred, which is the sum of the client waiting times. We obtain an -approximation for the version where vehicles may be located at multiple depots and a -approximation for the version where all vehicles are located at the same depot, both of which are the first improvements on this problem in a decade. Perhaps more significantly, our algorithms exploit various LP-relaxations for minimum-latency problems. We show how to effectively leverage two classes of LPs--- and ---that are often believed to be quite powerful but have only sporadically been effectively leveraged for network-design and vehicle-routing problems. This gives the first concrete evidence of the effectiveness of LP-relaxations for this class of problems. The -approximation the multiple-depot version is obtained by rounding a near-optimal solution to an underlying configuration LP for the problem. The -approximation can be obtained both via rounding a bidirected LP for the single-depot problem or via more combinatorial means. The latter approach uses a bidirected LP to obtain the following key result that is of independent interest: for any , we can efficiently compute a rooted tree that is at least as good, with respect to the prize-collecting objective (i.e., edge cost + number of uncovered nodes) as the best collection of rooted paths. Our algorithms are versatile and extend easily to handle various extensions involving: (i) weighted sum of latencies, (ii) constraints specifying which depots may serve which nodes, (iii) node service times.',
	 'authors': u'Ian Post, Chaitanya Swamy,',
	 'category': u'Computer Science ',
	 'date': '2014-11-17',
	 'pdflink': u'http://arxiv.org/pdf/1411.4573',
	 'subjects': u'Data Structures and Algorithms (cs.DS)',
	 'title': u'\nLinear-Programming based Approximation Algorithms for Multi-Vehicle  Minimum Latency Problems',
	 'urllink': u'http://arxiv.org/abs/1411.4573'}
2015-04-10 19:31:03+0000 [xxu46_10] INFO: Crawled 943 pages (at 1 pages/min), scraped 936 items (at 1 items/min)
2015-04-10 19:31:59+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1411.4568> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 19:31:59+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1411.4568>
	{'abstract': u'We introduce a learning-based approach to detect repeatable keypoints under drastic imaging changes of weather and lighting conditions to which state-of-the-art keypoint detectors are surprisingly sensitive. We first identify good keypoint candidates in multiple training images taken from the same viewpoint. We then train a regressor to predict a score map whose maxima are those points so that they can be found by simple non-maximum suppression. As there are no standard datasets to test the influence of these kinds of changes, we created our own, which we will make publicly available. We will show that our method significantly outperforms the state-of-the-art methods in such challenging conditions, while still achieving state-of-the-art performance on the untrained standard Oxford dataset.',
	 'authors': u'Yannick Verdie, Kwang Moo Yi, Pascal Fua, Vincent Lepetit,',
	 'category': u'Computer Science ',
	 'date': '2014-11-17',
	 'pdflink': u'http://arxiv.org/pdf/1411.4568',
	 'subjects': u'Computer Vision and Pattern Recognition (cs.CV)',
	 'title': u'\nTILDE: A Temporally Invariant Learned DEtector',
	 'urllink': u'http://arxiv.org/abs/1411.4568'}
2015-04-10 19:32:03+0000 [xxu46_10] INFO: Crawled 944 pages (at 1 pages/min), scraped 937 items (at 1 items/min)
2015-04-10 19:33:03+0000 [xxu46_10] INFO: Crawled 944 pages (at 0 pages/min), scraped 937 items (at 0 items/min)
2015-04-10 19:33:10+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1411.4565> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 19:33:10+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1411.4565>
	{'abstract': u'This paper presents a parallel genetic algorithm for three dimensional bin packing with heterogeneous bins using Hadoop Map-Reduce framework. The most common three dimensional bin packing problem which packs given set of boxes into minimum number of equal sized bins is proven to be NP Hard. The variation of three dimensional bin packing problem that allows heterogeneous bin sizes and rotation of boxes is computationally more harder than common three dimensional bin packing problem. The proposed Map-Reduce implementation helps to run the genetic algorithm for three dimensional bin packing with heterogeneous bins on multiple machines parallely and computes the solution in relatively short time.',
	 'authors': u'Drona Pratap Chandu,',
	 'category': u'Computer Science ',
	 'date': '2014-11-17',
	 'pdflink': u'http://arxiv.org/pdf/1411.4565',
	 'subjects': u'Distributed, Parallel, and Cluster Computing (cs.DC)',
	 'title': u'\nA Parallel Genetic Algorithm for Three Dimensional Bin Packing with  Heterogeneous Bins',
	 'urllink': u'http://arxiv.org/abs/1411.4565'}
2015-04-10 19:34:03+0000 [xxu46_10] INFO: Crawled 945 pages (at 1 pages/min), scraped 938 items (at 1 items/min)
2015-04-10 19:34:08+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1411.4555> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 19:34:08+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1411.4555>
	{'abstract': u'Automatically describing the content of an image is a fundamental problem in artificial intelligence that connects computer vision and natural language processing. In this paper, we present a generative model based on a deep recurrent architecture that combines recent advances in computer vision and machine translation and that can be used to generate natural sentences describing an image. The model is trained to maximize the likelihood of the target description sentence given the training image. Experiments on several datasets show the accuracy of the model and the fluency of the language it learns solely from image descriptions. Our model is often quite accurate, which we verify both qualitatively and quantitatively. For instance, while the current state-of-the-art BLEU score (the higher the better) on the Pascal dataset is 25, our approach yields 59, to be compared to human performance around 69. We also show BLEU score improvements on Flickr30k, from 55 to 66, and on SBU, from 19 to 27.',
	 'authors': u'Oriol Vinyals, Alexander Toshev, Samy Bengio, Dumitru Erhan,',
	 'category': u'Computer Science ',
	 'date': '2014-11-17',
	 'pdflink': u'http://arxiv.org/pdf/1411.4555',
	 'subjects': u'Computer Vision and Pattern Recognition (cs.CV)',
	 'title': u'\nShow and Tell: A Neural Image Caption Generator',
	 'urllink': u'http://arxiv.org/abs/1411.4555'}
2015-04-10 19:34:55+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1411.4516> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 19:34:55+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1411.4516>
	{'abstract': u'We study the extension of relational multiagent systems (RMASs), where agents manipulate full-fledged relational databases, with data types and facets equipped with domain-specific, rigid relations (such as total orders). Specifically, we focus on design-time verification of RMASs against rich first-order temporal properties expressed in a variant of first-order mu-calculus with quantification across states. We build on previous decidability results under the "state-bounded" assumption, i.e., in each single state only a bounded number of data objects is stored in the agent databases, while unboundedly many can be encountered over time. We recast this condition, showing decidability in presence of dense, linear orders, and facets defined on top of them. Our approach is based on the construction of a finite-state, sound and complete abstraction of the original system, in which dense linear orders are reformulated as non-rigid relations working on the active domain of the system only. We also show undecidability when including a data type equipped with the successor relation.',
	 'authors': u'Diego Calvanese, Giorgio Delzanno, Marco Montali,',
	 'category': u'Computer Science ',
	 'date': '2014-11-17',
	 'pdflink': u'http://arxiv.org/pdf/1411.4516',
	 'subjects': u'Artificial Intelligence (cs.AI)',
	 'title': u'\nVerification of Relational Multiagent Systems with Data Types (Extended  Version)',
	 'urllink': u'http://arxiv.org/abs/1411.4516'}
2015-04-10 19:35:03+0000 [xxu46_10] INFO: Crawled 947 pages (at 2 pages/min), scraped 940 items (at 2 items/min)
2015-04-10 19:36:03+0000 [xxu46_10] INFO: Crawled 947 pages (at 0 pages/min), scraped 940 items (at 0 items/min)
2015-04-10 19:36:04+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1411.4503> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 19:36:04+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1411.4503>
	{'abstract': u'We derive a convex optimization problem for the task of segmenting sequential data, which explicitly treats presence of outliers. We describe two algorithms for solving this problem, one exact and one a top-down novel approach, and we derive a consistency results for the case of two segments and no outliers. Robustness to outliers is evaluated on two real-world tasks related to speech segmentation. Our algorithms outperform baseline segmentation algorithms.',
	 'authors': u'Itamar Katz, Koby Crammer,',
	 'category': u'Computer Science ',
	 'date': '2014-11-17',
	 'pdflink': u'http://arxiv.org/pdf/1411.4503',
	 'subjects': u'Learning (cs.LG)',
	 'title': u'\nOutlier-Robust Convex Segmentation',
	 'urllink': u'http://arxiv.org/abs/1411.4503'}
2015-04-10 19:37:03+0000 [xxu46_10] INFO: Crawled 948 pages (at 1 pages/min), scraped 941 items (at 1 items/min)
2015-04-10 19:37:09+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1411.4498> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 19:37:09+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1411.4498>
	{'abstract': u'We consider waking up a single-hop radio network with multiple channels. There are stations connected to channels without collision detection. Some stations may become active spontaneously at arbitrary times, where is unknown, and the goal is for all the stations to hear a successful transmission as soon as possible after the first spontaneous activation. We present a deterministic algorithm for the general problem that wakes up the network in time. We prove a lower bound that any deterministic algorithm requires time. We give a deterministic algorithm for the special case when , for some constant , which wakes up the network in time. This algorithm misses time optimality by at most a factor of . We give a randomized algorithm that wakes up the network within rounds with the probability of at least , for any unknown . We also consider a model of jamming, in which each channel in any round may be jammed to prevent a successful transmission, which happens with some known parameter probability , independently across all channels and rounds. For this model, we give a deterministic algorithm that wakes up the network in time with the probability of at least .',
	 'authors': u'Bogdan S. Chlebus, Gianluca De Marco, Dariusz R. Kowalski,',
	 'category': u'Computer Science ',
	 'date': '2014-11-17',
	 'pdflink': u'http://arxiv.org/pdf/1411.4498',
	 'subjects': u'Data Structures and Algorithms (cs.DS)',
	 'title': u'\nScalable Wake-up of Multi-Channel Single-Hop Radio Networks',
	 'urllink': u'http://arxiv.org/abs/1411.4498'}
2015-04-10 19:38:03+0000 [xxu46_10] INFO: Crawled 949 pages (at 1 pages/min), scraped 942 items (at 1 items/min)
2015-04-10 19:38:25+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1411.4495> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 19:38:25+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1411.4495>
	{'abstract': u'We present an institution for UML state machines without hierarchical states. The interaction with UML class diagrams is handled via institutions for guards and actions, which provide dynamic components of states (such as valuations of attributes) but abstract away from details of class diagrams. We also study a notion of interleaving product, which captures the interaction of several state machines. The interleaving product construction is the basis for a semantics of composite structure diagrams, which can be used to specify the interaction of state machines. This work is part of a larger effort to build a framework for formal software development with UML, based on a heterogeneous approach using institutions.',
	 'authors': u'Alexander Knapp, Till Mossakowski, Markus Roggenbach, Martin Glauer,',
	 'category': u'Computer Science ',
	 'date': '2014-11-17',
	 'pdflink': u'http://arxiv.org/pdf/1411.4495',
	 'subjects': u'Software Engineering (cs.SE)',
	 'title': u'\nAn Institution for Simple UML State Machines',
	 'urllink': u'http://arxiv.org/abs/1411.4495'}
2015-04-10 19:39:03+0000 [xxu46_10] INFO: Crawled 950 pages (at 1 pages/min), scraped 943 items (at 1 items/min)
2015-04-10 19:39:30+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1411.4491> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 19:39:30+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1411.4491>
	{'abstract': u'Domain adaptation aims at adapting a prediction function trained on a source domain, for a new different but related target domain. Recently several subspace learning methods have proposed adaptive solutions in the unsupervised case, where no labeled data are available for the target. Most of the attention has been dedicated to searching a new low-dimensional domain-invariant representation, leaving the definition of the prediction function to a second stage. Here we propose to learn both jointly. Specifically we learn the source subspace that best matches the target subspace while at the same time minimizing a regularized misclassification loss. We provide an alternating optimization technique based on stochastic sub-gradient descent to solve the learning problem and we demonstrate its performance on several domain adaptation tasks.',
	 'authors': u'Basura Fernando, Tatiana Tommasi, Tinne Tuytelaars,',
	 'category': u'Computer Science ',
	 'date': '2014-11-17',
	 'pdflink': u'http://arxiv.org/pdf/1411.4491',
	 'subjects': u'Computer Vision and Pattern Recognition (cs.CV)',
	 'title': u'\nJoint cross-domain classification and subspace learning',
	 'urllink': u'http://arxiv.org/abs/1411.4491'}
2015-04-10 19:40:03+0000 [xxu46_10] INFO: Crawled 951 pages (at 1 pages/min), scraped 944 items (at 1 items/min)
2015-04-10 19:40:43+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1411.4484> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 19:40:43+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1411.4484>
	{'abstract': u'For many people, Wikipedia represents one of the primary sources of knowledge about foreign cultures. Yet, different Wikipedia language editions offer different descriptions of cultural phenomena. Unveiling diverging representations of cultures is an important problem since they may foster the formation of cross-cultural stereotypes, misunderstandings and potentially even conflict. In this work we present an approach for mining cultural relations between different language communities by exploring how they describe their own culture and those of other communities on Wikipedia. We demonstrate the utility of our approach via a case study that focuses on food cultures and validate our results using 1) various external reference data sources (i.e., the European Social Value Survey, migration statistics), 2) crowdsourcing methods and 3) simulations.',
	 'authors': u'Paul Laufer, Claudia Wagner, Fabian Fl\xf6ck, Markus Strohmaier,',
	 'category': u'Computer Science ',
	 'date': '2014-11-17',
	 'pdflink': u'http://arxiv.org/pdf/1411.4484',
	 'subjects': u'Social and Information Networks (cs.SI)',
	 'title': u'\nMining cross-cultural relations from Wikipedia - A study of 31 European  food cultures',
	 'urllink': u'http://arxiv.org/abs/1411.4484'}
2015-04-10 19:41:03+0000 [xxu46_10] INFO: Crawled 952 pages (at 1 pages/min), scraped 945 items (at 1 items/min)
2015-04-10 19:42:02+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1411.4476> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 19:42:02+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1411.4476>
	{'abstract': u'The emph is a generalization of the classic facility location problem proposed by Eisenstat, Mathieu, and Schabanel to model the dynamics of evolving social/infrastructure networks. The generalization lies in that the distance metric between clients and facilities changes over time. This leads to a trade-off between optimizing the classic objective function and the "stability" of the solution: there is a switching cost charged every time a client changes the facility to which it is connected. While the standard linear program (LP) relaxation for the classic problem naturally extends to this problem, traditional LP-rounding techniques do not, as they are often sensitive to small changes in the metric resulting in frequent switches. We present a new LP-rounding algorithm for facility location problems, which yields the first constant approximation algorithm for the dynamic facility location problem. Our algorithm installs competing exponential clocks on the clients and facilities, and connect every client by the path that repeatedly follows the smallest clock in the neighborhood. The use of exponential clocks gives rise to several properties that distinguish our approach from previous LP-roundings for facility location problems. In particular, we use emph and we allow clients to connect through paths of emph. In fact, the clustering-free nature of our algorithm is crucial for applying our LP-rounding approach to the dynamic problem.',
	 'authors': u'Hyung-Chan An, Ashkan Norouzi-Fard, Ola Svensson,',
	 'category': u'Computer Science ',
	 'date': '2014-11-17',
	 'pdflink': u'http://arxiv.org/pdf/1411.4476',
	 'subjects': u'Data Structures and Algorithms (cs.DS)',
	 'title': u'\nDynamic Facility Location via Exponential Clocks',
	 'urllink': u'http://arxiv.org/abs/1411.4476'}
2015-04-10 19:42:03+0000 [xxu46_10] INFO: Crawled 953 pages (at 1 pages/min), scraped 946 items (at 1 items/min)
2015-04-10 19:42:53+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1411.4472> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 19:42:53+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1411.4472>
	{'abstract': u'The ability to extract public opinion from web portals such as review sites, social networks and blogs will enable companies and individuals to form a view, an attitude and make decisions without having to do lengthy and costly researches and surveys. In this paper machine learning techniques are used for determining the polarity of forum posts on kajgana which are written in Macedonian language. The posts are classified as being positive, negative or neutral. We test different feature metrics and classifiers and provide detailed evaluation of their participation in improving the overall performance on a manually generated dataset. By achieving 92% accuracy, we show that the performance of systems for automated opinion mining is comparable to a human evaluator, thus making it a viable option for text data analysis. Finally, we present a few statistics derived from the forum posts using the developed system.',
	 'authors': u'Andrej Gajduk, Ljupco Kocarev,',
	 'category': u'Computer Science ',
	 'date': '2014-11-17',
	 'pdflink': u'http://arxiv.org/pdf/1411.4472',
	 'subjects': u'Computation and Language (cs.CL)',
	 'title': u'\nOpinion mining of text documents written in Macedonian language',
	 'urllink': u'http://arxiv.org/abs/1411.4472'}
2015-04-10 19:43:03+0000 [xxu46_10] INFO: Crawled 954 pages (at 1 pages/min), scraped 947 items (at 1 items/min)
2015-04-10 19:44:03+0000 [xxu46_10] INFO: Crawled 954 pages (at 0 pages/min), scraped 947 items (at 0 items/min)
2015-04-10 19:44:08+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1411.4465> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 19:44:08+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1411.4465>
	{'abstract': u'Network coding has been successfully used in the past for efficient broadcasting in wireless multi-hop networks. Two coding approaches are suitable for mobile networks; Random Linear Network Coding (RLNC) and XOR-based coding. In this work, we make the observation that RLNC provides increased resilience to packet losses compared to XOR-based coding. We develop an analytical model that justifies our intuition. However, the model also reveals that combining RLNC with probabilistic forwarding, which is the approach taken in the literature, may significantly impact RLNC\'s performance. Therefore, we take the novel approach to combine RLNC with a deterministic broadcasting algorithm in order to prune transmissions. More specifically, we propose a Connected Dominating Set (CDS) based algorithm that works in synergy with RLNC on the "packet generation level". Since managing packet generations is a key issue in RLNC, we propose a distributed scheme, which is also suitable for mobile environments and does not compromise the coding efficiency. We show that the proposed algorithm outperforms XOR-based as well as RLNC-based schemes even when global knowledge is used for managing packet generations.',
	 'authors': u'Nikolaos Papanikos, Evangelos Papapetrou,',
	 'category': u'Computer Science ',
	 'date': '2014-11-17',
	 'pdflink': u'http://arxiv.org/pdf/1411.4465',
	 'subjects': u'Networking and Internet Architecture (cs.NI)',
	 'title': u'\nDeterministic Broadcasting and Random Linear Network Coding in Mobile Ad  Hoc Networks',
	 'urllink': u'http://arxiv.org/abs/1411.4465'}
2015-04-10 19:45:03+0000 [xxu46_10] INFO: Crawled 955 pages (at 1 pages/min), scraped 948 items (at 1 items/min)
2015-04-10 19:45:21+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1411.4464> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 19:45:21+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1411.4464>
	{'abstract': u'In this paper, we propose a fast fully convolutional neural network (FCNN) for crowd segmentation. By replacing the fully connected layers in CNN with 1 by 1 convolution kernels, FCNN takes whole images as inputs and directly outputs segmentation maps by one pass of forward propagation. It has the property of translation invariance like patch-by-patch scanning but with much lower computation cost. Once FCNN is learned, it can process input images of any sizes without warping them to a standard size. These attractive properties make it extendable to other general image segmentation problems. Based on FCNN, a multi-stage deep learning is proposed to integrate appearance and motion cues for crowd segmentation. Both appearance filters and motion filers are pretrained stage-by-stage and then jointly optimized. Different combination methods are investigated. The effectiveness of our approach and component-wise analysis are evaluated on two crowd segmentation datasets created by us, which include image frames from 235 and 11 scenes, respectively. They are currently the largest crowd segmentation datasets and will be released to the public.',
	 'authors': u'Kai Kang, Xiaogang Wang,',
	 'category': u'Computer Science ',
	 'date': '2014-11-17',
	 'pdflink': u'http://arxiv.org/pdf/1411.4464',
	 'subjects': u'Computer Vision and Pattern Recognition (cs.CV)',
	 'title': u'\nFully Convolutional Neural Networks for Crowd Segmentation',
	 'urllink': u'http://arxiv.org/abs/1411.4464'}
2015-04-10 19:45:53+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1411.4455> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 19:45:53+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1411.4455>
	{'abstract': u'The essence of distantly supervised relation extraction is that it is an incomplete multi-label classification problem with sparse and noisy features. To tackle the sparsity and noise challenges, we propose solving the classification problem using matrix completion on factorized matrix of minimized rank. We formulate relation classification as completing the unknown labels of testing items (entity pairs) in a sparse matrix that concatenates training and testing textual features with training labels. Our algorithmic framework is based on the assumption that the rank of item-by-feature and item-by-label joint matrix is low. We apply two optimization models to recover the underlying low-rank matrix leveraging the sparsity of feature-label matrix. The matrix completion problem is then solved by the fixed point continuation (FPC) algorithm, which can find the global optimum. Experiments on two widely used datasets with different dimensions of textual features demonstrate that our low-rank matrix completion approach significantly outperforms the baseline and the state-of-the-art methods.',
	 'authors': u'Miao Fan, Deli Zhao, Qiang Zhou, Zhiyuan Liu, Thomas Fang Zheng, Edward Y. Chang,',
	 'category': u'Computer Science ',
	 'date': '2014-11-17',
	 'pdflink': u'http://arxiv.org/pdf/1411.4455',
	 'subjects': u'Computation and Language (cs.CL)',
	 'title': u'\nErrata: Distant Supervision for Relation Extraction with Matrix  Completion',
	 'urllink': u'http://arxiv.org/abs/1411.4455'}
2015-04-10 19:46:03+0000 [xxu46_10] INFO: Crawled 957 pages (at 2 pages/min), scraped 950 items (at 2 items/min)
2015-04-10 19:46:41+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1411.4449> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 19:46:41+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1411.4449>
	{'abstract': u"The purpose of this paper is twofold. The first is to point out that the Restricted Isometry Property does not hold in many applications where compressed sensing is successfully used. This includes some of the flagships of compressed sensing like Magnetic Resonance Imaging. We demonstrate that for natural compressed sensing matrices involving a level based reconstruction basis, e.g. wavelets, the number of measurements required to recover all -sparse signals for reasonable is excessive. In particular, uniform recovery of all -sparse signals is quite unrealistic. This realisation shows that the Restricted Isometry Property is insufficient for explaining the success of compressed sensing in various practical applications. The second purpose of the paper is to introduce a new realistic framework based on a new RIP-like definition that fits the actual applications where compressed sensing is used. We show that the shortcomings used to show that uniform recovery is unreasonable no longer apply if we instead ask for structured recovery that is uniform only within each of the levels. To analyse this phenomenon, a new tool, termed the 'Restricted Isometry Property in Levels' is described and analysed. We show that with certain conditions on the Restricted Isometry Property in Levels, a form of uniform recovery within each level is possible. Finally, we conclude the paper by providing examples that demonstrate the optimality of the results obtained.",
	 'authors': u'Alexander Bastounis, Anders C. Hansen,',
	 'category': u'Computer Science ',
	 'date': '2014-11-17',
	 'pdflink': u'http://arxiv.org/pdf/1411.4449',
	 'subjects': u'Information Theory (cs.IT)',
	 'title': u'\nOn the absence of the RIP in real-world applications of compressed  sensing and the RIP in levels',
	 'urllink': u'http://arxiv.org/abs/1411.4449'}
2015-04-10 19:47:03+0000 [xxu46_10] INFO: Crawled 958 pages (at 1 pages/min), scraped 951 items (at 1 items/min)
2015-04-10 19:47:36+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1411.4437> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 19:47:36+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1411.4437>
	{'abstract': u'Discovering the malicious or vulnerable anchor node is an essential problem in wireless sensor networks (WSNs). In wireless sensor networks, anchor nodes are the nodes that know its current location. Neighbouring nodes or non-anchor nodes calculate its location coordinate (or location reference) with the help of anchor nodes. Ingenuous localization is not possible in the presence of a cheating anchor node or a cheating node. Nowadays, its a challenging task to identify the cheating anchor node or cheating node in a network. Even after finding out the location of the cheating anchor node, there is no assurance, that the identified node is legitimate or not. This paper aims to localize the cheating anchor nodes using trilateration algorithm and later associate it with Mahalanobis distance to obtain maximum accuracy in detecting malicious or cheating anchor nodes during localization. We were able to attain a considerable reduction in the error achieved during localization. For implementation purpose, we simulated our scheme using ns3 network simulator.',
	 'authors': u'Jeril Kuriakose, V. Amruth, Swathy Nandhini, V. Abhilash,',
	 'category': u'Computer Science ',
	 'date': '2014-11-17',
	 'pdflink': u'http://arxiv.org/pdf/1411.4437',
	 'subjects': u'Networking and Internet Architecture (cs.NI)',
	 'title': u'\nSequestration of Malevolent Anchor Nodes in Wireless Sensor Networks  using Mahalanobis Distance',
	 'urllink': u'http://arxiv.org/abs/1411.4437'}
2015-04-10 19:48:03+0000 [xxu46_10] INFO: Crawled 959 pages (at 1 pages/min), scraped 952 items (at 1 items/min)
2015-04-10 19:48:52+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1411.4435> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 19:48:52+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1411.4435>
	{'abstract': u'In this manuscript we tackle the problem of semi-distributed user selection with distributed linear precoding for sum rate maximization in multiuser multicell systems. A set of adjacent base stations (BS) form a cluster in order to perform coordinated transmission to cell-edge users, and coordination is carried out through a central processing unit (CU). However, the message exchange between BSs and the CU is limited to scheduling control signaling and no user data or channel state information (CSI) exchange is allowed. In the considered multicell coordinated approach, each BS has its own set of cell-edge users and transmits only to one intended user while interference to non-intended users at other BSs is suppressed by signal steering (precoding). We use two distributed linear precoding schemes, Distributed Zero Forcing (DZF) and Distributed Virtual Signal-to-Interference-plus-Noise Ratio (DVSINR). Considering multiple users per cell and the backhaul limitations, the BSs rely on local CSI to solve the user selection problem. First we investigate how the signal-to-noise-ratio (SNR) regime and the number of antennas at the BSs affect the effective channel gain (the magnitude of the channels after precoding) and its relationship with multiuser diversity. Considering that user selection must be based on the type of implemented precoding, we develop metrics of compatibility (estimations of the effective channel gains) that can be computed from local CSI at each BS and reported to the CU for scheduling decisions. Based on such metrics, we design user selection algorithms that can find a set of users that potentially maximizes the sum rate. Numerical results show the effectiveness of the proposed metrics and algorithms for different configurations of users and antennas at the base stations.',
	 'authors': u'Eduardo Casta\xf1eda, Ad\xe3o Silva, Ramiro Samano-Robles, Atilio Gameiro,',
	 'category': u'Computer Science ',
	 'date': '2014-11-17',
	 'pdflink': u'http://arxiv.org/pdf/1411.4435',
	 'subjects': u'Information Theory (cs.IT)',
	 'title': u'\nDistributed Linear Precoding and User Selection in Coordinated Multicell  Systems',
	 'urllink': u'http://arxiv.org/abs/1411.4435'}
2015-04-10 19:49:03+0000 [xxu46_10] INFO: Crawled 960 pages (at 1 pages/min), scraped 953 items (at 1 items/min)
2015-04-10 19:49:41+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1411.4433> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 19:49:41+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1411.4433>
	{'abstract': u'Stochastic HYPE is a novel process algebra that models stochastic, instantaneous and continuous behaviour. It develops the flow-based approach of the hybrid process algebra HYPE by replacing non-urgent events with events with exponentially-distributed durations and also introduces random resets. The random resets allow for general stochasticity, and in particular allow for the use of event durations drawn from distributions other than the exponential distribution. To account for stochasticity, the semantics of stochastic HYPE target piecewise deterministic Markov processes (PDMPs), via intermediate transition-driven stochastic hybrid automata (TDSHA) in contrast to the hybrid automata used as semantic target for HYPE. Stochastic HYPE models have a specific structure where the controller of a system is separate from the continuous aspect of this system providing separation of concerns and supporting reasoning. A novel equivalence is defined which captures when two models have the same stochastic behaviour (as in stochastic bisimulation), instantaneous behaviour (as in classical bisimulation) and continuous behaviour. These techniques are illustrated via an assembly line example.',
	 'authors': u'Luca Bortolussi, Vashti Galpin, Jane Hillston,',
	 'category': u'Computer Science ',
	 'date': '2014-11-17',
	 'pdflink': u'http://arxiv.org/pdf/1411.4433',
	 'subjects': u'Systems and Control (cs.SY)',
	 'title': u'\nStochastic HYPE: Flow-based modelling of stochastic hybrid systems',
	 'urllink': u'http://arxiv.org/abs/1411.4433'}
2015-04-10 19:50:03+0000 [xxu46_10] INFO: Crawled 961 pages (at 1 pages/min), scraped 954 items (at 1 items/min)
2015-04-10 19:50:15+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1411.4423> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 19:50:15+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1411.4423>
	{'abstract': u'Unsupervised feature learning algorithms based on convolutional formulations of independent components analysis (ICA) have been demonstrated to yield state-of-the-art results in several action recognition benchmarks. However, existing approaches do not allow for the number of latent components (features) to be automatically inferred from the data in an unsupervised manner. This is a significant disadvantage of the state-of-the-art, as it results in considerable burden imposed on researchers and practitioners, who must resort to tedious cross-validation procedures to obtain the optimal number of latent features. To resolve these issues, in this paper we introduce a convolutional nonparametric Bayesian sparse ICA architecture for overcomplete feature learning from high-dimensional data. Our method utilizes an Indian buffet process prior to facilitate inference of the appropriate number of latent features under a hybrid variational inference algorithm, scalable to massive datasets. As we show, our model can be naturally used to obtain deep unsupervised hierarchical feature extractors, by greedily stacking successive model layers, similar to existing approaches. In addition, inference for this model is completely heuristics-free; thus, it obviates the need of tedious parameter tuning, which is a major challenge most deep learning approaches are faced with. We evaluate our method on several action recognition benchmarks, and exhibit its advantages over the state-of-the-art.',
	 'authors': u'S. Chatzis,',
	 'category': u'Computer Science ',
	 'date': '2014-11-17',
	 'pdflink': u'http://arxiv.org/pdf/1411.4423',
	 'subjects': u'Computer Vision and Pattern Recognition (cs.CV)',
	 'title': u'\nA Nonparametric Bayesian Approach Toward Stacked Convolutional  Independent Component Analysis',
	 'urllink': u'http://arxiv.org/abs/1411.4423'}
2015-04-10 19:51:03+0000 [xxu46_10] INFO: Crawled 962 pages (at 1 pages/min), scraped 955 items (at 1 items/min)
2015-04-10 19:51:14+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1411.4419> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 19:51:14+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1411.4419>
	{'abstract': u'Low Rank Representation (LRR) and its extensions seek the lowest rank representation of a given data set by solving a rank-minimization problem, which have attracted a lot of interests. Since the rank operator is nonconvex and discontinuous, LRR uses the Nuclear norm as a convex relaxation and most theoretical studies argue that the Nuclear norm may be the only one surrogate for the rank operator. In this paper, we prove the equivalence between the Frobenius-norm- and the Nuclear-norm-based representation. Specifically, when the data set is error-free, the Frobenius-norm-based representation is exactly the Nuclear-norm-based one; When the data set contains a small amount of additive errors, the Frobenius norm is equivalent to the truncated Nuclear norm. Our theoretical result not only provides a new surrogate (i.e., the Frobenius norm) for the rank-minimization problem, but also gives some novel theoretical insights to understand the success of Frobenius-norm-based methods in subspace clustering and pattern classification. Based on our theoretical results, we propose a robust subspace learning algorithm, i.e., Principal Coefficients Embedding (PCE), which builds a similarity graph using largest Frobenius-norm-based coefficients and embeds the graph into a low-dimensional space. Extensive experimental results show that PCE is superior to six feature extraction methods on four popular facial databases with respect to accuracy and robustness to corruptions and disguises.',
	 'authors': u'Xi Peng, Zhang Yi, Huajin Tang,',
	 'category': u'Computer Science ',
	 'date': '2014-11-17',
	 'pdflink': u'http://arxiv.org/pdf/1411.4419',
	 'subjects': u'Computer Vision and Pattern Recognition (cs.CV)',
	 'title': u'\nPrincipal Coefficients Embedding: Theory and Algorithm',
	 'urllink': u'http://arxiv.org/abs/1411.4419'}
2015-04-10 19:52:03+0000 [xxu46_10] INFO: Crawled 963 pages (at 1 pages/min), scraped 956 items (at 1 items/min)
2015-04-10 19:52:25+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1411.4407> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 19:52:25+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1411.4407>
	{'abstract': u'We are motivated by applications that need rich model classes to represent them. Examples of rich model classes include distributions over large, countably infinite supports, slow mixing Markov processes, etc. But such rich classes may be too complex to admit estimators that converge to the truth with convergence rates that can be uniformly bounded over the entire model class as the sample size increases (uniform consistency). However, these rich classes may still allow for estimators with pointwise guarantees whose performance can be bounded in a model dependent way. The pointwise angle of course has the drawback that the estimator performance is a function of the very unknown model that is being estimated, and is therefore unknown. Therefore, even if the estimator is consistent, how well it is doing may not be clear no matter what the sample size is. Departing from the dichotomy of uniform and pointwise consistency, a new analysis framework is explored by characterizing rich model classes that may only admit pointwise guarantees, yet all the information about the model needed to guage estimator accuracy can be inferred from the sample at hand. To retain focus, we analyze the universal compression problem in this data driven pointwise consistency framework.',
	 'authors': u'N. Santhanam, V. Anantharam, A. Kavcic, W. Szpankowski,',
	 'category': u'Computer Science ',
	 'date': '2014-11-17',
	 'pdflink': u'http://arxiv.org/pdf/1411.4407',
	 'subjects': u'Information Theory (cs.IT)',
	 'title': u'\nData driven consistency (working title)',
	 'urllink': u'http://arxiv.org/abs/1411.4407'}
2015-04-10 19:53:03+0000 [xxu46_10] INFO: Crawled 964 pages (at 1 pages/min), scraped 957 items (at 1 items/min)
2015-04-10 19:53:39+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1411.4399> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 19:53:39+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1411.4399>
	{'abstract': u'Interference alignment (IA) is a transmission strategy that attempts to exploit all degrees of freedom available at the transmitter or at the receiver in a multiuser channel. This paper proposes a new scheme named channel aided interference alignment. It makes use of the channel structure beside the linear interference alignment schemes to achieve the optimum degrees of freedom in a K user interference channel. In case the channel matrix does meet the specified structure, the proposed scheme achieves the optimum degrees of freedom at a finite signal to noise ratio and by using limited number of channel realizations; it turns to the usual linear vector interference alignment schemes otherwise. For the case of 3 user interference channel, it would be shown that if only one of interfering channel coefficients can be designed to a specific value, interference would be aligned perfectly at all the receivers. The case of interference channel with generalized message demands has been analyzed as well and also the X channel configuration is briefly investigated .',
	 'authors': u'Zainalabedin Samadi, Vahid Tabataba Vakily, Farzan Haddadi,',
	 'category': u'Computer Science ',
	 'date': '2014-11-17',
	 'pdflink': u'http://arxiv.org/pdf/1411.4399',
	 'subjects': u'Information Theory (cs.IT)',
	 'title': u'\nChannel Aided Interference Alignment',
	 'urllink': u'http://arxiv.org/abs/1411.4399'}
2015-04-10 19:54:03+0000 [xxu46_10] INFO: Crawled 965 pages (at 1 pages/min), scraped 958 items (at 1 items/min)
2015-04-10 19:54:53+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1411.4398> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 19:54:53+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1411.4398>
	{'abstract': u'In this work, we introduce a new, efficient and practical scheme based on the Rabin cryptosystem without using the Jacobi symbol, message redundancy technique or the needs of extra bits in order to specify the correct plaintext. Our system involves only a single prime number as the decryption key and does only one modular exponentiation. Consequently, this will practically reduce the computational efforts during decryption process. We demonstrate that the decryption is unique and proven to be equivalent to factoring.The scheme is performs better when compared to a number of Rabin cryptosystem variants.',
	 'authors': u'Muhammad Asyraf Asbullah, Muhammad Rezal Kamel Ariffin,',
	 'category': u'Computer Science ',
	 'date': '2014-11-17',
	 'pdflink': u'http://arxiv.org/pdf/1411.4398',
	 'subjects': u'Cryptography and Security (cs.CR)',
	 'title': u'\nRabin-$p$ Cryptosystem: Practical and Efficient Method for Rabin based  Encryption Scheme',
	 'urllink': u'http://arxiv.org/abs/1411.4398'}
2015-04-10 19:55:03+0000 [xxu46_10] INFO: Crawled 966 pages (at 1 pages/min), scraped 959 items (at 1 items/min)
2015-04-10 19:56:03+0000 [xxu46_10] INFO: Crawled 966 pages (at 0 pages/min), scraped 959 items (at 0 items/min)
2015-04-10 19:56:11+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1411.4389> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 19:56:11+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1411.4389>
	{'abstract': u'Models based on deep convolutional networks have dominated recent image interpretation tasks; we investigate whether models which are also recurrent, or "temporally deep", are effective for tasks involving sequences, visual and otherwise. We develop a novel recurrent convolutional architecture suitable for large-scale visual learning which is end-to-end trainable, and demonstrate the value of these models on benchmark video recognition tasks, image description and retrieval problems, and video narration challenges. In contrast to current models which assume a fixed spatio-temporal receptive field or simple temporal averaging for sequential processing, recurrent convolutional models are "doubly deep"\' in that they can be compositional in spatial and temporal "layers". Such models may have advantages when target concepts are complex and/or training data are limited. Learning long-term dependencies is possible when nonlinearities are incorporated into the network state updates. Long-term RNN models are appealing in that they directly can map variable-length inputs (e.g., video frames) to variable length outputs (e.g., natural language text) and can model complex temporal dynamics; yet they can be optimized with backpropagation. Our recurrent long-term models are directly connected to modern visual convnet models and can be jointly trained to simultaneously learn temporal dynamics and convolutional perceptual representations. Our results show such models have distinct advantages over state-of-the-art models for recognition or generation which are separately defined and/or optimized.',
	 'authors': u'Jeff Donahue, Lisa Anne Hendricks, Sergio Guadarrama, Marcus Rohrbach, Subhashini Venugopalan, Kate Saenko, Trevor Darrell,',
	 'category': u'Computer Science ',
	 'date': '2014-11-17',
	 'pdflink': u'http://arxiv.org/pdf/1411.4389',
	 'subjects': u'Computer Vision and Pattern Recognition (cs.CV)',
	 'title': u'\nLong-term Recurrent Convolutional Networks for Visual Recognition and  Description',
	 'urllink': u'http://arxiv.org/abs/1411.4389'}
2015-04-10 19:56:58+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1411.4384> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 19:56:58+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1411.4384>
	{'abstract': u'We study online combinatorial auctions with production costs proposed by Blum et al. using the online primal dual framework. In this model, buyers arrive online, and the seller can produce multiple copies of each item subject to a non-decreasing marginal cost per copy. The goal is to allocate items to maximize social welfare less total production cost. For arbitrary (strictly convex and differentiable) production cost functions, we characterize the optimal competitive ratio achievable by online mechanisms/algorithms. We show that online posted pricing mechanisms, which are incentive compatible, can achieve competitive ratios arbitrarily close to the optimal, and construct lower bound instances on which no online algorithms, not necessarily incentive compatible, can do better. Our positive results improve or match the results in several previous work, e.g., Bartal et al., Blum et al., and Buchbinder and Gonen. Our lower bounds apply to randomized algorithms and resolve an open problem by Buchbinder and Gonen.',
	 'authors': u'Zhiyi Huang, Anthony Kim,',
	 'category': u'Computer Science ',
	 'date': '2014-11-17',
	 'pdflink': u'http://arxiv.org/pdf/1411.4384',
	 'subjects': u'Data Structures and Algorithms (cs.DS)',
	 'title': u'\nWelfare Maximization with Production Costs: A Primal Dual Approach',
	 'urllink': u'http://arxiv.org/abs/1411.4384'}
2015-04-10 19:57:03+0000 [xxu46_10] INFO: Crawled 968 pages (at 2 pages/min), scraped 961 items (at 2 items/min)
2015-04-10 19:58:03+0000 [xxu46_10] INFO: Crawled 968 pages (at 0 pages/min), scraped 961 items (at 0 items/min)
2015-04-10 19:58:17+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1411.4380> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 19:58:17+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1411.4380>
	{'abstract': u'In this paper, we construct an infinitary variant of the relational model of linear logic, where the exponential modality is interpreted as the set of finite or countable multisets. We explain how to interpret in this model the fixpoint operator Y as a Conway operator alternatively defined in an inductive or a coinductive way. We then extend the relational semantics with a notion of color or priority in the sense of parity games. This extension enables us to define a new fixpoint operator Y combining both inductive and coinductive policies. We conclude the paper by sketching the connection between the resulting model of lambda-calculus with recursion and higher-order model-checking.',
	 'authors': u'Charles Grellois, Paul-Andr\xe9 Melli\xe8s,',
	 'category': u'Computer Science ',
	 'date': '2014-11-17',
	 'pdflink': u'http://arxiv.org/pdf/1411.4380',
	 'subjects': u'Logic in Computer Science (cs.LO)',
	 'title': u'\nAn infinitary model of linear logic',
	 'urllink': u'http://arxiv.org/abs/1411.4380'}
2015-04-10 19:59:03+0000 [xxu46_10] INFO: Crawled 969 pages (at 1 pages/min), scraped 962 items (at 1 items/min)
2015-04-10 19:59:34+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1411.4379> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 19:59:34+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1411.4379>
	{'abstract': u'Graph partitioning, a well studied problem of parallel computing has many applications in diversified fields such as distributed computing, social network analysis, data mining and many other domains. In this paper, we introduce FGPGA, an efficient genetic approach for producing feasible graph partitions. Our method takes into account the heterogeneity and capacity constraints of the partitions to ensure balanced partitioning. Such approach has various applications in mobile cloud computing that include feasible deployment of software applications on the more resourceful infrastructure in the cloud instead of mobile hand set. Our proposed approach is light weight and hence suitable for use in cloud architecture. We ensure feasibility of the partitions generated by not allowing over-sized partitions to be generated during the initialization and search. Our proposed method tested on standard benchmark datasets significantly outperforms the state-of-the-art methods in terms of quality of partitions and feasibility of the solutions.',
	 'authors': u'Md. Lisul Islam, Novia Nurain, Swakkhar Shatabda, M Sohel Rahman,',
	 'category': u'Computer Science ',
	 'date': '2014-11-17',
	 'pdflink': u'http://arxiv.org/pdf/1411.4379',
	 'subjects': u'Neural and Evolutionary Computing (cs.NE)',
	 'title': u'\nFGPGA: An Efficient Genetic Approach for Producing Feasible Graph  Partitions',
	 'urllink': u'http://arxiv.org/abs/1411.4379'}
2015-04-10 20:00:03+0000 [xxu46_10] INFO: Crawled 970 pages (at 1 pages/min), scraped 963 items (at 1 items/min)
2015-04-10 20:00:27+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1411.4373> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 20:00:27+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1411.4373>
	{'abstract': u'In this paper we consider the generalization of binary spatially coupled low-density parity-check (SC-LDPC) codes to finite fields GF, , and develop design rules for -ary SC-LDPC code ensembles based on their iterative belief propagation (BP) decoding thresholds, with particular emphasis on low-latency windowed decoding (WD). We consider transmission over both the binary erasure channel (BEC) and the binary-input additive white Gaussian noise channel (BIAWGNC) and present results for a variety of -regular SC-LDPC code ensembles constructed over GF using protographs. Thresholds are calculated using protograph versions of -ary density evolution (for the BEC) and -ary extrinsic information transfer analysis (for the BIAWGNC). We show that WD of -ary SC-LDPC codes provides significant threshold gains compared to corresponding (uncoupled) -ary LDPC block code (LDPC-BC) ensembles when the window size is large enough and that these gains increase as the finite field size increases. Moreover, we demonstrate that the new design rules provide WD thresholds that are close to capacity, even when both and are relatively small (thereby reducing decoding complexity and latency). The analysis further shows that, compared to standard flooding-schedule decoding, WD of -ary SC-LDPC code ensembles results in significant reductions in both decoding complexity and decoding latency, and that these reductions increase as increases. For applications with a near-threshold performance requirement and a constraint on decoding latency, we show that using -ary SC-LDPC code ensembles, with moderate , instead of their binary counterparts results in reduced decoding complexity.',
	 'authors': u'Lai Wei, David G. M. Mitchell, Thomas E. Fuja, Daniel J. Costello,',
	 'category': u'Computer Science ',
	 'date': '2014-11-17',
	 'pdflink': u'http://arxiv.org/pdf/1411.4373',
	 'subjects': u'Information Theory (cs.IT)',
	 'title': u'\nDesign of Spatially Coupled LDPC Codes over GF(q) for Windowed Decoding',
	 'urllink': u'http://arxiv.org/abs/1411.4373'}
2015-04-10 20:01:03+0000 [xxu46_10] INFO: Crawled 971 pages (at 1 pages/min), scraped 964 items (at 1 items/min)
2015-04-10 20:01:20+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1411.4369> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 20:01:20+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1411.4369>
	{'abstract': u'This report provides a comprehensive complexity study of line switching in the Linear DC model for the feasibility problem and the optimization problems of maximizing the load that can be served (maximum switching flow, MSF) and minimizing generation cost (optimal transmission switching, OTS). Our results show that these problems are NP-complete and that there is no fully polynomial-time approximation scheme for planar networks with a maximum-node degree of 3. Additionally, we demonstrate that the problems are still NP-hard if we restrict the network structure to cacti with a maximum degree of 3. We also show that the optimization problems can not be approximated within any constant factor.',
	 'authors': u'Karsten Lehmann, Alban Grastien, Pascal Van Hentenryck,',
	 'category': u'Computer Science ',
	 'date': '2014-11-17',
	 'pdflink': u'http://arxiv.org/pdf/1411.4369',
	 'subjects': u'Computational Complexity (cs.CC)',
	 'title': u'\nThe Complexity of DC-Switching Problems',
	 'urllink': u'http://arxiv.org/abs/1411.4369'}
2015-04-10 20:02:03+0000 [xxu46_10] INFO: Crawled 972 pages (at 1 pages/min), scraped 965 items (at 1 items/min)
2015-04-10 20:02:31+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1411.4366> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 20:02:31+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1411.4366>
	{'abstract': u'Majority of the computer or mobile phone enthusiasts make use of the web for searching activity. Web search engines are used for the searching; The results that the search engines get are provided to it by a software module known as the Web Crawler. The size of this web is increasing round-the-clock. The principal problem is to search this huge database for specific information. To state whether a web page is relevant to a search topic is a dilemma. This paper proposes a crawler called as PDD crawler which will follow both a link based as well as a content based approach. This crawler follows a completely new crawling strategy to compute the relevance of the page. It analyses the content of the page based on the information contained in various tags within the HTML source code and then computes the total weight of the page. The page with the highest weight, thus has the maximum content and highest relevance.',
	 'authors': u'Prashant Dahiwale, M M Raghuwanshi, Latesh malik,',
	 'category': u'Computer Science ',
	 'date': '2014-11-17',
	 'pdflink': u'http://arxiv.org/pdf/1411.4366',
	 'subjects': u'Information Retrieval (cs.IR)',
	 'title': u'\nPDD Crawler: A focused web crawler using link and content analysis for  relevance prediction',
	 'urllink': u'http://arxiv.org/abs/1411.4366'}
2015-04-10 20:03:03+0000 [xxu46_10] INFO: Crawled 973 pages (at 1 pages/min), scraped 966 items (at 1 items/min)
2015-04-10 20:03:49+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1411.4357> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 20:03:49+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1411.4357>
	{'abstract': u'This survey highlights the recent advances in algorithms for numerical linear algebra that have come from the technique of linear sketching, whereby given a matrix, one first compresses it to a much smaller matrix by multiplying it by a (usually) random matrix with certain properties. Much of the expensive computation can then be performed on the smaller matrix, thereby accelerating the solution for the original problem. In this survey we consider least squares as well as robust regression problems, low rank approximation, and graph sparsification. We also discuss a number of variants of these problems. Finally, we discuss the limitations of sketching methods.',
	 'authors': u'David P. Woodruff,',
	 'category': u'Computer Science ',
	 'date': '2014-11-17',
	 'pdflink': u'http://arxiv.org/pdf/1411.4357',
	 'subjects': u'Data Structures and Algorithms (cs.DS)',
	 'title': u'\nSketching as a Tool for Numerical Linear Algebra',
	 'urllink': u'http://arxiv.org/abs/1411.4357'}
2015-04-10 20:04:03+0000 [xxu46_10] INFO: Crawled 974 pages (at 1 pages/min), scraped 967 items (at 1 items/min)
2015-04-10 20:05:03+0000 [xxu46_10] INFO: Crawled 974 pages (at 0 pages/min), scraped 967 items (at 0 items/min)
2015-04-10 20:05:04+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1411.4351> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 20:05:04+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1411.4351>
	{'abstract': u"We present an unsupervised model for inducing signed social networks from the content exchanged across network edges. Inference in this model solves three problems simultaneously: (1) identifying the sign of each edge; (2) characterizing the distribution over content for each edge type; (3) estimating weights for triadic features that map to theoretical models such as structural balance. We apply this model to the problem of inducing the social function of address terms, such as 'Madame', 'comrade', and 'dude'. On a dataset of movie scripts, our system obtains a coherent clustering of address terms, while at the same time making intuitively plausible judgments of the formality of social relations in each film. As an additional contribution, we provide a bootstrapping technique for identifying and tagging address terms in dialogue.",
	 'authors': u'Vinodh Krishnan, Jacob Eisenstein,',
	 'category': u'Computer Science ',
	 'date': '2014-11-17',
	 'pdflink': u'http://arxiv.org/pdf/1411.4351',
	 'subjects': u'Social and Information Networks (cs.SI)',
	 'title': u'\n"You\'re Mr. Lebowski, I\'m the Dude": Inducing Address Term Formality in  Signed Social Networks',
	 'urllink': u'http://arxiv.org/abs/1411.4351'}
2015-04-10 20:06:03+0000 [xxu46_10] INFO: Crawled 975 pages (at 1 pages/min), scraped 968 items (at 1 items/min)
2015-04-10 20:06:19+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1411.4346> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 20:06:19+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1411.4346>
	{'abstract': u'This paper studies the containment control problem of multi-agent systems with multiple dynamic leaders in both the discrete-time domain and the continuous-time domain. The leaders\' motions are described by -order polynomial trajectories. This setting makes practical sense because given some critical points, the leaders\' trajectories are usually planned by the polynomial interpolations. In order to drive all followers into the convex hull spanned by the leaders, a -type ( and are short for and , respectively; implies that the algorithm includes high-order integral terms) containment algorithm is proposed. It is theoretically proved that the -type containment algorithm is able to solve the containment problem of multi-agent systems where the followers are described by any order integral dynamics. Compared with the previous results on the multi-agent systems with dynamic leaders, the distinguished features of this paper are that: (1) the containment problem is studied not only in the continuous-time domain but also in the discrete-time domain while most existing results only work in the continuous-time domain; (2) to deal with the leaders with the -order polynomial trajectories, existing results require the follower\'s dynamics to be -order integral while the followers considered in this paper can be described by any-order integral; and (3) the "sign" function is not employed in the proposed algorithm, which avoids the chattering phenomenon. Furthermore, in order to illustrate the practical value of the proposed approach, an application, the containment control of multiple mobile robots is studied. Finally, two simulation examples are given to demonstrate the effectiveness of the proposed algorithm.',
	 'authors': u'Yunpeng Wang, Long Cheng, Wei Ren, Zeng-Guang Hou, Min Tan,',
	 'category': u'Computer Science ',
	 'date': '2014-11-17',
	 'pdflink': u'http://arxiv.org/pdf/1411.4346',
	 'subjects': u'Systems and Control (cs.SY)',
	 'title': u'\nContainment Control of Multi-Agent Systems with Dynamic Leaders Based on  a $PI^n$-Type Approach',
	 'urllink': u'http://arxiv.org/abs/1411.4346'}
2015-04-10 20:07:03+0000 [xxu46_10] INFO: Crawled 976 pages (at 1 pages/min), scraped 969 items (at 1 items/min)
2015-04-10 20:07:13+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1411.4345> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 20:07:13+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1411.4345>
	{'abstract': u'The online learning tools and management also known as Learning Management System (LMS) have been adopted by higher education as it allows convenient and flexibility in learning process between students and instructors or tutors with minimal cost. The adoption of online learning tools in university has allowed users (students and instructors) to interact, share and discuss anytime-anywhere conveniently. Many students nowadays rely on online resources based using their mobile devices, substituting traditional learning interactions. Universities need strategy to sustain in providing intensive interactions and spreading word out mouth of good services through online learning tools by focusing on niche markets and creating close relationship with their stakeholders. The study presented in this paper analyses how universities design best practices in adopting LMS and evaluate its current state for future improvement. In fact, with proper strategies of LMS, universities have opportunities to sustain their business by offering interesting packages and to improve their services through intensive interactions with their users. In this study, we deploy Transaction Cost Economics (TCE) to understand the change business environment and to construct a model for higher institution to regulate their scenario on online learning strategies in fast changing and threatening business environment.',
	 'authors': u'Yabit Alas, Muhammad Anshari,',
	 'category': u'Computer Science ',
	 'date': '2014-11-17',
	 'pdflink': u'http://arxiv.org/pdf/1411.4345',
	 'subjects': u'Computers and Society (cs.CY)',
	 'title': u'\nConstructing Strategy of Online Learning in Higher Education:  Transaction Cost Economy',
	 'urllink': u'http://arxiv.org/abs/1411.4345'}
2015-04-10 20:08:03+0000 [xxu46_10] INFO: Crawled 977 pages (at 1 pages/min), scraped 970 items (at 1 items/min)
2015-04-10 20:08:28+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1411.4340> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 20:08:28+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1411.4340>
	{'abstract': u'Constructions of binary sequences with low autocorrelation are considered in the paper. Based on recent progresses about this topic, several more general constructions of binary sequences with optimal autocorrelations and other low autocorrelations are presented.',
	 'authors': u'Tongjiang Yan, Guang Gong,',
	 'category': u'Computer Science ',
	 'date': '2014-11-17',
	 'pdflink': u'http://arxiv.org/pdf/1411.4340',
	 'subjects': u'Information Theory (cs.IT)',
	 'title': u'\nSome Notes on Constructions of Binary Sequences with Optimal  Autocorrelation',
	 'urllink': u'http://arxiv.org/abs/1411.4340'}
2015-04-10 20:09:03+0000 [xxu46_10] INFO: Crawled 978 pages (at 1 pages/min), scraped 971 items (at 1 items/min)
2015-04-10 20:09:33+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1411.4332> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 20:09:33+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1411.4332>
	{'abstract': u'While it was defined long ago, the extension of CTL with quantification over atomic propositions has never been studied extensively. Considering two different semantics (depending whether propositional quantification refers to the Kripke structure or to its unwinding tree), we study its expressiveness (showing in particular that QCTL coincides with Monadic Second-Order Logic for both semantics) and characterise the complexity of its model-checking and satisfiability problems, depending on the number of nested propositional quantifiers (showing that the structure semantics populates the polynomial hierarchy while the tree semantics populates the exponential hierarchy).',
	 'authors': u'Fran\xe7ois Laroussinie, Nicolas Markey,',
	 'category': u'Computer Science ',
	 'date': '2014-11-16',
	 'pdflink': u'http://arxiv.org/pdf/1411.4332',
	 'subjects': u'Logic in Computer Science (cs.LO)',
	 'title': u'\nQuantified CTL: Expressiveness and Complexity',
	 'urllink': u'http://arxiv.org/abs/1411.4332'}
2015-04-10 20:10:03+0000 [xxu46_10] INFO: Crawled 979 pages (at 1 pages/min), scraped 972 items (at 1 items/min)
2015-04-10 20:10:53+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1411.4331> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 20:10:53+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1411.4331>
	{'abstract': u'As a fundamental technique that concerns several vision tasks such as image parsing, action recognition and clothing retrieval, human pose estimation (HPE) has been extensively investigated in recent years. To achieve accurate and reliable estimation of the human pose, it is well-recognized that the clothing attributes are useful and should be utilized properly. Most previous approaches, however, require to manually annotate the clothing attributes and are therefore very costly. In this paper, we shall propose and explore a emph clothing attribute approach for HPE. Unlike previous approaches, our approach models the clothing attributes as latent variables and thus requires no explicit labeling for the clothing attributes. The inference of the latent variables are accomplished by utilizing the framework of latent structured support vector machines (LSSVM). We employ the strategy of emph to train the LSSVM model: In each iteration, one kind of variables (e.g., human pose or clothing attribute) are fixed and the others are optimized. Our extensive experiments on two real-world benchmarks show the state-of-the-art performance of our proposed approach.',
	 'authors': u'Weipeng Zhang, Jie Shen, Guangcan Liu, Yong Yu,',
	 'category': u'Computer Science ',
	 'date': '2014-11-16',
	 'pdflink': u'http://arxiv.org/pdf/1411.4331',
	 'subjects': u'Computer Vision and Pattern Recognition (cs.CV)',
	 'title': u'\nA Latent Clothing Attribute Approach for Human Pose Estimation',
	 'urllink': u'http://arxiv.org/abs/1411.4331'}
2015-04-10 20:11:03+0000 [xxu46_10] INFO: Crawled 980 pages (at 1 pages/min), scraped 973 items (at 1 items/min)
2015-04-10 20:12:03+0000 [xxu46_10] INFO: Crawled 980 pages (at 0 pages/min), scraped 973 items (at 0 items/min)
2015-04-10 20:12:06+0000 [xxu46_10] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1411.4327> (referer: http://arxiv.org/list/cs/14?skip=14000&show=1000)
2015-04-10 20:12:06+0000 [xxu46_10] DEBUG: Scraped from <200 http://arxiv.org/abs/1411.4327>
	{'abstract': u'Software system can include redundant implementation elements, such as, different methods that can produce indistinguishable results. This type of redundancy is called intrinsic if it is already available in the software, although not intentionally planned. Redundancy can be a key element to increase the reliability of a system. Some fault tolerance and self-healing techniques exploit the redundancy to avoid failures at runtime. Unfortunately, inferring which operations are equivalent manually can be expensive and error prone. A technique proposed in previous work allows to automatically synthesizes method sequences that are equivalent to a target method. However this technique needs an execution scenario to work. Currently, this execution scenario is generated manually that is expensive and makes the technique hard to use. This paper proposes a technique to generate execution scenarios for a target method for which we are searching equivalent sequences. The experimental results obtained on the Java class Stack show that the proposed approach correctly generates execution scenarios within reasonable execution time. Besides, the execution scenarios generated allow to maximize the effectiveness of the technique described above.',
	 'authors': u'Matteo Brunetto,',
	 'category': u'Computer Science ',
	 'date': '2014-11-16',
	 'pdflink': u'http://arxiv.org/pdf/1411.4327',
	 'subjects': u'Software Engineering (cs.SE)',
	 'title': u'\nAutomatic Synthesis of Test Cases to Identify Software Redundancy',
	 'urllink': u'http://arxiv.org/abs/1411.4327'}
