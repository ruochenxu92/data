nohup: ignoring input
2015-03-23 19:50:48+0000 [scrapy] INFO: Scrapy 0.24.5 started (bot: superqq_spider)
2015-03-23 19:50:48+0000 [scrapy] INFO: Optional features available: ssl, http11, boto
2015-03-23 19:50:48+0000 [scrapy] INFO: Overridden settings: {'NEWSPIDER_MODULE': 'superqq_spider.spiders', 'SPIDER_MODULES': ['superqq_spider.spiders'], 'DOWNLOAD_DELAY': 4, 'BOT_NAME': 'superqq_spider'}
2015-03-23 19:50:48+0000 [scrapy] INFO: Enabled extensions: LogStats, TelnetConsole, CloseSpider, WebService, CoreStats, SpiderState
2015-03-23 19:50:48+0000 [scrapy] INFO: Enabled downloader middlewares: HttpAuthMiddleware, DownloadTimeoutMiddleware, UserAgentMiddleware, RetryMiddleware, DefaultHeadersMiddleware, MetaRefreshMiddleware, HttpCompressionMiddleware, RedirectMiddleware, CookiesMiddleware, ChunkedTransferMiddleware, DownloaderStats
2015-03-23 19:50:48+0000 [scrapy] INFO: Enabled spider middlewares: HttpErrorMiddleware, OffsiteMiddleware, RefererMiddleware, UrlLengthMiddleware, DepthMiddleware
2015-03-23 19:50:48+0000 [scrapy] INFO: Enabled item pipelines: JsonWriterPipeline
2015-03-23 19:50:48+0000 [xxu461000] INFO: Spider opened
2015-03-23 19:50:48+0000 [xxu461000] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2015-03-23 19:50:48+0000 [scrapy] DEBUG: Telnet console listening on 127.0.0.1:6023
2015-03-23 19:50:48+0000 [scrapy] DEBUG: Web service listening on 127.0.0.1:6080
2015-03-23 19:51:07+0000 [xxu461000] DEBUG: Crawled (200) <GET http://arxiv.org/list/cs/15?skip=3000&show=1000> (referer: None)
2015-03-23 19:51:14+0000 [xxu461000] DEBUG: Crawled (200) <GET http://arxiv.org/list/cs/15?skip=2000&show=1000> (referer: None)
2015-03-23 19:51:23+0000 [xxu461000] DEBUG: Crawled (200) <GET http://arxiv.org/list/cs/15?skip=0&show=1000> (referer: None)
2015-03-23 19:51:25+0000 [xxu461000] DEBUG: Crawled (200) <GET http://arxiv.org/list/cs/15?skip=1000&show=1000> (referer: None)
2015-03-23 19:51:25+0000 [xxu461000] DEBUG: Crawled (200) <GET http://arxiv.org/list/cs/14?skip=16000&show=1000> (referer: None)
2015-03-23 19:51:37+0000 [xxu461000] DEBUG: Crawled (200) <GET http://arxiv.org/list/cs/14?skip=14000&show=1000> (referer: None)
2015-03-23 19:51:48+0000 [xxu461000] INFO: Crawled 6 pages (at 6 pages/min), scraped 0 items (at 0 items/min)
2015-03-23 19:51:51+0000 [xxu461000] DEBUG: Crawled (200) <GET http://arxiv.org/list/cs/14?skip=13000&show=1000> (referer: None)
2015-03-23 19:51:56+0000 [xxu461000] DEBUG: Crawled (200) <GET http://arxiv.org/list/cs/14?skip=15000&show=1000> (referer: None)
2015-03-23 19:51:58+0000 [xxu461000] DEBUG: Crawled (200) <GET http://arxiv.org/list/cs/14?skip=11000&show=1000> (referer: None)
2015-03-23 19:52:04+0000 [xxu461000] DEBUG: Crawled (200) <GET http://arxiv.org/list/cs/14?skip=12000&show=1000> (referer: None)
2015-03-23 19:52:04+0000 [xxu461000] DEBUG: Crawled (200) <GET http://arxiv.org/list/cs/14?skip=9000&show=1000> (referer: None)
2015-03-23 19:52:11+0000 [xxu461000] DEBUG: Crawled (200) <GET http://arxiv.org/list/cs/14?skip=7000&show=1000> (referer: None)
2015-03-23 19:52:13+0000 [xxu461000] DEBUG: Crawled (200) <GET http://arxiv.org/list/cs/14?skip=10000&show=1000> (referer: None)
2015-03-23 19:52:15+0000 [xxu461000] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1503.04444> (referer: http://arxiv.org/list/cs/15?skip=3000&show=1000)
2015-03-23 19:52:15+0000 [xxu461000] DEBUG: Scraped from <200 http://arxiv.org/abs/1503.04444>
	{'abstract': u'A new diagnostic scheme is presented for ball bearing localized faults, which utilizes preprocessed time domain features based pattern recognition (PR). Vibration data is acquired from faulty bearings using a test rig, and the features are extracted from the data segments that are preprocessed prior to use in the fault classification process. The preprocessing involves smoothing of the features, which reduces the undesired impact of noise and vibration randomness on the PR process, and thus enhances the diagnostic accuracy. The results are compared with a similar scheme in terms of minimum features requirement to achieve an optimum classification accuracy, and the feature processing based proposed scheme provides better results.',
	 'authors': u'Muhammad Masood Tahir, Ayyaz Hussain,',
	 'category': u'Computer Science ',
	 'date': '2015-3-15',
	 'pdflink': u'http://arxiv.org/pdf/1503.04444',
	 'subjects': u'Computer Vision and Pattern Recognition (cs.CV)',
	 'title': u'\nPattern Recognition of Bearing Faults using Smoother Statistical  Features',
	 'urllink': u'http://arxiv.org/abs/1503.04444'}
2015-03-23 19:52:19+0000 [xxu461000] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1502.06103> (referer: http://arxiv.org/list/cs/15?skip=2000&show=1000)
2015-03-23 19:52:19+0000 [xxu461000] DEBUG: Scraped from <200 http://arxiv.org/abs/1502.06103>
	{'abstract': u"This paper considers the use of compressive sensing based algorithms for velocity estimation of moving vehicles. The procedure is based on sparse reconstruction algorithms combined with time-frequency analysis applied to video data. This algorithm provides an accurate estimation of object's velocity even in the case of a very reduced number of available video frames. The influence of crucial parameters is analysed for different types of moving vehicles.",
	 'authors': u'Ana Miletic, Nemanja Ivanovic,',
	 'category': u'Computer Science ',
	 'date': '2015-2-21',
	 'pdflink': u'http://arxiv.org/pdf/1502.06103',
	 'subjects': u'Multimedia (cs.MM)',
	 'title': u'\nCompressive sensing based velocity estimation in video data',
	 'urllink': u'http://arxiv.org/abs/1502.06103'}
2015-03-23 19:52:23+0000 [xxu461000] DEBUG: Crawled (200) <GET http://arxiv.org/list/cs/14?skip=4000&show=1000> (referer: None)
2015-03-23 19:52:24+0000 [xxu461000] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1501.00163> (referer: http://arxiv.org/list/cs/15?skip=0&show=1000)
2015-03-23 19:52:25+0000 [xxu461000] DEBUG: Scraped from <200 http://arxiv.org/abs/1501.00163>
	{'abstract': u'This paper investigates polynomial remainder codes with non-pairwise coprime moduli. We first consider a robust reconstruction problem for polynomials from erroneous residues when the degrees of all residue errors are assumed small, namely robust Chinese Remainder Theorem (CRT) for polynomials. It basically says that a polynomial can be reconstructed from erroneous residues such that the degree of the reconstruction error is upper bounded by whenever the degrees of all residue errors are upper bounded by , where a sufficient condition for and a reconstruction algorithm are obtained. By releasing the constraint that all residue errors have small degrees, another robust reconstruction is then presented when there are multiple unrestricted errors and an arbitrary number of errors with small degrees in the residues. By making full use of redundancy in moduli, we obtain a stronger residue error correction capability in the sense that apart from the number of errors that can be corrected in the previous existing result, some errors with small degrees can be also corrected in the residues. With this newly obtained result, improvements in uncorrected error probability and burst error correction capability in a data transmission are illustrated.',
	 'authors': u'Li Xiao, Xiang-Gen Xia,',
	 'category': u'Computer Science ',
	 'date': '2014-12-31',
	 'pdflink': u'http://arxiv.org/pdf/1501.00163',
	 'subjects': u'Information Theory (cs.IT)',
	 'title': u'\nError Correction in Polynomial Remainder Codes with Non-Pairwise Coprime  Moduli and Robust Chinese Remainder Theorem for Polynomials',
	 'urllink': u'http://arxiv.org/abs/1501.00163'}
2015-03-23 19:52:25+0000 [xxu461000] DEBUG: Crawled (200) <GET http://arxiv.org/list/cs/14?skip=8000&show=1000> (referer: None)
2015-03-23 19:52:30+0000 [xxu461000] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1501.06857> (referer: http://arxiv.org/list/cs/15?skip=1000&show=1000)
2015-03-23 19:52:30+0000 [xxu461000] DEBUG: Scraped from <200 http://arxiv.org/abs/1501.06857>
	{'abstract': u'Creating rankings might seem like a vain exercise in belly-button gazing, even more so for people so unlike that kind of things as programmers. However, in this paper we will try to prove how creating city (or province) based rankings in Spain has led to all kind of interesting effects, including increased productivity and community building. We describe the methodology we have used to search for programmers residing in a particular province focusing on those where most population is concentrated and apply different measures to show how these communities differ in structure, number and productivity.',
	 'authors': u'J.J. Merelo, Nuria Rico, Israel Blancas, M. G. Arenas, Fernando Tricas, Jos\xe9 Antonio Vacas,',
	 'category': u'Computer Science ',
	 'date': '2015-1-27',
	 'pdflink': u'http://arxiv.org/pdf/1501.06857',
	 'subjects': u'Social and Information Networks (cs.SI)',
	 'title': u'\nMeasuring the local GitHub developer community',
	 'urllink': u'http://arxiv.org/abs/1501.06857'}
2015-03-23 19:52:31+0000 [xxu461000] DEBUG: Crawled (200) <GET http://arxiv.org/list/cs/14?skip=6000&show=1000> (referer: None)
2015-03-23 19:52:33+0000 [xxu461000] DEBUG: Crawled (200) <GET http://arxiv.org/list/cs/14?skip=5000&show=1000> (referer: None)
2015-03-23 19:52:35+0000 [xxu461000] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1501.02245> (referer: http://arxiv.org/list/cs/15?skip=1000&show=1000)
2015-03-23 19:52:36+0000 [xxu461000] DEBUG: Scraped from <200 http://arxiv.org/abs/1501.02245>
	{'abstract': u'This study presented a type of image processing code which is used for sharpening photoelastic fringe patterns of transparent materials in photoelastic experiences to determine the stress distribution. C-Sharp software was utilized for coding the algorithm of this image processing method. For evaluation of this code, the results of a photoelastic experience of a sample contact problem between a half-plane with an oblique edge crack and a tilted wedge using this image processing method was compared with the FEM results of the same problem in order to obtain the stress intensity factors (SIF) of the specimen. A good agreement between experimental results extracted from this method of image processing and computational results was observed.',
	 'authors': u'Seyedmeysam Khaleghian, Anahita Emami, Nasser Soltani,',
	 'category': u'Computer Science ',
	 'date': '2015-1-6',
	 'pdflink': u'http://arxiv.org/pdf/1501.02245',
	 'subjects': u'Materials Science (cond-mat.mtrl-sci)',
	 'title': u'\nImage Processing Code for Sharpening Photoelastic Fringe Patterns and  Its Usage in Determination of Stress Intensity Factors in a Sample Contact  Problem',
	 'urllink': u'http://arxiv.org/abs/1501.02245'}
2015-03-23 19:52:49+0000 [xxu461000] INFO: Crawled 22 pages (at 16 pages/min), scraped 5 items (at 5 items/min)
2015-03-23 19:53:49+0000 [xxu461000] INFO: Crawled 22 pages (at 0 pages/min), scraped 5 items (at 0 items/min)
2015-03-23 19:54:49+0000 [xxu461000] INFO: Crawled 22 pages (at 0 pages/min), scraped 5 items (at 0 items/min)
2015-03-23 19:55:48+0000 [xxu461000] INFO: Crawled 22 pages (at 0 pages/min), scraped 5 items (at 0 items/min)
2015-03-23 19:55:49+0000 [xxu461000] DEBUG: Filtered duplicate request: <GET http://arxiv.org/abs/1405.1189> - no more duplicates will be shown (see DUPEFILTER_DEBUG to show all duplicates)
2015-03-23 19:55:49+0000 [xxu461000] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1405.2300> (referer: http://arxiv.org/list/cs/14?skip=5000&show=1000)
2015-03-23 19:55:49+0000 [xxu461000] DEBUG: Scraped from <200 http://arxiv.org/abs/1405.2300>
	{'abstract': u'We show that finding orthogonal grid-embeddings of plane graphs (planar with fixed combinatorial embedding) with the minimum number of bends in the so-called Kandinsky model (which allows vertices of degree ) is NP-complete, thus solving a long-standing open problem. On the positive side, we give an efficient algorithm for several restricted variants, such as graphs of bounded branch width and a subexponential exact algorithm for general plane graphs.',
	 'authors': u'Thomas Bl\xe4sius, Guido Br\xfcckner, Ignaz Rutter,',
	 'category': u'Computer Science ',
	 'date': '2014-4-14',
	 'pdflink': u'http://arxiv.org/pdf/1405.2300',
	 'subjects': u'Computational Geometry (cs.CG)',
	 'title': u'\nComplexity of Higher-Degree Orthogonal Graph Embedding in the Kandinsky  Model',
	 'urllink': u'http://arxiv.org/abs/1405.2300'}
2015-03-23 19:55:54+0000 [xxu461000] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1405.7584> (referer: http://arxiv.org/list/cs/14?skip=6000&show=1000)
2015-03-23 19:55:55+0000 [xxu461000] DEBUG: Scraped from <200 http://arxiv.org/abs/1405.7584>
	{'abstract': u'The Data Access System (DAS) is a metadata and data management software system, providing a reusable solution for the storage of data acquired both from telescopes and auxiliary data sources during the instrument development phases and operations. It is part of the Customizable Instrument WorkStation system (CIWS-FW), a framework for the storage, processing and quick-look at the data acquired from scientific instruments. The DAS provides a data access layer mainly targeted to software applications: quick-look displays, pre-processing pipelines and scientific workflows. It is logically organized in three main components: an intuitive and compact Data Definition Language (DAS DDL) in XML format, aimed for user-defined data types; an Application Programming Interface (DAS API), automatically adding classes and methods supporting the DDL data types, and providing an object-oriented query language; a data management component, which maps the metadata of the DDL data types in a relational Data Base Management System (DBMS), and stores the data in a shared (network) file system. With the DAS DDL, developers define the data model for a particular project, specifying for each data type the metadata attributes, the data format and layout (if applicable), and named references to related or aggregated data types. Together with the DDL user-defined data types, the DAS API acts as the only interface to store, query and retrieve the metadata and data in the DAS system, providing both an abstract interface and a data model specific one in C, C++ and Python. The mapping of metadata in the back-end database is automatic and supports several relational DBMSs, including MySQL, Oracle and PostgreSQL.',
	 'authors': u'Marco Frailis, Stefano Sartor, Andrea Zacchei, Marcello Lodi, Roberto Cirami, Fabio Pasian, Massimo Trifoglio, Andrea Bulgarelli, Fulvio Gianotti, Enrico Franceschi, Luciano Nicastro, Vito Conforti, Andrea Zoli, Ricky Smart, Roberto Morbidelli, Mauro Dadina,',
	 'category': u'Computer Science ',
	 'date': '2014-5-29',
	 'pdflink': u'http://arxiv.org/pdf/1405.7584',
	 'subjects': u'Instrumentation and Methods for Astrophysics (astro-ph.IM)',
	 'title': u'\nDAS: a data management system for instrument tests and operations',
	 'urllink': u'http://arxiv.org/abs/1405.7584'}
2015-03-23 19:55:57+0000 [xxu461000] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1407.5242> (referer: http://arxiv.org/list/cs/14?skip=8000&show=1000)
2015-03-23 19:55:57+0000 [xxu461000] DEBUG: Scraped from <200 http://arxiv.org/abs/1407.5242>
	{'abstract': u'Object proposal algorithms have shown great promise as a first step for object recognition and detection. Good object proposal generation algorithms require high object recall rate as well as low computational cost, because generating object proposals is usually utilized as a preprocessing step. The problem of how to accelerate the object proposal generation and evaluation process without decreasing recall is thus of great interest. In this paper, we propose a new object proposal generation method using two-stage cascade SVMs, where in the first stage linear filters are learned for predefined quantized scales/aspect-ratios independently, and in the second stage a global linear classifier is learned across all the quantized scales/aspect-ratios for calibration, so that all the proposals can be compared properly. The proposals with highest scores are our final output. Specifically, we explain our scale/aspect-ratio quantization scheme, and investigate the effects of combinations of and regularizers in cascade SVMs with/without ranking constraints in learning. Comprehensive experiments on VOC2007 dataset are conducted, and our results achieve the state-of-the-art performance with high object recall rate and high computational efficiency. Besides, our method has been demonstrated to be suitable for not only class-specific but also generic object proposal generation.',
	 'authors': u'Ziming Zhang, Philip H.S. Torr,',
	 'category': u'Computer Science ',
	 'date': '2014-7-20',
	 'pdflink': u'http://arxiv.org/pdf/1407.5242',
	 'subjects': u'Computer Vision and Pattern Recognition (cs.CV)',
	 'title': u'\nObject Proposal Generation using Two-Stage Cascade SVMs',
	 'urllink': u'http://arxiv.org/abs/1407.5242'}
2015-03-23 19:56:02+0000 [xxu461000] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1404.5372> (referer: http://arxiv.org/list/cs/14?skip=4000&show=1000)
2015-03-23 19:56:02+0000 [xxu461000] DEBUG: Scraped from <200 http://arxiv.org/abs/1404.5372>
	{'abstract': u'The linked open data (LOD) paradigm has emerged as a promising approach to structuring and sharing geospatial information. One of the major obstacles to this vision lies in the difficulties found in the automatic integration between heterogeneous vocabularies and ontologies that provides the semantic backbone of the growing constellation of open geo-knowledge bases. In this article, we show how to utilize WordNet as a semantic hub to increase the integration of LOD. With this purpose in mind, we devise Voc2WordNet, an unsupervised mapping technique between a given vocabulary and WordNet, combining intensional and extensional aspects of the geographic terms. Voc2WordNet is evaluated against a sample of human-generated alignments with the OpenStreetMap (OSM) Semantic Network, a crowdsourced geospatial resource, and the GeoNames ontology, the vocabulary of a large digital gazetteer. These empirical results indicate that the approach can obtain high precision and recall.',
	 'authors': u'Andrea Ballatore, Michela Bertolotto, David C. Wilson,',
	 'category': u'Computer Science ',
	 'date': '2014-4-22',
	 'pdflink': u'http://arxiv.org/pdf/1404.5372',
	 'subjects': u'Information Retrieval (cs.IR)',
	 'title': u'\nLinking Geographic Vocabularies through WordNet',
	 'urllink': u'http://arxiv.org/abs/1404.5372'}
2015-03-23 19:56:06+0000 [xxu461000] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1405.2295> (referer: http://arxiv.org/list/cs/14?skip=5000&show=1000)
2015-03-23 19:56:06+0000 [xxu461000] DEBUG: Scraped from <200 http://arxiv.org/abs/1405.2295>
	{'abstract': u'This paper studies the gains, in terms of served requests, attainable through out-of-band device-to-device (D2D) video exchanges in large cellular networks. A stochastic framework, in which users are clustered to exchange files, is introduced, considering several aspects of this problem: the file-caching policy, user matching for exchanges, aspects regarding scheduling and transmissions. A family of admissible protocols is introduced: each protocol is composed of a clustering strategy induced by any hard-core point process, and any suitable in-cluster coordination strategy, which dictates the dynamics inside the clusters. Two metrics, quantifying the "local" and "global" fraction of video requests served through D2D are defined, and relevant trade-off regions involving these metrics, as well as quality-of-service constraints are identified. A simple coordination strategy is proposed and analyzed, to obtain inner bounds to the trade-off regions, and draw conclusions on the performance attainable through D2D. To this end, an analysis of the time-varying interference that the nodes experience, and tight approximations of its Laplace transform are derived.',
	 'authors': u'Andr\xe9s Altieri, Pablo Piantanida, Leonardo Rey Vega, Cecilia Galarza,',
	 'category': u'Computer Science ',
	 'date': '2014-5-9',
	 'pdflink': u'http://arxiv.org/pdf/1405.2295',
	 'subjects': u'Information Theory (cs.IT)',
	 'title': u'\nOn Fundamental Trade-offs of Device-to-Device Communications in Large  Wireless Networks',
	 'urllink': u'http://arxiv.org/abs/1405.2295'}
2015-03-23 19:56:11+0000 [xxu461000] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1405.7375> (referer: http://arxiv.org/list/cs/14?skip=6000&show=1000)
2015-03-23 19:56:11+0000 [xxu461000] DEBUG: Scraped from <200 http://arxiv.org/abs/1405.7375>
	{'abstract': u'The computational cost of counting the number of solutions satisfying a Boolean formula, which is a problem instance of #SAT, has proven subtle to quantify. Even when finding individual satisfying solutions is computationally easy (e.g. 2-SAT, which is in P), determining the number of solutions is #P-hard. Recently, computational methods simulating quantum systems experienced advancements due to the development of tensor network algorithms and associated quantum physics-inspired techniques. By these methods, we give an algorithm using an axiomatic tensor contraction language for n-variable #SAT instances with complexity where is the number of COPY-tensors, is the number of gates, and is the maximal degree of any COPY-tensor. Thus, counting problems can be solved efficiently when their tensor network expression has at most COPY-tensors and polynomial fan-out. This framework also admits an intuitive proof of a variant of the Tovey conjecture (the r,1-SAT instance of the Dubois-Tovey theorem). This study increases the theory, expressiveness and application of tensor based algorithmic tools and provides an alternative insight on these problems which have a long history in statistical physics and computer science.',
	 'authors': u'Jacob D. Biamonte, Jason Morton, Jacob W. Turner,',
	 'category': u'Computer Science ',
	 'date': '2014-5-28',
	 'pdflink': u'http://arxiv.org/pdf/1405.7375',
	 'subjects': u'Quantum Physics (quant-ph)',
	 'title': u'\nTensor Network Contractions for #SAT',
	 'urllink': u'http://arxiv.org/abs/1405.7375'}
2015-03-23 19:56:16+0000 [xxu461000] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1407.5238> (referer: http://arxiv.org/list/cs/14?skip=8000&show=1000)
2015-03-23 19:56:16+0000 [xxu461000] DEBUG: Scraped from <200 http://arxiv.org/abs/1407.5238>
	{'abstract': u"We examine the process of engineering features for developing models that improve our understanding of learners' online behavior in MOOCs. Because feature engineering relies so heavily on human insight, we argue that extra effort should be made to engage the crowd for feature proposals and even their operationalization. We show two approaches where we have started to engage the crowd. We also show how features can be evaluated for their relevance in predictive accuracy. When we examined crowd-sourced features in the context of predicting stopout, not only were they nuanced, but they also considered more than one interaction mode between the learner and platform and how the learner was relatively performing. We were able to identify different influential features for stop out prediction that depended on whether a learner was in 1 of 4 cohorts defined by their level of engagement with the course discussion forum or wiki. This report is part of a compendium which considers different aspects of MOOC data science and stop out prediction.",
	 'authors': u"Kalyan Veeramachaneni, Una-May O'Reilly, Colin Taylor,",
	 'category': u'Computer Science ',
	 'date': '2014-7-20',
	 'pdflink': u'http://arxiv.org/pdf/1407.5238',
	 'subjects': u'Computers and Society (cs.CY)',
	 'title': u'\nTowards Feature Engineering at Scale for Data from Massive Open Online  Courses',
	 'urllink': u'http://arxiv.org/abs/1407.5238'}
2015-03-23 19:56:22+0000 [xxu461000] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1404.5367> (referer: http://arxiv.org/list/cs/14?skip=4000&show=1000)
2015-03-23 19:56:22+0000 [xxu461000] DEBUG: Scraped from <200 http://arxiv.org/abs/1404.5367>
	{'abstract': u'Most state-of-the-art approaches for named-entity recognition (NER) use semi supervised information in the form of word clusters and lexicons. Recently neural network-based language models have been explored, as they as a byproduct generate highly informative vector representations for words, known as word embeddings. In this paper we present two contributions: a new form of learning word embeddings that can leverage information from relevant lexicons to improve the representations, and the first system to use neural word embeddings to achieve state-of-the-art results on named-entity recognition in both CoNLL and Ontonotes NER. Our system achieves an F1 score of 90.90 on the test set for CoNLL 2003---significantly better than any previous system trained on public data, and matching a system employing massive private industrial query-log data.',
	 'authors': u'Alexandre Passos, Vineet Kumar, Andrew McCallum,',
	 'category': u'Computer Science ',
	 'date': '2014-4-22',
	 'pdflink': u'http://arxiv.org/pdf/1404.5367',
	 'subjects': u'Computation and Language (cs.CL)',
	 'title': u'\nLexicon Infused Phrase Embeddings for Named Entity Resolution',
	 'urllink': u'http://arxiv.org/abs/1404.5367'}
2015-03-23 19:56:27+0000 [xxu461000] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1409.2064> (referer: http://arxiv.org/list/cs/14?skip=10000&show=1000)
2015-03-23 19:56:27+0000 [xxu461000] DEBUG: Scraped from <200 http://arxiv.org/abs/1409.2064>
	{'abstract': u'By using experimental measurements of smart grid applications compliant with IEC 61850 in trace-driven WiMAX simulations, we show that the WiMAX MAC protocol efficiency decreases as a function of the number of stations. To avoid this shortcoming, we propose and analyze a novel WiMAX MAC protocol for smart grid applications, which uses lattice correlators to improve the throughput-delay performance significantly. For the considered configurations, the obtained maximum throughput of the proposed MAC protocol outperforms the current WiMAX MAC protocol by up to 41%.',
	 'authors': u'Martin L\xe9vesque,',
	 'category': u'Computer Science ',
	 'date': '2014-9-6',
	 'pdflink': u'http://arxiv.org/pdf/1409.2064',
	 'subjects': u'Networking and Internet Architecture (cs.NI)',
	 'title': u'\nRethinking the Contention Resolution Mechanism in WiMAX Networks using  Lattice Correlators for Improved Smart Grid Communication Performance',
	 'urllink': u'http://arxiv.org/abs/1409.2064'}
2015-03-23 19:56:31+0000 [xxu461000] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1405.2294> (referer: http://arxiv.org/list/cs/14?skip=5000&show=1000)
2015-03-23 19:56:31+0000 [xxu461000] DEBUG: Scraped from <200 http://arxiv.org/abs/1405.2294>
	{'abstract': u'An anomaly detection problem is investigated, in which there are totally n sequences with s anomalous sequences to be detected. Each normal sequence contains m independent and identically distributed (i.i.d.) samples drawn from a distribution p, whereas each anomalous sequence contains m i.i.d. samples drawn from a distribution q that is distinct from p. The distributions p and q are assumed to be unknown a priori. Two scenarios, respectively with and without a reference sequence generated by p, are studied. Distribution-free tests are constructed using maximum mean discrepancy (MMD) as the metric, which is based on mean embeddings of distributions into a reproducing kernel Hilbert space (RKHS). For both scenarios, it is shown that as the number n of sequences goes to infinity, if the value of s is known, then the number m of samples in each sequence should be at the order O(log n) or larger in order for the developed tests to consistently detect s anomalous sequences. If the value of s is unknown, then m should be at the order strictly larger than O(log n). Computational complexity of all developed tests is shown to be polynomial. Numerical results demonstrate that our tests outperform (or perform as well as) the tests based on other competitive traditional statistical approaches and kernel-based approaches under various cases. Consistency of the proposed test is also demonstrated on a real data set.',
	 'authors': u'Shaofeng Zou, Yingbin Liang, H. Vincent Poor, Xinghua Shi,',
	 'category': u'Computer Science ',
	 'date': '2014-4-25',
	 'pdflink': u'http://arxiv.org/pdf/1405.2294',
	 'subjects': u'Learning (cs.LG)',
	 'title': u'\nNonparametric Detection of Anomalous Data via Kernel Mean Embedding',
	 'urllink': u'http://arxiv.org/abs/1405.2294'}
2015-03-23 19:56:36+0000 [xxu461000] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1405.7292> (referer: http://arxiv.org/list/cs/14?skip=6000&show=1000)
2015-03-23 19:56:36+0000 [xxu461000] DEBUG: Scraped from <200 http://arxiv.org/abs/1405.7292>
	{'abstract': u'The results from most machine learning experiments are used for a specific purpose and then discarded. This results in a significant loss of information and requires rerunning experiments to compare learning algorithms. This also requires implementation of another algorithm for comparison, that may not always be correctly implemented. By storing the results from previous experiments, machine learning algorithms can be compared easily and the knowledge gained from them can be used to improve their performance. The purpose of this work is to provide easy access to previous experimental results for learning and comparison. These stored results are comprehensive -- storing the prediction for each test instance as well as the learning algorithm, hyperparameters, and training set that were used. Previous results are particularly important for meta-learning, which, in a broad sense, is the process of learning from previous machine learning results such that the learning process is improved. While other experiment databases do exist, one of our focuses is on easy access to the data. We provide meta-learning data sets that are ready to be downloaded for meta-learning experiments. In addition, queries to the underlying database can be made if specific information is desired. We also differ from previous experiment databases in that our databases is designed at the instance level, where an instance is an example in a data set. We store the predictions of a learning algorithm trained on a specific training set for each instance in the test set. Data set level information can then be obtained by aggregating the results from the instances. The instance level information can be used for many tasks such as determining the diversity of a classifier or algorithmically determining the optimal subset of training instances for a learning algorithm.',
	 'authors': u'Michael R. Smith, Andrew White, Christophe Giraud-Carrier, Tony Martinez,',
	 'category': u'Computer Science ',
	 'date': '2014-5-28',
	 'pdflink': u'http://arxiv.org/pdf/1405.7292',
	 'subjects': u'Machine Learning (stat.ML)',
	 'title': u'\nAn Easy to Use Repository for Comparing and Improving Machine Learning  Algorithm Usage',
	 'urllink': u'http://arxiv.org/abs/1405.7292'}
2015-03-23 19:56:42+0000 [xxu461000] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1407.5234> (referer: http://arxiv.org/list/cs/14?skip=8000&show=1000)
2015-03-23 19:56:42+0000 [xxu461000] DEBUG: Scraped from <200 http://arxiv.org/abs/1407.5234>
	{'abstract': u'We consider the general problem of matching a subspace to a signal in R^N that has been observed indirectly (compressed) through a random projection. We are interested in the case where the collection of K-dimensional subspaces is continuously parameterized, i.e. naturally indexed by an interval from the real line, or more generally a region of R^D. Our main results show that if the dimension of the random projection is on the order of K times a geometrical constant that describes the complexity of the collection, then the match obtained from the compressed observation is nearly as good as one obtained from a full observation of the signal. We give multiple concrete examples of collections of subspaces for which this geometrical constant can be estimated, and discuss the relevance of the results to the general problems of template matching and source localization.',
	 'authors': u'William Mantzel, Justin Romberg,',
	 'category': u'Computer Science ',
	 'date': '2014-7-20',
	 'pdflink': u'http://arxiv.org/pdf/1407.5234',
	 'subjects': u'Information Theory (cs.IT)',
	 'title': u'\nCompressed Subspace Matching on the Continuum',
	 'urllink': u'http://arxiv.org/abs/1407.5234'}
2015-03-23 19:56:47+0000 [xxu461000] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1404.5357> (referer: http://arxiv.org/list/cs/14?skip=4000&show=1000)
2015-03-23 19:56:47+0000 [xxu461000] DEBUG: Scraped from <200 http://arxiv.org/abs/1404.5357>
	{'abstract': u'In this work we present a morphological analysis of Bishnupriya Manipuri language, an Indo-Aryan language spoken in the north eastern India. As of now, there is no computational work available for the language. Finite state morphology is one of the successful approaches applied in a wide variety of languages over the year. Therefore we adapted the finite state approach to analyse morphology of the Bishnupriya Manipuri language.',
	 'authors': u'Nayan Jyoti Kalita, Navanath Saharia, Smriti Kumar Sinha,',
	 'category': u'Computer Science ',
	 'date': '2014-4-22',
	 'pdflink': u'http://arxiv.org/pdf/1404.5357',
	 'subjects': u'Computation and Language (cs.CL)',
	 'title': u'\nMorphological Analysis of the Bishnupriya Manipuri Language using Finite  State Transducers',
	 'urllink': u'http://arxiv.org/abs/1404.5357'}
2015-03-23 19:56:48+0000 [xxu461000] INFO: Crawled 35 pages (at 13 pages/min), scraped 18 items (at 13 items/min)
2015-03-23 19:56:52+0000 [xxu461000] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1409.2056> (referer: http://arxiv.org/list/cs/14?skip=10000&show=1000)
2015-03-23 19:56:52+0000 [xxu461000] DEBUG: Scraped from <200 http://arxiv.org/abs/1409.2056>
	{'abstract': u"Many proofs of the fundamental theorem of algebra rely on the fact that the minimum of the modulus of a complex polynomial over the complex plane is attained at some complex number. The proof then follows by arguing the minimum value is zero. This can be done by proving that at any complex number that is not a zero of the polynomial we can exhibit a direction of descent for the modulus. In this note we present a very short and simple proof of the existence of such descent direction. In particular, our descent direction gives rise to Newton's method for solving a polynomial equation via modulus minimization and also makes the iterates definable at any critical point.",
	 'authors': u'Bahman Kalantari,',
	 'category': u'Computer Science ',
	 'date': '2014-9-6',
	 'pdflink': u'http://arxiv.org/pdf/1409.2056',
	 'subjects': u'Numerical Analysis (cs.NA)',
	 'title': u"\nA One-Line Proof of the Fundamental Theorem of Algebra with Newton's  Method as a Consequence",
	 'urllink': u'http://arxiv.org/abs/1409.2056'}
2015-03-23 19:56:54+0000 [xxu461000] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1405.2281> (referer: http://arxiv.org/list/cs/14?skip=5000&show=1000)
2015-03-23 19:56:54+0000 [xxu461000] DEBUG: Scraped from <200 http://arxiv.org/abs/1405.2281>
	{'abstract': u'This volume contains the papers accepted at the 1st Workshop on Resource Awareness and Adaptivity in Multi-Core Computing (Racing 2014), held in Paderborn, Germany, May 29-30, 2014. Racing 2014 was co-located with the IEEE European Test Symposium (ETS).',
	 'authors': u'Frank Hannig, J\xfcrgen Teich,',
	 'category': u'Computer Science ',
	 'date': '2014-5-8',
	 'pdflink': u'http://arxiv.org/html/1405.2281',
	 'subjects': u'Distributed, Parallel, and Cluster Computing (cs.DC)',
	 'title': u'\nProceedings of the First Workshop on Resource Awareness and Adaptivity  in Multi-Core Computing (Racing 2014)',
	 'urllink': u'http://arxiv.org/abs/1405.2281'}
2015-03-23 19:57:00+0000 [xxu461000] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1405.7147> (referer: http://arxiv.org/list/cs/14?skip=6000&show=1000)
2015-03-23 19:57:00+0000 [xxu461000] DEBUG: Scraped from <200 http://arxiv.org/abs/1405.7147>
	{'abstract': u'In this work, quadratic double and quadratic bordered double circulant constructions are applied to F_4 + uF_4 as well as F_4, as a result of which extremal binary self-dual codes of length 56 and 64 are obtained. The binary extension theorems as well as the ring extension version are used to obtain 7 extremal self-dual binary codes of length 58, 24 extremal self-dual binary codes of length 66 and 29 extremal self-dual binary codes of length 68, all with new weight enumerators, updating the list of all the known extremal self-dual codes in the literature.',
	 'authors': u'Abidin Kaya, Bahattin Yildiz, Irfan Siap,',
	 'category': u'Computer Science ',
	 'date': '2014-5-28',
	 'pdflink': u'http://arxiv.org/pdf/1405.7147',
	 'subjects': u'Combinatorics (math.CO)',
	 'title': u'\nNew extremal binary self-dual codes from F_4 + uF_4-lifts of quadratic  double circulant codes over F_4',
	 'urllink': u'http://arxiv.org/abs/1405.7147'}
2015-03-23 19:57:05+0000 [xxu461000] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1405.2406> (referer: http://arxiv.org/list/cs/14?skip=5000&show=1000)
2015-03-23 19:57:05+0000 [xxu461000] DEBUG: Scraped from <200 http://arxiv.org/abs/1405.2406>
	{'abstract': u"In this study, we show that a movement policy can be improved efficiently using the previous experiences of a real robot. Reinforcement Learning (RL) is becoming a popular approach to acquire a nonlinear optimal policy through trial and error. However, it is considered very difficult to apply RL to real robot control since it usually requires many learning trials. Such trials cannot be executed in real environments because unrealistic time is necessary and the real system's durability is limited. Therefore, in this study, instead of executing many learning trials, we propose to use a recently developed RL algorithm, importance-weighted PGPE, by which the robot can efficiently reuse previously sampled data to improve it's policy parameters. We apply importance-weighted PGPE to CB-i, our real humanoid robot, and show that it can learn a target reaching movement and a cart-pole swing up movement in a real environment without using any prior knowledge of the task or any carefully designed initial trajectory.",
	 'authors': u'Norikazu Sugimoto, Voot Tangkaratt, Thijs Wensveen, Tingting Zhao, Masashi Sugiyama, Jun Morimoto,',
	 'category': u'Computer Science ',
	 'date': '2014-5-10',
	 'pdflink': u'http://arxiv.org/pdf/1405.2406',
	 'subjects': u'Robotics (cs.RO)',
	 'title': u'\nEfficient Reuse of Previous Experiences to Improve Policies in Real  Environment',
	 'urllink': u'http://arxiv.org/abs/1405.2406'}
2015-03-23 19:57:10+0000 [xxu461000] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1405.2872> (referer: http://arxiv.org/list/cs/14?skip=5000&show=1000)
2015-03-23 19:57:10+0000 [xxu461000] DEBUG: Scraped from <200 http://arxiv.org/abs/1405.2872>
	{'abstract': u"Over the last 20 years significant effort has been dedicated to the development of sampling-based motion planning algorithms such as the Rapidly-exploring Random Trees (RRT) and its asymptotically optimal version (e.g. RRT*). However, asymptotic optimality for RRT* only holds for linear and fully actuated systems or for a small number of non-linear systems (e.g. Dubin's car) for which a steering function is available. The purpose of this paper is to show that asymptotically optimal motion planning for dynamical systems with differential constraints can be achieved without the use of a steering function. We develop a novel analysis on sampling-based planning algorithms that sample the control space. This analysis demonstrated that asymptotically optimal path planning for any Lipschitz continuous dynamical system can be achieved by sampling the control space directly. We also determine theoretical bounds on the convergence rates for this class of algorithms. As the number of iterations increases, the trajectory generated by these algorithms, approaches the optimal control trajectory, with probability one. Simulation results are promising.",
	 'authors': u'Georgios Papadopoulos, Hanna Kurniawati, Nicholas M. Patrikalakis,',
	 'category': u'Computer Science ',
	 'date': '2014-5-12',
	 'pdflink': u'http://arxiv.org/pdf/1405.2872',
	 'subjects': u'Robotics (cs.RO)',
	 'title': u'\nAnalysis of Asymptotically Optimal Sampling-based Motion Planning  Algorithms for Lipschitz Continuous Dynamical Systems',
	 'urllink': u'http://arxiv.org/abs/1405.2872'}
2015-03-23 19:57:14+0000 [xxu461000] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1405.2861> (referer: http://arxiv.org/list/cs/14?skip=5000&show=1000)
2015-03-23 19:57:14+0000 [xxu461000] DEBUG: Scraped from <200 http://arxiv.org/abs/1405.2861>
	{'abstract': u"Content-Centric Networking (CCN) is a communications paradigm that emphasizes content distribution. Named-Data Networking (NDN) is an instantiation of CCN, a candidate Future Internet Architecture. NDN supports human-readable content naming and router-based content caching which lends itself to efficient, secure and scalable content distribution. Because of NDN's fundamental requirement that each content object must be signed by its producer, fragmentation has been considered incompatible with NDN since it precludes authentication of individual content fragments by routers. The alternative of hop-by-hop reassembly is problematic due to the substantial incurred delay. In this paper, we show that secure and efficient content fragmentation is both possible and even advantageous in NDN and similar information-centric architectures that involve signed content. We design a concrete technique that facilitates efficient and secure content fragmentation in NDN, discuss its security guarantees and assess performance. We also describe a prototype implementation and compare performance of cut-through with hop-by-hop fragmentation and reassembly.",
	 'authors': u'Cesar Ghali, Ashok Narayanan, David Oran, Gene Tsudik,',
	 'category': u'Computer Science ',
	 'date': '2014-5-12',
	 'pdflink': u'http://arxiv.org/pdf/1405.2861',
	 'subjects': u'Networking and Internet Architecture (cs.NI)',
	 'title': u'\nSecure Fragmentation for Content-Centric Networks',
	 'urllink': u'http://arxiv.org/abs/1405.2861'}
2015-03-23 19:57:17+0000 [xxu461000] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1405.2856> (referer: http://arxiv.org/list/cs/14?skip=5000&show=1000)
2015-03-23 19:57:17+0000 [xxu461000] DEBUG: Scraped from <200 http://arxiv.org/abs/1405.2856>
	{'abstract': u'This paper maps the national UK web presence on the basis of an analysis of the .uk domain from 1996 to 2010. It reviews previous attempts to use web archives to understand national web domains and describes the dataset. Next, it presents an analysis of the .uk domain, including the overall number of links in the archive and changes in the link density of different second-level domains over time. We then explore changes over time within a particular second-level domain, the academic subdomain .ac.uk, and compare linking practices with variables, including institutional affiliation, league table ranking, and geographic location. We do not detect institutional affiliation affecting linking practices and find only partial evidence of league table ranking affecting network centrality, but find a clear inverse relationship between the density of links and the geographical distance between universities. This echoes prior findings regarding offline academic activity, which allows us to argue that real-world factors like geography continue to shape academic relationships even in the Internet age. We conclude with directions for future uses of web archive resources in this emerging area of research.',
	 'authors': u'Scott A. Hale, Taha Yasseri, Josh Cowls, Eric T. Meyer, Ralph Schroeder, Helen Margetts,',
	 'category': u'Computer Science ',
	 'date': '2014-5-12',
	 'pdflink': u'http://arxiv.org/pdf/1405.2856',
	 'subjects': u'Digital Libraries (cs.DL)',
	 'title': u'\nMapping the UK Webspace: Fifteen Years of British Universities on the  Web',
	 'urllink': u'http://arxiv.org/abs/1405.2856'}
2015-03-23 19:57:22+0000 [xxu461000] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1405.2852> (referer: http://arxiv.org/list/cs/14?skip=5000&show=1000)
2015-03-23 19:57:22+0000 [xxu461000] DEBUG: Scraped from <200 http://arxiv.org/abs/1405.2852>
	{'abstract': u'Labelled Markov chains (LMCs) are widely used in probabilistic verification, speech recognition, computational biology, and many other fields. Checking two LMCs for equivalence is a classical problem subject to extensive studies, while the total variation distance provides a natural measure for the "inequivalence" of two LMCs: it is the maximum difference between probabilities that the LMCs assign to the same event. In this paper we develop a theory of the total variation distance between two LMCs, with emphasis on the algorithmic aspects: (1) we provide a polynomial-time algorithm for determining whether two LMCs have distance 1, i.e., whether they can almost always be distinguished; (2) we provide an algorithm for approximating the distance with arbitrary precision; and (3) we show that the threshold problem, i.e., whether the distance exceeds a given threshold, is NP-hard and hard for the square-root-sum problem. We also make a connection between the total variation distance and Bernoulli convolutions.',
	 'authors': u'Taolue Chen, Stefan Kiefer,',
	 'category': u'Computer Science ',
	 'date': '2014-5-12',
	 'pdflink': u'http://arxiv.org/pdf/1405.2852',
	 'subjects': u'Logic in Computer Science (cs.LO)',
	 'title': u'\nOn the Total Variation Distance of Labelled Markov Chains',
	 'urllink': u'http://arxiv.org/abs/1405.2852'}
2015-03-23 19:57:26+0000 [xxu461000] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1405.2850> (referer: http://arxiv.org/list/cs/14?skip=5000&show=1000)
2015-03-23 19:57:26+0000 [xxu461000] DEBUG: Scraped from <200 http://arxiv.org/abs/1405.2850>
	{'abstract': u'A critical component in the implementation of a concurrent tabling system is the design of the table space. One of the most successful proposals for representing tables is based on a two-level trie data structure, where one trie level stores the tabled subgoal calls and the other stores the computed answers. In this work, we present a simple and efficient lock-free design where both levels of the tries can be shared among threads in a concurrent environment. To implement lock-freedom we took advantage of the CAS atomic instruction that nowadays can be widely found on many common architectures. CAS reduces the granularity of the synchronization when threads access concurrent areas, but still suffers from low-level problems such as false sharing or cache memory side-effects. In order to be as effective as possible in the concurrent search and insert operations over the table space data structures, we based our design on a hash trie data structure in such a way that it minimizes potential low-level synchronization problems by dispersing as much as possible the concurrent areas. Experimental results in the Yap Prolog system show that our new lock-free hash trie design can effectively reduce the execution time and scale better than previous designs.',
	 'authors': u'Miguel Areias, Ricardo Rocha,',
	 'category': u'Computer Science ',
	 'date': '2014-5-9',
	 'pdflink': u'http://arxiv.org/pdf/1405.2850',
	 'subjects': u'Programming Languages (cs.PL)',
	 'title': u'\nA Simple and Efficient Lock-Free Hash Trie Design for Concurrent Tabling',
	 'urllink': u'http://arxiv.org/abs/1405.2850'}
2015-03-23 19:57:32+0000 [xxu461000] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1405.2848> (referer: http://arxiv.org/list/cs/14?skip=5000&show=1000)
2015-03-23 19:57:32+0000 [xxu461000] DEBUG: Scraped from <200 http://arxiv.org/abs/1405.2848>
	{'abstract': u'Ontological queries are evaluated against a knowledge base consisting of an extensional database and an ontology (i.e., a set of logical assertions and constraints which derive new intensional knowledge from the extensional database), rather than directly on the extensional database. The evaluation and optimization of such queries is an intriguing new problem for database research. In this paper, we discuss two important aspects of this problem: query rewriting and query optimization. Query rewriting consists of the compilation of an ontological query into an equivalent first-order query against the underlying extensional database. We present a novel query rewriting algorithm for rather general types of ontological constraints which is well-suited for practical implementations. In particular, we show how a conjunctive query against a knowledge base, expressed using linear and sticky existential rules, that is, members of the recently introduced Datalog+/- family of ontology languages, can be compiled into a union of conjunctive queries (UCQ) against the underlying database. Ontological query optimization, in this context, attempts to improve this rewriting process so to produce possibly small and cost-effective UCQ rewritings for an input query.',
	 'authors': u'Georg Gottlob, Giorgio Orsi, Andreas Pieris,',
	 'category': u'Computer Science ',
	 'date': '2014-5-12',
	 'pdflink': u'http://arxiv.org/pdf/1405.2848',
	 'subjects': u'Databases (cs.DB)',
	 'title': u'\nQuery Rewriting and Optimization for Ontological Databases',
	 'urllink': u'http://arxiv.org/abs/1405.2848'}
2015-03-23 19:57:35+0000 [xxu461000] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1405.2833> (referer: http://arxiv.org/list/cs/14?skip=5000&show=1000)
2015-03-23 19:57:35+0000 [xxu461000] DEBUG: Scraped from <200 http://arxiv.org/abs/1405.2833>
	{'abstract': u'Distributed (Cloud) Storage Systems (DSS) exhibit heterogeneity in several dimensions such as the volume (size) of data, frequency of data access and the desired degree of reliability. Ultimately, the complex interplay between these dimensions impacts the latency performance of cloud storage systems. To this end, we propose and analyze a heterogeneous distributed storage model in which storage servers (disks) store the data of distinct classes. Data of class is encoded using a erasure code and the (random) data retrieval requests can also vary from class to class. We present a queuing theoretic analysis of the proposed model and establish upper and lower bounds on the average latency for each data class under various scheduling policies for data retrieval. Using simulations, we verify the accuracy of the proposed bounds and present qualitative insights which reveal the impact of heterogeneity and scheduling policies on the mean latency of different data classes. Lastly, we conclude with a discussion on per-class fairness in heterogeneous DSS.',
	 'authors': u'Akshay Kumar, Ravi Tandon, T. Charles Clancy,',
	 'category': u'Computer Science ',
	 'date': '2014-5-12',
	 'pdflink': u'http://arxiv.org/pdf/1405.2833',
	 'subjects': u'Distributed, Parallel, and Cluster Computing (cs.DC)',
	 'title': u'\nOn the Latency of Erasure-Coded Cloud Storage Systems',
	 'urllink': u'http://arxiv.org/abs/1405.2833'}
2015-03-23 19:57:39+0000 [xxu461000] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1405.2826> (referer: http://arxiv.org/list/cs/14?skip=5000&show=1000)
2015-03-23 19:57:39+0000 [xxu461000] DEBUG: Scraped from <200 http://arxiv.org/abs/1405.2826>
	{'abstract': u"Public transit systems in urban areas usually require large state subsidies, primarily due to high fare evasion rates. In this paper, we study new models for optimizing fare inspection strategies in transit networks based on bilevel programming. In the first level, the leader (the network operator) determines probabilities for inspecting passengers at different locations, while in the second level, the followers (the fare-evading passengers) respond by optimizing their routes given the inspection probabilities and travel times. To model the followers' behavior we study both a non-adaptive variant, in which passengers select a path a priori and continue along it throughout their journey, and an adaptive variant, in which they gain information along the way and use it to update their route. For these problems, which are interesting in their own right, we design exact and approximation algorithms and we prove a tight bound of 3/4 on the ratio of the optimal cost between adaptive and non-adaptive strategies. For the leader's optimization problem, we study a fixed-fare and a flexible-fare variant, where ticket prices may or may not be set at the operator's will. For the latter variant, we design an LP based approximation algorithm. Finally, using a local search procedure that shifts inspection probabilities within an initially determined support set, we perform an extensive computational study for all variants of the problem on instances of the Dutch railway and the Amsterdam subway network. This study reveals that our solutions are within 95% of theoretical upper bounds drawn from the LP relaxation.",
	 'authors': u'Jos\xe9 R. Correa, Tobias Harks, Vincent J.C. Kreuzen, Jannik Matuschke,',
	 'category': u'Computer Science ',
	 'date': '2014-5-12',
	 'pdflink': u'http://arxiv.org/pdf/1405.2826',
	 'subjects': u'Computer Science and Game Theory (cs.GT)',
	 'title': u'\nFare Evasion in Transit Networks',
	 'urllink': u'http://arxiv.org/abs/1405.2826'}
2015-03-23 19:57:41+0000 [xxu461000] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1406.0554> (referer: http://arxiv.org/list/cs/14?skip=6000&show=1000)
2015-03-23 19:57:41+0000 [xxu461000] DEBUG: Scraped from <200 http://arxiv.org/abs/1406.0554>
	{'abstract': u'We develop a framework for convexifying a fairly general class of optimization problems. Under additional assumptions, we analyze the suboptimality of the solution to the convexified problem relative to the original nonconvex problem and prove additive approximation guarantees. We then develop algorithms based on stochastic gradient methods to solve the resulting optimization problems and show bounds on convergence rates. %We show a simple application of this framework to supervised learning, where one can perform integration explicitly and can use standard (non-stochastic) optimization algorithms with better convergence guarantees. We then extend this framework to apply to a general class of discrete-time dynamical systems. In this context, our convexification approach falls under the well-studied paradigm of risk-sensitive Markov Decision Processes. We derive the first known model-based and model-free policy gradient optimization algorithms with guaranteed convergence to the optimal solution. Finally, we present numerical results validating our formulation in different applications.',
	 'authors': u'Krishnamurthy Dvijotham, Maryam Fazel, Emanuel Todorov,',
	 'category': u'Computer Science ',
	 'date': '2014-6-3',
	 'pdflink': u'http://arxiv.org/pdf/1406.0554',
	 'subjects': u'Systems and Control (cs.SY)',
	 'title': u'\nUniversal Convexification via Risk-Aversion',
	 'urllink': u'http://arxiv.org/abs/1406.0554'}
2015-03-23 19:57:47+0000 [xxu461000] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1405.2822> (referer: http://arxiv.org/list/cs/14?skip=5000&show=1000)
2015-03-23 19:57:47+0000 [xxu461000] DEBUG: Scraped from <200 http://arxiv.org/abs/1405.2822>
	{'abstract': u'Dynamic spectrum sharing is a promising technology for improving the spectrum utilization. In this paper, we study how secondary users can share the spectrum in a distributed fashion based on social imitations. The imitation-based mechanism leverages the social intelligence of the secondary user crowd and only requires a low computational power for each individual user. We introduce the information sharing graph to model the social information sharing relationship among the secondary users. We propose an imitative spectrum access mechanism on a general information sharing graph such that each secondary user first estimates its expected throughput based on local observations, and then imitates the channel selection of another neighboring user who achieves a higher throughput. We show that the imitative spectrum access mechanism converges to an imitation equilibrium, where no beneficial imitation can be further carried out on the time average. Numerical results show that the imitative spectrum access mechanism can achieve efficient spectrum utilization and meanwhile provide good fairness across secondary users.',
	 'authors': u'Xu Chen, Jianwei Huang,',
	 'category': u'Computer Science ',
	 'date': '2014-5-12',
	 'pdflink': u'http://arxiv.org/pdf/1405.2822',
	 'subjects': u'Networking and Internet Architecture (cs.NI)',
	 'title': u'\nImitation-based Social Spectrum Sharing',
	 'urllink': u'http://arxiv.org/abs/1405.2822'}
2015-03-23 19:57:48+0000 [xxu461000] INFO: Crawled 49 pages (at 14 pages/min), scraped 32 items (at 14 items/min)
2015-03-23 19:57:52+0000 [xxu461000] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1406.0532> (referer: http://arxiv.org/list/cs/14?skip=6000&show=1000)
2015-03-23 19:57:52+0000 [xxu461000] DEBUG: Scraped from <200 http://arxiv.org/abs/1406.0532>
	{'abstract': u"(arXiv abridged abstract) In the last two decades, videogames have evolved in a nearly explosive way from the pixelated graphics to today's near-realistic 3D environments. The interaction devices traditionally used in videogames have not evolved with the same intensity, but recent HCI studies have explored biofeedback interaction - the explicit manipulation of a person's physiological data as input to a system - as an alternative to them. Traditional biofeedback prototypes apply 1 sensor to each game mechanic (unimodality). In this dissertation, we introduce the combination of 2 physiological sensors simultaneously per game mechanic (multimodality) and present a First-Person Shooter game comprised of 8 game mechanics with three interaction flavours (no biofeedback/vanilla, unimodal and multimodal). An empirical study with 32 regular players was employed to explore and study differences between the three interaction types and where they can be best employed. Players compared the three games in terms of Fun, Ease of Use, Originality, Playability and Favourite Condition. For the sake of completeness, other evaluation methods were used as well: IMI Questionnaire, keywords association and open-ended commentaries. The vanilla version was considered easier to use, but both biofeedback versions were considered the most fun. Both versions were praised differently: the unimodal version for its simplicity of use, and the multimodal for its realism, activation safety of game mechanics and depth added to the game. Our conclusion is that multimodal biofeedback can have a relevant impact in terms of added depth, depending on the way it is used inside the game. On a boundary case, it can be used to increase the feeling of empowerment on the player when using certain abilities, or to intentionally make in-game actions more difficult by demanding more physical effort from the player.",
	 'authors': u'Gon\xe7alo Amaral da Silva,',
	 'category': u'Computer Science ',
	 'date': '2014-6-2',
	 'pdflink': u'http://arxiv.org/pdf/1406.0532',
	 'subjects': u'Human-Computer Interaction (cs.HC)',
	 'title': u'\nMultimodal vs. Unimodal Physiological Control in Videogames for Enhanced  Realism and Depth',
	 'urllink': u'http://arxiv.org/abs/1406.0532'}
2015-03-23 19:57:55+0000 [xxu461000] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1405.2820> (referer: http://arxiv.org/list/cs/14?skip=5000&show=1000)
2015-03-23 19:57:55+0000 [xxu461000] DEBUG: Scraped from <200 http://arxiv.org/abs/1405.2820>
	{'abstract': u'We consider a bound on the bias reduction of a random number generator by processing based on binary linear codes. We introduce a new bound on the total variation distance of the processed output based on the weight distribution of the code generated by the chosen binary matrix. Starting from this result we show a lower bound for the entropy rate of the output of linear binary extractors.',
	 'authors': u'Alessio Meneghetti, Massimiliano Sala, Alessandro Tomasi,',
	 'category': u'Computer Science ',
	 'date': '2014-5-12',
	 'pdflink': u'http://arxiv.org/pdf/1405.2820',
	 'subjects': u'Information Theory (cs.IT)',
	 'title': u'\nA weight-distribution bound for entropy extractors using linear binary  codes',
	 'urllink': u'http://arxiv.org/abs/1405.2820'}
2015-03-23 19:57:59+0000 [xxu461000] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1406.0516> (referer: http://arxiv.org/list/cs/14?skip=6000&show=1000)
2015-03-23 19:57:59+0000 [xxu461000] DEBUG: Scraped from <200 http://arxiv.org/abs/1406.0516>
	{'abstract': u"The emergence and wide-spread use of social networks and microblogging sites has led to a dramatic increase on the availability of users' activity data. Importantly, this data can be exploited to solve some of the problems that have captured the attention of economists and marketers for decades as, e.g., product adoption, product competition and product life cycle. In this paper, we leverage on users' activity data from a popular microblogging site to model and predict the competing dynamics of products and social conventions adoptions. To this aim, we propose a data-driven model, based on continuous-time Hawkes processes, for the adoption and frequency of use of competing products and conventions. We then develop an inference method to efficiently fit the model parameters by solving a convex program. The problem decouples into a collection of smaller subproblems, thus scaling easily to networks with hundred of thousands of nodes. We validate our method over synthetic and real diffusion data gathered from Twitter, and show that the proposed model does not only present a good predictive power but also provides interpretable model parameters, which allow us to gain insights into the fundamental principles that drive product and convention adoptions.",
	 'authors': u'Isabel Valera, Manuel Gomez-Rodriguez, Krishna Gummadi,',
	 'category': u'Computer Science ',
	 'date': '2014-6-2',
	 'pdflink': u'http://arxiv.org/pdf/1406.0516',
	 'subjects': u'Social and Information Networks (cs.SI)',
	 'title': u'\nModeling Diffusion of Competing Products and Conventions in Social Media',
	 'urllink': u'http://arxiv.org/abs/1406.0516'}
2015-03-23 19:58:05+0000 [xxu461000] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1405.2816> (referer: http://arxiv.org/list/cs/14?skip=5000&show=1000)
2015-03-23 19:58:05+0000 [xxu461000] DEBUG: Scraped from <200 http://arxiv.org/abs/1405.2816>
	{'abstract': u'This paper proposes a cooperation protocol between a secondary user (SU) and a primary user (PU) which dedicates a free frequency subband for the SU if cooperation results in energy saving. Time is slotted and users are equipped with buffers. Under the proposed protocol, the PU releases portion of its bandwidth for secondary transmission. Moreover, it assigns a portion of the time slot duration for the SU to relay primary packets and achieve a higher successful packet reception probability at the primary receiver. We assume that the PU has three states: idle, forward, and retransmission states. At each of these states, the SU accesses the channel with adaptive transmission parameters. The PU cooperates with the SU if and only if the achievable average number of transmitted primary packets per joule is higher than the number of transmitted packets per joule when it operates alone. The numerical results show the beneficial gains of the proposed cooperative cognitive protocol.',
	 'authors': u'Ahmed El Shafie, Ahmed Sultan, Tamer Khattab,',
	 'category': u'Computer Science ',
	 'date': '2014-5-12',
	 'pdflink': u'http://arxiv.org/pdf/1405.2816',
	 'subjects': u'Networking and Internet Architecture (cs.NI)',
	 'title': u'\nMaximum Throughput of a Secondary User Cooperating with an Energy-Aware  Primary User',
	 'urllink': u'http://arxiv.org/abs/1405.2816'}
2015-03-23 19:58:10+0000 [xxu461000] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1406.0495> (referer: http://arxiv.org/list/cs/14?skip=6000&show=1000)
2015-03-23 19:58:10+0000 [xxu461000] DEBUG: Scraped from <200 http://arxiv.org/abs/1406.0495>
	{'abstract': u"This article describes the recognition part of a system that will be used for personalized therapy of dyslalia affecting pre scholars. Dyslalia is a speech disorder that affect pronunciation of one ore many sounds. The full system targets interdisciplinary research (computer science, psychology, electronics) - having as main objective the development of methods, models, algorithms, System on Chip architectures with regards to the elaboration and implementation of a complete system addressing the therapy of dyslalia affecting pre scholars, in a personalized and user centered manner. The system addresses the number of 10% of children with age between 4 and 7 that, according to the statistics, present different variations of speech impairments. Although these impairments do not create major difficulties concerning common communication, it has been noticed that problems are likely to appear affecting negatively the child's personality as well as his social environment.",
	 'authors': u'Stefan-Gheorghe Pentiuc, Ovidiu-Andrei Schipor, Mirela Danubianu, Doina-Maria Schipor,',
	 'category': u'Computer Science ',
	 'date': '2014-5-29',
	 'pdflink': u'http://arxiv.org/pdf/1406.0495',
	 'subjects': u'Computers and Society (cs.CY)',
	 'title': u'\nAutomatic Recognition of Dyslalia Affecting Pre-Scholars',
	 'urllink': u'http://arxiv.org/abs/1406.0495'}
2015-03-23 19:58:14+0000 [xxu461000] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1405.2815> (referer: http://arxiv.org/list/cs/14?skip=5000&show=1000)
2015-03-23 19:58:14+0000 [xxu461000] DEBUG: Scraped from <200 http://arxiv.org/abs/1405.2815>
	{'abstract': u"In this work, we study the problem of band allocation of buffered secondary users (SUs) to primary bands licensed to (owned by) buffered primary users (PUs). The bands are assigned to SUs in an orthogonal (one-to-one) fashion such that neither band sharing nor multi-band allocations are permitted. In order to study the stability region of the secondary network, the optimization problem used to obtain the stability region's envelope (closure) is established and is shown to be a linear program which can be solved efficiently and reliably. We compare our orthogonal allocation system with two typical low-complexity and intuitive band allocation systems. In one system, each cognitive user chooses a band randomly in each time slot with some assignment probability designed such that the system maintained stable, while in the other system fixed (deterministic) band assignment is adopted throughout the lifetime of the network. We derive the stability regions of these two systems. We prove mathematically, as well as through numerical results, the advantages of our proposed orthogonal system over the other two systems.",
	 'authors': u'Ahmed El Shafie, Tamer Khattab,',
	 'category': u'Computer Science ',
	 'date': '2014-5-12',
	 'pdflink': u'http://arxiv.org/pdf/1405.2815',
	 'subjects': u'Information Theory (cs.IT)',
	 'title': u'\nOn Orthogonal Band Allocation for Multi-User Multi-Band Cognitive Radio  Networks: Stability Analysis',
	 'urllink': u'http://arxiv.org/abs/1405.2815'}
2015-03-23 19:58:19+0000 [xxu461000] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1406.0492> (referer: http://arxiv.org/list/cs/14?skip=6000&show=1000)
2015-03-23 19:58:19+0000 [xxu461000] DEBUG: Scraped from <200 http://arxiv.org/abs/1406.0492>
	{'abstract': u'We present a new exact algorithm for the Steiner tree problem in graphs which is based on dynamic programming. Known empirically fast algorithms are primarily based on reductions, heuristics and branching. Our algorithm combines the best known worst-case run time with a fast, often superior, practical performance.',
	 'authors': u'Stefan Hougardy, Jannik Silvanus, Jens Vygen,',
	 'category': u'Computer Science ',
	 'date': '2014-6-2',
	 'pdflink': u'http://arxiv.org/pdf/1406.0492',
	 'subjects': u'Data Structures and Algorithms (cs.DS)',
	 'title': u'\nDijkstra meets Steiner: a fast exact goal-oriented Steiner tree  algorithm',
	 'urllink': u'http://arxiv.org/abs/1406.0492'}
2015-03-23 19:58:25+0000 [xxu461000] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1405.2814> (referer: http://arxiv.org/list/cs/14?skip=5000&show=1000)
2015-03-23 19:58:25+0000 [xxu461000] DEBUG: Scraped from <200 http://arxiv.org/abs/1405.2814>
	{'abstract': u'In this paper, we propose a cognitive protocol that involves cooperation between the primary and secondary users. In addition to its own queue, the secondary user (SU) has a queue to store, and then relay, the undelivered primary packets. When the primary queue is nonempty, the SU remains idle and attempts to decode the primary packet. When the primary queue is empty, the SU splits the total channel bandwidth into two orthogonal subbands and assigns each to a queue probabilistically. We show the advantage of the proposed protocol over the prioritized cognitive relaying (PCR) protocol in which the SU assigns a priority in transmission to the primary packets over its own packets. We present two problem formulations, one based on throughput and the other on delay. Both optimization problems are shown to be linear programs for a given bandwidth assignment. Numerical results demonstrate the benefits of the proposed protocol.',
	 'authors': u'Ahmed El Shafie, Ahmed Sultan, Tamer Khattab,',
	 'category': u'Computer Science ',
	 'date': '2014-5-12',
	 'pdflink': u'http://arxiv.org/pdf/1405.2814',
	 'subjects': u'Networking and Internet Architecture (cs.NI)',
	 'title': u'\nProbabilistic Band-Splitting for a Buffered Cooperative Cognitive  Terminal',
	 'urllink': u'http://arxiv.org/abs/1405.2814'}
2015-03-23 19:58:28+0000 [xxu461000] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1406.0486> (referer: http://arxiv.org/list/cs/14?skip=6000&show=1000)
2015-03-23 19:58:28+0000 [xxu461000] DEBUG: Scraped from <200 http://arxiv.org/abs/1406.0486>
	{'abstract': u'Monte Carlo Tree Search (MCTS) has improved the performance of game engines in domains such as Go, Hex, and general game playing. MCTS has been shown to outperform classic alpha-beta search in games where good heuristic evaluations are difficult to obtain. In recent years, combining ideas from traditional minimax search in MCTS has been shown to be advantageous in some domains, such as Lines of Action, Amazons, and Breakthrough. In this paper, we propose a new way to use heuristic evaluations to guide the MCTS search by storing the two sources of information, estimated win rates and heuristic evaluations, separately. Rather than using the heuristic evaluations to replace the playouts, our technique backs them up implicitly during the MCTS simulations. These minimax values are then used to guide future simulations. We show that using implicit minimax backups leads to stronger play performance in Kalah, Breakthrough, and Lines of Action.',
	 'authors': u'Marc Lanctot, Mark H.M. Winands, Tom Pepels, Nathan R. Sturtevant,',
	 'category': u'Computer Science ',
	 'date': '2014-6-2',
	 'pdflink': u'http://arxiv.org/pdf/1406.0486',
	 'subjects': u'Artificial Intelligence (cs.AI)',
	 'title': u'\nMonte Carlo Tree Search with Heuristic Evaluations using Implicit  Minimax Backups',
	 'urllink': u'http://arxiv.org/abs/1406.0486'}
2015-03-23 19:58:34+0000 [xxu461000] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1405.2809> (referer: http://arxiv.org/list/cs/14?skip=5000&show=1000)
2015-03-23 19:58:34+0000 [xxu461000] DEBUG: Scraped from <200 http://arxiv.org/abs/1405.2809>
	{'abstract': u"The effect of the primary traffic on a secondary user's (SU) throughput under Rayleigh flat fading channel is investigated. For this case, closed form expressions are derived for the average probability of detection and the average probability of false alarm. Based on these expressions, the average SU throughput under the desired signal-to-noise ratio (SNR) constraint in order to maintain the quality of the secondary link is found analytically considering the random arrival or departure of the primary user. It is shown that the spectrum sensing performance and SU throughput degrade with increase in the primary traffic and the deep fade condition of the channel over which the detection is performed. The degree of degradation in SU throughput is seen to be severed further due to the interference link from the primary transmitter to the secondary receiver. Under these detrimental effects, a sensing-throughput trade-off for SU is illustrated. Finally, the combined effect of the primary traffic, fading, imperfect spectrum sensing and the interference link from a primary transmitter is studied on the outage probability at SU.",
	 'authors': u'Sanket Kalamkar, Adrish Banerjee,',
	 'category': u'Computer Science ',
	 'date': '2014-5-12',
	 'pdflink': u'http://arxiv.org/pdf/1405.2809',
	 'subjects': u'Information Theory (cs.IT)',
	 'title': u'\nOn the Effect of Primary User Traffic on Secondary Throughput and Outage  Probability Under Rayleigh Flat Fading Channel',
	 'urllink': u'http://arxiv.org/abs/1405.2809'}
2015-03-23 19:58:40+0000 [xxu461000] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1406.0455> (referer: http://arxiv.org/list/cs/14?skip=6000&show=1000)
2015-03-23 19:58:40+0000 [xxu461000] DEBUG: Scraped from <200 http://arxiv.org/abs/1406.0455>
	{'abstract': u'The majority of recommender systems are designed to recommend items (such as movies and products) to users. We focus on the problem of recommending buyers to sellers which comes with new challenges: (1) constraints on the number of recommendations buyers are part of before they become overwhelmed, (2) constraints on the number of recommendations sellers receive within their budget, and (3) constraints on the set of buyers that sellers want to receive (e.g., no more than two people from the same household). We propose the following critical problems of recommending buyers to sellers: Constrained Recommendation (C-REC) capturing the first two challenges, and Conflict-Aware Constrained Recommendation (CAC-REC) capturing all three challenges at the same time. We show that C-REC can be modeled using linear programming and can be efficiently solved using modern solvers. On the other hand, we show that CAC-REC is NP-hard. We propose two approximate algorithms to solve CAC-REC and show that they achieve close to optimal solutions via comprehensive experiments using real-world datasets.',
	 'authors': u'Cheng Chen, Lan Zheng, Venkatesh Srinivasan, Alex Thomo, Kui Wu, Anthony Sukow,',
	 'category': u'Computer Science ',
	 'date': '2014-6-2',
	 'pdflink': u'http://arxiv.org/pdf/1406.0455',
	 'subjects': u'Social and Information Networks (cs.SI)',
	 'title': u'\nBuyer to Seller Recommendation under Constraints',
	 'urllink': u'http://arxiv.org/abs/1406.0455'}
2015-03-23 19:58:46+0000 [xxu461000] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1405.2806> (referer: http://arxiv.org/list/cs/14?skip=5000&show=1000)
2015-03-23 19:58:46+0000 [xxu461000] DEBUG: Scraped from <200 http://arxiv.org/abs/1405.2806>
	{'abstract': u'In order to operate an electrical distribution network in a secure and cost-efficient way, it is necessary, due to the rise of renewable energy-based distributed generation, to develop Active Network Management (ANM) strategies. These strategies rely on short-term policies that control the power injected by generators and/or taken off by loads in order to avoid congestion or voltage problems. While simple ANM strategies would curtail the production of generators, more advanced ones would move the consumption of loads to relevant time periods to maximize the potential of renewable energy sources. However, such advanced strategies imply solving large-scale optimal sequential decision-making problems under uncertainty, something that is understandably complicated. In order to promote the development of computational techniques for active network management, we detail a generic procedure for formulating ANM decision problems as Markov decision processes. We also specify it to a 75-bus distribution network. The resulting test instance is available at this http URL It can be used as a test bed for comparing existing computational techniques, as well as for developing new ones. A solution technique that consists in an approximate multistage program is also illustrated on the test instance.',
	 'authors': u'Quentin Gemine, Damien Ernst, Bertrand Corn\xe9lusse,',
	 'category': u'Computer Science ',
	 'date': '2014-5-12',
	 'pdflink': u'http://arxiv.org/pdf/1405.2806',
	 'subjects': u'Systems and Control (cs.SY)',
	 'title': u'\nActive network management for electrical distribution systems: problem  formulation and benchmark',
	 'urllink': u'http://arxiv.org/abs/1405.2806'}
2015-03-23 19:58:48+0000 [xxu461000] INFO: Crawled 61 pages (at 12 pages/min), scraped 44 items (at 12 items/min)
2015-03-23 19:58:51+0000 [xxu461000] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1406.0440> (referer: http://arxiv.org/list/cs/14?skip=6000&show=1000)
2015-03-23 19:58:51+0000 [xxu461000] DEBUG: Scraped from <200 http://arxiv.org/abs/1406.0440>
	{'abstract': u"Software-Defined Networking (SDN) is an emerging paradigm that promises to change this state of affairs, by breaking vertical integration, separating the network's control logic from the underlying routers and switches, promoting (logical) centralization of network control, and introducing the ability to program the network. The separation of concerns introduced between the definition of network policies, their implementation in switching hardware, and the forwarding of traffic, is key to the desired flexibility: by breaking the network control problem into tractable pieces, SDN makes it easier to create and introduce new abstractions in networking, simplifying network management and facilitating network evolution. In this paper we present a comprehensive survey on SDN. We start by introducing the motivation for SDN, explain its main concepts and how it differs from traditional networking, its roots, and the standardization activities regarding this novel paradigm. Next, we present the key building blocks of an SDN infrastructure using a bottom-up, layered approach. We provide an in-depth analysis of the hardware infrastructure, southbound and northbound APIs, network virtualization layers, network operating systems (SDN controllers), network programming languages, and network applications. We also look at cross-layer problems such as debugging and troubleshooting. In an effort to anticipate the future evolution of this new paradigm, we discuss the main ongoing research efforts and challenges of SDN. In particular, we address the design of switches and control platforms -- with a focus on aspects such as resiliency, scalability, performance, security and dependability -- as well as new opportunities for carrier transport networks and cloud providers. Last but not least, we analyze the position of SDN as a key enabler of a software-defined environment.",
	 'authors': u'Diego Kreutz, Fernando M. V. Ramos, Paulo Verissimo, Christian Esteve Rothenberg, Siamak Azodolmolky, Steve Uhlig,',
	 'category': u'Computer Science ',
	 'date': '2014-6-2',
	 'pdflink': u'http://arxiv.org/pdf/1406.0440',
	 'subjects': u'Networking and Internet Architecture (cs.NI)',
	 'title': u'\nSoftware-Defined Networking: A Comprehensive Survey',
	 'urllink': u'http://arxiv.org/abs/1406.0440'}
2015-03-23 19:58:56+0000 [xxu461000] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1405.2801> (referer: http://arxiv.org/list/cs/14?skip=5000&show=1000)
2015-03-23 19:58:56+0000 [xxu461000] DEBUG: Scraped from <200 http://arxiv.org/abs/1405.2801>
	{'abstract': u'This paper introduces a new constraint domain for reasoning about data with uncertainty. It extends convex modeling with the notion of p-box to gain additional quantifiable information on the data whereabouts. Unlike existing approaches, the p-box envelops an unknown probability instead of approximating its representation. The p-box bounds are uniform cumulative distribution functions (cdf) in order to employ linear computations in the probabilistic domain. The reasoning by means of p-box cdf-intervals is an interval computation which is exerted on the real domain then it is projected onto the cdf domain. This operation conveys additional knowledge represented by the obtained probabilistic bounds. The empirical evaluation of our implementation shows that, with minimal overhead, the output solution set realizes a full enclosure of the data along with tighter bounds on its probabilistic distributions.',
	 'authors': u'Aya Saad, Thom Fruehwirth, Carmen Gervet,',
	 'category': u'Computer Science ',
	 'date': '2014-5-12',
	 'pdflink': u'http://arxiv.org/pdf/1405.2801',
	 'subjects': u'Logic in Computer Science (cs.LO)',
	 'title': u'\nThe P-Box CDF-Intervals: Reliable Constraint Reasoning with Quantifiable  Information',
	 'urllink': u'http://arxiv.org/abs/1405.2801'}
2015-03-23 19:59:01+0000 [xxu461000] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1406.0435> (referer: http://arxiv.org/list/cs/14?skip=6000&show=1000)
2015-03-23 19:59:01+0000 [xxu461000] DEBUG: Scraped from <200 http://arxiv.org/abs/1406.0435>
	{'abstract': u'The relational DBMS (RDBMS) has been widely used since it supports various high-level functionalities such as SQL, schemas, indexes, and transactions that do not exist in the O/S file system. But, a recent advent of big data technology facilitates development of new systems that sacrifice the DBMS functionality in order to efficiently manage large-scale data. Those so-called NoSQL systems use a distributed file system, which support scalability and reliability. They support scalability of the system by storing data into a large number of low-cost commodity hardware and support reliability by storing the data in replica. However, they have a drawback that they do not adequately support high-level DBMS functionality. In this paper, we propose an architecture of a DBMS that uses the DFS as storage. With this novel architecture, the DBMS is capable of supporting scalability and reliability of the DFS as well as high-level functionality of DBMS. Thus, a DBMS can utilize a virtually unlimited storage space provided by the DFS, rendering it to be suitable for big data analytics. As part of the architecture of the DBMS, we propose the notion of the meta DFS file, which allows the DBMS to use the DFS as the storage, and an efficient transaction management method including recovery and concurrency control. We implement this architecture in Odysseus/DFS, an integration of the Odysseus relational DBMS, that has been being developed at KAIST for over 24 years, with the DFS. Our experiments on transaction processing show that, due to the high-level functionality of Odysseus/DFS, it outperforms Hbase, which is a representative open-source NoSQL system. We also show that, compared with an RDBMS with local storage, the performance of Odysseus/DFS is comparable or marginally degraded, showing that the overhead of Odysseus/DFS for supporting scalability by using the DFS as the storage is not significant.',
	 'authors': u'Jun-Sung Kim, Kyu-Young Whang, Hyuk-Yoon Kwon, Il-Yeol Song,',
	 'category': u'Computer Science ',
	 'date': '2014-6-2',
	 'pdflink': u'http://arxiv.org/pdf/1406.0435',
	 'subjects': u'Databases (cs.DB)',
	 'title': u'\nOdysseus/DFS: Integration of DBMS and Distributed File System for  Transaction Processing of Big Data',
	 'urllink': u'http://arxiv.org/abs/1406.0435'}
2015-03-23 19:59:06+0000 [xxu461000] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1405.2798> (referer: http://arxiv.org/list/cs/14?skip=5000&show=1000)
2015-03-23 19:59:06+0000 [xxu461000] DEBUG: Scraped from <200 http://arxiv.org/abs/1405.2798>
	{'abstract': u'In this paper, we present a novel two-stage metric learning algorithm. We first map each learning instance to a probability distribution by computing its similarities to a set of fixed anchor points. Then, we define the distance in the input data space as the Fisher information distance on the associated statistical manifold. This induces in the input data space a new family of distance metric with unique properties. Unlike kernelized metric learning, we do not require the similarity measure to be positive semi-definite. Moreover, it can also be interpreted as a local metric learning algorithm with well defined distance approximation. We evaluate its performance on a number of datasets. It outperforms significantly other metric learning methods and SVM.',
	 'authors': u'Jun Wang, Ke Sun, Fei Sha, Stephane Marchand-Maillet, Alexandros Kalousis,',
	 'category': u'Computer Science ',
	 'date': '2014-5-12',
	 'pdflink': u'http://arxiv.org/pdf/1405.2798',
	 'subjects': u'Learning (cs.LG)',
	 'title': u'\nTwo-Stage Metric Learning',
	 'urllink': u'http://arxiv.org/abs/1405.2798'}
2015-03-23 19:59:11+0000 [xxu461000] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1406.0416> (referer: http://arxiv.org/list/cs/14?skip=6000&show=1000)
2015-03-23 19:59:11+0000 [xxu461000] DEBUG: Scraped from <200 http://arxiv.org/abs/1406.0416>
	{'abstract': u'Within the context of evolution, an altruistic act that benefits the receiving individual at the expense of the acting individual is a puzzling phenomenon. An extreme form of altruism can be found in colicinogenic E. coli. These suicidal altruists explode, releasing colicins that kill unrelated individuals, which are not colicin resistant. By committing suicide, the altruist makes it more likely that its kin will have less competition. The benefits of this strategy rely on the number of competitors and kin nearby. If the organism explodes at an inopportune time, the suicidal act may not harm any competitors. Communication could enable organisms to act altruistically when environmental conditions suggest that that strategy would be most beneficial. Quorum sensing is a form of communication in which bacteria produce a protein and gauge the amount of that protein around them. Quorum sensing is one means by which bacteria sense the biotic factors around them and determine when to produce products, such as antibiotics, that influence competition. Suicidal altruists could use quorum sensing to determine when exploding is most beneficial, but it is challenging to study the selective forces at work in microbes. To address these challenges, we use digital evolution (a form of experimental evolution that uses self-replicating computer programs as organisms) to investigate the effects of enabling altruistic organisms to communicate via quorum sensing. We found that quorum-sensing altruists killed a greater number of competitors per explosion, winning competitions against non-communicative altruists. These findings indicate that quorum sensing could increase the beneficial effect of altruism and the suite of conditions under which it will evolve.',
	 'authors': u'Anya Elaine Johnson, Eli Strauss, Rodney Pickett, Christoph Adami, Ian Dworkin, Heather J. Goldsby,',
	 'category': u'Computer Science ',
	 'date': '2014-6-2',
	 'pdflink': u'http://arxiv.org/pdf/1406.0416',
	 'subjects': u'Neural and Evolutionary Computing (cs.NE)',
	 'title': u'\nMore Bang For Your Buck: Quorum-Sensing Capabilities Improve the  Efficacy of Suicidal Altruism',
	 'urllink': u'http://arxiv.org/abs/1406.0416'}
2015-03-23 19:59:17+0000 [xxu461000] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1405.2795> (referer: http://arxiv.org/list/cs/14?skip=5000&show=1000)
2015-03-23 19:59:17+0000 [xxu461000] DEBUG: Scraped from <200 http://arxiv.org/abs/1405.2795>
	{'abstract': u'In this paper, the dynamic modeling of a seven linked humanoid robot, is accurately developed, in the three dimensional space using the Newton-Euler formalism. The aim of this study is to provide a clear and a systematic approach so that starting from generalized motion equations of all rigid bodies of the humanoid robot one can establish a reduced dynamical model. The resulting model can be expended either for simulation propositions or implemented for any given control law. In addition, transformations and developments, proposed here, can be exploited for modeling any other three-dimensional humanoid robot with a different morphology and variable number of rigid bodies and degrees of freedom.',
	 'authors': u'Amira Aloulou, Olfa Boubaker,',
	 'category': u'Computer Science ',
	 'date': '2014-5-8',
	 'pdflink': u'http://arxiv.org/pdf/1405.2795',
	 'subjects': u'Robotics (cs.RO)',
	 'title': u'\nA Relevant Reduction Method for Dynamic Modeling of a Seven-linked  Humanoid Robot in the Three-dimensional Space',
	 'urllink': u'http://arxiv.org/abs/1405.2795'}
2015-03-23 19:59:22+0000 [xxu461000] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1406.0403> (referer: http://arxiv.org/list/cs/14?skip=6000&show=1000)
2015-03-23 19:59:22+0000 [xxu461000] DEBUG: Scraped from <200 http://arxiv.org/abs/1406.0403>
	{'abstract': u'Deeply embedded systems often have the tightest constraints on energy consumption, requiring that they consume tiny amounts of current and run on batteries for years. However, they typically execute code directly from flash, instead of the more energy efficient RAM. We implement a novel compiler optimization that exploits the relative efficiency of RAM by statically moving carefully selected basic blocks from flash to RAM. Our technique uses integer linear programming, with an energy cost model to select a good set of basic blocks to place into RAM, without impacting stack or data storage. We evaluate our optimization on a common ARM microcontroller and succeed in reducing the average power consumption by up to 41% and reducing energy consumption by up to 22%, while increasing execution time. A case study is presented, where an application executes code then sleeps for a period of time. For this example we show that our optimization could allow the application to run on battery for up to 32% longer. We also show that for this scenario the total application energy can be reduced, even if the optimization increases the execution time of the code.',
	 'authors': u'James Pallister, Kerstin Eder, Simon Hollis,',
	 'category': u'Computer Science ',
	 'date': '2014-5-30',
	 'pdflink': u'http://arxiv.org/pdf/1406.0403',
	 'subjects': u'Other Computer Science (cs.OH)',
	 'title': u'\nOptimizing the flash-RAM energy trade-off in deeply embedded systems',
	 'urllink': u'http://arxiv.org/abs/1406.0403'}
2015-03-23 19:59:28+0000 [xxu461000] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1405.2794> (referer: http://arxiv.org/list/cs/14?skip=5000&show=1000)
2015-03-23 19:59:28+0000 [xxu461000] DEBUG: Scraped from <200 http://arxiv.org/abs/1405.2794>
	{'abstract': u"To appear in Theory and Practice of Logic Programming (TPLP). Tabling is a commonly used technique in logic programming for avoiding cyclic behavior of logic programs and enabling more declarative program definitions. Furthermore, tabling often improves computational performance. Rational term are terms with one or more infinite sub-terms but with a finite representation. Rational terms can be generated in Prolog by omitting the occurs check when unifying two terms. Applications of rational terms include definite clause grammars, constraint handling systems, and coinduction. In this paper, we report our extension of YAP's Prolog tabling mechanism to support rational terms. We describe the internal representation of rational terms within the table space and prove its correctness. We then use this extension to implement a tabling based approach to coinduction. We compare our approach with current coinductive transformations and describe the implementation. In addition, we present an algorithm that ensures a canonical representation for rational terms.",
	 'authors': u'Thepfrastos Mantadelis, Ricardo Rocha, Paulo Moura,',
	 'category': u'Computer Science ',
	 'date': '2014-5-9',
	 'pdflink': u'http://arxiv.org/pdf/1405.2794',
	 'subjects': u'Programming Languages (cs.PL)',
	 'title': u'\nTabling, Rational Terms, and Coinduction Finally Together!',
	 'urllink': u'http://arxiv.org/abs/1405.2794'}
2015-03-23 19:59:33+0000 [xxu461000] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1406.0380> (referer: http://arxiv.org/list/cs/14?skip=6000&show=1000)
2015-03-23 19:59:33+0000 [xxu461000] DEBUG: Scraped from <200 http://arxiv.org/abs/1406.0380>
	{'abstract': u'This article presents a new approach to the real-time solution of inverse problems on embedded systems. The class of problems addressed corresponds to ordinary differential equations (ODEs) with generalized linear constraints, whereby the data from an array of sensors forms the forcing function. The solution of the equation is formulated as a least squares (LS) problem with linear constraints. The LS approach makes the method suitable for the explicit solution of inverse problems where the forcing function is perturbed by noise. The algebraic computation is partitioned into a initial preparatory step, which precomputes the matrices required for the run-time computation; and the cyclic run-time computation, which is repeated with each acquisition of sensor data. The cyclic computation consists of a single matrix-vector multiplication, in this manner computation complexity is known a-priori, fulfilling the definition of a real-time computation. Numerical testing of the new method is presented on perturbed as well as unperturbed problems; the results are compared with known analytic solutions and solutions acquired from state-of-the-art implicit solvers. The solution is implemented with model based design and uses only fundamental linear algebra; consequently, this approach supports automatic code generation for deployment on embedded systems. The targeting concept was tested via software- and processor-in-the-loop verification on two systems with different processor architectures. Finally, the method was tested on a laboratory prototype with real measurement data for the monitoring of flexible structures. The problem solved is: the real-time overconstrained reconstruction of a curve from measured gradients. Such systems are commonly encountered in the monitoring of structures and/or ground subsidence.',
	 'authors': u"Christoph Gugg, Matthew Harker, Paul O'Leary, Gerhard Rath,",
	 'category': u'Computer Science ',
	 'date': '2014-6-2',
	 'pdflink': u'http://arxiv.org/pdf/1406.0380',
	 'subjects': u'Discrete Mathematics (cs.DM)',
	 'title': u'\nAn Algebraic Framework for the Real-Time Solution of Inverse Problems on  Embedded Systems',
	 'urllink': u'http://arxiv.org/abs/1406.0380'}
2015-03-23 19:59:38+0000 [xxu461000] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1405.2786> (referer: http://arxiv.org/list/cs/14?skip=5000&show=1000)
2015-03-23 19:59:38+0000 [xxu461000] DEBUG: Scraped from <200 http://arxiv.org/abs/1405.2786>
	{'abstract': u'To fully utilize the spatial multiplexing gains or array gains of massive MIMO, the channel state information must be obtained at the transmitter side (CSIT). However, conventional CSIT estimation approaches are not suitable for FDD massive MIMO systems because of the overwhelming training and feedback overhead. In this paper, we consider multi-user massive MIMO systems and deploy the compressive sensing (CS) technique to reduce the training as well as the feedback overhead in the CSIT estimation. The multi-user massive MIMO systems exhibits a hidden joint sparsity structure in the user channel matrices due to the shared local scatterers in the physical propagation environment. As such, instead of naively applying the conventional CS to the CSIT estimation, we propose a distributed compressive CSIT estimation scheme so that the compressed measurements are observed at the users locally, while the CSIT recovery is performed at the base station jointly. A joint orthogonal matching pursuit recovery algorithm is proposed to perform the CSIT recovery, with the capability of exploiting the hidden joint sparsity in the user channel matrices. We analyze the obtained CSIT quality in terms of the normalized mean absolute error, and through the closed-form expressions, we obtain simple insights into how the joint channel sparsity can be exploited to improve the CSIT recovery performance.',
	 'authors': u'Xiongbin Rao, Vincent K. N. Lau,',
	 'category': u'Computer Science ',
	 'date': '2014-5-12',
	 'pdflink': u'http://arxiv.org/pdf/1405.2786',
	 'subjects': u'Information Theory (cs.IT)',
	 'title': u'\nDistributed Compressive CSIT Estimation and Feedback for FDD Multi-user  Massive MIMO Systems',
	 'urllink': u'http://arxiv.org/abs/1405.2786'}
2015-03-23 19:59:42+0000 [xxu461000] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1406.0379> (referer: http://arxiv.org/list/cs/14?skip=6000&show=1000)
2015-03-23 19:59:42+0000 [xxu461000] DEBUG: Scraped from <200 http://arxiv.org/abs/1406.0379>
	{'abstract': u"With an increasing emphasis on network security, much more attention has been attracted to the vulnerability of complex networks. The multi-scale evaluation of vulnerability is widely used since it makes use of combined powers of the links' betweenness and has an effective evaluation to vulnerability. However, how to determine the coefficient in existing multi-scale evaluation model to measure the vulnerability of different networks is still an open issue. In this paper, an improved model based on the fractal dimension of complex networks is proposed to obtain a more reasonable evaluation of vulnerability with more physical significance. Not only the structure and basic physical properties of networks is characterized, but also the covering ability of networks, which is related to the vulnerability of the network, is taken into consideration in our proposed method. The numerical examples and real applications are used to illustrate the efficiency of our proposed method.",
	 'authors': u'Li Gou, Bo Wei, Rehan Sadiq, Sankaran Mahadevan, Yong Deng,',
	 'category': u'Computer Science ',
	 'date': '2014-6-2',
	 'pdflink': u'http://arxiv.org/pdf/1406.0379',
	 'subjects': u'Social and Information Networks (cs.SI)',
	 'title': u'\nAn improved vulnerability index of complex networks based on fractal  dimension',
	 'urllink': u'http://arxiv.org/abs/1406.0379'}
2015-03-23 19:59:45+0000 [xxu461000] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1405.2767> (referer: http://arxiv.org/list/cs/14?skip=5000&show=1000)
2015-03-23 19:59:45+0000 [xxu461000] DEBUG: Scraped from <200 http://arxiv.org/abs/1405.2767>
	{'abstract': u'In this work we propose a novel iterative estimation algorithm for linear observation systems called S-AMP whose fixed points are the stationary points of the exact Gibbs free energy under a set of (first- and second-) moment consistency constraints in the large system limit. S-AMP extends the approximate message-passing (AMP) algorithm to general matrix ensembles. The generalization is based on the S-transform (in free probability) of the spectrum of the measurement matrix. Furthermore, we show that the optimality of S-AMP follows directly from its design rather than from solving a separate optimization problem as done for AMP.',
	 'authors': u'Burak \xc7akmak, Ole Winther, Bernard H. Fleury,',
	 'category': u'Computer Science ',
	 'date': '2014-5-12',
	 'pdflink': u'http://arxiv.org/pdf/1405.2767',
	 'subjects': u'Information Theory (cs.IT)',
	 'title': u'\nS-AMP: Approximate Message Passing for General Matrix Ensembles',
	 'urllink': u'http://arxiv.org/abs/1405.2767'}
2015-03-23 19:59:48+0000 [xxu461000] INFO: Crawled 73 pages (at 12 pages/min), scraped 56 items (at 12 items/min)
2015-03-23 19:59:51+0000 [xxu461000] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1406.0375> (referer: http://arxiv.org/list/cs/14?skip=6000&show=1000)
2015-03-23 19:59:51+0000 [xxu461000] DEBUG: Scraped from <200 http://arxiv.org/abs/1406.0375>
	{'abstract': u'This paper proposes an assessment model, based on a new taxonomy, which comprises an evaluation guideline with performance metrics and experimental setup to aid designers in evaluating solutions through fair comparisons. Simulation results are provided based on the proposed model considering Epidemic, PROPHET, Bubble Rap, and Spray and Wait, and showing how they perform under the same set of metrics and scenario',
	 'authors': u'Waldir Moreira, Paulo Mendes, Susana Sargento,',
	 'category': u'Computer Science ',
	 'date': '2014-6-2',
	 'pdflink': u'http://arxiv.org/pdf/1406.0375',
	 'subjects': u'Networking and Internet Architecture (cs.NI)',
	 'title': u'\nAssessment Model for Opportunistic Routing',
	 'urllink': u'http://arxiv.org/abs/1406.0375'}
2015-03-23 19:59:56+0000 [xxu461000] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1405.2760> (referer: http://arxiv.org/list/cs/14?skip=5000&show=1000)
2015-03-23 19:59:56+0000 [xxu461000] DEBUG: Scraped from <200 http://arxiv.org/abs/1405.2760>
	{'abstract': u'Searching in the Internet for some object characterised by its attributes in the form of data, such as a hotel in a certain city whose price is less than something, is one of our most common activities when we access the Web. We discuss this problem in a general setting, and compute the average amount of time and the energy it takes to find an object in an infinitely large search space. We consider the use of N search agents which act concurrently. Both the case where the search agent knows which way it needs to go to find the object, and the case where the search agent is perfectly ignorant and may even head away from the object being sought. We show that under mild conditions regarding the randomness of the search and the use of a time-out, the search agent will always find the object despite the fact that the search space is infinite. We obtain a formula for the average search time and the average energy expended by N search agents acting concurrently and independently of each other. We see that the time-out itself can be used to minimise the search time and the amount of energy that is consumed to find an object. An approximate formula is derived for the number of search agents that can help us guarantee that an object is found in a given time, and we discuss how the competition between search agents and other agents that try to hide the data object, can be used by opposing parties to guarantee their own success.',
	 'authors': u'Erol Gelenbe, Omer H. Abdelrahman,',
	 'category': u'Computer Science ',
	 'date': '2014-5-12',
	 'pdflink': u'http://arxiv.org/pdf/1405.2760',
	 'subjects': u'Networking and Internet Architecture (cs.NI)',
	 'title': u'\nSearch in the Universe of Big Networks and Data',
	 'urllink': u'http://arxiv.org/abs/1405.2760'}
2015-03-23 20:00:01+0000 [xxu461000] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1406.0373> (referer: http://arxiv.org/list/cs/14?skip=6000&show=1000)
2015-03-23 20:00:01+0000 [xxu461000] DEBUG: Scraped from <200 http://arxiv.org/abs/1406.0373>
	{'abstract': u"Goldreich suggested candidates of one-way functions and pseudorandom generators included in . It is known that randomly generated Goldreich's generator using -wise independent predicates with input variables and output variables is not pseudorandom generator with high probability for sufficiently large constant . Most of the previous works assume that the alphabet is binary and use techniques available only for the binary alphabet. In this paper, we deal with non-binary generalization of Goldreich's generator and derives the tight threshold for linear programming relaxation attack using local marginal polytope for randomly generated Goldreich's generators. We assume that input variables are known. In that case, we show that when , there is an exact threshold such that for , the LP relaxation can determine linearly many input variables of Goldreich's generator if , and that the LP relaxation cannot determine input variables of Goldreich's generator if . This paper uses characterization of LP solutions by combinatorial structures called stopping sets on a bipartite graph, which is related to a simple algorithm called peeling algorithm.",
	 'authors': u'Ryuhei Mori, Takeshi Koshiba, Osamu Watanabe, Masaki Yamamoto,',
	 'category': u'Computer Science ',
	 'date': '2014-6-2',
	 'pdflink': u'http://arxiv.org/pdf/1406.0373',
	 'subjects': u'Cryptography and Security (cs.CR)',
	 'title': u"\nLinear Programming Relaxations for Goldreich's Generators over  Non-Binary Alphabets",
	 'urllink': u'http://arxiv.org/abs/1406.0373'}
2015-03-23 20:00:10+0000 [xxu461000] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1405.2738> (referer: http://arxiv.org/list/cs/14?skip=5000&show=1000)
2015-03-23 20:00:11+0000 [xxu461000] DEBUG: Scraped from <200 http://arxiv.org/abs/1405.2738>
	{'abstract': u'The design and verification of cryptographic protocols is a notoriously difficult task, even in symbolic models which take an abstract view of cryptography. This is mainly due to the fact that protocols may interact with an arbitrary attacker which yields a verification problem that has several sources of unboundedness (size of messages, number of sessions, etc. In this paper, we characterize a class of protocols for which deciding security for an unbounded number of sessions is decidable. More precisely, we present a simple transformation which maps a protocol that is secure for a bounded number of protocol sessions (a decidable problem) to a protocol that is secure for an unbounded number of sessions. The precise number of sessions that need to be considered is a function of the security property and we show that for several classical security properties a single session is sufficient. Therefore, in many cases our results yields a design strategy for security protocols: (i) design a protocol intended to be secure for a ; and (ii) apply our transformation to obtain a protocol which is secure for an unbounded number of sessions.',
	 'authors': u'Myrto Arapinis, St\xe9phanie Delaune, Steve Kremer,',
	 'category': u'Computer Science ',
	 'date': '2014-5-12',
	 'pdflink': u'http://arxiv.org/pdf/1405.2738',
	 'subjects': u'Cryptography and Security (cs.CR)',
	 'title': u'\nDynamic Tags for Security Protocols',
	 'urllink': u'http://arxiv.org/abs/1405.2738'}
2015-03-23 20:00:13+0000 [xxu461000] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1406.0370> (referer: http://arxiv.org/list/cs/14?skip=6000&show=1000)
2015-03-23 20:00:13+0000 [xxu461000] DEBUG: Scraped from <200 http://arxiv.org/abs/1406.0370>
	{'abstract': u'Prototyping is an important part in research and development of tangible user interfaces (TUIs). On the way from the idea to a working prototype, new hardware prototypes usually have to be crafted repeatedly in numerous iterations. This brings us to think about virtual prototypes that exhibit the same functionality as a real TUI, but reduce the amount of time and resources that have to be spent. Building upon existing open-source software - the middleware Robot Operating System (ROS) and the 3D simulator Gazebo - we have created a toolkit that can be used for developing and testing fully functional implementations of a tangible user interface as a virtual device. The entire interaction between the TUI and other hardware and software components is controlled by the middleware, while the human interaction with the TUI can be explored using the 3D simulator and 3D input/output technologies. We argue that by simulating parts of the hardware-software co-design process, the overall development effort can be reduced.',
	 'authors': u'Stefan Diewald, Andreas M\xf6ller, Luis Roalter, Matthias Kranz,',
	 'category': u'Computer Science ',
	 'date': '2014-6-2',
	 'pdflink': u'http://arxiv.org/pdf/1406.0370',
	 'subjects': u'Human-Computer Interaction (cs.HC)',
	 'title': u'\nSimulation and Virtual Prototyping of Tangible User Interfaces',
	 'urllink': u'http://arxiv.org/abs/1406.0370'}
2015-03-23 20:00:17+0000 [xxu461000] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1405.2736> (referer: http://arxiv.org/list/cs/14?skip=5000&show=1000)
2015-03-23 20:00:17+0000 [xxu461000] DEBUG: Scraped from <200 http://arxiv.org/abs/1405.2736>
	{'abstract': u'In this paper we give new constructions of Ferrer diagram rank metric codes, which achieve the largest possible dimension. In particular, we prove several cases of a conjecture by T. Etzion and N. Silberstein. We also establish a sharp lower bound on the dimension of linear rank metric anticodes with a given profile. Combining our results with the multilevel construction, we produce examples of subspace codes with the largest known cardinality for the given parameters.',
	 'authors': u'Elisa Gorla, Alberto Ravagnani,',
	 'category': u'Computer Science ',
	 'date': '2014-5-12',
	 'pdflink': u'http://arxiv.org/pdf/1405.2736',
	 'subjects': u'Information Theory (cs.IT)',
	 'title': u'\nSubspace codes from Ferrers diagrams',
	 'urllink': u'http://arxiv.org/abs/1405.2736'}
2015-03-23 20:00:22+0000 [xxu461000] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1406.0349> (referer: http://arxiv.org/list/cs/14?skip=6000&show=1000)
2015-03-23 20:00:22+0000 [xxu461000] DEBUG: Scraped from <200 http://arxiv.org/abs/1406.0349>
	{'abstract': u'We consider expressions built up from binary relation names using the operators union, composition, and set difference. We show that it is undecidable to test whether a given such expression is finitely satisfiable, i.e., whether there exist finite binary relations that can be substituted for the relation names so that evaluates to a nonempty result. This result already holds in restriction to expressions that mention just a single relation name, and where the difference operator can be nested at most once.',
	 'authors': u'Tony Tan, Jan Van den Bussche, Xiaowang Zhang,',
	 'category': u'Computer Science ',
	 'date': '2014-6-2',
	 'pdflink': u'http://arxiv.org/pdf/1406.0349',
	 'subjects': u'Logic in Computer Science (cs.LO)',
	 'title': u'\nUndecidability of satisfiability in the algebra of finite binary  relations with union, composition, and difference',
	 'urllink': u'http://arxiv.org/abs/1406.0349'}
2015-03-23 20:00:27+0000 [xxu461000] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1405.2735> (referer: http://arxiv.org/list/cs/14?skip=5000&show=1000)
2015-03-23 20:00:27+0000 [xxu461000] DEBUG: Scraped from <200 http://arxiv.org/abs/1405.2735>
	{'abstract': u"We review the characteristics of signalling storms that have been caused by certain common apps and recently observed in cellular networks, leading to system outages. We then develop a mathematical model of a mobile user's signalling behaviour which focuses on the potential of causing such storms, and represent it by a large Markov chain. The analysis of this model allows us to determine the key parameters of mobile user device behaviour that can lead to signalling storms. We then identify the parameter values that will lead to worst case load for the network itself in the presence of such storms. This leads to explicit results regarding the manner in which individual mobile behaviour can cause overload conditions on the network and its signalling servers, and provides insight into how this may be avoided.",
	 'authors': u'Omer H. Abdelrahman, Erol Gelenbe,',
	 'category': u'Computer Science ',
	 'date': '2014-5-12',
	 'pdflink': u'http://arxiv.org/pdf/1405.2735',
	 'subjects': u'Networking and Internet Architecture (cs.NI)',
	 'title': u'\nSignalling Storms in 3G Mobile Networks',
	 'urllink': u'http://arxiv.org/abs/1405.2735'}
2015-03-23 20:00:31+0000 [xxu461000] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1406.0342> (referer: http://arxiv.org/list/cs/14?skip=6000&show=1000)
2015-03-23 20:00:31+0000 [xxu461000] DEBUG: Scraped from <200 http://arxiv.org/abs/1406.0342>
	{'abstract': u"This paper considers Gama-Nguyen-Regev's strategy [GNR10] for optimizing pruning coefficients for lattice vector enumeration. We give a table of optimized coefficients and proposes a faster method for computing near-optimized coefficients for any parameters by interpolation.",
	 'authors': u'Yoshinori Aono,',
	 'category': u'Computer Science ',
	 'date': '2014-6-2',
	 'pdflink': u'http://arxiv.org/pdf/1406.0342',
	 'subjects': u'Cryptography and Security (cs.CR)',
	 'title': u"\nA faster method for computing Gama-Nguyen-Regev's extreme pruning  coefficients",
	 'urllink': u'http://arxiv.org/abs/1406.0342'}
2015-03-23 20:00:37+0000 [xxu461000] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1405.2733> (referer: http://arxiv.org/list/cs/14?skip=5000&show=1000)
2015-03-23 20:00:37+0000 [xxu461000] DEBUG: Scraped from <200 http://arxiv.org/abs/1405.2733>
	{'abstract': u'We give an optimal-size representation for the elements of the trace zero subgroup of the Picard group of an elliptic or hyperelliptic curve of any genus, with respect to a field extension of any prime degree. The representation is via the coefficients of a rational function, and it is compatible with scalar multiplication of points. We provide efficient compression and decompression algorithms, and complement them with implementation results. We discuss in detail the practically relevant cases of small genus and extension degree, and compare with the other known compression methods.',
	 'authors': u'Elisa Gorla, Maike Massierer,',
	 'category': u'Computer Science ',
	 'date': '2014-5-12',
	 'pdflink': u'http://arxiv.org/pdf/1405.2733',
	 'subjects': u'Cryptography and Security (cs.CR)',
	 'title': u'\nAn optimal representation for the trace zero subgroup',
	 'urllink': u'http://arxiv.org/abs/1405.2733'}
2015-03-23 20:00:41+0000 [xxu461000] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1406.0333> (referer: http://arxiv.org/list/cs/14?skip=6000&show=1000)
2015-03-23 20:00:41+0000 [xxu461000] DEBUG: Scraped from <200 http://arxiv.org/abs/1406.0333>
	{'abstract': u'This workshop about triangulations of manifolds in computational geometry and topology was held at the 2014 CG-Week in Kyoto, Japan. It focussed on computational and combinatorial questions regarding triangulations, with the goal of bringing together researchers working on various aspects of triangulations and of fostering a closer collaboration within the computational geometry and topology community. Triangulations are highly suitable for computations due to their clear combinatorial structure. As a consequence, they have been successfully employed in discrete algorithms to solve purely theoretical problems in a broad variety of mathematical research areas (knot theory, polytope theory, 2- and 3-manifold topology, geometry, and others). However, due to the large variety of applications, requirements vary from field to field and thus different types of triangulations, different tools, and different frameworks are used in different areas of research. This is why today closely related research areas are sometimes largely disjoint leaving potential reciprocal benefits unused. To address these potentials a workshop on Triangulations was held at Oberwolfach Research Institute in 2012. Since then many new collaborations between researchers of different mathematical communities have been established. Regarding the computational geometry community, the theory of manifolds continues to contribute to advances in more applied areas of the field. Many researchers are interested in fundamental mathematical research about triangulations and thus will benefit from a broad set of knowledge about different research areas using different techniques. We hope that this workshop brought together researchers from many different fields of computational geometry to have fruitful discussions which will lead to new interdisciplinary collaborations and solutions.',
	 'authors': u'Jonathan Spreer, Uli Wagner, Benjamin A. Burton, Satoshi Murai, Eric Sedgwick, Henry Segerman,',
	 'category': u'Computer Science ',
	 'date': '2014-6-2',
	 'pdflink': u'http://arxiv.org/pdf/1406.0333',
	 'subjects': u'Computational Geometry (cs.CG)',
	 'title': u'\nCollection of abstracts of the Workshop on Triangulations in Geometry  and Topology at CG Week 2014 in Kyoto',
	 'urllink': u'http://arxiv.org/abs/1406.0333'}
2015-03-23 20:00:47+0000 [xxu461000] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1405.2708> (referer: http://arxiv.org/list/cs/14?skip=5000&show=1000)
2015-03-23 20:00:47+0000 [xxu461000] DEBUG: Scraped from <200 http://arxiv.org/abs/1405.2708>
	{'abstract': u'This paper presents a modified multi model predictive control algorithm for the control of riser outlet temperature and regenerator temperature for the fluid catalytic cracking unit (FCCU). The models of the fluid catalytic cracking unit are estimated using subspace identification (N4SID) algorithm. The PRBS signal is applied as an input signal to estimate the FCCU models. Since the estimated model does not give 100% fit; especially for nonlinear systems having more than one operating conditions, multi-model approach is proposed. In multi model, more than one model of FCCU used in MPC design. The main advantages of proposed method are that it can handle hard input and output constraints and it can be used for multi input multi output processes (MIMO) without increasing the complexity in control design. MATLAB/Simulink is used to estimate the models of FCCU and simulate the results for the controller. The simulation results show that the proposed algorithm provides better result for both reference tracking and disturbance rejection.',
	 'authors': u'Nafay Hifzur Rehman, Neelam Verma,',
	 'category': u'Computer Science ',
	 'date': '2014-5-12',
	 'pdflink': u'http://arxiv.org/pdf/1405.2708',
	 'subjects': u'Systems and Control (cs.SY)',
	 'title': u'\nApplication of Modified Multi Model Predictive Control Algorithm to  Fluid Catalytic Cracking Unit',
	 'urllink': u'http://arxiv.org/abs/1405.2708'}
2015-03-23 20:00:48+0000 [xxu461000] INFO: Crawled 85 pages (at 12 pages/min), scraped 68 items (at 12 items/min)
2015-03-23 20:00:52+0000 [xxu461000] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1406.0312> (referer: http://arxiv.org/list/cs/14?skip=6000&show=1000)
2015-03-23 20:00:52+0000 [xxu461000] DEBUG: Scraped from <200 http://arxiv.org/abs/1406.0312>
	{'abstract': u'State-of-the-art patch-based image representations involve a pooling operation that aggregates statistics computed from local descriptors. Standard pooling operations include sum- and max-pooling. Sum-pooling lacks discriminability because the resulting representation is strongly influenced by frequent yet often uninformative descriptors, but only weakly influenced by rare yet potentially highly-informative ones. Max-pooling equalizes the influence of frequent and rare descriptors but is only applicable to representations that rely on count statistics, such as the bag-of-visual-words (BOV) and its soft- and sparse-coding extensions. We propose a novel pooling mechanism that achieves the same effect as max-pooling but is applicable beyond the BOV and especially to the state-of-the-art Fisher Vector -- hence the name Generalized Max Pooling (GMP). It involves equalizing the similarity between each patch and the pooled representation, which is shown to be equivalent to re-weighting the per-patch statistics. We show on five public image classification benchmarks that the proposed GMP can lead to significant performance gains with respect to heuristic alternatives.',
	 'authors': u'Naila Murray, Florent Perronnin,',
	 'category': u'Computer Science ',
	 'date': '2014-6-2',
	 'pdflink': u'http://arxiv.org/pdf/1406.0312',
	 'subjects': u'Computer Vision and Pattern Recognition (cs.CV)',
	 'title': u'\nGeneralized Max Pooling',
	 'urllink': u'http://arxiv.org/abs/1406.0312'}
2015-03-23 20:00:56+0000 [xxu461000] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1405.2705> (referer: http://arxiv.org/list/cs/14?skip=5000&show=1000)
2015-03-23 20:00:56+0000 [xxu461000] DEBUG: Scraped from <200 http://arxiv.org/abs/1405.2705>
	{'abstract': u'In this paper we survey recent work on the use of statistical model checking techniques for biological applications. We begin with an overview of the basic modelling techniques for biochemical reactions and their corresponding stochastic simulation algorithm - the Gillespie algorithm. We continue by giving a brief description of the relation between stochastic models and continuous (ordinary differential equation) models. Next we present a literature survey, divided in two general areas. In the first area we focus on works addressing verification of biological models, while in the second area we focus on papers tackling the parameter synthesis problem. We conclude with some open problems and directions for further research.',
	 'authors': u'Paolo Zuliani,',
	 'category': u'Computer Science ',
	 'date': '2014-5-12',
	 'pdflink': u'http://arxiv.org/pdf/1405.2705',
	 'subjects': u'Logic in Computer Science (cs.LO)',
	 'title': u'\nStatistical Model Checking for Biological Applications',
	 'urllink': u'http://arxiv.org/abs/1405.2705'}
2015-03-23 20:01:02+0000 [xxu461000] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1406.0309> (referer: http://arxiv.org/list/cs/14?skip=6000&show=1000)
2015-03-23 20:01:02+0000 [xxu461000] DEBUG: Scraped from <200 http://arxiv.org/abs/1406.0309>
	{'abstract': u'Network Function Virtualization (NFV) refers to the use of commodity hardware resources as the basic platform to perform specialized network functions as opposed to specialized hardware devices. Currently, NFV is mainly implemented based on general purpose processors, or general purpose network processors. In this paper we propose the use of FPGAs as an ideal platform for NFV that can be used to provide both the flexibility of virtualizations and the high performance of the specialized hardware. We present the early attempts of using FPGAs dynamic reconfiguration in network processing applications to provide flexible network functions and we present the opportunities for an FPGA-based NFV platform.',
	 'authors': u'Christoforos Kachris, Georgios Sirakoulis, Dimitrios Soudris,',
	 'category': u'Computer Science ',
	 'date': '2014-6-2',
	 'pdflink': u'http://arxiv.org/pdf/1406.0309',
	 'subjects': u'Networking and Internet Architecture (cs.NI)',
	 'title': u'\nNetwork Function Virtualization based on FPGAs:A Framework for  all-Programmable network devices',
	 'urllink': u'http://arxiv.org/abs/1406.0309'}
2015-03-23 20:01:07+0000 [xxu461000] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1405.2702> (referer: http://arxiv.org/list/cs/14?skip=5000&show=1000)
2015-03-23 20:01:07+0000 [xxu461000] DEBUG: Scraped from <200 http://arxiv.org/abs/1405.2702>
	{'abstract': u'In this paper we present the comparison of the linguistic networks from literature and blog texts. The linguistic networks are constructed from texts as directed and weighted co-occurrence networks of words. Words are nodes and links are established between two nodes if they are directly co-occurring within the sentence. The comparison of the networks structure is performed at global level (network) in terms of: average node degree, average shortest path length, diameter, clustering coefficient, density and number of components. Furthermore, we perform analysis on the local level (node) by comparing the rank plots of in and out degree, strength and selectivity. The selectivity-based results point out that there are differences between the structure of the networks constructed from literature and blogs.',
	 'authors': u'Sabina \u0160i\u0161ovi\u0107, Sanda Martin\u010di\u0107-Ip\u0161i\u0107, Ana Me\u0161trovi\u0107,',
	 'category': u'Computer Science ',
	 'date': '2014-5-12',
	 'pdflink': u'http://arxiv.org/pdf/1405.2702',
	 'subjects': u'Computation and Language (cs.CL)',
	 'title': u'\nComparison of the language networks from literature and blogs',
	 'urllink': u'http://arxiv.org/abs/1405.2702'}
2015-03-23 20:01:12+0000 [xxu461000] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1406.0306> (referer: http://arxiv.org/list/cs/14?skip=6000&show=1000)
2015-03-23 20:01:12+0000 [xxu461000] DEBUG: Scraped from <200 http://arxiv.org/abs/1406.0306>
	{'abstract': u'An isogeometric boundary element method for problems in elasticity is presented, which is based on an independent approximation for the geometry, traction and displacement field. This enables a flexible choice of refinement strategies, permits an efficient evaluation of geometry related information, a mixed collocation scheme which deals with discontinuous tractions along non-smooth boundaries and a significant reduction of the right hand side of the system of equations for common boundary conditions. All these benefits are achieved without any loss of accuracy compared to conventional isogeometric formulations. The system matrices are approximated by means of hierarchical matrices to reduce the computational complexity for large scale analysis. For the required geometrical bisection of the domain, a strategy for the evaluation of bounding boxes containing the supports of NURBS basis functions is presented. The versatility and accuracy of the proposed methodology is demonstrated by convergence studies showing optimal rates and real world examples in two and three dimensions.',
	 'authors': u'Benjamin Marussig, J\xfcrgen Zechner, Gernot Beer, Thomas-Peter Fries,',
	 'category': u'Computer Science ',
	 'date': '2014-6-2',
	 'pdflink': u'http://arxiv.org/pdf/1406.0306',
	 'subjects': u'Numerical Analysis (cs.NA)',
	 'title': u'\nFast Isogeometric Boundary Element Method based on Independent Field  Approximation',
	 'urllink': u'http://arxiv.org/abs/1406.0306'}
2015-03-23 20:01:17+0000 [xxu461000] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1405.2693> (referer: http://arxiv.org/list/cs/14?skip=5000&show=1000)
2015-03-23 20:01:17+0000 [xxu461000] DEBUG: Scraped from <200 http://arxiv.org/abs/1405.2693>
	{'abstract': u'Integration techniques for combining programs written in distinct language paradigms facilitate the implementation of specialised modules in the best language for their task. In the case of Java-Prolog integration, a known problem is the proper representation of references to Java objects on the Prolog side. To solve it adequately, multiple dimensions should be considered, including reference representation, opacity of the representation, identity preservation, reference life span, and scope of the inter-language conversion policies. This paper presents an approach that addresses all these dimensions, generalising and building on existing representation patterns of foreign references in Prolog, and taking inspiration from similar inter-language representation techniques found in other domains. Our approach maximises portability by making few assumptions about the Prolog engine interacting with Java (e.g., embedded or executed as an external process). We validate our work by extending JPC, an open-source integration library, with features supporting our approach. Our JPC library is currently compatible with three different open source Prolog engines (SWI, YAP and XSB) by means of drivers. To appear in Theory and Practice of Logic Programming (TPLP).',
	 'authors': u'Sergio Castro, Kim Mens, Paulo Moura,',
	 'category': u'Computer Science ',
	 'date': '2014-5-12',
	 'pdflink': u'http://arxiv.org/pdf/1405.2693',
	 'subjects': u'Programming Languages (cs.PL)',
	 'title': u'\nCustomisable Handling of Java References in Prolog Programs',
	 'urllink': u'http://arxiv.org/abs/1405.2693'}
2015-03-23 20:01:22+0000 [xxu461000] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1406.0304> (referer: http://arxiv.org/list/cs/14?skip=6000&show=1000)
2015-03-23 20:01:22+0000 [xxu461000] DEBUG: Scraped from <200 http://arxiv.org/abs/1406.0304>
	{'abstract': u'We tackle the problem of multi-task learning with copula process. Multivariable prediction in spatial and spatial-temporal processes such as natural resource estimation and pollution monitoring have been typically addressed using techniques based on Gaussian processes and co-Kriging. While the Gaussian prior assumption is convenient from analytical and computational perspectives, nature is dominated by non-Gaussian likelihoods. Copula processes are an elegant and flexible solution to handle various non-Gaussian likelihoods by capturing the dependence structure of random variables with cumulative distribution functions rather than their marginals. We show how multi-task learning for copula processes can be used to improve multivariable prediction for problems where the simple Gaussianity prior assumption does not hold. Then, we present a transductive approximation for multi-task learning and derive analytical expressions for the copula process model. The approach is evaluated and compared to other techniques in one artificial dataset and two publicly available datasets for natural resource estimation and concrete slump prediction.',
	 'authors': u'Markus Schneider, Fabio Ramos,',
	 'category': u'Computer Science ',
	 'date': '2014-6-2',
	 'pdflink': u'http://arxiv.org/pdf/1406.0304',
	 'subjects': u'Learning (cs.LG)',
	 'title': u'\nTransductive Learning for Multi-Task Copula Processes',
	 'urllink': u'http://arxiv.org/abs/1406.0304'}
2015-03-23 20:01:27+0000 [xxu461000] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1405.2685> (referer: http://arxiv.org/list/cs/14?skip=5000&show=1000)
2015-03-23 20:01:27+0000 [xxu461000] DEBUG: Scraped from <200 http://arxiv.org/abs/1405.2685>
	{'abstract': u'In cognitive radio it is of prime importance that the presence of Primary Users (PU) is detected correctly at each of the time. In order to do so the help from all present Secondary Users (SU) is taken and such a taken is known as co-operative spectrum sensing. Ideally it is assumed that all the secondary users give the correct result to the control center. But there are certain conditions under which the secondary users deliberately forward wrong result to the control center so as to degrade the performance of the cognitive network. In this paper we study the different techniques for detecting the malicious users or outliers. We take into consideration practical environmental condition such that the received signal of the secondary users is made to undergo fading and noise is also introduced in the signal. We further go on to examine each of the outlier detector techniques and find out the most suitable at various instants.',
	 'authors': u'Manish B Dave, Mitesh B Nakrani,',
	 'category': u'Computer Science ',
	 'date': '2014-5-12',
	 'pdflink': u'http://arxiv.org/pdf/1405.2685',
	 'subjects': u'Networking and Internet Architecture (cs.NI)',
	 'title': u'\nMalicious User Detection in Spectrum Sensing for WRAN Using Different  Outliers Detection Techniques',
	 'urllink': u'http://arxiv.org/abs/1405.2685'}
2015-03-23 20:01:33+0000 [xxu461000] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1406.0303> (referer: http://arxiv.org/list/cs/14?skip=6000&show=1000)
2015-03-23 20:01:33+0000 [xxu461000] DEBUG: Scraped from <200 http://arxiv.org/abs/1406.0303>
	{'abstract': u'We present a modification of the superposition calculus that is meant to generate consequences of sets of first-order axioms. This approach is proven to be sound and deductive-complete in the presence of redundancy elimination rules, provided the considered consequences are built on a given finite set of ground terms, represented by constant symbols. In contrast to other approaches, most existing results about the termination of the superposition calculus can be carried over to our procedure. This ensures in particular that the calculus is terminating for many theories of interest to the SMT community.',
	 'authors': u'Mnacho Echenim, Nicolas Peltier,',
	 'category': u'Computer Science ',
	 'date': '2014-6-2',
	 'pdflink': u'http://arxiv.org/pdf/1406.0303',
	 'subjects': u'Logic in Computer Science (cs.LO)',
	 'title': u'\nA Superposition Calculus for Abductive Reasoning',
	 'urllink': u'http://arxiv.org/abs/1406.0303'}
2015-03-23 20:01:39+0000 [xxu461000] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1405.2684> (referer: http://arxiv.org/list/cs/14?skip=5000&show=1000)
2015-03-23 20:01:39+0000 [xxu461000] DEBUG: Scraped from <200 http://arxiv.org/abs/1405.2684>
	{'abstract': u'Steganography (literally meaning covered writing) is the art and science of embedding secret message into seemingly harmless message. Stenography is practice from olden days where in ancient Greece people used wooden blocks to inscribe secret data and cover the date with wax and write normal message on it. Today stenography is used in various field like multimedia, networks, medical, military etc. With increasing technology trends steganography is becoming more and more advanced where people not only interested on hiding messages in multimedia data (cover data) but also at the receiving end they are willing to obtain original cover data without any distortion after extracting secret message. This paper will discuss few irreversible data hiding techniques and also, some recently proposed reversible data hiding approach using images.',
	 'authors': u'Tanmoy Sarkar, Sugata Sanyal,',
	 'category': u'Computer Science ',
	 'date': '2014-5-12',
	 'pdflink': u'http://arxiv.org/pdf/1405.2684',
	 'subjects': u'Cryptography and Security (cs.CR)',
	 'title': u'\nReversible and Irreversible Data Hiding Technique',
	 'urllink': u'http://arxiv.org/abs/1405.2684'}
2015-03-23 20:01:43+0000 [xxu461000] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1406.0298> (referer: http://arxiv.org/list/cs/14?skip=6000&show=1000)
2015-03-23 20:01:43+0000 [xxu461000] DEBUG: Scraped from <200 http://arxiv.org/abs/1406.0298>
	{'abstract': u'Wireless Sensor Network(WSN) is an emerging technology and explored field of researchers worldwide in the past few years, so does the need for effective security mechanisms. The sensing technology combined with processing power and wireless communication makes it lucrative for being exploited in abundance in future. The inclusion of wireless communication technology also incurs various types of security threats due to unattended installation of sensor nodes as sensor networks may interact with sensitive data and /or operate in hostile unattended environments. These security concerns be addressed from the beginning of the system design. The intent of this paper is to investigate the security related issues in wireless sensor networks. In this paper we have explored general security threats in wireless sensor network with extensive study.',
	 'authors': u'Sahabul Alam, Debashis De,',
	 'category': u'Computer Science ',
	 'date': '2014-6-2',
	 'pdflink': u'http://arxiv.org/pdf/1406.0298',
	 'subjects': u'Networking and Internet Architecture (cs.NI)',
	 'title': u'\nAnalysis of Security Threats in Wireless Sensor Network',
	 'urllink': u'http://arxiv.org/abs/1406.0298'}
2015-03-23 20:01:48+0000 [xxu461000] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1407.5716> (referer: http://arxiv.org/list/cs/14?skip=8000&show=1000)
2015-03-23 20:01:48+0000 [xxu461000] DEBUG: Scraped from <200 http://arxiv.org/abs/1407.5716>
	{'abstract': u'In this paper, we study the downlink performance of a heterogeneous cellular network (HetNet) where both macro and small cells share the same spectrum and hence interfere with each other. We assume that the users are concentrated at certain areas in the cell, i.e., they form hotspots. While some of the hotspots are assumed to have a small cell in their vicinity, the others are directly served by the macrocell. Due to a relatively small area of each hotspot, the users lying in a particular hotspot appear to be almost co-located to the macrocells, which are typically deployed at some elevation. Assuming large number of antennas at the macrocell, we exploit this directionality in the channel vectors to obtain spatial blanking, i.e., concentrating transmission energy only in certain directions while creating transmission opportunities for the small cells lying in the other directions. In addition to this inherent interference suppression, we also develop three low-complexity interference coordination strategies: (i) turn off small cells based on the amount of cross-tier interference they receive or cause to the scheduled macrocell hotspots, (ii) schedule hotspots such that treating interference as noise is approximately optimal for the resulting Gaussian interference channel, and (iii) offload some of the macrocell hotspots to nearby small cells in order to improve throughput fairness across all hotspots. For all these schemes, we study the relative merits and demerits of uniform deployment of small cells vs. deploying more small cells towards the cell center or the cell edge.',
	 'authors': u'Ansuman Adhikary, Harpreet S. Dhillon, Giuseppe Caire,',
	 'category': u'Computer Science ',
	 'date': '2014-7-22',
	 'pdflink': u'http://arxiv.org/pdf/1407.5716',
	 'subjects': u'Information Theory (cs.IT)',
	 'title': u'\nMassive-MIMO Meets HetNet: Interference Coordination Through Spatial  Blanking',
	 'urllink': u'http://arxiv.org/abs/1407.5716'}
2015-03-23 20:01:48+0000 [xxu461000] INFO: Crawled 97 pages (at 12 pages/min), scraped 80 items (at 12 items/min)
2015-03-23 20:01:53+0000 [xxu461000] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1405.2664> (referer: http://arxiv.org/list/cs/14?skip=5000&show=1000)
2015-03-23 20:01:53+0000 [xxu461000] DEBUG: Scraped from <200 http://arxiv.org/abs/1405.2664>
	{'abstract': u"The maximum mean discrepancy (MMD) is a recently proposed test statistic for two-sample test. Its quadratic time complexity, however, greatly hampers its availability to large-scale applications. To accelerate the MMD calculation, in this study we propose an efficient method called FastMMD. The core idea of FastMMD is to equivalently transform the MMD with shift-invariant kernels into the amplitude expectation of a linear combination of sinusoid components based on Bochner's theorem and Fourier transform cite. Taking advantage of sampling of Fourier transform, FastMMD decreases the time complexity for MMD calculation from to , where and are the size and dimension of the sample set, respectively. For kernels that are spherically invariant, the computation can be further accelerated to by using the Fastfood technique cite. The uniform convergence of our method has also been theoretically proved in both unbiased and biased estimates. We have further provided a geometric explanation for our method, namely ensemble of circular discrepancy, which facilitates us to understand the insight of MMD, and is hopeful to help arouse more extensive metrics for assessing two-sample test. Experimental results substantiate that FastMMD is with similar accuracy as exact MMD, while with faster computation speed and lower variance than the existing MMD approximation methods.",
	 'authors': u'Ji Zhao, Deyu Meng,',
	 'category': u'Computer Science ',
	 'date': '2014-5-12',
	 'pdflink': u'http://arxiv.org/pdf/1405.2664',
	 'subjects': u'Artificial Intelligence (cs.AI)',
	 'title': u'\nFastMMD: Ensemble of Circular Discrepancy for Efficient Two-Sample Test',
	 'urllink': u'http://arxiv.org/abs/1405.2664'}
2015-03-23 20:01:58+0000 [xxu461000] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1406.0296> (referer: http://arxiv.org/list/cs/14?skip=6000&show=1000)
2015-03-23 20:01:58+0000 [xxu461000] DEBUG: Scraped from <200 http://arxiv.org/abs/1406.0296>
	{'abstract': u'This paper presents an architecture of an information retrieval system that use the advantages offered by mobile agents to collect information from different sources and bring the result to the calling user. Mobile agent technology will be used for determine the traceability of a product and also for searching information about a specific entity.',
	 'authors': u'Felicia Florentina Giza, Cristina Elena Turcu, Ovidiu Andrei Schipor,',
	 'category': u'Computer Science ',
	 'date': '2014-6-2',
	 'pdflink': u'http://arxiv.org/pdf/1406.0296',
	 'subjects': u'Information Retrieval (cs.IR)',
	 'title': u'\nUsing Mobile Agents for Information Retrival in B2B Systems',
	 'urllink': u'http://arxiv.org/abs/1406.0296'}
2015-03-23 20:02:03+0000 [xxu461000] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1407.5714> (referer: http://arxiv.org/list/cs/14?skip=8000&show=1000)
2015-03-23 20:02:03+0000 [xxu461000] DEBUG: Scraped from <200 http://arxiv.org/abs/1407.5714>
	{'abstract': u'As the amount of digital devices suspected of containing digital evidence increases, case backlogs for digital investigations are also increasing in many organizations. To ensure timely investigation of requests, this work proposes the use of signature-based methods for automated action instance approximation to automatically reconstruct past user activities within a compromised or suspect system. This work specifically explores how multiple instances of a user action may be detected using signature-based methods during a post-mortem digital forensic analysis. A system is formally defined as a set of objects, where a subset of objects may be altered on the occurrence of an action. A novel action-trace update time threshold is proposed that enables objects to be categorized by their respective update patterns over time. By integrating time into event reconstruction, the most recent action instance approximation as well as limited past instances of the action may be differentiated and their time values approximated. After the formal theory if signature-based event reconstruction is defined, a case study is given to evaluate the practicality of the proposed method.',
	 'authors': u'Joshua I. James, Pavel Gladyshev,',
	 'category': u'Computer Science ',
	 'date': '2014-7-22',
	 'pdflink': u'http://arxiv.org/pdf/1407.5714',
	 'subjects': u'Cryptography and Security (cs.CR)',
	 'title': u'\nAutomated Inference of Past Action Instances in Digital Investigations',
	 'urllink': u'http://arxiv.org/abs/1407.5714'}
2015-03-23 20:02:08+0000 [xxu461000] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1405.2652> (referer: http://arxiv.org/list/cs/14?skip=5000&show=1000)
2015-03-23 20:02:08+0000 [xxu461000] DEBUG: Scraped from <200 http://arxiv.org/abs/1405.2652>
	{'abstract': u'We consider a reinforcement learning setting introduced in (Maillard et al., NIPS 2011) where the learner does not have explicit access to the states of the underlying Markov decision process (MDP). Instead, she has access to several models that map histories of past interactions to states. Here we improve over known regret bounds in this setting, and more importantly generalize to the case where the models given to the learner do not contain a true model resulting in an MDP representation but only approximations of it. We also give improved error bounds for state aggregation.',
	 'authors': u'Ronald Ortner, Odalric-Ambrym Maillard, Daniil Ryabko,',
	 'category': u'Computer Science ',
	 'date': '2014-5-12',
	 'pdflink': u'http://arxiv.org/pdf/1405.2652',
	 'subjects': u'Learning (cs.LG)',
	 'title': u'\nSelecting Near-Optimal Approximate State Representations in  Reinforcement Learning',
	 'urllink': u'http://arxiv.org/abs/1405.2652'}
2015-03-23 20:02:13+0000 [xxu461000] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1406.0295> (referer: http://arxiv.org/list/cs/14?skip=6000&show=1000)
2015-03-23 20:02:13+0000 [xxu461000] DEBUG: Scraped from <200 http://arxiv.org/abs/1406.0295>
	{'abstract': u"The growth of Internet has led to new approaches for distance education. The main mechanisms involved in this process are distance learning and distance evaluation. Distance learning have multiple forms, from browsing and finding information when needed or collecting information onto ready-to-use packages, to interactive content where the learner is able to affect and control the content in some way. From the teacher's point of view, evaluation of learning aims to determine whether the students have achieved the goals set for a particular topic or course. Typically this is tested somehow and based on the test results, evaluation is performed. This kind of method is always restricted to the test, however. Many times the students may actually learn something completely different from those that are tested. Furthermore, testing does not necessarily take the students' individual needs into account. However, when properly designed, the testing method is a practical and even fairly reliable way of evaluating learning.",
	 'authors': u'Stefan Gheorghe Pentiuc, Felicia Giza, Ovidiu Andrei Schipor,',
	 'category': u'Computer Science ',
	 'date': '2014-6-2',
	 'pdflink': u'http://arxiv.org/pdf/1406.0295',
	 'subjects': u'Computers and Society (cs.CY)',
	 'title': u'\nMobile Agents for Distance Evaluation Procedures',
	 'urllink': u'http://arxiv.org/abs/1406.0295'}
2015-03-23 20:02:18+0000 [xxu461000] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1407.5711> (referer: http://arxiv.org/list/cs/14?skip=8000&show=1000)
2015-03-23 20:02:18+0000 [xxu461000] DEBUG: Scraped from <200 http://arxiv.org/abs/1407.5711>
	{'abstract': u'Intelligent terminals often produce a large number of data packets of small lengths. For these packets, it is inefficient to follow the conventional medium access control (MAC) protocols because they lead to poor utilization of service resources. We propose a novel multiple access scheme that targets massive multiple-input multiple-output (MIMO) systems based on compressive sensing (CS). We employ block precoding in the time domain to enable the simultaneous transmissions of many users, which could be even more than the number of receive antennas at the base station. We develop a block-sparse system model and adopt the block orthogonal matching pursuit (BOMP) algorithm to recover the transmitted signals. Conditions for data recovery guarantees are identified and numerical results demonstrate that our scheme is efficient for uplink small packet transmission.',
	 'authors': u'Ronggui Xie, Huarui Yin, Zhengdao Wang, Xiaohui Chen,',
	 'category': u'Computer Science ',
	 'date': '2014-7-22',
	 'pdflink': u'http://arxiv.org/pdf/1407.5711',
	 'subjects': u'Information Theory (cs.IT)',
	 'title': u'\nA Novel Uplink Data Transmission Scheme For Small Packets In Massive  MIMO System',
	 'urllink': u'http://arxiv.org/abs/1407.5711'}
2015-03-23 20:02:23+0000 [xxu461000] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1405.2642> (referer: http://arxiv.org/list/cs/14?skip=5000&show=1000)
2015-03-23 20:02:23+0000 [xxu461000] DEBUG: Scraped from <200 http://arxiv.org/abs/1405.2642>
	{'abstract': u"The dynamics of belief and knowledge is one of the major components of any autonomous system that should be able to incorporate new pieces of information. We introduced the Horn knowledge base dynamics to deal with two important points: first, to handle belief states that need not be deductively closed; and the second point is the ability to declare certain parts of the belief as immutable. In this paper, we address another, radically new approach to this problem. This approach is very close to the Hansson's dyadic representation of belief. Here, we consider the immutable part as defining a new logical system. By a logical system, we mean that it defines its own consequence relation and closure operator. Based on this, we provide an abductive framework for Horn knowledge base dynamics.",
	 'authors': u'Radhakrishnan Delhibabu,',
	 'category': u'Computer Science ',
	 'date': '2014-5-12',
	 'pdflink': u'http://arxiv.org/pdf/1405.2642',
	 'subjects': u'Logic in Computer Science (cs.LO)',
	 'title': u'\nAn Abductive Framework for Horn Knowledge Base Dynamics',
	 'urllink': u'http://arxiv.org/abs/1405.2642'}
2015-03-23 20:02:28+0000 [xxu461000] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1406.0292> (referer: http://arxiv.org/list/cs/14?skip=6000&show=1000)
2015-03-23 20:02:28+0000 [xxu461000] DEBUG: Scraped from <200 http://arxiv.org/abs/1406.0292>
	{'abstract': u"The Isabelle proof assistant comes equipped with a very powerful tactic for term simplification. While tremendously useful, the results of simplifying a term do not always match the user's expectation: sometimes, the resulting term is not in the form the user expected, or the simplifier fails to apply a rule. We describe a new, interactive tracing facility which offers insight into the hierarchical structure of the simplification with user-defined filtering, memoization and search. The new simplifier trace is integrated into the Isabelle/jEdit Prover IDE.",
	 'authors': u'Lars Hupel,',
	 'category': u'Computer Science ',
	 'date': '2014-6-2',
	 'pdflink': u'http://arxiv.org/pdf/1406.0292',
	 'subjects': u'Mathematical Software (cs.MS)',
	 'title': u'\nInteractive Simplifier Tracing and Debugging in Isabelle',
	 'urllink': u'http://arxiv.org/abs/1406.0292'}
2015-03-23 20:02:33+0000 [xxu461000] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1407.5701> (referer: http://arxiv.org/list/cs/14?skip=8000&show=1000)
2015-03-23 20:02:33+0000 [xxu461000] DEBUG: Scraped from <200 http://arxiv.org/abs/1407.5701>
	{'abstract': u"The development of scientific software is often a partnership between domain scientists and scientific software engineers. It is especially important to embrace these collaborations when developing advanced scientific software, where sustainability, reproducibility, and extensibility are important. In the ideal case, as discussed in this manuscript, this brings together teams composed of the world's foremost scientific experts in a given field with seasoned software developers experienced in forming highly collaborative teams working on software to further scientific research.",
	 'authors': u"Marcus D. Hanwell, Patrick O'Leary, Bob O'Bara,",
	 'category': u'Computer Science ',
	 'date': '2014-7-22',
	 'pdflink': u'http://arxiv.org/pdf/1407.5701',
	 'subjects': u'Software Engineering (cs.SE)',
	 'title': u'\nSustainable Software Ecosystems: Software Engineers, Domain Scientists,  and Engineers Collaborating for Science',
	 'urllink': u'http://arxiv.org/abs/1407.5701'}
2015-03-23 20:02:37+0000 [xxu461000] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1405.2641> (referer: http://arxiv.org/list/cs/14?skip=5000&show=1000)
2015-03-23 20:02:37+0000 [xxu461000] DEBUG: Scraped from <200 http://arxiv.org/abs/1405.2641>
	{'abstract': u'In this paper, we present multimodal 2D +3D face recognition method using block based curvelet features. The 3D surface of face (Depth Map) is computed from the stereo face images using stereo vision technique. The statistical measures such as mean, standard deviation, variance and entropy are extracted from each block of curvelet subband for both depth and intensity images independently.In order to compute the decision score, the KNN classifier is employed independently for both intensity and depth map. Further, computed decision scoresof intensity and depth map are combined at decision level to improve the face recognition rate. The combination of intensity and depth map is verified experimentally using benchmark face database. The experimental results show that the proposed multimodal method is better than individual modality.',
	 'authors': u'Jyothi K, Prabhakar C.J,',
	 'category': u'Computer Science ',
	 'date': '2014-5-12',
	 'pdflink': u'http://arxiv.org/pdf/1405.2641',
	 'subjects': u'Computer Vision and Pattern Recognition (cs.CV)',
	 'title': u'\nMulti Modal Face Recognition Using Block Based Curvelet Features',
	 'urllink': u'http://arxiv.org/abs/1405.2641'}
2015-03-23 20:02:42+0000 [xxu461000] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1406.0289> (referer: http://arxiv.org/list/cs/14?skip=6000&show=1000)
2015-03-23 20:02:42+0000 [xxu461000] DEBUG: Scraped from <200 http://arxiv.org/abs/1406.0289>
	{'abstract': u'Scope of this paper is to consider a mean field neural model which takes into account the functional neurogeometry of the visual cortex modelled as a group of rotations and translations. The model generalizes well known results of Bressloff and Cowan which, in absence of input, accounts for hallucination patterns. The main result of our study consists in showing that in presence of a visual input, the eigenmodes of the linearized operator which become stable represent perceptual units present in the image. The result is strictly related to dimensionality reduction and clustering problems.',
	 'authors': u'Alessandro Sarti, Giovanna Citti,',
	 'category': u'Computer Science ',
	 'date': '2014-6-2',
	 'pdflink': u'http://arxiv.org/pdf/1406.0289',
	 'subjects': u'Computer Vision and Pattern Recognition (cs.CV)',
	 'title': u'\nThe constitution of visual perceptual units in the functional  architecture of V1',
	 'urllink': u'http://arxiv.org/abs/1406.0289'}
2015-03-23 20:02:47+0000 [xxu461000] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1407.5699> (referer: http://arxiv.org/list/cs/14?skip=8000&show=1000)
2015-03-23 20:02:47+0000 [xxu461000] DEBUG: Scraped from <200 http://arxiv.org/abs/1407.5699>
	{'abstract': u"This paper investigates the feasibility of using a discriminate pricing scheme to offset the inconvenience that is experienced by an energy user (EU) in trading its energy with an energy controller in smart grid. The main objective is to encourage EUs with small distributed energy resources (DERs), or with high sensitivity to their inconvenience, to take part in the energy trading via providing incentive to them with relatively higher payment at the same time as reducing the total cost to the energy controller. The proposed scheme is modeled through a two-stage Stackelberg game that describes the energy trading between a shared facility authority (SFA) and EUs in a smart community. A suitable cost function is proposed for the SFA to leverage the generation of discriminate pricing according to the inconvenience experienced by each EU. It is shown that the game has a unique sub-game perfect equilibrium (SPE), under the certain condition at which the SFA's total cost is minimized, and that each EU receives its best utility according to its associated inconvenience for the given price. A backward induction technique is used to derive a closed form expression for the price function at SPE, and thus the dependency of price on an EU's different decision parameters is explained for the studied system. Numerical examples are provided to show the beneficial properties of the proposed scheme.",
	 'authors': u'Wayes Tushar, Chau Yuen, Bo Chai, David B. Smith, H. Vincent Poor,',
	 'category': u'Computer Science ',
	 'date': '2014-7-22',
	 'pdflink': u'http://arxiv.org/pdf/1407.5699',
	 'subjects': u'Systems and Control (cs.SY)',
	 'title': u'\nFeasibility of Using Discriminate Pricing Schemes for Energy Trading in  Smart Grid',
	 'urllink': u'http://arxiv.org/abs/1407.5699'}
2015-03-23 20:02:48+0000 [xxu461000] INFO: Crawled 109 pages (at 12 pages/min), scraped 92 items (at 12 items/min)
2015-03-23 20:02:52+0000 [xxu461000] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1405.2636> (referer: http://arxiv.org/list/cs/14?skip=5000&show=1000)
2015-03-23 20:02:52+0000 [xxu461000] DEBUG: Scraped from <200 http://arxiv.org/abs/1405.2636>
	{'abstract': u'The ongoing hardware evolution exhibits an escalation in the number, as well as in the heterogeneity, of computing resources. The pressure to maintain reasonable levels of performance and portability forces application developers to leave the traditional programming paradigms and explore alternative solutions. PaStiX is a parallel sparse direct solver, based on a dynamic scheduler for modern hierarchical manycore architectures. In this paper, we study the benefits and limits of replacing the highly specialized internal scheduler of the PaStiX solver with two generic runtime systems: PaRSEC and StarPU. The tasks graph of the factorization step is made available to the two runtimes, providing them the opportunity to process and optimize its traversal in order to maximize the algorithm efficiency for the targeted hardware platform. A comparative study of the performance of the PaStiX solver on top of its native internal scheduler, PaRSEC, and StarPU frameworks, on different execution environments, is performed. The analysis highlights that these generic task-based runtimes achieve comparable results to the application-optimized embedded scheduler on homogeneous platforms. Furthermore, they are able to significantly speed up the solver on heterogeneous environments by taking advantage of the accelerators while hiding the complexity of their efficient manipulation from the programmer.',
	 'authors': u'Xavier Lacoste, Mathieu Faverge, George Bosilca, Pierre Ramet, Samuel Thibault,',
	 'category': u'Computer Science ',
	 'date': '2014-5-12',
	 'pdflink': u'http://arxiv.org/pdf/1405.2636',
	 'subjects': u'Distributed, Parallel, and Cluster Computing (cs.DC)',
	 'title': u'\nTaking advantage of hybrid systems for sparse direct solvers via  task-based runtimes',
	 'urllink': u'http://arxiv.org/abs/1405.2636'}
2015-03-23 20:02:57+0000 [xxu461000] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1406.0288> (referer: http://arxiv.org/list/cs/14?skip=6000&show=1000)
2015-03-23 20:02:57+0000 [xxu461000] DEBUG: Scraped from <200 http://arxiv.org/abs/1406.0288>
	{'abstract': u'Continuous action recognition is more challenging than isolated recognition because classification and segmentation must be simultaneously carried out. We build on the well known dynamic time warping (DTW) framework and devise a novel visual alignment technique, namely dynamic frame warping (DFW), which performs isolated recognition based on per-frame representation of videos, and on aligning a test sequence with a model sequence. Moreover, we propose two extensions which enable to perform recognition concomitant with segmentation, namely one-pass DFW and two-pass DFW. These two methods have their roots in the domain of continuous recognition of speech and, to the best of our knowledge, their extension to continuous visual action recognition has been overlooked. We test and illustrate the proposed techniques with a recently released dataset (RAVEL) and with two public-domain datasets widely used in action recognition (Hollywood-1 and Hollywood-2). We also compare the performances of the proposed isolated and continuous recognition algorithms with several recently published methods.',
	 'authors': u'Kaustubh Kulkarni, Georgios Evangelidis, Jan Cech, Radu Horaud,',
	 'category': u'Computer Science ',
	 'date': '2014-6-2',
	 'pdflink': u'http://arxiv.org/pdf/1406.0288',
	 'subjects': u'Computer Vision and Pattern Recognition (cs.CV)',
	 'title': u'\nContinuous Action Recognition Based on Sequence Alignment',
	 'urllink': u'http://arxiv.org/abs/1406.0288'}
2015-03-23 20:03:03+0000 [xxu461000] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1407.5674> (referer: http://arxiv.org/list/cs/14?skip=8000&show=1000)
2015-03-23 20:03:03+0000 [xxu461000] DEBUG: Scraped from <200 http://arxiv.org/abs/1407.5674>
	{'abstract': u'We consider variants of the following multi-covering problem with disks. We are given two point sets (servers) and (clients) in the plane, a coverage function , and a constant . Centered at each server is a single disk whose radius we are free to set. The requirement is that each client be covered by at least of the server disks. The objective function we wish to minimize is the sum of the -th powers of the disk radii. We present a polynomial time algorithm for this problem achieving an approximation.',
	 'authors': u'Santanu Bhowmick, Kasturi Varadarajan, Shi-Ke Xue,',
	 'category': u'Computer Science ',
	 'date': '2014-7-21',
	 'pdflink': u'http://arxiv.org/pdf/1407.5674',
	 'subjects': u'Computational Geometry (cs.CG)',
	 'title': u'\nA Constant-Factor Approximation for Multi-Covering with Disks',
	 'urllink': u'http://arxiv.org/abs/1407.5674'}
2015-03-23 20:03:08+0000 [xxu461000] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1405.2627> (referer: http://arxiv.org/list/cs/14?skip=5000&show=1000)
2015-03-23 20:03:08+0000 [xxu461000] DEBUG: Scraped from <200 http://arxiv.org/abs/1405.2627>
	{'abstract': u"Networking is undergoing a transformation throughout our industry. The need for scalable network control and automation shifts the focus from hardware driven products with ad hoc control to Software Defined Networks. This process is now well underway. In this paper, we adopt the perspective of the Promise Theory to examine the current and future states of networking technologies. The goal is to see beyond specific technologies, topologies and approaches and define principles. Promise Theory's bottom-up modeling has been applied to server management for many years and lends itself to principles of self-healing, scalability and robustness.",
	 'authors': u'Paul Borril, Mark Burgess, Todd Craw, Mike Dvorkin,',
	 'category': u'Computer Science ',
	 'date': '2014-5-12',
	 'pdflink': u'http://arxiv.org/pdf/1405.2627',
	 'subjects': u'Networking and Internet Architecture (cs.NI)',
	 'title': u'\nA Promise Theory Perspective on Data Networks',
	 'urllink': u'http://arxiv.org/abs/1405.2627'}
2015-03-23 20:03:13+0000 [xxu461000] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1406.0285> (referer: http://arxiv.org/list/cs/14?skip=6000&show=1000)
2015-03-23 20:03:13+0000 [xxu461000] DEBUG: Scraped from <200 http://arxiv.org/abs/1406.0285>
	{'abstract': u'Supermarket models are a class of parallel queueing networks with an adaptive control scheme that play a key role in the study of resource management of, such as, computer networks, manufacturing systems and transportation networks. When the arrival processes are non-Poisson and the service times are non-exponential, analysis of such a supermarket model is always limited, interesting, and challenging. This paper describes a supermarket model with non-Poisson inputs: Markovian Arrival Processes (MAPs) and with non-exponential service times: Phase-type (PH) distributions, and provides a generalized matrix-analytic method which is first combined with the operator semigroup and the mean-field limit. When discussing such a more general supermarket model, this paper makes some new results and advances as follows: (1) Providing a detailed probability analysis for setting up an infinite-dimensional system of differential vector equations satisfied by the expected fraction vector, where "the invariance of environment factors" is given as an important result. (2) Introducing the phase-type structure to the operator semigroup and to the mean-field limit, and a Lipschitz condition can be obtained by means of a unified matrix-differential algorithm. (3) The matrix-analytic method is used to compute the fixed point which leads to performance computation of this system. Finally, we use some numerical examples to illustrate how the performance measures of this supermarket model depend on the non-Poisson inputs and on the non-exponential service times. Thus the results of this paper give new highlight on understanding influence of non-Poisson inputs and of non-exponential service times on performance measures of more general supermarket models.',
	 'authors': u'Quan-Lin Li, John C.S. Lui,',
	 'category': u'Computer Science ',
	 'date': '2014-6-2',
	 'pdflink': u'http://arxiv.org/pdf/1406.0285',
	 'subjects': u'Performance (cs.PF)',
	 'title': u'\nBlock-Structured Supermarket Models',
	 'urllink': u'http://arxiv.org/abs/1406.0285'}
2015-03-23 20:03:18+0000 [xxu461000] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1407.5670> (referer: http://arxiv.org/list/cs/14?skip=8000&show=1000)
2015-03-23 20:03:18+0000 [xxu461000] DEBUG: Scraped from <200 http://arxiv.org/abs/1407.5670>
	{'abstract': u'This article provides an introduction to Rust, a systems language by Mozilla, to programmers already familiar with Haskell, OCaml or other functional languages.',
	 'authors': u'Raphael Poss,',
	 'category': u'Computer Science ',
	 'date': '2014-7-21',
	 'pdflink': u'http://arxiv.org/pdf/1407.5670',
	 'subjects': u'Programming Languages (cs.PL)',
	 'title': u'\nRust for functional programmers',
	 'urllink': u'http://arxiv.org/abs/1407.5670'}
2015-03-23 20:03:23+0000 [xxu461000] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1405.2622> (referer: http://arxiv.org/list/cs/14?skip=5000&show=1000)
2015-03-23 20:03:23+0000 [xxu461000] DEBUG: Scraped from <200 http://arxiv.org/abs/1405.2622>
	{'abstract': u'In this paper, we study news group modeling and forecasting methods using quantitative data generated by our large-scale natural language processing (NLP) text analysis system. A news group is a set of news entities, like top U.S. cities, governors, senators, golfers, or movie actors. Our fame distribution analysis of news groups shows that log-normal and power-law distributions generally could describe news groups in many aspects. We use several real news groups including cities, politicians, and CS professors, to evaluate our news group models in terms of time series data distribution analysis, group-fame probability analysis, and fame-changing analysis over long time. We also build a practical news generation model using a HMM (Hidden Markov Model) based approach. Most importantly, our analysis shows the future entity fame distribution has a power-law tail. That is, only a small number of news entities in a group could become famous in the future. Based on these analysis we are able to answer some interesting forecasting problems - for example, what is the future average fame (or maximum fame) of a specific news group? And what is the probability that some news entity become very famous within a certain future time range? We also give concrete examples to illustrate our forecasting approaches.',
	 'authors': u'Wenbin Zhang, Steven Skiena,',
	 'category': u'Computer Science ',
	 'date': '2014-5-12',
	 'pdflink': u'http://arxiv.org/pdf/1405.2622',
	 'subjects': u'Social and Information Networks (cs.SI)',
	 'title': u'\nNews-Based Group Modeling and Forecasting',
	 'urllink': u'http://arxiv.org/abs/1405.2622'}
2015-03-23 20:03:25+0000 [xxu461000] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1406.0271> (referer: http://arxiv.org/list/cs/14?skip=6000&show=1000)
2015-03-23 20:03:25+0000 [xxu461000] DEBUG: Scraped from <200 http://arxiv.org/abs/1406.0271>
	{'abstract': u'The simple scheme of treating interference as noise (TIN) is studied in this paper for the 3 x 2 X channel. A new sum-capacity upper bound is derived. This upper bound is transformed into a generalized degrees-of-freedom (GDoF) upper bound, and is shown to coincide with the achievable GDoF of scheme that combines TDMA and TIN for some conditions on the channel parameters. These conditions specify a noisy interference regime which extends noisy interference regimes available in literature. As a by-product, the sum-capacity of the 3 x 2 X channel is characterized within a constant gap in the given noisy interference regime.',
	 'authors': u'Soheil Gherekhloo, Anas Chaaban, Aydin Sezgin,',
	 'category': u'Computer Science ',
	 'date': '2014-6-2',
	 'pdflink': u'http://arxiv.org/pdf/1406.0271',
	 'subjects': u'Information Theory (cs.IT)',
	 'title': u'\nExtended Generalized DoF Optimality Regime of Treating Interference as  Noise in the X Channel',
	 'urllink': u'http://arxiv.org/abs/1406.0271'}
2015-03-23 20:03:31+0000 [xxu461000] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1407.5661> (referer: http://arxiv.org/list/cs/14?skip=8000&show=1000)
2015-03-23 20:03:31+0000 [xxu461000] DEBUG: Scraped from <200 http://arxiv.org/abs/1407.5661>
	{'abstract': u"Streaming, big data applications face challenges in creating scalable data flow pipelines, in which multiple data streams must be collected, stored, queried, and analyzed. These data sources are characterized by their volume (in terms of dataset size), velocity (in terms of data rates), and variety (in terms of fields and types). For many applications, distributed NoSQL databases are effective alternatives to traditional relational database management systems. This paper considers a cyber situational awareness system that uses the Apache Accumulo database to provide scalable data warehousing, real-time data ingest, and responsive querying for human users and analytic algorithms. We evaluate Accumulo's ingestion scalability as a function of number of client processes and servers. We also describe a flexible data model with effective techniques for query planning and query batching to deliver responsive results. Query performance is evaluated in terms of latency of the client receiving initial result sets. Accumulo performance is measured on a database of up to 8 nodes using real cyber data.",
	 'authors': u"Scott M. Sawyer, B. David O'Gwynn,",
	 'category': u'Computer Science ',
	 'date': '2014-7-21',
	 'pdflink': u'http://arxiv.org/pdf/1407.5661',
	 'subjects': u'Databases (cs.DB)',
	 'title': u'\nEvaluating Accumulo Performance for a Scalable Cyber Data Processing  Pipeline',
	 'urllink': u'http://arxiv.org/abs/1407.5661'}
2015-03-23 20:03:37+0000 [xxu461000] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1405.2605> (referer: http://arxiv.org/list/cs/14?skip=5000&show=1000)
2015-03-23 20:03:37+0000 [xxu461000] DEBUG: Scraped from <200 http://arxiv.org/abs/1405.2605>
	{'abstract': u'A discrete-time Wiener phase noise channel model is introduced in which multiple samples are available at the output for every input symbol. A lower bound on the capacity is developed. At high signal-to-noise ratio (SNR), if the number of samples per symbol grows with the square root of the SNR, the capacity pre-log is at least 3/4. This is strictly greater than the capacity pre-log of the Wiener phase noise channel with only one sample per symbol, which is 1/2. It is shown that amplitude modulation achieves a pre-log of 1/2 while phase modulation achieves a pre-log of at least 1/4.',
	 'authors': u'Hassan Ghozlan, Gerhard Kramer,',
	 'category': u'Computer Science ',
	 'date': '2014-5-12',
	 'pdflink': u'http://arxiv.org/pdf/1405.2605',
	 'subjects': u'Information Theory (cs.IT)',
	 'title': u'\nPhase Modulation for Discrete-time Wiener Phase Noise Channels with  Oversampling at High SNR',
	 'urllink': u'http://arxiv.org/abs/1405.2605'}
2015-03-23 20:03:41+0000 [xxu461000] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1406.0263> (referer: http://arxiv.org/list/cs/14?skip=6000&show=1000)
2015-03-23 20:03:41+0000 [xxu461000] DEBUG: Scraped from <200 http://arxiv.org/abs/1406.0263>
	{'abstract': u'We give a new characterization of maximal repetitions (or runs) in strings based on Lyndon words. The characterization leads to a proof of what was known as the "runs" conjecture (Kolpakov &amp; Kucherov (FOCS \'99)), which states that the maximum number of runs in a string of length is less than . The proof is remarkably simple, considering the numerous endeavors to tackle this problem in the last 15 years, and significantly improves our understanding of how runs can occur in strings. In addition, we obtain an upper bound of for the maximum sum of exponents of runs in a string of length , improving on the best known bound of by Crochemore et al. (JDA 2012). We also give improved analyses on related problems. Furthermore, the characterization also gives rise to a new, conceptually simple linear-time algorithm for computing all the runs in a string. A notable characteristic of our algorithm is that, unlike all existing linear-time algorithms, it does not utilize the Lempel-Ziv factorization of the string.',
	 'authors': u'Hideo Bannai, Tomohiro I, Shunsuke Inenaga, Yuto Nakashima, Masayuki Takeda, Kazuya Tsuruta,',
	 'category': u'Computer Science ',
	 'date': '2014-6-2',
	 'pdflink': u'http://arxiv.org/pdf/1406.0263',
	 'subjects': u'Discrete Mathematics (cs.DM)',
	 'title': u'\nThe "Runs" Theorem',
	 'urllink': u'http://arxiv.org/abs/1406.0263'}
2015-03-23 20:03:47+0000 [xxu461000] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1407.5659> (referer: http://arxiv.org/list/cs/14?skip=8000&show=1000)
2015-03-23 20:03:47+0000 [xxu461000] DEBUG: Scraped from <200 http://arxiv.org/abs/1407.5659>
	{'abstract': u'The rate regions of multilevel diversity coding systems (MDCS), a sub-class of the broader family of multi-source multi-sink networks with special structure, are investigated. After showing how to enumerate all non-isomorphic MDCS instances of a given size, the Shannon outer bound and several achievable inner bounds based on linear codes are given for the rate region of each non-isomorphic instance. For thousands of MDCS instances, the bounds match, and hence exact rate regions are proven. Results gained from these computations are summarized in key statistics involving aspects such as the sufficiency of scalar binary codes, the necessary size of vector binary codes, etc. Also, it is shown how to generate computer aided human readable converse proofs, as well as how to construct the codes for an achievability proof. Based on this large repository of rate regions, a series of results about general MDCS cases that they inspired are introduced and proved. In particular, a series of embedding operations that preserve the property of sufficiency of scalar or vector codes are presented. The utility of these operations is demonstrated by boiling the thousands of MDCS instances for which binary scalar codes are insufficient down to 12 forbidden smallest embedded MDCS instances.',
	 'authors': u'Congduan Li, Steven Weber, John MacLaren Walsh,',
	 'category': u'Computer Science ',
	 'date': '2014-7-21',
	 'pdflink': u'http://arxiv.org/pdf/1407.5659',
	 'subjects': u'Information Theory (cs.IT)',
	 'title': u'\nMultilevel Diversity Coding Systems: Rate Regions, Codes, Computation, &  Forbidden Minors',
	 'urllink': u'http://arxiv.org/abs/1407.5659'}
2015-03-23 20:03:48+0000 [xxu461000] INFO: Crawled 121 pages (at 12 pages/min), scraped 104 items (at 12 items/min)
2015-03-23 20:03:52+0000 [xxu461000] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1404.5701> (referer: http://arxiv.org/list/cs/14?skip=4000&show=1000)
2015-03-23 20:03:52+0000 [xxu461000] DEBUG: Scraped from <200 http://arxiv.org/abs/1404.5701>
	{'abstract': u'In this paper we consider a wiretap channel with a secret key buffer. We use the coding scheme of [1] to enhance the secrecy rate to the capacity of the main channel, while storing each securely transmitted message in the secret key buffer. We use the oldest secret bits from the buffer to be used as a secret key to transmit a message in a slot and then remove those bits. With this scheme we are able to prove stronger results than those in [1]. i.e., not only the message which is being transmitted currently, but all the messages transmitted in last slots are secure with respect to all the information that the eavesdropper possesses, where can be chosen arbitrarily large.',
	 'authors': u'Shahid Mehraj Shah, Vinod Sharma,',
	 'category': u'Computer Science ',
	 'date': '2014-4-23',
	 'pdflink': u'http://arxiv.org/pdf/1404.5701',
	 'subjects': u'Information Theory (cs.IT)',
	 'title': u'\nAchieving Shannon Capacity in a Wiretap Channel via Previous Messages',
	 'urllink': u'http://arxiv.org/abs/1404.5701'}
2015-03-23 20:03:58+0000 [xxu461000] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1405.2602> (referer: http://arxiv.org/list/cs/14?skip=5000&show=1000)
2015-03-23 20:03:58+0000 [xxu461000] DEBUG: Scraped from <200 http://arxiv.org/abs/1405.2602>
	{'abstract': u'Let be a finite commutative chain ring with unique maximal ideal , and let be a positive integer coprime with the characteristic of . In this paper, the algebraic structure of cyclic codes of length over is investigated. Some new necessary and sufficient conditions for the existence of nontrivial self-dual cyclic codes are provided. An enumeration formula for the self-dual cyclic codes is also studied.',
	 'authors': u'Bocong Chen, San Ling, Guanghui Zhang,',
	 'category': u'Computer Science ',
	 'date': '2014-5-11',
	 'pdflink': u'http://arxiv.org/pdf/1405.2602',
	 'subjects': u'Information Theory (cs.IT)',
	 'title': u'\nSelf-dual cyclic codes over finite chain rings',
	 'urllink': u'http://arxiv.org/abs/1405.2602'}
2015-03-23 20:04:03+0000 [xxu461000] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1406.0256> (referer: http://arxiv.org/list/cs/14?skip=6000&show=1000)
2015-03-23 20:04:03+0000 [xxu461000] DEBUG: Scraped from <200 http://arxiv.org/abs/1406.0256>
	{'abstract': u'Simulations play a vital role in implementing, testing and validating proposed algorithms and protocols in VANET. Mobility model, defined as the movement pattern of vehicles, is one of the main factors that contribute towards the efficient implementation of VANET algorithms and protocols. Using near reality mobility models ensure that accurate results are obtained from simulations. Mobility models that have been proposed and used to implement and test VANET protocols and algorithms are either the urban mobility model or highway mobility model. Algorithms and protocols implemented using urban or highway mobility models may not produce accurate results in hybrid mobility models without enhancement due to the vast differences in mobility patterns. It is on this score the Hybrist, a novel hybrid mobility model is proposed. The realistic mobility pattern trace file of the proposed Hybrist hybrid mobility model can be imported to VANET simulators such as Veins and network simulators such as ns2 and Qualnet to simulate VANET algorithms and protocols.',
	 'authors': u'Wiseborn Manfe Danquah, Turgay D Altilar,',
	 'category': u'Computer Science ',
	 'date': '2014-6-2',
	 'pdflink': u'http://arxiv.org/pdf/1406.0256',
	 'subjects': u'Networking and Internet Architecture (cs.NI)',
	 'title': u'\nHYBRIST Mobility Model- A Novel Hybrid Mobility Model for VANET  Simulations',
	 'urllink': u'http://arxiv.org/abs/1406.0256'}
2015-03-23 20:04:07+0000 [xxu461000] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1407.5656> (referer: http://arxiv.org/list/cs/14?skip=8000&show=1000)
2015-03-23 20:04:07+0000 [xxu461000] DEBUG: Scraped from <200 http://arxiv.org/abs/1407.5656>
	{'abstract': u'In the big data era, scalability has become a crucial requirement for any useful computational model. Probabilistic graphical models are very useful for mining and discovering data insights, but they are not scalable enough to be suitable for big data problems. Bayesian Networks particularly demonstrate this limitation when their data is represented using few random variables while each random variable has a massive set of values. With hierarchical data - data that is arranged in a treelike structure with several levels - one would expect to see hundreds of thousands or millions of values distributed over even just a small number of levels. When modeling this kind of hierarchical data across large data sets, Bayesian networks become infeasible for representing the probability distributions for the following reasons: i) Each level represents a single random variable with hundreds of thousands of values, ii) The number of levels is usually small, so there are also few random variables, and iii) The structure of the network is predefined since the dependency is modeled top-down from each parent to each of its child nodes, so the network would contain a single linear path for the random variables from each parent to each child node. In this paper we present a scalable probabilistic graphical model to overcome these limitations for massive hierarchical data. We believe the proposed model will lead to an easily-scalable, more readable, and expressive implementation for problems that require probabilistic-based solutions for massive amounts of hierarchical data. We successfully applied this model to solve two different challenging probabilistic-based problems on massive hierarchical data sets for different domains, namely, bioinformatics and latent semantic discovery over search logs.',
	 'authors': u'Khalifeh AlJadda, Mohammed Korayem, Camilo Ortiz, Trey Grainger, John A. Miller, William S. York,',
	 'category': u'Computer Science ',
	 'date': '2014-7-21',
	 'pdflink': u'http://arxiv.org/pdf/1407.5656',
	 'subjects': u'Artificial Intelligence (cs.AI)',
	 'title': u'\nPGMHD: A Scalable Probabilistic Graphical Model for Massive Hierarchical  Data Problems',
	 'urllink': u'http://arxiv.org/abs/1407.5656'}
2015-03-23 20:04:11+0000 [xxu461000] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1404.5692> (referer: http://arxiv.org/list/cs/14?skip=4000&show=1000)
2015-03-23 20:04:11+0000 [xxu461000] DEBUG: Scraped from <200 http://arxiv.org/abs/1404.5692>
	{'abstract': u'In many signal processing applications, one aims to reconstruct a signal that has a simple representation with respect to a certain basis or frame. Fundamental elements of the basis known as "atoms" allow us to define "atomic norms" that can be used to construct convex regularizers for the reconstruction problem. Efficient algorithms are available to solve the reconstruction problems in certain special cases, but an approach that works well for general atomic norms remains to be found. This paper describes an optimization algorithm called CoGEnT , which produces solutions with succinct atomic representations for reconstruction problems, generally formulated with atomic norm constraints. CoGEnT combines a greedy selection scheme based on the conditional gradient approach with a backward (or "truncation") step that exploits the quadratic nature of the objective to reduce the basis size. We establish convergence properties and validate the algorithm via extensive numerical experiments on a suite of signal processing applications. Our algorithm and analysis are also novel in that they allow for inexact forward steps. In practice, CoGEnT significantly outperforms the basic conditional gradient method, and indeed many methods that are tailored to specific applications, when the truncation steps are defined appropriately. We also introduce several novel applications that are enabled by the atomic norm framework, including tensor completion, moment problems in signal processing, and graph deconvolution.',
	 'authors': u'Nikhil Rao, Parikshit Shah, Stephen Wright,',
	 'category': u'Computer Science ',
	 'date': '2014-4-23',
	 'pdflink': u'http://arxiv.org/pdf/1404.5692',
	 'subjects': u'Data Structures and Algorithms (cs.DS)',
	 'title': u'\nForward - Backward Greedy Algorithms for Atomic Norm Regularization',
	 'urllink': u'http://arxiv.org/abs/1404.5692'}
2015-03-23 20:04:17+0000 [xxu461000] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1405.2600> (referer: http://arxiv.org/list/cs/14?skip=5000&show=1000)
2015-03-23 20:04:17+0000 [xxu461000] DEBUG: Scraped from <200 http://arxiv.org/abs/1405.2600>
	{'abstract': u'Many machine learning algorithms are based on the assumption that training examples are drawn identically and independently. However, this assumption does not hold anymore when learning from a networked sample because two or more training examples may share some common objects, and hence share the features of these shared objects. We first show that the classic approach of ignoring this problem potentially can have a disastrous effect on the accuracy of statistics, and then consider alternatives. One of these is to only use independent examples, discarding other information. However, this is clearly suboptimal. We analyze sample error bounds in a networked setting, providing both improved and new results. Next, we propose an efficient weighting method which achieves a better sample error bound than those of previous methods. Our approach is based on novel concentration inequalities for networked variables.',
	 'authors': u'Yuyi Wang, Jan Ramon, Zheng-Chu Guo,',
	 'category': u'Computer Science ',
	 'date': '2014-5-11',
	 'pdflink': u'http://arxiv.org/pdf/1405.2600',
	 'subjects': u'Artificial Intelligence (cs.AI)',
	 'title': u'\nLearning from networked examples',
	 'urllink': u'http://arxiv.org/abs/1405.2600'}
2015-03-23 20:04:23+0000 [xxu461000] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1406.0253> (referer: http://arxiv.org/list/cs/14?skip=6000&show=1000)
2015-03-23 20:04:23+0000 [xxu461000] DEBUG: Scraped from <200 http://arxiv.org/abs/1406.0253>
	{'abstract': u'Mobility is an important attribute in todays computing world. Mobile devices,smartphone and tablet PC are becoming an integral part of human life because they are most effective and convenient communication tools. This paper proposes a system to connect and access the desktops of remote computer systems using an android based Smartphone. Virtual Network Computing based architecture is used to develop the proposed system. Through a VirtuMob viewer provided on the users Smartphone, the user will be able to access and manipulate the desktops of remote computers. Several functionality such as viewing the desktop, mouse operations, keyboard operations, manipulation of documents can be performed from the Smartphone. VirtuMob server should be running on the remote system and it must be attached to a network. VirtuMob Accelerator is used to process the RFB frames of the desktop, perform Encoding of the frames and then relay the frames to the viewer over the internet. Several Encoding techniques are studied and analysed to determine which is best suited for the proposed system.',
	 'authors': u'M H Soorajprasad, Balapradeep K N, Antony P J,',
	 'category': u'Computer Science ',
	 'date': '2014-6-2',
	 'pdflink': u'http://arxiv.org/pdf/1406.0253',
	 'subjects': u'Other Computer Science (cs.OH)',
	 'title': u'\nVirtuMob : Remote Desktop Virtualization Solution for Smarphones',
	 'urllink': u'http://arxiv.org/abs/1406.0253'}
2015-03-23 20:04:28+0000 [xxu461000] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1407.5648> (referer: http://arxiv.org/list/cs/14?skip=8000&show=1000)
2015-03-23 20:04:28+0000 [xxu461000] DEBUG: Scraped from <200 http://arxiv.org/abs/1407.5648>
	{'abstract': u'We describe two pilot studies of code review by and for scientists. Our principal findings are that scientists are enthusiastic, but need to be shown code review in action, and that just-in-time review of small code changes is more likely to succeed than large-scale end-of-work reviews.',
	 'authors': u'Marian Petre, Greg Wilson,',
	 'category': u'Computer Science ',
	 'date': '2014-7-21',
	 'pdflink': u'http://arxiv.org/pdf/1407.5648',
	 'subjects': u'Software Engineering (cs.SE)',
	 'title': u'\nCode Review For and By Scientists',
	 'urllink': u'http://arxiv.org/abs/1407.5648'}
2015-03-23 20:04:32+0000 [xxu461000] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1404.5686> (referer: http://arxiv.org/list/cs/14?skip=4000&show=1000)
2015-03-23 20:04:32+0000 [xxu461000] DEBUG: Scraped from <200 http://arxiv.org/abs/1404.5686>
	{'abstract': u'In Smart Grid applications, as the number of deployed electric smart meters increases, massive amounts of valuable meter data is generated and collected every day. To enable reliable data collection and make business decisions fast, high throughput storage and high-performance analysis of massive meter data become crucial for grid companies. Considering the advantage of high efficiency, fault tolerance, and price-performance of Hadoop and Hive systems, they are frequently deployed as underlying platform for big data processing. However, in real business use cases, these data analysis applications typically involve multidimensional range queries (MDRQ) as well as batch reading and statistics on the meter data. While Hive is high-performance at complex data batch reading and analysis, it lacks efficient indexing techniques for MDRQ. In this paper, we propose DGFIndex, an index structure for Hive that efficiently supports MDRQ for massive meter data. DGFIndex divides the data space into cubes using the grid file technique. Unlike the existing indexes in Hive, which stores all combinations of multiple dimensions, DGFIndex only stores the information of cubes. This leads to smaller index size and faster query processing. Furthermore, with pre-computing user-defined aggregations of each cube, DGFIndex only needs to access the boundary region for aggregation query. Our comprehensive experiments show that DGFIndex can save significant disk space in comparison with the existing indexes in Hive and the query performance with DGFIndex is 2-50 times faster than existing indexes in Hive and HadoopDB for aggregation query, 2-5 times faster than both for non-aggregation query, 2-75 times faster than scanning the whole table in different query selectivity.',
	 'authors': u'Yue Liu, Songlin Hu, Tilmann Rabl, Wantao Liu, Hans-Arno Jacobsen, Kaifeng Wu, Jian Chen, Jintao Li,',
	 'category': u'Computer Science ',
	 'date': '2014-4-23',
	 'pdflink': u'http://arxiv.org/pdf/1404.5686',
	 'subjects': u'Databases (cs.DB)',
	 'title': u'\nDGFIndex for Smart Grid: Enhancing Hive with a Cost-Effective  Multidimensional Range Index',
	 'urllink': u'http://arxiv.org/abs/1404.5686'}
2015-03-23 20:04:36+0000 [xxu461000] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1405.2597> (referer: http://arxiv.org/list/cs/14?skip=5000&show=1000)
2015-03-23 20:04:36+0000 [xxu461000] DEBUG: Scraped from <200 http://arxiv.org/abs/1405.2597>
	{'abstract': u'This paper is concerned with linear superposition systems in which all components of the superimposed signal are coded with an identical binary low-density parity-check (LDPC) code. We focus on the design of decoding and computing algorithms. The main contributions of this paper include: 1) we present three types of iterative multistage decoding/computing algorithms, which are referred to as decoding-computing (DC) type, computing-decoding (CD) type and computing-decoding computing (CDC) type, respectively; 2) we propose a joint decoding/computing algorithm by treating the system as a nonbinary LDPC (NB-LDPC) coded system; 3) we propose a time-varying signaling scheme for multi-user communication channels. The proposed algorithms may find applications in superposition modulation (SM), multiple-access channels (MAC), Gaussian interference channels (GIFC) and two-way relay channels (TWRC). For SM system, numerical results show that 1) the proposed CDC type iterative multistage algorithm performs better than the standard DC type iterative multistage algorithm, and 2) the joint decoding/computing algorithm performs better than the proposed iterative multistage algorithms in high spectral efficiency regime. For GIFC, numerical results show that, from moderate to strong interference, the time-varying signaling scheme significantly outperforms the constant signaling scheme when decoded with the joint decoding/computing algorithm (about 8.5 dB for strong interference). For TWRC, numerical results show that the joint decoding/computing algorithm performs better than the CD type algorithm.',
	 'authors': u'Shancheng Zhao, Xiao Ma, Baoming Bai,',
	 'category': u'Computer Science ',
	 'date': '2014-5-11',
	 'pdflink': u'http://arxiv.org/pdf/1405.2597',
	 'subjects': u'Information Theory (cs.IT)',
	 'title': u'\nDecoding and Computing Algorithms for Linear Superposition LDPC Coded  Systems',
	 'urllink': u'http://arxiv.org/abs/1405.2597'}
2015-03-23 20:04:40+0000 [xxu461000] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1406.0234> (referer: http://arxiv.org/list/cs/14?skip=6000&show=1000)
2015-03-23 20:04:40+0000 [xxu461000] DEBUG: Scraped from <200 http://arxiv.org/abs/1406.0234>
	{'abstract': u'In this work, we propose a cross-layer design strategy based on the parallel interference cancellation (PIC) detection technique and a multi-relay selection algorithm for the uplink of cooperative direct-sequence code-division multiple access (DS-CDMA) systems. We devise a low-cost greedy list-based PIC (GL-PIC) strategy with RAKE receivers as the front-end that can approach the maximum likelihood detector performance. We also present a low-complexity multi-relay selection algorithm based on greedy techniques that can approach the performance of an exhaustive search. Simulations show an excellent bit error rate performance of the proposed detection and relay selection algorithms as compared to existing techniques.',
	 'authors': u'J. Gu, R. C. de Lamare,',
	 'category': u'Computer Science ',
	 'date': '2014-6-2',
	 'pdflink': u'http://arxiv.org/pdf/1406.0234',
	 'subjects': u'Information Theory (cs.IT)',
	 'title': u'\nJoint PIC and relay selection based on greedy techniques for cooperative  DS-CDMA systems',
	 'urllink': u'http://arxiv.org/abs/1406.0234'}
2015-03-23 20:04:46+0000 [xxu461000] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1407.5610> (referer: http://arxiv.org/list/cs/14?skip=8000&show=1000)
2015-03-23 20:04:46+0000 [xxu461000] DEBUG: Scraped from <200 http://arxiv.org/abs/1407.5610>
	{'abstract': u"Performance Testing is critical for applications like web services and e-commerce platforms to ensure enhanced end user experience. In such cases, starting to test the system's performance early should significantly reduce the overall development cost. Test-first Performance (TFP) is one such paradigm that allows performance testing right from the early stage of development. Given such potential benefit, this paper proposes the design of a testing framework IVRIDIO which introduces TFP as a Service (TFPaaS). IVRIDIO incorporates the Plugin for TFP in the Cloud (PTFPC) aiming to provide instant feedbacks - a prime requirement of TFP to immediately fix critical performance issues. Furthermore, the Convention over Configuration (CoC) design paradigm has been applied by introducing a configurable project template to maintain TFP test cases. The prototyping details of the framework are given and the variation of response time to the inclusion of PTFPC has also been discussed. The Summated Usability Metric (SUM) score has been provided so that it can later be used for comparing the PTFPC plugin's usability.",
	 'authors': u'Alim Ul Gias, Rayhanur Rahman, Asif Imran, Kazi Sakib,',
	 'category': u'Computer Science ',
	 'date': '2014-7-21',
	 'pdflink': u'http://arxiv.org/pdf/1407.5610',
	 'subjects': u'Software Engineering (cs.SE)',
	 'title': u'\nTFPaaS : Test-first Performance as a Service to Cloud for Software  Testing Environment',
	 'urllink': u'http://arxiv.org/abs/1407.5610'}
2015-03-23 20:04:48+0000 [xxu461000] INFO: Crawled 133 pages (at 12 pages/min), scraped 116 items (at 12 items/min)
2015-03-23 20:04:51+0000 [xxu461000] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1404.5683> (referer: http://arxiv.org/list/cs/14?skip=4000&show=1000)
2015-03-23 20:04:51+0000 [xxu461000] DEBUG: Scraped from <200 http://arxiv.org/abs/1404.5683>
	{'abstract': u'In this work, a likelihood encoder is studied in the context of lossy source compression. The analysis of the likelihood encoder is based on a soft-covering lemma. It is demonstrated that the use of a likelihood encoder together with the soft-covering lemma gives alternative achievability proofs for classical source coding problems. The case of the rate-distortion function with side information at the decoder (i.e. the Wyner-Ziv problem) is carefully examined and an application of the likelihood encoder to the multi-terminal source coding inner bound (i.e. the Berger-Tung region) is outlined.',
	 'authors': u'Eva C. Song, Paul Cuff, H. Vincent Poor,',
	 'category': u'Computer Science ',
	 'date': '2014-4-23',
	 'pdflink': u'http://arxiv.org/pdf/1404.5683',
	 'subjects': u'Information Theory (cs.IT)',
	 'title': u'\nThe Likelihood Encoder for Lossy Source Compression',
	 'urllink': u'http://arxiv.org/abs/1404.5683'}
2015-03-23 20:04:56+0000 [xxu461000] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1405.2590> (referer: http://arxiv.org/list/cs/14?skip=5000&show=1000)
2015-03-23 20:04:56+0000 [xxu461000] DEBUG: Scraped from <200 http://arxiv.org/abs/1405.2590>
	{'abstract': u'Data originating from the Web, sensor readings and social media result in increasingly huge datasets. The so called Big Data comes with new scientific and technological challenges while creating new opportunities, hence the increasing interest in academia and industry. Traditionally, logic programming has focused on complex knowledge structures/programs, so the question arises whether and how it can work in the face of Big Data. In this paper, we examine how the well-founded semantics can process huge amounts of data through mass parallelization. More specifically, we propose and evaluate a parallel approach using the MapReduce framework. Our experimental results indicate that our approach is scalable and that well-founded semantics can be applied to billions of facts. To the best of our knowledge, this is the first work that addresses large scale nonmonotonic reasoning without the restriction of stratification for predicates of arbitrary arity. To appear in Theory and Practice of Logic Programming (TPLP).',
	 'authors': u'Ilias Tachmazidis, Grigoris Antoniou, Wolfgang Faber,',
	 'category': u'Computer Science ',
	 'date': '2014-5-11',
	 'pdflink': u'http://arxiv.org/pdf/1405.2590',
	 'subjects': u'Artificial Intelligence (cs.AI)',
	 'title': u'\nEfficient Computation of the Well-Founded Semantics over Big Data',
	 'urllink': u'http://arxiv.org/abs/1405.2590'}
2015-03-23 20:05:02+0000 [xxu461000] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1406.0231> (referer: http://arxiv.org/list/cs/14?skip=6000&show=1000)
2015-03-23 20:05:02+0000 [xxu461000] DEBUG: Scraped from <200 http://arxiv.org/abs/1406.0231>
	{'abstract': u'Proximity Distribution Kernel is an effective method for bag-of-featues based image representation. In this paper, we investigate the soft assignment of visual words to image features for proximity distribution. Visual word contribution function is proposed to model ambiguous proximity distributions. Three ambiguous proximity distributions is developed by three ambiguous contribution functions. The experiments are conducted on both classification and retrieval of medical image data sets. The results show that the performance of the proposed methods, Proximity Distribution Kernel (PDK), is better or comparable to the state-of-the-art bag-of-features based image representation methods.',
	 'authors': u'Quanquan Wang, Yongping Li,',
	 'category': u'Computer Science ',
	 'date': '2014-6-2',
	 'pdflink': u'http://arxiv.org/pdf/1406.0231',
	 'subjects': u'Computer Vision and Pattern Recognition (cs.CV)',
	 'title': u'\nAmbiguous Proximity Distribution',
	 'urllink': u'http://arxiv.org/abs/1406.0231'}
2015-03-23 20:05:06+0000 [xxu461000] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1407.5609> (referer: http://arxiv.org/list/cs/14?skip=8000&show=1000)
2015-03-23 20:05:06+0000 [xxu461000] DEBUG: Scraped from <200 http://arxiv.org/abs/1407.5609>
	{'abstract': u'The closest pair problem (CPP) is one of the well studied and fundamental problems in computing. Given a set of points in a metric space, the problem is to identify the pair of closest points. Another closely related problem is the fixed radius nearest neighbors problem (FRNNP). Given a set of points and a radius , the problem is, for every input point , to identify all the other input points that are within a distance of from . A naive deterministic algorithm can solve these problems in quadratic time. CPP as well as FRNNP play a vital role in computational biology, computational finance, share market analysis, weather prediction, entomology, electro cardiograph, N-body simulations, molecular simulations, etc. As a result, any improvements made in solving CPP and FRNNP will have immediate implications for the solution of numerous problems in these domains. We live in an era of big data and processing these data take large amounts of time. Speeding up data processing algorithms is thus much more essential now than ever before. In this paper we present algorithms for CPP and FRNNP that improve (in theory and/or practice) the best-known algorithms reported in the literature for CPP and FRNNP. These algorithms also improve the best-known algorithms for related applications including time series motif mining and the two locus problem in Genome Wide Association Studies (GWAS).',
	 'authors': u'Sanguthevar Rajasekaran, Sudipta Pathak,',
	 'category': u'Computer Science ',
	 'date': '2014-7-21',
	 'pdflink': u'http://arxiv.org/pdf/1407.5609',
	 'subjects': u'Data Structures and Algorithms (cs.DS)',
	 'title': u'\nEfficient Algorithms for the Closest Pair Problem and Applications',
	 'urllink': u'http://arxiv.org/abs/1407.5609'}
2015-03-23 20:05:10+0000 [xxu461000] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1404.5668> (referer: http://arxiv.org/list/cs/14?skip=4000&show=1000)
2015-03-23 20:05:10+0000 [xxu461000] DEBUG: Scraped from <200 http://arxiv.org/abs/1404.5668>
	{'abstract': u"Recently, there has been a growing interest in modeling planning with information constraints. Accordingly, an agent maximizes a regularized expected utility known as the free energy, where the regularizer is given by the information divergence from a prior to a posterior policy. While this approach can be justified in various ways, including from statistical mechanics and information theory, it is still unclear how it relates to decision-making against adversarial environments. This connection has previously been suggested in work relating the free energy to risk-sensitive control and to extensive form games. Here, we show that a single-agent free energy optimization is equivalent to a game between the agent and an imaginary adversary. The adversary can, by paying an exponential penalty, generate costs that diminish the decision maker's payoffs. It turns out that the optimal strategy of the adversary consists in choosing costs so as to render the decision maker indifferent among its choices, which is a definining property of a Nash equilibrium, thus tightening the connection between free energy optimization and game theory.",
	 'authors': u'Pedro A. Ortega, Daniel D. Lee,',
	 'category': u'Computer Science ',
	 'date': '2014-4-22',
	 'pdflink': u'http://arxiv.org/pdf/1404.5668',
	 'subjects': u'Artificial Intelligence (cs.AI)',
	 'title': u'\nAn Adversarial Interpretation of Information-Theoretic Bounded  Rationality',
	 'urllink': u'http://arxiv.org/abs/1404.5668'}
2015-03-23 20:05:16+0000 [xxu461000] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1405.2584> (referer: http://arxiv.org/list/cs/14?skip=5000&show=1000)
2015-03-23 20:05:16+0000 [xxu461000] DEBUG: Scraped from <200 http://arxiv.org/abs/1405.2584>
	{'abstract': u'Sentiment analysis (also known as opinion mining) refers to the use of natural language processing, text analysis and computational linguistics to identify and extract subjective information in source materials. Mining opinions expressed in the user generated content is a challenging yet practically very useful problem. This survey would cover various approaches and methodology used in Sentiment Analysis and Opinion Mining in general. The focus would be on Internet text like, Product review, tweets and other social media.',
	 'authors': u'Rahul Tejwani,',
	 'category': u'Computer Science ',
	 'date': '2014-5-11',
	 'pdflink': u'http://arxiv.org/pdf/1405.2584',
	 'subjects': u'Information Retrieval (cs.IR)',
	 'title': u'\nSentiment Analysis: A Survey',
	 'urllink': u'http://arxiv.org/abs/1405.2584'}
2015-03-23 20:05:22+0000 [xxu461000] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1406.0223> (referer: http://arxiv.org/list/cs/14?skip=6000&show=1000)
2015-03-23 20:05:22+0000 [xxu461000] DEBUG: Scraped from <200 http://arxiv.org/abs/1406.0223>
	{'abstract': u'The performance of prediction models is often based on "abstract metrics" that estimate the model\'s ability to limit residual errors between the observed and predicted values. However, meaningful evaluation and selection of prediction models for end-user domains requires holistic and application-sensitive performance measures. Inspired by energy consumption prediction models used in the emerging "big data" domain of Smart Power Grids, we propose a suite of performance measures to rationally compare models along the dimensions of scale independence, reliability, volatility and cost. We include both application independent and dependent measures, the latter parameterized to allow customization by domain experts to fit their scenario. While our measures are generalizable to other domains, we offer an empirical analysis using real energy use data for three Smart Grid applications: planning, customer education and demand response, which are relevant for energy sustainability. Our results underscore the value of the proposed measures to offer a deeper insight into models\' behavior and their impact on real applications, which benefit both data mining researchers and practitioners.',
	 'authors': u'Saima Aman, Yogesh Simmhan, Viktor K. Prasanna,',
	 'category': u'Computer Science ',
	 'date': '2014-6-2',
	 'pdflink': u'http://arxiv.org/pdf/1406.0223',
	 'subjects': u'Learning (cs.LG)',
	 'title': u'\nHolistic Measures for Evaluating Prediction Models in Smart Grids',
	 'urllink': u'http://arxiv.org/abs/1406.0223'}
2015-03-23 20:05:27+0000 [xxu461000] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1407.5599> (referer: http://arxiv.org/list/cs/14?skip=8000&show=1000)
2015-03-23 20:05:27+0000 [xxu461000] DEBUG: Scraped from <200 http://arxiv.org/abs/1407.5599>
	{'abstract': u'The general perception is that kernel methods are not scalable, and neural nets are the methods of choice for nonlinear learning problems. Or have we simply not tried hard enough for kernel methods? Here we propose an approach that scales up kernel methods using a novel concept called "doubly stochastic functional gradients". Our approach relies on the fact that many kernel methods can be expressed as convex optimization problems, and we solve the problems by making two unbiased stochastic approximations to the functional gradient, one using random training points and another using random functions associated with the kernel, and then descending using this noisy functional gradient. We show that a function produced by this procedure after iterations converges to the optimal function in the reproducing kernel Hilbert space in rate , and achieves a generalization performance of . This doubly stochasticity also allows us to avoid keeping the support vectors and to implement the algorithm in a small memory footprint, which is linear in number of iterations and independent of data dimension. Our approach can readily scale kernel methods up to the regimes which are dominated by neural nets. We show that our method can achieve competitive performance to neural nets in datasets such as 8 million handwritten digits from MNIST, 2.3 million energy materials from MolecularSpace, and 1 million photos from ImageNet.',
	 'authors': u'Bo Dai, Bo Xie, Niao He, Yingyu Liang, Anant Raj, Maria-Florina Balcan, Le Song,',
	 'category': u'Computer Science ',
	 'date': '2014-7-21',
	 'pdflink': u'http://arxiv.org/pdf/1407.5599',
	 'subjects': u'Learning (cs.LG)',
	 'title': u'\nScalable Kernel Methods via Doubly Stochastic Gradients',
	 'urllink': u'http://arxiv.org/abs/1407.5599'}
2015-03-23 20:05:31+0000 [xxu461000] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1404.5665> (referer: http://arxiv.org/list/cs/14?skip=4000&show=1000)
2015-03-23 20:05:31+0000 [xxu461000] DEBUG: Scraped from <200 http://arxiv.org/abs/1404.5665>
	{'abstract': u'The vast quantity of data generated and captured every day has led to a pressing need for tools and processes to organize, analyze and interrelate this data. Automated reasoning and optimization tools with inherent support for data could enable advancements in a variety of contexts, from data-backed decision making to data-intensive scientific research. To this end, we introduce a decidable logic aimed at database analysis. Our logic extends quantifier-free Linear Integer Arithmetic with operators from Relational Algebra, like selection and cross product. We provide a scalable decision procedure that is based on the BC(T) architecture for ILP Modulo Theories. Our decision procedure makes use of database techniques. We also experimentally evaluate our approach, and discuss potential applications.',
	 'authors': u'Panagiotis Manolios, Vasilis Papavasileiou, Mirek Riedewald,',
	 'category': u'Computer Science ',
	 'date': '2014-4-22',
	 'pdflink': u'http://arxiv.org/pdf/1404.5665',
	 'subjects': u'Logic in Computer Science (cs.LO)',
	 'title': u'\nILP Modulo Data',
	 'urllink': u'http://arxiv.org/abs/1404.5665'}
2015-03-23 20:05:36+0000 [xxu461000] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1405.2580> (referer: http://arxiv.org/list/cs/14?skip=5000&show=1000)
2015-03-23 20:05:36+0000 [xxu461000] DEBUG: Scraped from <200 http://arxiv.org/abs/1405.2580>
	{'abstract': u'In this paper we show that inverses of well-conditioned, finite-time Gramians and impulse response matrices of large-scale interconnected systems described by sparse state-space models, can be approximated by sparse matrices. The approximation methodology established in this paper opens the door to the development of novel methods for distributed estimation, identification and control of large-scale interconnected systems. The novel estimators (controllers) compute local estimates (control actions) simply as linear combinations of inputs and outputs (states) of local subsystems. The size of these local data sets essentially depends on the condition number of the finite-time observability (controllability) Gramian. Furthermore, the developed theory shows that the sparsity patterns of the system matrices of the distributed estimators (controllers) are primarily determined by the sparsity patterns of state-space matrices of large-scale systems. The computational and memory complexity of the approximation algorithms are , where is the number of local subsystems of the interconnected system. Consequently, the proposed approximation methodology is computationally feasible for interconnected systems with an extremely large number of local subsystems.',
	 'authors': u'Aleksandar Haber, Michel Verhaegen,',
	 'category': u'Computer Science ',
	 'date': '2014-5-11',
	 'pdflink': u'http://arxiv.org/pdf/1405.2580',
	 'subjects': u'Systems and Control (cs.SY)',
	 'title': u'\nSparse approximate inverses of Gramians and impulse response matrices of  large-scale interconnected systems',
	 'urllink': u'http://arxiv.org/abs/1405.2580'}
2015-03-23 20:05:41+0000 [xxu461000] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1406.0216> (referer: http://arxiv.org/list/cs/14?skip=6000&show=1000)
2015-03-23 20:05:41+0000 [xxu461000] DEBUG: Scraped from <200 http://arxiv.org/abs/1406.0216>
	{'abstract': u'Numerous digital humanities projects maintain their data collections in the form of text, images, and metadata. While data may be stored in many formats, from plain text to XML to relational databases, the use of the resource description framework (RDF) as a standardized representation has gained considerable traction during the last five years. Almost every digital humanities meeting has at least one session concerned with the topic of digital humanities, RDF, and linked data. While most existing work in linked data has focused on improving algorithms for entity matching, the aim of the LinkedHumanities project is to build digital humanities tools that work "out of the box," enabling their use by humanities scholars, computer scientists, librarians, and information scientists alike. With this paper, we report on the Linked Open Data Enhancer (LODE) framework developed as part of the LinkedHumanities project. With LODE we support non-technical users to enrich a local RDF repository with high-quality data from the Linked Open Data cloud. LODE links and enhances the local RDF repository without compromising the quality of the data. In particular, LODE supports the user in the enhancement and linking process by providing intuitive user-interfaces and by suggesting high-quality linking candidates using tailored matching algorithms. We hope that the LODE framework will be useful to digital humanities scholars complementing other digital humanities tools.',
	 'authors': u'Jakob Huber, Timo Sztyler, Jan Noessner, Jaimie Murdock, Colin Allen, Mathias Niepert,',
	 'category': u'Computer Science ',
	 'date': '2014-6-1',
	 'pdflink': u'http://arxiv.org/pdf/1406.0216',
	 'subjects': u'Digital Libraries (cs.DL)',
	 'title': u'\nLODE: Linking Digital Humanities Content to the Web of Data',
	 'urllink': u'http://arxiv.org/abs/1406.0216'}
2015-03-23 20:05:47+0000 [xxu461000] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1407.5593> (referer: http://arxiv.org/list/cs/14?skip=8000&show=1000)
2015-03-23 20:05:47+0000 [xxu461000] DEBUG: Scraped from <200 http://arxiv.org/abs/1407.5593>
	{'abstract': u"Kaczmarz's alternating projection method has been widely used for solving a consistent (mostly over-determined) linear system of equations Ax=b. Because of its simple iterative nature with light computation, this method was successfully applied in computerized tomography. Since tomography generates a matrix A with highly coherent rows, randomized Kaczmarz algorithm is expected to provide faster convergence as it picks a row for each iteration at random, based on a certain probability distribution. It was recently shown that picking a row at random, proportional with its norm, makes the iteration converge exponentially in expectation with a decay constant that depends on the scaled condition number of A and not the number of equations. Since Kaczmarz's method is a subspace projection method, the convergence rate for simple Kaczmarz algorithm was developed in terms of subspace angles. This paper provides analyses of simple and randomized Kaczmarz algorithms and explain the link between them. It also propose new versions of randomization that may speed up convergence.",
	 'authors': u'Tim Wallace, Ali Sekmen,',
	 'category': u'Computer Science ',
	 'date': '2014-7-21',
	 'pdflink': u'http://arxiv.org/pdf/1407.5593',
	 'subjects': u'Numerical Analysis (cs.NA)',
	 'title': u'\nDeterministic Versus Randomized Kaczmarz Iterative Projection',
	 'urllink': u'http://arxiv.org/abs/1407.5593'}
2015-03-23 20:05:48+0000 [xxu461000] INFO: Crawled 145 pages (at 12 pages/min), scraped 128 items (at 12 items/min)
2015-03-23 20:05:53+0000 [xxu461000] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1404.5660> (referer: http://arxiv.org/list/cs/14?skip=4000&show=1000)
2015-03-23 20:05:53+0000 [xxu461000] DEBUG: Scraped from <200 http://arxiv.org/abs/1404.5660>
	{'abstract': u'Karloff? and Shirley recently proposed summary trees as a new way to visualize large rooted trees (Eurovis 2013) and gave algorithms for generating a maximum-entropy k-node summary tree of an input n-node rooted tree. However, the algorithm generating optimal summary trees was only pseudo-polynomial (and worked only for integral weights); the authors left open existence of a olynomial-time algorithm. In addition, the authors provided an additive approximation algorithm and a greedy heuristic, both working on real weights. This paper shows how to construct maximum entropy k-node summary trees in time O(k^2 n + n log n) for real weights (indeed, as small as the time bound for the greedy heuristic given previously); how to speed up the approximation algorithm so that it runs in time O(n + (k^4/eps?) log(k/eps?)), and how to speed up the greedy algorithm so as to run in time O(kn + n log n). Altogether, these results make summary trees a much more practical tool than before.',
	 'authors': u'Richard Cole, Howard Karloff,',
	 'category': u'Computer Science ',
	 'date': '2014-4-22',
	 'pdflink': u'http://arxiv.org/pdf/1404.5660',
	 'subjects': u'Data Structures and Algorithms (cs.DS)',
	 'title': u'\nFast Algorithms for Constructing Maximum Entropy Summary Trees',
	 'urllink': u'http://arxiv.org/abs/1404.5660'}
2015-03-23 20:05:58+0000 [xxu461000] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1405.2576> (referer: http://arxiv.org/list/cs/14?skip=5000&show=1000)
2015-03-23 20:05:58+0000 [xxu461000] DEBUG: Scraped from <200 http://arxiv.org/abs/1405.2576>
	{'abstract': u'Ultra network densification is considered a major trend in the evolution of cellular networks, due to its ability to bring the network closer to the user side and reuse resources to the maximum extent. In this paper we explore spatial resources coordination as a key empowering technology for next generation (5G) ultra-dense networks. We propose an optimization framework for flexibly associating system users with a densely deployed network of access nodes, opting for the exploitation of densification and the control of overhead signaling. Combined with spatial precoding processing strategies, we design network resources management strategies reflecting various features, namely local vs global channel state information knowledge exploitation, centralized vs distributed implementation, and non-cooperative vs joint multi-node data processing. We apply these strategies to future UDN setups, and explore the impact of critical network parameters, that is, the densification levels of users and access nodes as well as the power budget constraints, to users performance. We demonstrate that spatial resources coordination is a key factor for capitalizing on the gains of ultra dense network deployments.',
	 'authors': u'Antonis G. Gotsis, Stelios Stefanatos, Angeliki Alexiou,',
	 'category': u'Computer Science ',
	 'date': '2014-5-11',
	 'pdflink': u'http://arxiv.org/pdf/1405.2576',
	 'subjects': u'Information Theory (cs.IT)',
	 'title': u'\nSpatial Coordination Strategies in Future Ultra-Dense Wireless Networks',
	 'urllink': u'http://arxiv.org/abs/1405.2576'}
2015-03-23 20:06:04+0000 [xxu461000] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1406.0215> (referer: http://arxiv.org/list/cs/14?skip=6000&show=1000)
2015-03-23 20:06:04+0000 [xxu461000] DEBUG: Scraped from <200 http://arxiv.org/abs/1406.0215>
	{'abstract': u'Formal coordination mechanisms are of growing importance as human-based service delivery becomes more globalized and informal mechanisms are no longer effective. Further it is becoming apparent that business environments, communication among distributed teams, and work performance are all subject to endogenous and exogenous uncertainty. This paper describes a stochastic model of service requests in global service delivery and then puts forth a cognitive approach for coordination in the face of uncertainty, based on a perception-action loop and receding horizon control. Optimization algorithms used are a mix of myopic dynamic programming and constraint-based programming. The coordination approach described has been deployed by a globally integrated enterprise in a very large-scale global delivery system and has been demonstrated to improve work efficiency by 10-15% as compared to manual planning.',
	 'authors': u'Lav R. Varshney, Shivali Agarwal, Yi-Min Chee, Renuka R. Sindhgatta, Daniel V. Oppenheim, Juhnyoung Lee, Krishna Ratakonda,',
	 'category': u'Computer Science ',
	 'date': '2014-6-1',
	 'pdflink': u'http://arxiv.org/pdf/1406.0215',
	 'subjects': u'Other Computer Science (cs.OH)',
	 'title': u'\nCognitive Coordination of Global Service Delivery',
	 'urllink': u'http://arxiv.org/abs/1406.0215'}
2015-03-23 20:06:08+0000 [xxu461000] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1407.5587> (referer: http://arxiv.org/list/cs/14?skip=8000&show=1000)
2015-03-23 20:06:08+0000 [xxu461000] DEBUG: Scraped from <200 http://arxiv.org/abs/1407.5587>
	{'abstract': u'Various operations related to infinite sequential games are classified in the Weihrauch lattice.',
	 'authors': u'Stephane Le Roux, Arno Pauly,',
	 'category': u'Computer Science ',
	 'date': '2014-7-21',
	 'pdflink': u'http://arxiv.org/pdf/1407.5587',
	 'subjects': u'Logic in Computer Science (cs.LO)',
	 'title': u'\nWeihrauch degrees of finding equilibria in sequential games',
	 'urllink': u'http://arxiv.org/abs/1407.5587'}
2015-03-23 20:06:14+0000 [xxu461000] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1404.5643> (referer: http://arxiv.org/list/cs/14?skip=4000&show=1000)
2015-03-23 20:06:14+0000 [xxu461000] DEBUG: Scraped from <200 http://arxiv.org/abs/1404.5643>
	{'abstract': u'Research on multi-agent planning has been popular in recent years. While previous research has been motivated by the understanding that, through cooperation, multi-agent systems can achieve tasks that are unachievable by single-agent systems, there are no formal characterizations of situations where cooperation is required to achieve a goal, thus warranting the application of multi-agent systems. In this paper, we provide such a formal discussion from the planning aspect. We first show that determining whether there is required cooperation (RC) is intractable is general. Then, by dividing the problems that require cooperation (referred to as RC problems) into two classes -- problems with heterogeneous and homogeneous agents, we aim to identify all the conditions that can cause RC in these two classes. We establish that when none of these identified conditions hold, the problem is single-agent solvable. Furthermore, with a few assumptions, we provide an upper bound on the minimum number of agents required for RC problems with homogeneous agents. This study not only provides new insights into multi-agent planning, but also has many applications. For example, in human-robot teaming, when a robot cannot achieve a task, it may be due to RC. In such cases, the human teammate should be informed and, consequently, coordinate with other available robots for a solution.',
	 'authors': u'Yu Zhang, Subbarao Kambhampati,',
	 'category': u'Computer Science ',
	 'date': '2014-4-22',
	 'pdflink': u'http://arxiv.org/pdf/1404.5643',
	 'subjects': u'Artificial Intelligence (cs.AI)',
	 'title': u'\nA Formal Analysis of Required Cooperation in Multi-agent Planning',
	 'urllink': u'http://arxiv.org/abs/1404.5643'}
2015-03-23 20:06:19+0000 [xxu461000] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1405.2571> (referer: http://arxiv.org/list/cs/14?skip=5000&show=1000)
2015-03-23 20:06:19+0000 [xxu461000] DEBUG: Scraped from <200 http://arxiv.org/abs/1405.2571>
	{'abstract': u'A partial Latin square (PLS) is a partial assignment of n symbols to an nxn grid such that, in each row and in each column, each symbol appears at most once. The partial Latin square extension problem is an NP-hard problem that asks for a largest extension of a given PLS. In this paper we propose an efficient local search for this problem. We focus on the local search such that the neighborhood is defined by (p,q)-swap, i.e., removing exactly p symbols and then assigning symbols to at most q empty cells. For p in , our neighborhood search algorithm finds an improved solution or concludes that no such solution exists in O(n^) time. We also propose a novel swap operation, Trellis-swap, which is a generalization of (1,q)-swap and (2,q)-swap. Our Trellis-neighborhood search algorithm takes O(n^) time to do the same thing. Using these neighborhood search algorithms, we design a prototype iterated local search algorithm and show its effectiveness in comparison with state-of-the-art optimization solvers such as IBM ILOG CPLEX and LocalSolver.',
	 'authors': u'Kazuya Haraguchi,',
	 'category': u'Computer Science ',
	 'date': '2014-5-11',
	 'pdflink': u'http://arxiv.org/pdf/1405.2571',
	 'subjects': u'Data Structures and Algorithms (cs.DS)',
	 'title': u'\nAn Efficient Local Search for Partial Latin Square Extension Problem',
	 'urllink': u'http://arxiv.org/abs/1405.2571'}
2015-03-23 20:06:24+0000 [xxu461000] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1406.0214> (referer: http://arxiv.org/list/cs/14?skip=6000&show=1000)
2015-03-23 20:06:24+0000 [xxu461000] DEBUG: Scraped from <200 http://arxiv.org/abs/1406.0214>
	{'abstract': u'We introduce the first unified theory for target tracking using Multiple Hypothesis Tracking, Topological Data Analysis, and machine learning. Our string of innovations are 1) robust topological features are used to encode behavioral information, 2) statistical models are fitted to distributions over these topological features, and 3) the target type classification methods of Wigren and Bar Shalom et al. are employed to exploit the resulting likelihoods for topological features inside of the tracking procedure. To demonstrate the efficacy of our approach, we test our procedure on synthetic vehicular data generated by the Simulation of Urban Mobility package.',
	 'authors': u'Paul Bendich, Sang Chin, Jesse Clarke, Jonathan deSena, John Harer, Elizabeth Munch, Andrew Newman, David Porter, David Rouse, Nate Strawn, Adam Watkins,',
	 'category': u'Computer Science ',
	 'date': '2014-6-1',
	 'pdflink': u'http://arxiv.org/pdf/1406.0214',
	 'subjects': u'Systems and Control (cs.SY)',
	 'title': u'\nTopological and Statistical Behavior Classifiers for Tracking  Applications',
	 'urllink': u'http://arxiv.org/abs/1406.0214'}
2015-03-23 20:06:29+0000 [xxu461000] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1407.5574> (referer: http://arxiv.org/list/cs/14?skip=8000&show=1000)
2015-03-23 20:06:29+0000 [xxu461000] DEBUG: Scraped from <200 http://arxiv.org/abs/1407.5574>
	{'abstract': u'Artificial bee colony (ABC) algorithm has proved its importance in solving a number of problems including engineering optimization problems. ABC algorithm is one of the most popular and youngest member of the family of population based nature inspired meta-heuristic swarm intelligence method. ABC has been proved its superiority over some other Nature Inspired Algorithms (NIA) when applied for both benchmark functions and real world problems. The performance of search process of ABC depends on a random value which tries to balance exploration and exploitation phase. In order to increase the performance it is required to balance the exploration of search space and exploitation of optimal solution of the ABC. This paper outlines a new hybrid of ABC algorithm with Genetic Algorithm. The proposed method integrates crossover operation from Genetic Algorithm (GA) with original ABC algorithm. The proposed method is named as Crossover based ABC (CbABC). The CbABC strengthens the exploitation phase of ABC as crossover enhances exploration of search space. The CbABC tested over four standard benchmark functions and a popular continuous optimization problem.',
	 'authors': u'Sandeep Kumar, Vivek Kumar Sharma, Rajani Kumari,',
	 'category': u'Computer Science ',
	 'date': '2014-7-21',
	 'pdflink': u'http://arxiv.org/pdf/1407.5574',
	 'subjects': u'Artificial Intelligence (cs.AI)',
	 'title': u'\nA Novel Hybrid Crossover based Artificial Bee Colony Algorithm for  Optimization Problem',
	 'urllink': u'http://arxiv.org/abs/1407.5574'}
2015-03-23 20:06:33+0000 [xxu461000] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1404.5611> (referer: http://arxiv.org/list/cs/14?skip=4000&show=1000)
2015-03-23 20:06:33+0000 [xxu461000] DEBUG: Scraped from <200 http://arxiv.org/abs/1404.5611>
	{'abstract': u'"Science gateway" (SG) ideology means a user-friendly intuitive interface between scientists (or scientific communities) and different software components + various distributed computing infrastructures (DCIs) (like grids, clouds, clusters), where researchers can focus on their scientific goals and less on peculiarities of software/DCI. "IMP Science Gateway Portal" (this http URL) for complex workflow management and integration of distributed computing resources (like clusters, service grids, desktop grids, clouds) is presented. It is created on the basis of WS-PGRADE and gUSE technologies, where WS-PGRADE is designed for science workflow operation and gUSE - for smooth integration of available resources for parallel and distributed computing in various heterogeneous distributed computing infrastructures (DCI). The typical scientific workflows with possible scenarios of its preparation and usage are presented. Several typical use cases for these science applications (scientific workflows) are considered for molecular dynamics (MD) simulations of complex behavior of various nanostructures (nanoindentation of graphene layers, defect system relaxation in metal nanocrystals, thermal stability of boron nitride nanotubes, etc.). The user experience is analyzed in the context of its practical applications for MD simulations in materials science, physics and nanotechnologies with available heterogeneous DCIs. In conclusion, the "science gateway" approach - workflow manager (like WS-PGRADE) + DCI resources manager (like gUSE)- gives opportunity to use the SG portal (like "IMP Science Gateway Portal") in a very promising way, namely, as a hub of various virtual experimental labs (different software components + various requirements to resources) in the context of its practical MD applications in materials science, physics, chemistry, biology, and nanotechnologies.',
	 'authors': u'Yuri Gordienko, Lev Bekenev, Olexandra Baskova, Olexander Gatsenko, Elena Zasimchuk, Sergii Stirenko,',
	 'category': u'Computer Science ',
	 'date': '2014-4-22',
	 'pdflink': u'http://arxiv.org/pdf/1404.5611',
	 'subjects': u'Computational Engineering, Finance, and Science (cs.CE)',
	 'title': u'\nIMP Science Gateway: from the Portal to the Hub of Virtual Experimental  Labs in Materials Science',
	 'urllink': u'http://arxiv.org/abs/1404.5611'}
2015-03-23 20:06:37+0000 [xxu461000] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1405.2564> (referer: http://arxiv.org/list/cs/14?skip=5000&show=1000)
2015-03-23 20:06:37+0000 [xxu461000] DEBUG: Scraped from <200 http://arxiv.org/abs/1405.2564>
	{'abstract': u'To appear in Theory and Practice of Logic Programming (TPLP). Several Prolog interpreters are based on the Warren Abstract Machine (WAM), an elegant model to compile Prolog programs. In order to improve the performance several strategies have been proposed, such as: optimize the selection of clauses, specialize the unification, global analysis, native code generation and tabling. This paper proposes a different strategy to implement an efficient Prolog System, the creation of specialized emulators on the fly. The proposed strategy was implemented and evaluated at YAP Prolog System, and the experimental evaluation showed interesting results.',
	 'authors': u'George Souza Oliveira, Anderson Faustino da Silva,',
	 'category': u'Computer Science ',
	 'date': '2014-5-11',
	 'pdflink': u'http://arxiv.org/pdf/1405.2564',
	 'subjects': u'Programming Languages (cs.PL)',
	 'title': u'\nTowards an Efficient Prolog System by Code Introspection',
	 'urllink': u'http://arxiv.org/abs/1405.2564'}
2015-03-23 20:06:41+0000 [xxu461000] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1406.0200> (referer: http://arxiv.org/list/cs/14?skip=6000&show=1000)
2015-03-23 20:06:41+0000 [xxu461000] DEBUG: Scraped from <200 http://arxiv.org/abs/1406.0200>
	{'abstract': u'A dual-layer multiple-input multiple-output (MIMO) system with multi-level modulation is considered. A computationally efficient soft-input soft-output receiver based on the exact max-log maximum a posteriori (max-log-MAP) principle is presented in the context of iterative detection and decoding. We show that the computational complexity of our exact max-log-MAP solution grows linearly with the constellation size and is, furthermore, less than that of the best known methods of Turbo-LORD that only provide approximate solutions. Using decoder feedback to change the decision thresholds of the constellation symbols, we show that the exhaustive search operation boils down to a simple slicing operation. For various simulation parameters, we verify that our solution performs identically to the brute-force exhaustive search max-log MAP solution.',
	 'authors': u'Ahmad Gomaa, Louay M.A. Jalloul,',
	 'category': u'Computer Science ',
	 'date': '2014-6-1',
	 'pdflink': u'http://arxiv.org/pdf/1406.0200',
	 'subjects': u'Information Theory (cs.IT)',
	 'title': u'\nEfficient Soft-Input Soft-Output Detection of Dual-Layer MIMO Systems',
	 'urllink': u'http://arxiv.org/abs/1406.0200'}
2015-03-23 20:06:47+0000 [xxu461000] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1407.5572> (referer: http://arxiv.org/list/cs/14?skip=8000&show=1000)
2015-03-23 20:06:47+0000 [xxu461000] DEBUG: Scraped from <200 http://arxiv.org/abs/1407.5572>
	{'abstract': u'This work investigates the secrecy capacity of the Wiretap Broadcast Channel (WBC) with an external eavesdropper where a source wishes to communicate two private messages over a Broadcast Channel (BC) while keeping them secret from the eavesdropper. We derive a non-trivial outer bound on the secrecy capacity region of this channel which, in absence of security constraints, reduces to the best known outer bound to the capacity of the standard BC. An inner bound is also derived which follows the behaviour of both the best known inner bound for the BC and the Wiretap Channel. These bounds are shown to be tight for the deterministic BC with a general eavesdropper, the semi-deterministic BC with a more-noisy eavesdropper and the Wiretap BC where users exhibit a less-noisiness order between them. Finally, by rewriting our outer bound to encompass the characteristics of parallel channels, we also derive the secrecy capacity region of the product of two inversely less-noisy BCs with a more-noisy eavesdropper. We illustrate our results by studying the impact of security constraints on the capacity of the WBC with binary erasure (BEC) and binary symmetric (BSC) components.',
	 'authors': u'Meryem Benammar, Pablo Piantanida,',
	 'category': u'Computer Science ',
	 'date': '2014-7-21',
	 'pdflink': u'http://arxiv.org/pdf/1407.5572',
	 'subjects': u'Information Theory (cs.IT)',
	 'title': u'\nSecrecy Capacity Region of Some Classes of Wiretap Broadcast Channels',
	 'urllink': u'http://arxiv.org/abs/1407.5572'}
2015-03-23 20:06:48+0000 [xxu461000] INFO: Crawled 157 pages (at 12 pages/min), scraped 140 items (at 12 items/min)
2015-03-23 20:06:52+0000 [xxu461000] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1404.5605> (referer: http://arxiv.org/list/cs/14?skip=4000&show=1000)
2015-03-23 20:06:52+0000 [xxu461000] DEBUG: Scraped from <200 http://arxiv.org/abs/1404.5605>
	{'abstract': u'Sparse code multiple access (SCMA) is a new frequency domain non-orthogonal multiple-access technique which can improve spectral efficiency of wireless radio access. With SCMA, different incoming data streams are directly mapped to codewords of different multi-dimensional cookbooks, where each codeword represents a spread transmission layer. Multiple SCMA layers share the same time-frequency resources of OFDMA. The sparsity of codewords makes the near-optimal detection feasible through iterative message passing algorithm (MPA). Such low complexity of multi-layer detection allows excessive codeword overloading in which the dimension of multiplexed layers exceeds the dimension of codewords. Optimization of overloading factor along with modulation-coding levels of layers provides a more flexible and efficient link-adaptation mechanism. On the other hand, the signal spreading feature of SCMA can improve link-adaptation as a result of less colored interference. In this paper a technique is developed to enable multi-user SCMA (MU-SCMA) for downlink wireless access. User pairing, power sharing, rate adjustment, and scheduling algorithms are designed to improve the downlink throughput of a heavily loaded network. The advantage of SCMA spreading for lightly loaded networks is also evaluated.',
	 'authors': u'Hosein Nikopour, Eric Yi, Alireza Bayesteh, Kelvin Au, Mark Hawryluck, Hadi Baligh, Jianglei Ma,',
	 'category': u'Computer Science ',
	 'date': '2014-4-22',
	 'pdflink': u'http://arxiv.org/pdf/1404.5605',
	 'subjects': u'Information Theory (cs.IT)',
	 'title': u'\nSCMA for Downlink Multiple Access of 5G Wireless Networks',
	 'urllink': u'http://arxiv.org/abs/1404.5605'}
2015-03-23 20:06:56+0000 [xxu461000] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1405.2555> (referer: http://arxiv.org/list/cs/14?skip=5000&show=1000)
2015-03-23 20:06:56+0000 [xxu461000] DEBUG: Scraped from <200 http://arxiv.org/abs/1405.2555>
	{'abstract': u'In secure multiparty computation, mutually distrusting users in a network want to collaborate to compute functions of data which is distributed among the users. The users should not learn any additional information about the data of others than what they may infer from their own data and the functions they are computing. Previous works have mostly considered the worst case context (i.e., without assuming any distribution for the data); Lee and Abbe (2014) is a notable exception. Here, we study the average case (i.e., we work with a distribution on the data) where correctness and privacy is only desired asymptotically. For concreteness and simplicity, we consider a secure version of the function computation problem of K "orner and Marton (1979) where two users observe a doubly symmetric binary source with parameter p and the third user wants to compute the XOR. We show that the amount of communication and randomness resources required depends on the level of correctness desired. When zero-error and perfect privacy are required, the results of Data et al. (2014) show that it can be achieved if and only if a total rate of 1 bit is communicated between every pair of users and private randomness at the rate of 1 is used up. In contrast, we show here that, if we only want the probability of error to vanish asymptotically in block length, it can be achieved by a lower rate (binary entropy of p) for all the links and for private randomness; this also guarantees perfect privacy. We also show that no smaller rates are possible even if privacy is only required asymptotically.',
	 'authors': u'Deepesh Data, Bikash Kumar Dey, Manoj Mishra, Vinod M. Prabhakaran,',
	 'category': u'Computer Science ',
	 'date': '2014-5-11',
	 'pdflink': u'http://arxiv.org/pdf/1405.2555',
	 'subjects': u'Information Theory (cs.IT)',
	 'title': u'\nHow to Securely Compute the Modulo-Two Sum of Binary Sources',
	 'urllink': u'http://arxiv.org/abs/1405.2555'}
2015-03-23 20:07:01+0000 [xxu461000] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1406.0196> (referer: http://arxiv.org/list/cs/14?skip=6000&show=1000)
2015-03-23 20:07:01+0000 [xxu461000] DEBUG: Scraped from <200 http://arxiv.org/abs/1406.0196>
	{'abstract': u'A coloring of a graph G = (V,E) is a partition of V into independent sets or color classes. A vertex v Vi is a Grundy vertex if it is adjacent to at least one vertex in each color class Vj . A coloring is a Grundy coloring if every color class contains at least one Grundy vertex, and the Grundy number of a graph is the maximum number of colors in a Grundy coloring. We derive a natural upper bound on this parameter and show that graphs with sufficiently large girth achieve equality in the bound. In particular, this gives a linear time algorithm to determine the Grundy number of a tree.',
	 'authors': u'Ali Mansouri, Mohamed Salim Bouhlel,',
	 'category': u'Computer Science ',
	 'date': '2014-6-1',
	 'pdflink': u'http://arxiv.org/pdf/1406.0196',
	 'subjects': u'Discrete Mathematics (cs.DM)',
	 'title': u'\nA linear algorithm for the grundy number of a tree',
	 'urllink': u'http://arxiv.org/abs/1406.0196'}
2015-03-23 20:07:06+0000 [xxu461000] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1407.5553> (referer: http://arxiv.org/list/cs/14?skip=8000&show=1000)
2015-03-23 20:07:06+0000 [xxu461000] DEBUG: Scraped from <200 http://arxiv.org/abs/1407.5553>
	{'abstract': u'Many large-scale systems such as intelligent transportation systems, smart grids or smart buildings collect data about the activities of their users to optimize their operations. In a typical scenario, signals originate from many sensors capturing events involving these users, and several statistics of interest need to be continuously published in real-time. In addition, in order to encourage user participation, privacy issues need to be taken into consideration. This paper considers the problem of providing differential privacy guarantees for such multi-input multi-output systems operating continuously. We show in particular how to construct various extensions of the zero-forcing equalization mechanism, which we previously proposed for single-input single-output systems. We also describe an application to privately monitoring and forecasting occupancy in a building equipped with a dense network of motion detection sensors, which is useful for example to control its HVAC system.',
	 'authors': u'Jerome Le Ny, Meisam Mohammady,',
	 'category': u'Computer Science ',
	 'date': '2014-7-21',
	 'pdflink': u'http://arxiv.org/pdf/1407.5553',
	 'subjects': u'Systems and Control (cs.SY)',
	 'title': u'\nDifferentially Private MIMO Filtering for Event Streams and  Spatio-Temporal Monitoring',
	 'urllink': u'http://arxiv.org/abs/1407.5553'}
2015-03-23 20:07:11+0000 [xxu461000] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1404.5588> (referer: http://arxiv.org/list/cs/14?skip=4000&show=1000)
2015-03-23 20:07:11+0000 [xxu461000] DEBUG: Scraped from <200 http://arxiv.org/abs/1404.5588>
	{'abstract': u'In this paper, we propose a novel image set representation and classification method by maximizing the margin of image sets. The margin of an image set is defined as the difference of the distance to its nearest image set from different classes and the distance to its nearest image set of the same class. By modeling the image sets by using both their image samples and their affine hull models, and maximizing the margins of the images sets, the image set representation parameter learning problem is formulated as an minimization problem, which is further optimized by an expectation -maximization (EM) strategy with accelerated proximal gradient (APG) optimization in an iterative algorithm. To classify a given test image set, we assign it to the class which could provide the largest margin. Experiments on two applications of video-sequence-based face recognition demonstrate that the proposed method significantly outperforms state-of-the-art image set classification methods in terms of both effectiveness and efficiency.',
	 'authors': u'Jim Jing-Yan Wang, Majed Alzahrani, Xin Gao,',
	 'category': u'Computer Science ',
	 'date': '2014-4-22',
	 'pdflink': u'http://arxiv.org/pdf/1404.5588',
	 'subjects': u'Computer Vision and Pattern Recognition (cs.CV)',
	 'title': u'\nLarge Margin Image Set Representation and Classification',
	 'urllink': u'http://arxiv.org/abs/1404.5588'}
2015-03-23 20:07:16+0000 [xxu461000] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1405.2553> (referer: http://arxiv.org/list/cs/14?skip=5000&show=1000)
2015-03-23 20:07:16+0000 [xxu461000] DEBUG: Scraped from <200 http://arxiv.org/abs/1405.2553>
	{'abstract': u'We prove that a minimal automaton has a minimal adjacency matrix rank and a minimal adjacency matrix nullity using equitable partition (from graph spectra theory) and Nerode partition (from automata theory). This result naturally introduces the notion of matrix rank into a regular language L, the minimal adjacency matrix rank of a deterministic automaton that recognises L. We then define and focus on rank-one languages: the class of languages for which the rank of minimal automaton is one. We also define the expanded canonical automaton of a rank-one language.',
	 'authors': u"Ryoma Sin'ya,",
	 'category': u'Computer Science ',
	 'date': '2014-5-11',
	 'pdflink': u'http://arxiv.org/pdf/1405.2553',
	 'subjects': u'Formal Languages and Automata Theory (cs.FL)',
	 'title': u'\nGraph Spectral Properties of Deterministic Finite Automata',
	 'urllink': u'http://arxiv.org/abs/1405.2553'}
2015-03-23 20:07:19+0000 [xxu461000] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1406.0187> (referer: http://arxiv.org/list/cs/14?skip=6000&show=1000)
2015-03-23 20:07:19+0000 [xxu461000] DEBUG: Scraped from <200 http://arxiv.org/abs/1406.0187>
	{'abstract': u'This paper proposes a set of piecewise Toeplitz matrices as the linear mapping/sensing operator for recovering low rank matrices from few measurements. We prove that such operators efficiently encode the information so there exists a unique reconstruction matrix under mild assumptions. This work provides a significant extension of the compressed sensing and rank minimization theory, and it achieves a tradeoff between reducing the memory required for storing the sampling operator from to but at the expense of increasing the number of measurements by . Simulation results show that the proposed operator can recover low rank matrices efficiently with a reconstruction performance close to the cases of using random unstructured operators.',
	 'authors': u'Kezhi Li, Cristian R. Rojas, Saikat Chatterjee, H\\rakan Hjalmarsson,',
	 'category': u'Computer Science ',
	 'date': '2014-6-1',
	 'pdflink': u'http://arxiv.org/pdf/1406.0187',
	 'subjects': u'Information Theory (cs.IT)',
	 'title': u'\nPiecewise Toeplitz Matrices-based Sensing for Rank Minimization',
	 'urllink': u'http://arxiv.org/abs/1406.0187'}
2015-03-23 20:07:24+0000 [xxu461000] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1407.5547> (referer: http://arxiv.org/list/cs/14?skip=8000&show=1000)
2015-03-23 20:07:24+0000 [xxu461000] DEBUG: Scraped from <200 http://arxiv.org/abs/1407.5547>
	{'abstract': u'Though online social network research has exploded during the past years, not much thought has been given to the exploration of the nature of social links. Online interactions have been interpreted as indicative of one social process or another (e.g., status exchange or trust), often with little systematic justification regarding the relation between observed data and theoretical concept. Our research aims to breach this gap in computational social science by proposing an unsupervised, parameter-free method to discover, with high accuracy, the fundamental domains of interaction occurring in social networks. By applying this method on two online datasets different by scope and type of interaction (aNobii and Flickr) we observe the spontaneous emergence of three domains of interaction representing the exchange of status, knowledge and social support. By finding significant relations between the domains of interaction and classic social network analysis issues (e.g., tie strength, dyadic interaction over time) we show how the network of interactions induced by the extracted domains can be used as a starting point for more nuanced analysis of online social data that may one day incorporate the normative grammar of social interaction. Our methods finds applications in online social media services ranging from recommendation to visual link summarization.',
	 'authors': u'Luca Maria Aiello, Rossano Schifanella, Bogdan State,',
	 'category': u'Computer Science ',
	 'date': '2014-7-21',
	 'pdflink': u'http://arxiv.org/pdf/1407.5547',
	 'subjects': u'Computers and Society (cs.CY)',
	 'title': u'\nReading the Source Code of Social Ties',
	 'urllink': u'http://arxiv.org/abs/1407.5547'}
2015-03-23 20:07:29+0000 [xxu461000] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1404.5585> (referer: http://arxiv.org/list/cs/14?skip=4000&show=1000)
2015-03-23 20:07:29+0000 [xxu461000] DEBUG: Scraped from <200 http://arxiv.org/abs/1404.5585>
	{'abstract': u'The IDSgrep structural query system for Han character dictionaries is presented. This system includes a data model and syntax for describing the spatial structure of Han characters using Extended Ideographic Description Sequences (EIDSes) based on the Unicode IDS syntax; a language for querying EIDS databases, designed to suit the needs of font developers and foreign language learners; a bit vector index inspired by Bloom filters for faster query operations; a freely available implementation; and format translation from popular third-party IDS and XML character databases. Experimental results are included, with a comparison to other software used for similar applications.',
	 'authors': u'Matthew Skala,',
	 'category': u'Computer Science ',
	 'date': '2014-4-22',
	 'pdflink': u'http://arxiv.org/pdf/1404.5585',
	 'subjects': u'Computation and Language (cs.CL)',
	 'title': u'\nA Structural Query System for Han Characters',
	 'urllink': u'http://arxiv.org/abs/1404.5585'}
2015-03-23 20:07:32+0000 [xxu461000] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1405.2539> (referer: http://arxiv.org/list/cs/14?skip=5000&show=1000)
2015-03-23 20:07:32+0000 [xxu461000] DEBUG: Scraped from <200 http://arxiv.org/abs/1405.2539>
	{'abstract': u'Image Mosaicing is a method of constructing multiple images of the same scene into a larger image. The output of the image mosaic will be the union of two input images. Image-mosaicing algorithms are used to get mosaiced image. Image Mosaicing processed is basically divided in to 5 phases. Which includes; Feature point extraction, Image registration, Homography computation, Warping and Blending if Image. Various corner detection algorithm is being used for Feature extraction. This corner produces an efficient and informative output mosaiced image. Image mosaicing is widely used in creating 3D images, medical imaging, computer vision, data from satellites, and military automatic target recognition.',
	 'authors': u'Dushyant Vaghela, Prof. Kapildev Naina,',
	 'category': u'Computer Science ',
	 'date': '2014-5-11',
	 'pdflink': u'http://arxiv.org/pdf/1405.2539',
	 'subjects': u'Computer Vision and Pattern Recognition (cs.CV)',
	 'title': u'\nA Review of Image Mosaicing Techniques',
	 'urllink': u'http://arxiv.org/abs/1405.2539'}
2015-03-23 20:07:38+0000 [xxu461000] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1406.0184> (referer: http://arxiv.org/list/cs/14?skip=6000&show=1000)
2015-03-23 20:07:38+0000 [xxu461000] DEBUG: Scraped from <200 http://arxiv.org/abs/1406.0184>
	{'abstract': u'In this paper we examine the key elements determining the best performance of computing by increasing the frequency of a single chip and to get the minimum latency during execution of the programs to achieve best possible output. It is not enough to provide concurrent improvements in the hardware as Software also have to introduce concurrency in order to exploit the parallelism. The software parallelism is defined by the control and data dependency of programs whereas Hardware refers to the type of parallelism defined by the machine architecture and hardware multiplicity.',
	 'authors': u'Kamran Latif,',
	 'category': u'Computer Science ',
	 'date': '2014-6-1',
	 'pdflink': u'http://arxiv.org/pdf/1406.0184',
	 'subjects': u'Performance (cs.PF)',
	 'title': u'\nParallelism Via Concurrency at Multiple Levels',
	 'urllink': u'http://arxiv.org/abs/1406.0184'}
2015-03-23 20:07:43+0000 [xxu461000] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1407.5537> (referer: http://arxiv.org/list/cs/14?skip=8000&show=1000)
2015-03-23 20:07:43+0000 [xxu461000] DEBUG: Scraped from <200 http://arxiv.org/abs/1407.5537>
	{'abstract': u'Millimeter wave (mmW) cellular systems will require high gain directional antennas and dense base station (BS) deployments to overcome high near field path loss and poor diffraction. As a desirable side effect, high gain antennas provide interference isolation, providing an opportunity to incorporate self-backhauling--BSs backhauling among themselves in a mesh architecture without significant loss in throughput--to enable the requisite large BS densities. The use of directional antennas and resource sharing between access and backhaul links leads to coverage and rate trends that differ significantly from conventional microwave (W) cellular systems. In this paper, we propose a general and tractable mmW cellular model capturing these key trends and characterize the associated rate distribution. The developed model and analysis is validated using actual building locations from dense urban settings and empirically-derived path loss models. The analysis shows that in sharp contrast to the interference limited nature of W cellular networks, the spectral efficiency of mmW networks (besides total rate) also increases with BS density particularly at the cell edge. Increasing the system bandwidth, although boosting median and peak rates, does not significantly influence the cell edge rate. With self-backhauling, different combinations of the wired backhaul fraction (i.e. the faction of BSs with a wired connection) and BS density are shown to guarantee the same median rate (QoS).',
	 'authors': u'Sarabjot Singh, Mandar N. Kulkarni, Amitava Ghosh, Jeffrey G. Andrews,',
	 'category': u'Computer Science ',
	 'date': '2014-7-21',
	 'pdflink': u'http://arxiv.org/pdf/1407.5537',
	 'subjects': u'Information Theory (cs.IT)',
	 'title': u'\nTractable Model for Rate in Self-Backhauled Millimeter Wave Cellular  Networks',
	 'urllink': u'http://arxiv.org/abs/1407.5537'}
2015-03-23 20:07:46+0000 [xxu461000] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1404.5584> (referer: http://arxiv.org/list/cs/14?skip=4000&show=1000)
2015-03-23 20:07:46+0000 [xxu461000] DEBUG: Scraped from <200 http://arxiv.org/abs/1404.5584>
	{'abstract': u'In the last years the vertex enumeration problem of polyhedra has seen a revival in the study of metabolic networks, which increased the demand for efficient vertex enumeration algorithms for high-dimensional polyhedra given by inequalities. In this paper we apply the concept of branch-decomposition to the vertex enumeration problem of polyhedra . Therefore, we introduce the concept of -module and show how it relates to the separators of the linear matroid generated by the columns of . This then translates structural properties of the matroidal branch-decomposition to the context of polyhedra. We then use this to present a total polynomial time algorithm for polytopes for which the branch-width of the linear matroid generated by is bounded by a constant .',
	 'authors': u'Arne C. Reimers, Leen Stougie,',
	 'category': u'Computer Science ',
	 'date': '2014-4-22',
	 'pdflink': u'http://arxiv.org/pdf/1404.5584',
	 'subjects': u'Computational Geometry (cs.CG)',
	 'title': u'\nA decomposition theory for vertex enumeration of convex polyhedra',
	 'urllink': u'http://arxiv.org/abs/1404.5584'}
2015-03-23 20:07:48+0000 [xxu461000] INFO: Crawled 170 pages (at 13 pages/min), scraped 153 items (at 13 items/min)
2015-03-23 20:07:50+0000 [xxu461000] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1405.2538> (referer: http://arxiv.org/list/cs/14?skip=5000&show=1000)
2015-03-23 20:07:50+0000 [xxu461000] DEBUG: Scraped from <200 http://arxiv.org/abs/1405.2538>
	{'abstract': u"Picat, a new member of the logic programming family, follows a different doctrine than Prolog in offering the core logic programming concepts: arrays and maps as built-in data types; implicit pattern matching with explicit unification and explicit non-determinism; functions for deterministic computations; and loops for convenient scripting and modeling purposes. Picat provides facilities for solving combinatorial search problems, including a common interface with CP, SAT, and MIP solvers, tabling for dynamic programming, and a module for planning. Picat's planner module, which is implemented by the use of tabling, has produced surprising and encouraging results. Thanks to term-sharing and resource-bounded tabled search, Picat overwhelmingly outperforms the cutting-edge ASP and PDDL planners on the planning benchmarks used in recent ASP competitions.",
	 'authors': u'Neng-Fa Zhou,',
	 'category': u'Computer Science ',
	 'date': '2014-5-11',
	 'pdflink': u'http://arxiv.org/pdf/1405.2538',
	 'subjects': u'Programming Languages (cs.PL)',
	 'title': u'\nCombinatorial Search With Picat',
	 'urllink': u'http://arxiv.org/abs/1405.2538'}
2015-03-23 20:07:56+0000 [xxu461000] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1406.0175> (referer: http://arxiv.org/list/cs/14?skip=6000&show=1000)
2015-03-23 20:07:56+0000 [xxu461000] DEBUG: Scraped from <200 http://arxiv.org/abs/1406.0175>
	{'abstract': u"Games have always been a popular test bed for artificial intelligence techniques. Game developers are always in constant search for techniques that can automatically create computer games minimizing the developer's task. In this work we present an evolutionary strategy based solution towards the automatic generation of two player board games. To guide the evolutionary process towards games, which are entertaining, we propose a set of metrics. These metrics are based upon different theories of entertainment in computer games. This work also compares the entertainment value of the evolved games with the existing popular board based games. Further to verify the entertainment value of the evolved games with the entertainment value of the human user a human user survey is conducted. In addition to the user survey we check the learnability of the evolved games using an artificial neural network based controller. The proposed metrics and the evolutionary process can be employed for generating new and entertaining board games, provided an initial search space is given to the evolutionary algorithm.",
	 'authors': u'Zahid Halim,',
	 'category': u'Computer Science ',
	 'date': '2014-6-1',
	 'pdflink': u'http://arxiv.org/pdf/1406.0175',
	 'subjects': u'Neural and Evolutionary Computing (cs.NE)',
	 'title': u'\nEvolutionary Search in the Space of Rules for Creation of New Two-Player  Board Games',
	 'urllink': u'http://arxiv.org/abs/1406.0175'}
2015-03-23 20:08:01+0000 [xxu461000] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1407.5527> (referer: http://arxiv.org/list/cs/14?skip=8000&show=1000)
2015-03-23 20:08:01+0000 [xxu461000] DEBUG: Scraped from <200 http://arxiv.org/abs/1407.5527>
	{'abstract': u'In this work, a new approach to the definition of the quality of experience is presented. By considering the quality of service as a baseline, that portion of the QoE that can be inferred from the QoS is excluded, and then the rest of the QoE is approached with the notion of QoE at a Boundary (QoEaaB). With the QoEaaB as the core of the proposed approach, various potential boundaries, and their associated unseen opportunities to improve the QoE are discussed. In particular, property, contract, SLA, and content are explored in terms of their boundaries and also their associated QoEaaB. With an interest in online video delivery, management of resource sharing and isolation associated with multi-tenant operations is considered. It is concluded that the proposed QoEaaB can bring a new perspective in QoE modeling and assessment toward a more enriched approach to improving the experience based on innovation and deep connectivity among actors.',
	 'authors': u'Reza Farrahi Moghaddam, Mohamed Cheriet,',
	 'category': u'Computer Science ',
	 'date': '2014-7-18',
	 'pdflink': u'http://arxiv.org/pdf/1407.5527',
	 'subjects': u'Human-Computer Interaction (cs.HC)',
	 'title': u'\nQuality of Experience (QoE) beyond Quality of Service (QoS) as its  baseline: QoE at the Interface of Experience Domains',
	 'urllink': u'http://arxiv.org/abs/1407.5527'}
2015-03-23 20:08:06+0000 [xxu461000] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1404.5581> (referer: http://arxiv.org/list/cs/14?skip=4000&show=1000)
2015-03-23 20:08:06+0000 [xxu461000] DEBUG: Scraped from <200 http://arxiv.org/abs/1404.5581>
	{'abstract': u'The Strukov model was the phenomenological model that accompanied the announcement of the first recognised physical instantiation of the memristor and, as such, it has been widely used. This model described the motion of a boundary, , between two types of inter-converting material, and , seemingly under a uniform field across the entire device. In fact, what was intended was a field with a discontinuity at , that was uniform between . In this paper we show that the discontinuity is required for the Strukov model derivation to be completed, and thus the derivation as given does not describe a situation with a uniform field across the entire device. The discontinuity can be described as a Heaviside function, , located on , for which there are three common single-valued approximations for . The Strukov model as intended includes an approximation for the Heaviside function (the field is taken to be the same as that across the part of the device). We compare approximations and give solutions. We then extend the description of the field to a more-realistic continuously varying sigmoidal transition between two uniform fields and demonstrate that the centro-symmetric approximation model (taking the field as being the average of the fields across and ) is a better single-point model of that situation: the other two approximations over or underestimate the field.',
	 'authors': u'Ella Gale,',
	 'category': u'Computer Science ',
	 'date': '2014-4-22',
	 'pdflink': u'http://arxiv.org/pdf/1404.5581',
	 'subjects': u'Emerging Technologies (cs.ET)',
	 'title': u'\nUniform and Piece-wise Uniform Fields in Memristor Models',
	 'urllink': u'http://arxiv.org/abs/1404.5581'}
2015-03-23 20:08:11+0000 [xxu461000] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1405.2530> (referer: http://arxiv.org/list/cs/14?skip=5000&show=1000)
2015-03-23 20:08:11+0000 [xxu461000] DEBUG: Scraped from <200 http://arxiv.org/abs/1405.2530>
	{'abstract': u'We consider the problem of scheduling jobs to minimize the makespan on unrelated machines, where job requires time if processed on machine . A classic algorithm of Lenstra et al. yields the best known approximation ratio of for the problem. Improving this bound has been a prominent open problem for over two decades. In this paper we obtain a tighter bound for a wide subclass of instances which can be identified efficiently. Specifically, we define the feasibility factor of a given instance as the minimum fraction of machines on which each job can be processed. We show that there is a polynomial-time algorithm that, given values and , and an instance having a sufficiently large feasibility factor , either proves that no schedule of mean machine completion time and makespan exists, or else finds a schedule of makespan at most . For the restricted version of the problem, where for each job and machine , , we show that a simpler algorithm yields a better bound, thus improving for highly feasible instances the best known ratio of , for any fixed , due to Svensson.',
	 'authors': u'Dor Arad, Yael Mordechai, Hadas Shachnai,',
	 'category': u'Computer Science ',
	 'date': '2014-5-11',
	 'pdflink': u'http://arxiv.org/pdf/1405.2530',
	 'subjects': u'Data Structures and Algorithms (cs.DS)',
	 'title': u'\nTighter Bounds for Makespan Minimization on Unrelated Machines',
	 'urllink': u'http://arxiv.org/abs/1405.2530'}
2015-03-23 20:08:17+0000 [xxu461000] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1406.0173> (referer: http://arxiv.org/list/cs/14?skip=6000&show=1000)
2015-03-23 20:08:17+0000 [xxu461000] DEBUG: Scraped from <200 http://arxiv.org/abs/1406.0173>
	{'abstract': u'Common ISAR radar images and signals can be reconstructed from much fewer samples than the sampling theorem requires since they are usually sparse. Unavailable randomly positioned samples can result from heavily corrupted parts of the signal. Since these samples can be omitted and declared as unavailable, the application of the compressive sensing methods in the recovery of heavily corrupted signal and radar images is possible. A simple direct method for the recovery of unavailable signal samples and the calculation of the restored ISAR image is reviewed. An analysis of the noise influence is performed. For fast maneuvering ISAR targets the sparsity property is lost since the ISAR image is blurred. A nonparametric quadratic time-frequency representations based method is used to restore the ISAR image sparsity. However, the linear relation between the signal and the sparsity domain transformation is lost. A recently proposed gradient recovery algorithm is adapted for this kind of analysis. It does not require the linear relation of the signal and its sparsity domain transformation in the process of unavailable data recovery. The presented methods and results are tested on several numerical examples proving the expected accuracy and improvements.',
	 'authors': u'Ljubisa Stankovic,',
	 'category': u'Computer Science ',
	 'date': '2014-6-1',
	 'pdflink': u'http://arxiv.org/pdf/1406.0173',
	 'subjects': u'Information Theory (cs.IT)',
	 'title': u'\nOn the ISAR Image Analysis and Recovery with Unavailable or Heavily  Corrupted Data',
	 'urllink': u'http://arxiv.org/abs/1406.0173'}
2015-03-23 20:08:22+0000 [xxu461000] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1407.5524> (referer: http://arxiv.org/list/cs/14?skip=8000&show=1000)
2015-03-23 20:08:22+0000 [xxu461000] DEBUG: Scraped from <200 http://arxiv.org/abs/1407.5524>
	{'abstract': u'We introduce process-oriented programming as a natural extension of object-oriented programming for parallel computing. It is based on the observation that every class of an object-oriented language can be instantiated as a process, accessible via a remote pointer. The introduction of process pointers requires no syntax extension, identifies processes with programming objects, and enables processes to exchange information simply by executing remote methods. Process-oriented programming is a high-level language alternative to multithreading, MPI and many other languages, environments and tools currently used for parallel computations. It implements natural object-based parallelism using only minimal syntax extension of existing languages, such as C++ and Python, and has therefore the potential to lead to widespread adoption of parallel programming. We implemented a prototype system for running processes using C++ with MPI and used it to compute a large three-dimensional Fourier transform on a computer cluster built of commodity hardware components. Three-dimensional Fourier transform is a prototype of a data-intensive application with a complex data-access pattern. The process-oriented code is only a few hundred lines long, and attains very high data throughput by achieving massive parallelism and maximizing hardware utilization.',
	 'authors': u'Edward Givelberg,',
	 'category': u'Computer Science ',
	 'date': '2014-7-21',
	 'pdflink': u'http://arxiv.org/pdf/1407.5524',
	 'subjects': u'Programming Languages (cs.PL)',
	 'title': u'\nProcess-Oriented Parallel Programming with an Application to  Data-Intensive Computing',
	 'urllink': u'http://arxiv.org/abs/1407.5524'}
2015-03-23 20:08:27+0000 [xxu461000] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1404.5569> (referer: http://arxiv.org/list/cs/14?skip=4000&show=1000)
2015-03-23 20:08:27+0000 [xxu461000] DEBUG: Scraped from <200 http://arxiv.org/abs/1404.5569>
	{'abstract': u'Online Bin Stretching is a semi-online variant of bin packing in which the algorithm has to use the same number of bins as the optimal packing, but is allowed to slightly overpack the bins. The goal is to minimize the amount of overpacking, i.e., the maximum size packed into any bin. We give an algorithm for Online Bin Stretching with a stretching factor of 1.5 for any number of bins. We also show a specialized algorithm for three bins with a stretching factor of 11/8 = 1.375.',
	 'authors': u'Martin B\xf6hm, Ji\u0159\xed Sgall, Rob van Stee, Pavel Vesel\xfd,',
	 'category': u'Computer Science ',
	 'date': '2014-4-22',
	 'pdflink': u'http://arxiv.org/pdf/1404.5569',
	 'subjects': u'Data Structures and Algorithms (cs.DS)',
	 'title': u'\nBetter Algorithms for Online Bin Stretching',
	 'urllink': u'http://arxiv.org/abs/1404.5569'}
2015-03-23 20:08:30+0000 [xxu461000] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1405.2524> (referer: http://arxiv.org/list/cs/14?skip=5000&show=1000)
2015-03-23 20:08:30+0000 [xxu461000] DEBUG: Scraped from <200 http://arxiv.org/abs/1405.2524>
	{'abstract': u'For any given short code (referred to as the basic code), block Markov superposition transmission (BMST) provides a simple way to obtain predictable extra coding gain by spatial coupling the generator matrix of the basic code. This paper presents a systematic design methodology for BMST systems to approach the channel capacity at any given target bit-error-rate (BER) of interest. To simplify the design, we choose the basic code as the Cartesian product of a short block code. The encoding memory is then inferred from the genie-aided lower bound according to the performance gap of the short block code to the corresponding Shannon limit at the target BER. In addition to the sliding-window decoding algorithm, we propose to perform one more phase decoding to remove residual (rare) errors. A new technique that assumes a noisy genie is proposed to upper bound the performance. Under some mild assumptions, these genie-aided bounds can be used to predict the performance of the proposed two-phase decoding algorithm in the extremely low BER region. Using the Cartesian product of a repetition code as the basic code, we construct a BMST system with an encoding memory 30 whose performance at the BER of can be predicted within one dB away from the Shannon limit over the binary-input additive white Gaussian noise channel (BI-AWGNC).',
	 'authors': u'Chulong Liang, Xiao Ma, Qiutao Zhuang, Baoming Bai,',
	 'category': u'Computer Science ',
	 'date': '2014-5-11',
	 'pdflink': u'http://arxiv.org/pdf/1405.2524',
	 'subjects': u'Information Theory (cs.IT)',
	 'title': u'\nSpatial Coupling of Generator Matrix: A General Approach to Design of  Good Codes at a Target BER',
	 'urllink': u'http://arxiv.org/abs/1405.2524'}
2015-03-23 20:08:34+0000 [xxu461000] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1406.0157> (referer: http://arxiv.org/list/cs/14?skip=6000&show=1000)
2015-03-23 20:08:34+0000 [xxu461000] DEBUG: Scraped from <200 http://arxiv.org/abs/1406.0157>
	{'abstract': u"A rateless code encodes a finite length information word into an infinitely long codeword such that longer prefixes of the codeword can tolerate a larger fraction of errors. A rateless code achieves capacity for a family of channels if, for every channel in the family, reliable communication is obtained by a prefix of the code whose rate is arbitrarily close to the channel's capacity. As a result, a universal encoder can communicate over all channels in the family while simultaneously achieving optimal communication overhead. In this paper, we construct the first emph rateless code for the binary symmetric channel. Our code can be encoded and decoded in time per bit and in almost logarithmic parallel time of , where is any (arbitrarily slow) super-constant function. Furthermore, the error probability of our code is almost exponentially small . Previous rateless codes are probabilistic (i.e., based on code ensembles), require polynomial time per bit for decoding, and have inferior asymptotic error probabilities. Our main technical contribution is a constructive proof for the existence of an infinite generating matrix that each of its prefixes induce a weight distribution that approximates the expected weight distribution of a random linear code.",
	 'authors': u'Benny Applebaum, Liron David, Guy Even,',
	 'category': u'Computer Science ',
	 'date': '2014-6-1',
	 'pdflink': u'http://arxiv.org/pdf/1406.0157',
	 'subjects': u'Information Theory (cs.IT)',
	 'title': u'\nDeterministic Rateless Codes for BSC',
	 'urllink': u'http://arxiv.org/abs/1406.0157'}
2015-03-23 20:08:38+0000 [xxu461000] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1407.5514> (referer: http://arxiv.org/list/cs/14?skip=8000&show=1000)
2015-03-23 20:08:38+0000 [xxu461000] DEBUG: Scraped from <200 http://arxiv.org/abs/1407.5514>
	{'abstract': u'We present the concept of an acoustic rake receiver---a microphone beamformer that uses echoes to improve the noise and interference suppression. The rake idea is well-known in wireless communications; it involves constructively combining different multipath components that arrive at the receiver antennas. Unlike spread-spectrum signals used in wireless communications, speech signals are not orthogonal to their shifts. Therefore, we focus on the spatial structure, rather than temporal. Instead of explicitly estimating the channel, we create correspondences between early echoes in time and image sources in space. These multiple sources of the desired and the interfering signal offer additional spatial diversity that we can exploit in the beamformer design. We present several "intuitive" and optimal formulations of acoustic rake receivers, and show theoretically and numerically that the rake formulation of the maximum signal-to-interference-and-noise beamformer offers significant performance boosts in terms of noise and interference suppression. Beyond signal-to-noise ratio, we observe gains in terms of the emph (PESQ) metric for the speech quality. We accompany the paper by the complete simulation and processing chain written in Python. The code and the sound samples are available online at url.',
	 'authors': u'Ivan Dokmani\u0107, Robin Scheibler, Martin Vetterli,',
	 'category': u'Computer Science ',
	 'date': '2014-7-21',
	 'pdflink': u'http://arxiv.org/pdf/1407.5514',
	 'subjects': u'Sound (cs.SD)',
	 'title': u'\nRaking the Cocktail Party',
	 'urllink': u'http://arxiv.org/abs/1407.5514'}
2015-03-23 20:08:42+0000 [xxu461000] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1404.5568> (referer: http://arxiv.org/list/cs/14?skip=4000&show=1000)
2015-03-23 20:08:42+0000 [xxu461000] DEBUG: Scraped from <200 http://arxiv.org/abs/1404.5568>
	{'abstract': u'We study a basic problem of approximating the size of an unknown set in a known universe . We consider two versions of the problem. In both versions the algorithm can specify subsets . In the first version, which we refer to as the group query or subset query version, the algorithm is told whether is non-empty. In the second version, which we refer to as the subset sampling version, if is non-empty, then the algorithm receives a uniformly selected element from . We study the difference between these two versions under different conditions on the subsets that the algorithm may query/sample, and in both the case that the algorithm is adaptive and the case where it is non-adaptive. In particular we focus on a natural family of allowed subsets, which correspond to intervals, as well as variants of this family.',
	 'authors': u'Dana Ron, Gilad Tsur,',
	 'category': u'Computer Science ',
	 'date': '2014-4-20',
	 'pdflink': u'http://arxiv.org/pdf/1404.5568',
	 'subjects': u'Data Structures and Algorithms (cs.DS)',
	 'title': u'\nThe Power of an Example: Hidden Set Size Approximation Using Group  Queries and Conditional Sampling',
	 'urllink': u'http://arxiv.org/abs/1404.5568'}
2015-03-23 20:08:47+0000 [xxu461000] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1405.2523> (referer: http://arxiv.org/list/cs/14?skip=5000&show=1000)
2015-03-23 20:08:47+0000 [xxu461000] DEBUG: Scraped from <200 http://arxiv.org/abs/1405.2523>
	{'abstract': u'In recent years, the issue of energy consumption in high performance computing (HPC) systems has attracted a great deal of attention. In response to this, many energy-aware algorithms have been developed in different layers of HPC systems, including the hardware layer, service layer and system layer. These algorithms are of two types: first, algorithms which directly try to improve the energy by tweaking frequency operation or scheduling algorithms; and second, algorithms which focus on improving the performance of the system, with the assumption that efficient running of a system may indirectly save more energy. In this thesis, we develop algorithms in both layers. First, we introduce three algorithms to directly improve the energy of scheduled tasks at the hardware level by using Dynamic Voltage Frequency Scaling (DVFS). Second, we propose two algorithms for modelling and resource provisioning of MapReduce applications (a well-known parametric distributed framework currently used by Google, Yahoo, Facebook and LinkedIn) based on its configuration parameters. Certainly, estimating the performance (e.g., execution time or CPU clock ticks) of a MapReduce application can be later used for smart scheduling of such applications in clouds or clusters. To evaluate the algorithms, we have conducted extensive simulation and real experiments on a 5-node physical cluster with up to 25 virtual nodes, using both synthetic and real world applications. Also, the proposed new algorithms are compared with existing algorithms by experimentation, and the experimental results reveal new information on the performance of these algorithms, as well as on the properties of MapReduce and DVFS. In the end, three open problems are revealed by the experimental observations, and their importance is explained.',
	 'authors': u'Nikzad Babaii Rizvandi,',
	 'category': u'Computer Science ',
	 'date': '2014-5-11',
	 'pdflink': u'http://arxiv.org/pdf/1405.2523',
	 'subjects': u'Distributed, Parallel, and Cluster Computing (cs.DC)',
	 'title': u'\nPerformance Provisioning and Energy Efficiency in Cloud and Distributed  Computing Systems',
	 'urllink': u'http://arxiv.org/abs/1405.2523'}
2015-03-23 20:08:48+0000 [xxu461000] INFO: Crawled 183 pages (at 13 pages/min), scraped 166 items (at 13 items/min)
2015-03-23 20:08:51+0000 [xxu461000] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1406.0156> (referer: http://arxiv.org/list/cs/14?skip=6000&show=1000)
2015-03-23 20:08:51+0000 [xxu461000] DEBUG: Scraped from <200 http://arxiv.org/abs/1406.0156>
	{'abstract': u"This paper proposed a new regression model called -regularized outlier isolation and regression (LOIRE) and a fast algorithm based on block coordinate descent to solve this model. Besides, assuming outliers are gross errors following a Bernoulli process, this paper also presented a Bernoulli estimate model which, in theory, should be very accurate and robust due to its complete elimination of affections caused by outliers. Though this Bernoulli estimate is hard to solve, it could be approximately achieved through a process which takes LOIRE as an important intermediate step. As a result, the approximate Bernoulli estimate is a good combination of Bernoulli estimate's accuracy and LOIRE regression's efficiency with several simulations conducted to strongly verify this point. Moreover, LOIRE can be further extended to realize robust rank factorization which is powerful in recovering low-rank component from massive corruptions. Extensive experimental results showed that the proposed method outperforms state-of-the-art methods like RPCA and GoDec in the aspect of computation speed with a competitive performance.",
	 'authors': u'Sheng Han, Suzhen Wang, Xinyu Wu,',
	 'category': u'Computer Science ',
	 'date': '2014-6-1',
	 'pdflink': u'http://arxiv.org/pdf/1406.0156',
	 'subjects': u'Computer Vision and Pattern Recognition (cs.CV)',
	 'title': u'\n$l_1$-regularized Outlier Isolation and Regression',
	 'urllink': u'http://arxiv.org/abs/1406.0156'}
2015-03-23 20:08:54+0000 [xxu461000] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1407.5495> (referer: http://arxiv.org/list/cs/14?skip=8000&show=1000)
2015-03-23 20:08:54+0000 [xxu461000] DEBUG: Scraped from <200 http://arxiv.org/abs/1407.5495>
	{'abstract': u'Fifth generation (5G) wireless networks are expected to support very diverse applications and terminals. Massive connectivity with a large number of devices is an important requirement for 5G networks. Current LTE system is not able to efficiently support massive connectivity, especially on the uplink (UL). Among the issues arise due to massive connectivity is the cost of signaling overhead and latency. In this paper, an uplink contention-based sparse code multiple access (SCMA) design is proposed as a solution. First, the system design aspects of the proposed multiple-access scheme are described. The SCMA parameters can be adjusted to provide different levels of overloading, thus suitable to meet the diverse traffic connectivity requirements. In addition, the system-level evaluations of a small packet application scenario are provided for contention-based UL SCMA. SCMA is compared to OFDMA in terms of connectivity and drop rate under a tight latency requirement. The simulation results demonstrate that contention-based SCMA can provide around 2.8 times gain over contention-based OFDMA in terms of supported active users. The uplink contention-based SCMA scheme can be a promising technology for 5G wireless networks for data transmission with low signaling overhead, low delay, and support of massive connectivity.',
	 'authors': u'Kelvin Au, Liqing Zhang, Hosein Nikopour, Eric Yi, Alireza Bayesteh, Usa Vilaipornsawai, Jianglei Ma, Peiying Zhu,',
	 'category': u'Computer Science ',
	 'date': '2014-7-21',
	 'pdflink': u'http://arxiv.org/pdf/1407.5495',
	 'subjects': u'Information Theory (cs.IT)',
	 'title': u'\nUplink Contention Based SCMA for 5G Radio Access',
	 'urllink': u'http://arxiv.org/abs/1407.5495'}
2015-03-23 20:09:00+0000 [xxu461000] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1404.5566> (referer: http://arxiv.org/list/cs/14?skip=4000&show=1000)
2015-03-23 20:09:00+0000 [xxu461000] DEBUG: Scraped from <200 http://arxiv.org/abs/1404.5566>
	{'abstract': u'The problem of finding an optimal vertex cover in a graph is a classic NP-complete problem, and is a special case of the hitting set question. On the other hand, the hitting set problem, when asked in the context of induced geometric objects, often turns out to be exactly the vertex cover problem on restricted classes of graphs. In this work we explore a particular instance of such a phenomenon. We consider the problem of hitting all axis-parallel slabs induced by a point set P, and show that it is equivalent to the problem of finding a vertex cover on a graph whose edge set is the union of two Hamiltonian Paths. We show the latter problem to be NP-complete, and we also give an algorithm to find a vertex cover of size at most k, on graphs of maximum degree four, whose running time is 1.2637^k n^O(1).',
	 'authors': u'Akanksha Agrawal, Sathish Govindarajan, Neeldhara Misra,',
	 'category': u'Computer Science ',
	 'date': '2014-4-21',
	 'pdflink': u'http://arxiv.org/pdf/1404.5566',
	 'subjects': u'Data Structures and Algorithms (cs.DS)',
	 'title': u'\nVertex Cover Gets Faster and Harder on Low Degree Graphs',
	 'urllink': u'http://arxiv.org/abs/1404.5566'}
2015-03-23 20:09:04+0000 [xxu461000] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1405.2517> (referer: http://arxiv.org/list/cs/14?skip=5000&show=1000)
2015-03-23 20:09:04+0000 [xxu461000] DEBUG: Scraped from <200 http://arxiv.org/abs/1405.2517>
	{'abstract': u'A number of recent efforts aim to bridge the global digital divide, particularly with respect to Internet access. We take this endeavor one step further and argue that Internet access and web security go hand in glove in the developing world. To remedy the situation, we explore whether low-cost platforms, such as Raspberry Pi ( 59), can be used to implement security mechanisms. Using a firewall as a motivating security application we benchmark its performance on these platforms to test our thesis. Our results show that these platforms can indeed serve as enablers of security functions for small sized deployments in the developing world, while only consuming less than',
	 'authors': u'Zubair Nabi,',
	 'category': u'Computer Science ',
	 'date': '2014-5-11',
	 'pdflink': u'http://arxiv.org/pdf/1405.2517',
	 'subjects': u'Computers and Society (cs.CY)',
	 'title': u'\nA $35 Firewall for the Developing World',
	 'urllink': u'http://arxiv.org/abs/1405.2517'}
2015-03-23 20:09:08+0000 [xxu461000] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1406.0155> (referer: http://arxiv.org/list/cs/14?skip=6000&show=1000)
2015-03-23 20:09:08+0000 [xxu461000] DEBUG: Scraped from <200 http://arxiv.org/abs/1406.0155>
	{'abstract': u'Measuring inconsistency is viewed as an important issue related to handling inconsistencies. Good measures are supposed to satisfy a set of rational properties. However, defining sound properties is sometimes problematic. In this paper, we emphasize one such property, named Decomposability, rarely discussed in the literature due to its modeling difficulties. To this end, we propose an independent decomposition which is more intuitive than existing proposals. To analyze inconsistency in a more fine-grained way, we introduce a graph representation of a knowledge base and various MUSdecompositions. One particular MUS-decomposition, named distributable MUS-decomposition leads to an interesting partition of inconsistencies in a knowledge base such that multiple experts can check inconsistencies in parallel, which is impossible under existing measures. Such particular MUSdecomposition results in an inconsistency measure that satisfies a number of desired properties. Moreover, we give an upper bound complexity of the measure that can be computed using 0/1 linear programming or Min Cost Satisfiability problems, and conduct preliminary experiments to show its feasibility.',
	 'authors': u'Said Jabbour, Yue Ma, Badran Raddaoui, Lakhdar Sais, Yakoub Salhi,',
	 'category': u'Computer Science ',
	 'date': '2014-6-1',
	 'pdflink': u'http://arxiv.org/pdf/1406.0155',
	 'subjects': u'Artificial Intelligence (cs.AI)',
	 'title': u'\nOn the measure of conflicts: A MUS-Decomposition Based Framework',
	 'urllink': u'http://arxiv.org/abs/1406.0155'}
2015-03-23 20:09:12+0000 [xxu461000] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1407.5488> (referer: http://arxiv.org/list/cs/14?skip=8000&show=1000)
2015-03-23 20:09:12+0000 [xxu461000] DEBUG: Scraped from <200 http://arxiv.org/abs/1407.5488>
	{'abstract': u'An L(2,1)-labelling of a graph is a function from the vertex set V (G) to the set of non-negative integers such that adjacent vertices get numbers at least two apart, and vertices at distance two get distinct numbers. The L(2,1)-labelling number denoted by of is the minimum range of labels over all such labelling. In this article, it is shown that, for a circular-arc graph , the upper bound of is , where and represents the maximum degree of the vertices and size of maximum clique respectively.',
	 'authors': u'Satyabrata Paul, Madhumangal Pal, Anita Pal,',
	 'category': u'Computer Science ',
	 'date': '2014-7-21',
	 'pdflink': u'http://arxiv.org/pdf/1407.5488',
	 'subjects': u'Discrete Mathematics (cs.DM)',
	 'title': u'\nL(2,1)-labelling of Circular-arc Graph',
	 'urllink': u'http://arxiv.org/abs/1407.5488'}
2015-03-23 20:09:16+0000 [xxu461000] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1404.5565> (referer: http://arxiv.org/list/cs/14?skip=4000&show=1000)
2015-03-23 20:09:16+0000 [xxu461000] DEBUG: Scraped from <200 http://arxiv.org/abs/1404.5565>
	{'abstract': u'It has been known since long time that many NP-hard optimization problems can be solved in polynomial time when restricted to structures of constant treewidth. In this work we provide the first extension of such results to the quantum setting. We show that given a quantum circuit with uninitialized inputs, gates and treewidth , one can compute in time a classical witness that maximizes the acceptance probability of up to a additive factor. In particular our algorithm runs in polynomial time if is constant and . For unrestricted values of this problem is known to be hard for the complexity class QCMA, a quantum generalization of NP. In contrast, we show that the same problem is already NP-hard if even when is constant. Finally, we show that for and constant , it is QMA-hard to find a quantum witness that maximizes the acceptance probability of a quantum circuit of treewidth up to a additive factor.',
	 'authors': u'Mateus de Oliveira Oliveira,',
	 'category': u'Computer Science ',
	 'date': '2014-4-22',
	 'pdflink': u'http://arxiv.org/pdf/1404.5565',
	 'subjects': u'Computational Complexity (cs.CC)',
	 'title': u'\nOn the Satisfiability of Quantum Circuits of Small Treewidth',
	 'urllink': u'http://arxiv.org/abs/1404.5565'}
2015-03-23 20:09:21+0000 [xxu461000] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1405.2512> (referer: http://arxiv.org/list/cs/14?skip=5000&show=1000)
2015-03-23 20:09:21+0000 [xxu461000] DEBUG: Scraped from <200 http://arxiv.org/abs/1405.2512>
	{'abstract': u'Consider high-dimensional data set such that for every data-point there exist an information at only part of its dimensions, and the rest is unknown. We assume that the data emerge from real-world data, i.e., the true (unknown) points lie close to each other such that they may be group together.',
	 'authors': u'Hadassa Daltrophe, Shlomi Dolev, Zvi Lotker,',
	 'category': u'Computer Science ',
	 'date': '2014-5-11',
	 'pdflink': u'http://arxiv.org/e-print/1405.2512',
	 'subjects': u'Other Computer Science (cs.OH)',
	 'title': u'\nBig Data Representation as High Dimensional Configuration',
	 'urllink': u'http://arxiv.org/abs/1405.2512'}
2015-03-23 20:09:27+0000 [xxu461000] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1406.0154> (referer: http://arxiv.org/list/cs/14?skip=6000&show=1000)
2015-03-23 20:09:27+0000 [xxu461000] DEBUG: Scraped from <200 http://arxiv.org/abs/1406.0154>
	{'abstract': u'We give a complete characterization of bipartite graphs having tree-like Galois lattices. We prove that the poset obtained by deleting bottom and top elements from the Galois lattice of a bipartite graph is tree-like if and only if the graph is a Bipartite Distance Hereditary graph. By relying on the interplay between bipartite distance hereditary graphs and series-parallel graphs, we show that the lattice can be realized as the containment relation among directed paths in an arborescence. Moreover, a compact encoding of Bipartite Distance Hereditary graphs is proposed, that allows optimal time computation of neighborhood intersections and maximal bicliques.',
	 'authors': u'Nicola Apollonio, Massimiliano Caramia, Paolo Giulio Franciosa,',
	 'category': u'Computer Science ',
	 'date': '2014-6-1',
	 'pdflink': u'http://arxiv.org/pdf/1406.0154',
	 'subjects': u'Discrete Mathematics (cs.DM)',
	 'title': u'\nOn the Galois Lattice of Bipartite Distance Hereditary Graphs',
	 'urllink': u'http://arxiv.org/abs/1406.0154'}
2015-03-23 20:09:33+0000 [xxu461000] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1407.5483> (referer: http://arxiv.org/list/cs/14?skip=8000&show=1000)
2015-03-23 20:09:33+0000 [xxu461000] DEBUG: Scraped from <200 http://arxiv.org/abs/1407.5483>
	{'abstract': u'In this letter we propose a new hybrid code called "RM-Polar" codes. This new codes are constructed by combining the construction of Reed-Muller (RM) code and Polar code. It has much larger minimum Hamming distance than Polar codes, therefore it has much better error performance than Polar codes.',
	 'authors': u'Bin Li, Hui Shen, David Tse,',
	 'category': u'Computer Science ',
	 'date': '2014-7-21',
	 'pdflink': u'http://arxiv.org/pdf/1407.5483',
	 'subjects': u'Information Theory (cs.IT)',
	 'title': u'\nA RM-Polar Codes',
	 'urllink': u'http://arxiv.org/abs/1407.5483'}
2015-03-23 20:09:37+0000 [xxu461000] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1404.5562> (referer: http://arxiv.org/list/cs/14?skip=4000&show=1000)
2015-03-23 20:09:37+0000 [xxu461000] DEBUG: Scraped from <200 http://arxiv.org/abs/1404.5562>
	{'abstract': u"Online social networks (OSNs) are changing the way in which the information spreads throughout the Internet. A deep understanding of the information spreading in OSNs leads to both social and commercial benefits. In this paper, we characterize the dynamic of information spreading (e.g., how fast and widely the information spreads against time) in OSNs by developing a general and accurate model based on the Interactive Markov Chains (IMCs) and mean-field theory. This model explicitly reveals the impacts of the network topology on information spreading in OSNs. Further, we extend our model to feature the time-varying user behaviors and the ever-changing information popularity. The complicated dynamic patterns of information spreading are captured by our model using six key parameters. Extensive tests based on Renren's dataset validate the accuracy of our model, which demonstrate that it can characterize the dynamic patterns of video sharing in Renren precisely and predict future spreading tendency successfully.",
	 'authors': u'Sai Zhang, Ke Xu, Xi Chen, Xue Liu,',
	 'category': u'Computer Science ',
	 'date': '2014-4-22',
	 'pdflink': u'http://arxiv.org/pdf/1404.5562',
	 'subjects': u'Social and Information Networks (cs.SI)',
	 'title': u'\nCharacterizing Information Spreading in Online Social Networks',
	 'urllink': u'http://arxiv.org/abs/1404.5562'}
2015-03-23 20:09:42+0000 [xxu461000] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1405.2501> (referer: http://arxiv.org/list/cs/14?skip=5000&show=1000)
2015-03-23 20:09:42+0000 [xxu461000] DEBUG: Scraped from <200 http://arxiv.org/abs/1405.2501>
	{'abstract': u'Tabling has been used for some time to improve efficiency of Prolog programs by memorizing answered queries. The same idea can be naturally used to memorize visited states during search for planning. In this paper we present a planner developed in the Picat language to solve the Petrobras planning problem. Picat is a novel Prolog-like language that provides pattern matching, deterministic and non-deterministic rules, and tabling as its core modelling and solving features. We demonstrate these capabilities using the Petrobras problem, where the goal is to plan transport of cargo items from ports to platforms using vessels with limited capacity. Monte Carlo Tree Search has been so far the best technique to tackle this problem and we will show that by using tabling we can achieve much better runtime efficiency and better plan quality.',
	 'authors': u'Roman Bart\xe1k, Neng-Fa Zhou,',
	 'category': u'Computer Science ',
	 'date': '2014-5-11',
	 'pdflink': u'http://arxiv.org/pdf/1405.2501',
	 'subjects': u'Artificial Intelligence (cs.AI)',
	 'title': u'\nUsing Tabled Logic Programming to Solve the Petrobras Planning Problem',
	 'urllink': u'http://arxiv.org/abs/1405.2501'}
2015-03-23 20:09:45+0000 [xxu461000] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1406.0143> (referer: http://arxiv.org/list/cs/14?skip=6000&show=1000)
2015-03-23 20:09:45+0000 [xxu461000] DEBUG: Scraped from <200 http://arxiv.org/abs/1406.0143>
	{'abstract': u"With the advancement of battery technology, energy harvesting communication systems attracted great research attention in recent years. However, energy harvesting communication systems with multiple transmitters and multiple receivers have not been considered yet. In this paper, the problem of broadcasting in a communication system with multiple energy harvesting transmitters and multiple receivers is studied. First, regarding the transmitters as a 'hole transmitter' [1], the optimal total transmission power is obtained and the optimal power allocation policy in [2] is extended to our system setup, with the aim of minimizing the transmission completion time. Then, a simpler power allocation policy is developed to allocate the optimal total transmission power to the data transmissions. As transmitter switching can provide flexibility and robustness to an energy harvesting communication system, especially when a transmitter is broken or the energy harvested by a transmitter is insufficient, a transmitter switching policy is further developed to choose a suitable transmitter to work whenever necessary. The results show that the proposed power allocation policy performs close to the optimal one and outperforms some heuristic ones in terms of transmission completion time. Besides, the proposed transmitter switching policy outperforms some heuristic ones in terms of number of switches.",
	 'authors': u'Fangfang Zhou, Hongbin Chen, Rong Yu, Lisheng Fan,',
	 'category': u'Computer Science ',
	 'date': '2014-6-1',
	 'pdflink': u'http://arxiv.org/pdf/1406.0143',
	 'subjects': u'Information Theory (cs.IT)',
	 'title': u'\nPower Allocation and Transmitter Switching for Broadcasting with  Multiple Energy Harvesting Transmitters',
	 'urllink': u'http://arxiv.org/abs/1406.0143'}
2015-03-23 20:09:48+0000 [xxu461000] INFO: Crawled 196 pages (at 13 pages/min), scraped 179 items (at 13 items/min)
2015-03-23 20:09:50+0000 [xxu461000] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1407.5456> (referer: http://arxiv.org/list/cs/14?skip=8000&show=1000)
2015-03-23 20:09:50+0000 [xxu461000] DEBUG: Scraped from <200 http://arxiv.org/abs/1407.5456>
	{'abstract': u'In this paper, we made the plan of a load testing, and got results by means of the LoadRunner which is an automatic load testing tool.We combined with the characteristics of electronic commerce system and did the load testing and analysis the result of load test by means of the LoadRunner. We fully described the characteristics of the electronic commerce application, designed the reasonable test cases,and simulated the practical scenario. In the process of running Load Runner, we arranged the appropriate transactions and rendezvous, and designed the truthful test network environment. The plan was applied to the load testing phase of the telecommunication equipment sales system of special products. We analyzed the load testing results, proposed the improving measures, and realized the optimization of the telecommunication equipment sales system and also found the defect of the system when the massive users access the system and guided the system improvement using the test result.',
	 'authors': u'Manju Kaushik,',
	 'category': u'Computer Science ',
	 'date': '2014-7-21',
	 'pdflink': u'http://arxiv.org/pdf/1407.5456',
	 'subjects': u'Software Engineering (cs.SE)',
	 'title': u'\nResearch of Load Testing and Result Based on Loadrunner',
	 'urllink': u'http://arxiv.org/abs/1407.5456'}
2015-03-23 20:09:56+0000 [xxu461000] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1404.5552> (referer: http://arxiv.org/list/cs/14?skip=4000&show=1000)
2015-03-23 20:09:56+0000 [xxu461000] DEBUG: Scraped from <200 http://arxiv.org/abs/1404.5552>
	{'abstract': u'We demonstrate algorithm-based fault tolerance for silent, transient data corruption in "black-box" preconditioners. We consider both additive Schwarz domain decomposition with an ILU(k) subdomain solver, and algebraic multigrid, both implemented in the Trilinos library. We evaluate faults that corrupt preconditioner results in both single and multiple MPI ranks. We then analyze how our approach behaves when then application is scaled. Our technique is based on a Selective Reliability approach that performs most operations in an unreliable mode, with only a few operations performed reliably. We also investigate two responses to faults and discuss the performance overheads imposed by each. For a non-symmetric problem solved using GMRES and ILU, we show that at scale our fault tolerance approach incurs only 22% overhead for the worst case. With detection techniques, we are able to reduce this overhead to 1.8% in the worst case.',
	 'authors': u'James Elliott, Mark Hoemmen, Frank Mueller,',
	 'category': u'Computer Science ',
	 'date': '2014-4-22',
	 'pdflink': u'http://arxiv.org/pdf/1404.5552',
	 'subjects': u'Distributed, Parallel, and Cluster Computing (cs.DC)',
	 'title': u'\nTolerating Silent Data Corruption in Opaque Preconditioners',
	 'urllink': u'http://arxiv.org/abs/1404.5552'}
2015-03-23 20:10:01+0000 [xxu461000] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1405.2496> (referer: http://arxiv.org/list/cs/14?skip=5000&show=1000)
2015-03-23 20:10:01+0000 [xxu461000] DEBUG: Scraped from <200 http://arxiv.org/abs/1405.2496>
	{'abstract': u"This paper proposes a strategy for the detection and triangulation of structural anomalies in solid media. The method revolves around the construction of sparse representations of the medium's dynamic response, obtained by learning instructive dictionaries which form a suitable basis for the response data. The resulting sparse coding problem is recast as a modified dictionary learning task with additional spatial sparsity constraints enforced on the atoms of the learned dictionaries, which provides them with a prescribed spatial topology that is designed to unveil anomalous regions in the physical domain. The proposed methodology is model agnostic, i.e., it forsakes the need for a physical model and requires virtually no a priori knowledge of the structure's material properties, as all the inferences are exclusively informed by the data through the layers of information that are available in the intrinsic salient structure of the material's dynamic response. This characteristic makes the approach powerful for anomaly identification in systems with unknown or heterogeneous property distribution, for which a model is unsuitable or unreliable. The method is validated using both synthetically",
	 'authors': u'Jeffrey M. Druce, Jarvis D. Haupt, Stefano Gonella,',
	 'category': u'Computer Science ',
	 'date': '2014-5-11',
	 'pdflink': u'http://arxiv.org/pdf/1405.2496',
	 'subjects': u'Computer Vision and Pattern Recognition (cs.CV)',
	 'title': u'\nAnomaly-Sensitive Dictionary Learning for Unsupervised Diagnostics of  Solid Media',
	 'urllink': u'http://arxiv.org/abs/1405.2496'}
2015-03-23 20:10:07+0000 [xxu461000] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1406.0140> (referer: http://arxiv.org/list/cs/14?skip=6000&show=1000)
2015-03-23 20:10:07+0000 [xxu461000] DEBUG: Scraped from <200 http://arxiv.org/abs/1406.0140>
	{'abstract': u"Given a random variable and a set of experts , we describe a method for finding a subset of experts whose aggregated opinion best predicts the outcome of . Therefore, the problem can be regarded as a team formation for performing a prediction task. We show that in case of aggregating experts' opinions by simple averaging, finding the best team (the team with the lowest total error during past turns) can be modeled with an integer quadratic programming and we prove its NP-hardness whereas its relaxation is solvable in polynomial time. Finally, we do an experimental comparison between different rounding and greedy heuristics and show that our suggested tabu search works effectively. Keywords: Team Selection, Information Aggregation, Opinion Pooling, Quadratic Programming, NP-Hard",
	 'authors': u'MohammadAmin Fazli, Azin Ghazimatin, Jafar Habibi, Hamid Haghshenas,',
	 'category': u'Computer Science ',
	 'date': '2014-6-1',
	 'pdflink': u'http://arxiv.org/pdf/1406.0140',
	 'subjects': u'Data Structures and Algorithms (cs.DS)',
	 'title': u'\nTeam Selection For Prediction Tasks',
	 'urllink': u'http://arxiv.org/abs/1406.0140'}
2015-03-23 20:10:12+0000 [xxu461000] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1407.5447> (referer: http://arxiv.org/list/cs/14?skip=8000&show=1000)
2015-03-23 20:10:12+0000 [xxu461000] DEBUG: Scraped from <200 http://arxiv.org/abs/1407.5447>
	{'abstract': u"This paper deals with the problem of efficient resource allocation in dynamic infrastructureless wireless networks. Assuming a reactive interference-limited scenario, each transmitter is allowed to select one frequency channel (from a common pool) together with a power level at each transmission trial; hence, for all transmitters, not only the fading gain, but also the number of interfering transmissions and their transmit powers are varying over time. Due to the absence of a central controller and time-varying network characteristics, it is highly inefficient for transmitters to acquire global channel and network knowledge. Therefore a reasonable assumption is that transmitters have no knowledge of fading gains, interference, and network topology. Each transmitting node selfishly aims at maximizing its average reward (or minimizing its average cost), which is a function of the action of that specific transmitter as well as those of all other transmitters. This scenario is modeled as a multi-player multi-armed adversarial bandit game, in which multiple players receive an a priori unknown reward with an arbitrarily time-varying distribution by sequentially pulling an arm, selected from a known and finite set of arms. Since players do not know the arm with the highest average reward in advance, they attempt to minimize their so-called regret, determined by the set of players' actions, while attempting to achieve equilibrium in some sense. To this end, we design in this paper two joint power level and channel selection strategies. We prove that the gap between the average reward achieved by our approaches and that based on the best fixed strategy converges to zero asymptotically. Moreover, the empirical joint frequencies of the game converge to the set of correlated equilibria. We further characterize this set for two special cases of our designed game.",
	 'authors': u'Setareh Maghsudi, Slawomir Stanczak,',
	 'category': u'Computer Science ',
	 'date': '2014-7-21',
	 'pdflink': u'http://arxiv.org/pdf/1407.5447',
	 'subjects': u'Computer Science and Game Theory (cs.GT)',
	 'title': u'\nJoint Channel Selection and Power Control in Infrastructureless Wireless  Networks: A Multi-Player Multi-Armed Bandit Framework',
	 'urllink': u'http://arxiv.org/abs/1407.5447'}
2015-03-23 20:10:17+0000 [xxu461000] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1404.5548> (referer: http://arxiv.org/list/cs/14?skip=4000&show=1000)
2015-03-23 20:10:17+0000 [xxu461000] DEBUG: Scraped from <200 http://arxiv.org/abs/1404.5548>
	{'abstract': u'In the Colored Bin Packing problem a sequence of items of sizes up to arrives to be packed into bins of unit capacity. Each item has one of colors and an additional constraint is that we cannot pack two items of the same color next to each other in the same bin. The objective is to minimize the number of bins. In the important special case when all items have size zero, we characterize the optimal value to be equal to color discrepancy. As our main result, we give an (asymptotically) 1.5-competitive algorithm which is optimal. In fact, the algorithm always uses at most bins and we show a matching lower bound of for any value of . In particular, the absolute ratio of our algorithm is and this is optimal. For items of unrestricted sizes we give an asymptotically -competitive algorithm. When the items have sizes at most for a real the asymptotic competitive ratio is . We also show that classical algorithms First Fit, Best Fit and Worst Fit are not constant competitive, which holds already for three colors and small items. In the case of two colors---the Black and White Bin Packing problem---we prove that all Any Fit algorithms have absolute competitive ratio . When the items have sizes at most for a real we show that the Worst Fit algorithm is absolutely -competitive.',
	 'authors': u'Martin B\xf6hm, Ji\u0159\xed Sgall, Pavel Vesel\xfd,',
	 'category': u'Computer Science ',
	 'date': '2014-4-22',
	 'pdflink': u'http://arxiv.org/pdf/1404.5548',
	 'subjects': u'Data Structures and Algorithms (cs.DS)',
	 'title': u'\nOnline Colored Bin Packing',
	 'urllink': u'http://arxiv.org/abs/1404.5548'}
2015-03-23 20:10:22+0000 [xxu461000] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1405.2494> (referer: http://arxiv.org/list/cs/14?skip=5000&show=1000)
2015-03-23 20:10:22+0000 [xxu461000] DEBUG: Scraped from <200 http://arxiv.org/abs/1405.2494>
	{'abstract': u'We study the framework of abductive logic programming extended with integrity constraints. For this framework, we introduce a new measure of the simplicity of an explanation based on its degree of emph: the more arbitrary the explanation, the less appealing it is, with explanations having no arbitrariness - they are called constrained - being the preferred ones. In the paper, we study basic properties of constrained explanations. For the case when programs in abductive theories are stratified we establish results providing a detailed picture of the complexity of the problem to decide whether constrained explanations exist. (To appear in Theory and Practice of Logic Programming (TPLP).)',
	 'authors': u'Luciano Caroprese, Irina Trubitsyna, Miroslaw Truszczynski, Ester Zumpano,',
	 'category': u'Computer Science ',
	 'date': '2014-5-11',
	 'pdflink': u'http://arxiv.org/pdf/1405.2494',
	 'subjects': u'Logic in Computer Science (cs.LO)',
	 'title': u'\nA Measure of Arbitrariness in Abductive Explanations',
	 'urllink': u'http://arxiv.org/abs/1405.2494'}
2015-03-23 20:10:27+0000 [xxu461000] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1406.0132> (referer: http://arxiv.org/list/cs/14?skip=6000&show=1000)
2015-03-23 20:10:27+0000 [xxu461000] DEBUG: Scraped from <200 http://arxiv.org/abs/1406.0132>
	{'abstract': u'In the Bag-of-Words (BoW) model based image retrieval task, the precision of visual matching plays a critical role in improving retrieval performance. Conventionally, local cues of a keypoint are employed. However, such strategy does not consider the contextual evidences of a keypoint, a problem which would lead to the prevalence of false matches. To address this problem, this paper defines "true match" as a pair of keypoints which are similar on three levels, i.e., local, regional, and global. Then, a principled probabilistic framework is established, which is capable of implicitly integrating discriminative cues from all these feature levels. Specifically, the Convolutional Neural Network (CNN) is employed to extract features from regional and global patches, leading to the so-called "Deep Embedding" framework. CNN has been shown to produce excellent performance on a dozen computer vision tasks such as image classification and detection, but few works have been done on BoW based image retrieval. In this paper, firstly we show that proper pre-processing techniques are necessary for effective usage of CNN feature. Then, in the attempt to fit it into our model, a novel indexing structure called "Deep Indexing" is introduced, which dramatically reduces memory usage. Extensive experiments on three benchmark datasets demonstrate that, the proposed Deep Embedding method greatly promotes the retrieval accuracy when CNN feature is integrated. We show that our method is efficient in terms of both memory and time cost, and compares favorably with the state-of-the-art methods.',
	 'authors': u'Liang Zheng, Shengjin Wang, Fei He, Qi Tian,',
	 'category': u'Computer Science ',
	 'date': '2014-6-1',
	 'pdflink': u'http://arxiv.org/pdf/1406.0132',
	 'subjects': u'Computer Vision and Pattern Recognition (cs.CV)',
	 'title': u'\nSeeing the Big Picture: Deep Embedding with Contextual Evidences',
	 'urllink': u'http://arxiv.org/abs/1406.0132'}
2015-03-23 20:10:33+0000 [xxu461000] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1407.5444> (referer: http://arxiv.org/list/cs/14?skip=8000&show=1000)
2015-03-23 20:10:33+0000 [xxu461000] DEBUG: Scraped from <200 http://arxiv.org/abs/1407.5444>
	{'abstract': u'Security protocols are used in many of our daily-life applications, and our privacy largely depends on their design. Formal verification techniques have proved their usefulness to analyse these protocols, but they become so complex that modular techniques have to be developed. We propose several results to safely compose security protocols. We consider arbitrary primitives modeled using an equational theory, and a rich process algebra close to the applied pi calculus. Relying on these composition results, we are able to derive some security properties on a protocol from the security analysis performed on each sub-protocol individually. We consider parallel composition and the case of key-exchange protocols. Our results apply to deal with confidentiality but also privacy-type properties (e.g. anonymity, unlinkability) expressed using a notion of equivalence. We illustrate the usefulness of our composition results on protocols from the 3G phone application and electronic passport.',
	 'authors': u'Myrto Arapinis, Vincent Cheval, St\xe9phanie Delaune,',
	 'category': u'Computer Science ',
	 'date': '2014-7-21',
	 'pdflink': u'http://arxiv.org/pdf/1407.5444',
	 'subjects': u'Cryptography and Security (cs.CR)',
	 'title': u'\nComposing security protocols: from confidentiality to privacy',
	 'urllink': u'http://arxiv.org/abs/1407.5444'}
2015-03-23 20:10:36+0000 [xxu461000] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1404.5545> (referer: http://arxiv.org/list/cs/14?skip=4000&show=1000)
2015-03-23 20:10:36+0000 [xxu461000] DEBUG: Scraped from <200 http://arxiv.org/abs/1404.5545>
	{'abstract': u"We consider the problem of -testing of class of bounded derivative properties over hypergrid domain with points distributed according to some product distribution. This class includes monotonicity, the Lipschitz property, -generalized Lipschitz and many more properties. Previous results for testing on for this class were known for monotonicity and -Lipschitz properties over uniformly distributed domains. medskip Our results imply testers that give the same upper bound for arbitrary product distributions as the hitherto known testers, which use uniformly randomly chosen samples from , for monotonicity and Lipschitz testing. Also, our testers are emph for a large class of bounded derivative properties, that includes -generalized Lipschitz property, over uniform distributions. Infact, each edge in is allowed to have it's own left and right Lipschitz constants. The time complexity is emph for arbitrary product distributions.",
	 'authors': u'Kashyap Dixit,',
	 'category': u'Computer Science ',
	 'date': '2014-4-22',
	 'pdflink': u'http://arxiv.org/pdf/1404.5545',
	 'subjects': u'Data Structures and Algorithms (cs.DS)',
	 'title': u'\n$L_p$-Testers for Bounded Derivative Properties on Product Distributions',
	 'urllink': u'http://arxiv.org/abs/1404.5545'}
2015-03-23 20:10:41+0000 [xxu461000] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1405.2492> (referer: http://arxiv.org/list/cs/14?skip=5000&show=1000)
2015-03-23 20:10:41+0000 [xxu461000] DEBUG: Scraped from <200 http://arxiv.org/abs/1405.2492>
	{'abstract': u'This paper addresses the cell association problem in the downlink of a multi-tier heterogeneous network (HetNet), where base stations (BSs) have finite number of resource blocks (RBs) available to distribute among their associated users. Two problems are defined and treated in this paper: sum utility of long term rate maximization with long term rate quality of service (QoS) constraints, and global outage probability minimization with outage QoS constraints. The first problem is well-suited for low mobility environments, while the second problem provides a framework to deal with environments with fast fading. The defined optimization problems in this paper are solved in two phases: cell association phase followed by the optional RB distribution phase. We show that the cell association phase of both problems have the same structure. Based on this similarity, we propose a unified distributed algorithm with low levels of message passing to for the cell association phase. This distributed algorithm is derived by relaxing the association constraints and using Lagrange dual decomposition method. In the RB distribution phase, the remaining RBs after the cell association phase are distributed among the users. Simulation results show the superiority of our distributed cell association scheme compared to schemes that are based on maximum signal to interference plus noise ratio (SINR).',
	 'authors': u'Hamidreza Boostanimehr, Vijay K. Bhargava,',
	 'category': u'Computer Science ',
	 'date': '2014-5-11',
	 'pdflink': u'http://arxiv.org/pdf/1405.2492',
	 'subjects': u'Networking and Internet Architecture (cs.NI)',
	 'title': u'\nUnified and Distributed QoS-Driven Cell Association Algorithms in  Heterogeneous Networks',
	 'urllink': u'http://arxiv.org/abs/1405.2492'}
2015-03-23 20:10:46+0000 [xxu461000] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1406.0124> (referer: http://arxiv.org/list/cs/14?skip=6000&show=1000)
2015-03-23 20:10:46+0000 [xxu461000] DEBUG: Scraped from <200 http://arxiv.org/abs/1406.0124>
	{'abstract': u'Plug-and-play information technology (IT) infrastructure has been expanding very rapidly in recent years. With the advent of cloud computing, many ecosystem and business paradigms are encountering potential changes and may be able to eliminate their IT infrastructure maintenance processes. Real-time performance and high availability requirements have induced telecom networks to adopt the new concepts of the cloud model: software-defined networking (SDN) and network function virtualization (NFV). NFV introduces and deploys new network functions in an open and standardized IT environment, while SDN aims to transform the way networks function. SDN and NFV are complementary technologies; they do not depend on each other. However, both concepts can be merged and have the potential to mitigate the challenges of legacy networks. In this paper, our aim is to describe the benefits of using SDN in a multitude of environments such as in data centers, data center networks, and Network as Service offerings. We also present the various challenges facing SDN, from scalability to reliability and security concerns, and discuss existing solutions to these challenges.',
	 'authors': u'Manar Jammal, Taranpreet Singh, Abdallah Shami, Rasool Asal, Yiming Li,',
	 'category': u'Computer Science ',
	 'date': '2014-6-1',
	 'pdflink': u'http://arxiv.org/pdf/1406.0124',
	 'subjects': u'Networking and Internet Architecture (cs.NI)',
	 'title': u'\nSoftware-Defined Networking: State of the Art and Research Challenges',
	 'urllink': u'http://arxiv.org/abs/1406.0124'}
2015-03-23 20:10:48+0000 [xxu461000] INFO: Crawled 208 pages (at 12 pages/min), scraped 191 items (at 12 items/min)
2015-03-23 20:10:51+0000 [xxu461000] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1407.5442> (referer: http://arxiv.org/list/cs/14?skip=8000&show=1000)
2015-03-23 20:10:51+0000 [xxu461000] DEBUG: Scraped from <200 http://arxiv.org/abs/1407.5442>
	{'abstract': u'The problem of finding the most critical nodes, referred to as the problem, is a very important one in several contexts such as information diffusion and preference aggregation in social networks, clustering of data points, etc. It has been observed in the literature that the value allotted to a node by most of the popular cooperative game theoretic solution concepts, acts as a good measure of appropriateness of that node (or a data point) to be included in the set, by itself. However, in general, nodes having the highest values are not the desirable nodes, because the appropriateness of a node to be a part of the set depends on other nodes in the set. As this is not explicitly captured by cooperative game theoretic solution concepts, it is necessary to post-process the obtained values in order to output the suitable nodes. In this paper, we propose several such post-processing methods and give reasoning behind each of them, and also propose a standalone algorithm that combines cooperative game theoretic solution concepts with the popular greedy hill-climbing algorithm.',
	 'authors': u'Swapnil Dhamal, Akanksha Meghlan, Y. Narahari,',
	 'category': u'Computer Science ',
	 'date': '2014-7-21',
	 'pdflink': u'http://arxiv.org/pdf/1407.5442',
	 'subjects': u'Social and Information Networks (cs.SI)',
	 'title': u'\nCooperative Game Theoretic Solution Concepts for top-$k$ Problems',
	 'urllink': u'http://arxiv.org/abs/1407.5442'}
2015-03-23 20:10:56+0000 [xxu461000] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1404.5539> (referer: http://arxiv.org/list/cs/14?skip=4000&show=1000)
2015-03-23 20:10:56+0000 [xxu461000] DEBUG: Scraped from <200 http://arxiv.org/abs/1404.5539>
	{'abstract': u"In the restructured electricity industry, electricity pooling markets are an oligopoly with strategic producers possessing private information (private production cost function). We focus on pooling markets where aggregate demand is represented by a non-strategic agent. Inelasticity of demand is a main difficulty in electricity markets which can potentially result in market failure and high prices. We consider demand to be inelastic. We propose a market mechanism that has the following features. (F1) It is individually rational. (F2) It is budget balanced. (F3) It is price efficient, that is, at equilibrium the price of electricity is equal to the marginal cost of production. (F4) The energy production profile corresponding to every non-zero Nash equilibrium of the game induced by the mechanism is a solution of the corresponding centralized problem where the objective is the maximization of the sum of the producers' and consumers' utilities. We identify some open problems associated with our approach to electricity pooling markets.",
	 'authors': u'Mohammad Rasouli, Demosthenis Teneketzis,',
	 'category': u'Computer Science ',
	 'date': '2014-4-21',
	 'pdflink': u'http://arxiv.org/pdf/1404.5539',
	 'subjects': u'Computer Science and Game Theory (cs.GT)',
	 'title': u'\nElectricity Pooling Markets with Strategic Producers Possessing  Asymmetric Information II: Inelastic Demand',
	 'urllink': u'http://arxiv.org/abs/1404.5539'}
2015-03-23 20:11:03+0000 [xxu461000] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1405.2489> (referer: http://arxiv.org/list/cs/14?skip=5000&show=1000)
2015-03-23 20:11:03+0000 [xxu461000] DEBUG: Scraped from <200 http://arxiv.org/abs/1405.2489>
	{'abstract': u'The ubiquity of technology in our daily lives and the economic stability of the technology sector in recent years, especially in areas with a computer science footing, has led to an increase in computer science enrollment in many parts of the world. To keep up with this trend, the undergraduate computer science curriculum has undergone many revisions, analysis, and discussion. Unfortunately, the graduate level curriculum is lagging far behind in computer science education literature and research. To remedy this, we present the blueprint and execution of a graduate level course in programming, designed specifically to cater to the needs of graduate students with a diverse background both in CS and other fields. To this end, the course is divided into two halves. In the first half, students are introduced to different programming concepts, such as multi-paradigm programming, data structures, concurrency, and security to bring them up to speed and provide a level playing field. In the second half, all of these concepts are employed as building blocks to solve real-world problems from data mining, natural language processing, computer vision, and other fields. In addition, the paper also discusses in detail the evaluation instruments employed for the course. Moreover, we also share anecdotal information around student feedback, course design, and grading that may be useful for others who wish to replicate our curriculum or sketch a similar course.',
	 'authors': u'Zubair Nabi,',
	 'category': u'Computer Science ',
	 'date': '2014-5-11',
	 'pdflink': u'http://arxiv.org/pdf/1405.2489',
	 'subjects': u'Computers and Society (cs.CY)',
	 'title': u'\nThe Zen of Graduate-level Programming',
	 'urllink': u'http://arxiv.org/abs/1405.2489'}
2015-03-23 20:11:08+0000 [xxu461000] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1406.0117> (referer: http://arxiv.org/list/cs/14?skip=6000&show=1000)
2015-03-23 20:11:08+0000 [xxu461000] DEBUG: Scraped from <200 http://arxiv.org/abs/1406.0117>
	{'abstract': u'Making energy consumption data accessible to software developers is an essential step towards energy efficient software engineering. The presence of various different, bespoke and incompatible, methods of instrumentation to obtain energy readings is currently limiting the widespread use of energy data in software development. This paper presents EACOF, a modular Energy-Aware Computing Framework that provides a layer of abstraction between sources of energy data and the applications that exploit them. EACOF replaces platform specific instrumentation through two APIs - one accepts input to the framework while the other provides access to application software. This allows developers to profile their code for energy consumption in an easy and portable manner using simple API calls. We outline the design of our framework and provide details of the API functionality. In a use case, where we investigate the impact of data bit width on the energy consumption of various sorting algorithms, we demonstrate that the data obtained using EACOF provides interesting, sometimes counter-intuitive, insights. All the code is available online under an open source license. this http URL',
	 'authors': u'Hayden Field, Glen Anderson, Kerstin Eder,',
	 'category': u'Computer Science ',
	 'date': '2014-5-31',
	 'pdflink': u'http://arxiv.org/pdf/1406.0117',
	 'subjects': u'Software Engineering (cs.SE)',
	 'title': u'\nEACOF: A Framework for Providing Energy Transparency to enable  Energy-Aware Software Development',
	 'urllink': u'http://arxiv.org/abs/1406.0117'}
2015-03-23 20:11:14+0000 [xxu461000] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1407.5425> (referer: http://arxiv.org/list/cs/14?skip=8000&show=1000)
2015-03-23 20:11:14+0000 [xxu461000] DEBUG: Scraped from <200 http://arxiv.org/abs/1407.5425>
	{'abstract': u'Information-theoretic methods have proven to be a very powerful tool in communication complexity, in particular giving an elegant proof of the linear lower bound for the two-party disjointness function, and tight lower bounds on disjointness in the multi-party number-in-the-hand (NIH) model. In this paper, we study the applicability of information theoretic methods to the multi-party number-on-the-forehead model (NOF), where determining the complexity of disjointness remains an important open problem. There are two basic parts to the NIH disjointness lower bound: a direct sum theorem and a lower bound on the one-bit AND function using a beautiful connection between Hellinger distance and protocols revealed by Bar-Yossef, Jayram, Kumar and Sivakumar [BYJKS04]. Inspired by this connection, we introduce the notion of Hellinger volume. We show that it lower bounds the information cost of multi-party NOF protocols and provide a small toolbox that allows one to manipulate several Hellinger volume terms and lower bound a Hellinger volume when the distributions involved satisfy certain conditions. In doing so, we prove a new upper bound on the difference between the arithmetic mean and the geometric mean in terms of relative entropy. We then apply these new tools to obtain a lower bound on the informational complexity of the AND_k function in the NOF setting. Finally, we discuss the difficulties of proving a direct sum theorem for information cost in the NOF model.',
	 'authors': u'Troy Lee, Nikos Leonardos, Michael Saks, Fengming Wang,',
	 'category': u'Computer Science ',
	 'date': '2014-7-21',
	 'pdflink': u'http://arxiv.org/pdf/1407.5425',
	 'subjects': u'Computational Complexity (cs.CC)',
	 'title': u'\nHellinger volume and number-on-the-forehead communication complexity',
	 'urllink': u'http://arxiv.org/abs/1407.5425'}
2015-03-23 20:11:18+0000 [xxu461000] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1404.5538> (referer: http://arxiv.org/list/cs/14?skip=4000&show=1000)
2015-03-23 20:11:18+0000 [xxu461000] DEBUG: Scraped from <200 http://arxiv.org/abs/1404.5538>
	{'abstract': u'In this paper, we consider a two-hop molecular communication network consisting of one nanotransmitter, one nanoreceiver, and one nanotransceiver acting as a relay. We consider two different schemes for relaying to improve the range of diffusion-based molecular communication. In the first scheme, two different types of messenger molecules are utilized at the relay node for transmission and detection. In the second scheme, we assume that there is only one type of molecule available to be used as an information carrier. We identify self-interference as the performance-limiting effect for the second relaying scheme. Self-interference occurs when the relay must detect the same type of molecule that it also emits. Furthermore, we consider two relaying modes analogous to those used in wireless communication systems, i.e., full-duplex and half-duplex. In particular, while our main focus is on full-duplex relaying, half-duplex relaying is employed as a means to mitigate self-interference. In addition, we propose the adaptation of the decision threshold as an effective mechanism to mitigate self-interference at the relay for full-duplex transmission. We derive closed-form expressions for the expected error probability of the network for both considered relaying schemes.',
	 'authors': u'Arman Ahmadzadeh, Adam Noel, Robert Schober,',
	 'category': u'Computer Science ',
	 'date': '2014-4-22',
	 'pdflink': u'http://arxiv.org/pdf/1404.5538',
	 'subjects': u'Information Theory (cs.IT)',
	 'title': u'\nAnalysis and Design of Two-Hop Diffusion-Based Molecular Communication  Networks',
	 'urllink': u'http://arxiv.org/abs/1404.5538'}
2015-03-23 20:11:24+0000 [xxu461000] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1405.2484> (referer: http://arxiv.org/list/cs/14?skip=5000&show=1000)
2015-03-23 20:11:24+0000 [xxu461000] DEBUG: Scraped from <200 http://arxiv.org/abs/1405.2484>
	{'abstract': u"Sponsored search auctions constitute one of the most successful applications of microeconomic mechanisms. In mechanism design, auctions are usually designed to incentivize advertisers to bid their truthful valuations and to assure both the advertisers and the auctioneer a non-negative utility. Nonetheless, in sponsored search auctions, the click-through-rates (CTRs) of the advertisers are often unknown to the auctioneer and thus standard truthful mechanisms cannot be directly applied and must be paired with an effective learning algorithm for the estimation of the CTRs. This introduces the critical problem of designing a learning mechanism able to estimate the CTRs at the same time as implementing a truthful mechanism with a revenue loss as small as possible compared to an optimal mechanism designed with the true CTRs. Previous work showed that, when dominant-strategy truthfulness is adopted, in single-slot auctions the problem can be solved using suitable exploration-exploitation mechanisms able to achieve a per-step regret (over the auctioneer's revenue) of order (where T is the number of times the auction is repeated). It is also known that, when truthfulness in expectation is adopted, a per-step regret (over the social welfare) of order can be obtained. In this paper we extend the results known in the literature to the case of multi-slot auctions. In this case, a model of the user is needed to characterize how the advertisers' valuations change over the slots. We adopt the cascade model that is the most famous model in the literature for sponsored search auctions. We prove a number of novel upper bounds and lower bounds both on the auctioneer's revenue loss and social welfare w.r.t. to the VCG auction and we report numerical simulations investigating the accuracy of the bounds in predicting the dependency of the regret on the auction parameters.",
	 'authors': u'Nicola Gatti, Alessandro Lazaric, Marco Rocco, Francesco Trov\xf2,',
	 'category': u'Computer Science ',
	 'date': '2014-5-11',
	 'pdflink': u'http://arxiv.org/pdf/1405.2484',
	 'subjects': u'Computer Science and Game Theory (cs.GT)',
	 'title': u'\nTruthful Learning Mechanisms for Multi-Slot Sponsored Search Auctions  with Externalities',
	 'urllink': u'http://arxiv.org/abs/1405.2484'}
2015-03-23 20:11:27+0000 [xxu461000] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1406.0090> (referer: http://arxiv.org/list/cs/14?skip=6000&show=1000)
2015-03-23 20:11:27+0000 [xxu461000] DEBUG: Scraped from <200 http://arxiv.org/abs/1406.0090>
	{'abstract': u"Cryptography is the science of encrypting the information so that it is rendered unreadable for an intruder. Cryptographic techniques are of utmost importance in today's world as the information to be sent might be of invaluable importance to both the sender and the receiver. Various cryptographic techniques ensure that even if an intruder intercepts the sent information, he is not able to decipher it thus render ending it useless for the intruder. Cryptography can be grouped into two types, that is Symmetric key cryptography and Asymmetric key cryptography. Symmetric key cryptography uses the same key for encryption as well as decryption thus making it faster compared to Asymmetric Key cryptography which uses different keys for encryption and decryption. Generation of dynamic keys for Symmetric key cryptography is an interesting field and in this we have tapped this field so as to generate dynamic keys for symmetric key cryptography. In this work, we have devised an algorithm for generating dynamic keys for sending messages over a communication channel and also solving key refreshment problem.",
	 'authors': u'Arjun Puri, Sudesh Kumar,',
	 'category': u'Computer Science ',
	 'date': '2014-5-31',
	 'pdflink': u'http://arxiv.org/pdf/1406.0090',
	 'subjects': u'Cryptography and Security (cs.CR)',
	 'title': u'\nError Control Codes: A Novel Solution for Secret Key Generation and Key  Refreshment Problem',
	 'urllink': u'http://arxiv.org/abs/1406.0090'}
2015-03-23 20:11:32+0000 [xxu461000] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1407.5416> (referer: http://arxiv.org/list/cs/14?skip=8000&show=1000)
2015-03-23 20:11:32+0000 [xxu461000] DEBUG: Scraped from <200 http://arxiv.org/abs/1407.5416>
	{'abstract': u'State-of-the-art important passage retrieval methods obtain very good results, but do not take into account privacy issues. In this paper, we present a privacy preserving method that relies on creating secure representations of documents. Our approach allows for third parties to retrieve important passages from documents without learning anything regarding their content. We use a hashing scheme known as Secure Binary Embeddings to convert a key phrase and bag-of-words representation to bit strings in a way that allows the computation of approximate distances, instead of exact ones. Experiments show that our secure system yield similar results to its non-private counterpart on both clean text and noisy speech recognized text.',
	 'authors': u'Luis Marujo, Jos\xe9 Port\xealo, David Martins de Matos, Jo\xe3o P. Neto, Anatole Gershman, Jaime Carbonell, Isabel Trancoso, Bhiksha Raj,',
	 'category': u'Computer Science ',
	 'date': '2014-7-21',
	 'pdflink': u'http://arxiv.org/pdf/1407.5416',
	 'subjects': u'Information Retrieval (cs.IR)',
	 'title': u'\nPrivacy-Preserving Important Passage Retrieval',
	 'urllink': u'http://arxiv.org/abs/1407.5416'}
2015-03-23 20:11:36+0000 [xxu461000] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1404.5528> (referer: http://arxiv.org/list/cs/14?skip=4000&show=1000)
2015-03-23 20:11:36+0000 [xxu461000] DEBUG: Scraped from <200 http://arxiv.org/abs/1404.5528>
	{'abstract': u'In this paper with the aid of genetic algorithm and fuzzy theory, we present a hybrid job scheduling approach, which considers the load balancing of the system and reduces total execution time and execution cost. We try to modify the standard Genetic algorithm and to reduce the iteration of creating population with the aid of fuzzy theory. The main goal of this research is to assign the jobs to the resources with considering the VM MIPS and length of jobs. The new algorithm assigns the jobs to the resources with considering the job length and resources capacities. We evaluate the performance of our approach with some famous cloud scheduling models. The results of the experiments show the efficiency of the proposed approach in term of execution time, execution cost and average Degree of Imbalance (DI).',
	 'authors': u'Saeed Javanmardi, Mohammad Shojafar, Danilo Amendola, Nicola Cordeschi, Hongbo Liu, Ajith Abraham,',
	 'category': u'Computer Science ',
	 'date': '2014-4-22',
	 'pdflink': u'http://arxiv.org/pdf/1404.5528',
	 'subjects': u'Distributed, Parallel, and Cluster Computing (cs.DC)',
	 'title': u'\nHybrid Genetic Algorithm for Cloud Computing Applications',
	 'urllink': u'http://arxiv.org/abs/1404.5528'}
2015-03-23 20:11:43+0000 [xxu461000] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1405.2476> (referer: http://arxiv.org/list/cs/14?skip=5000&show=1000)
2015-03-23 20:11:43+0000 [xxu461000] DEBUG: Scraped from <200 http://arxiv.org/abs/1405.2476>
	{'abstract': u'We prove the existence of a canonical form for semi-deterministic transducers with incomparable sets of output strings. Based on this, we develop an algorithm which learns semi-deterministic transducers given access to translation queries. We also prove that there is no learning algorithm for semi-deterministic transducers that uses only domain knowledge.',
	 'authors': u'Achilles Beros, Colin de la Higuera,',
	 'category': u'Computer Science ',
	 'date': '2014-5-10',
	 'pdflink': u'http://arxiv.org/pdf/1405.2476',
	 'subjects': u'Learning (cs.LG)',
	 'title': u'\nA Canonical Semi-Deterministic Transducer',
	 'urllink': u'http://arxiv.org/abs/1405.2476'}
2015-03-23 20:11:47+0000 [xxu461000] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1406.0089> (referer: http://arxiv.org/list/cs/14?skip=6000&show=1000)
2015-03-23 20:11:47+0000 [xxu461000] DEBUG: Scraped from <200 http://arxiv.org/abs/1406.0089>
	{'abstract': u'The forest-of-octrees approach to parallel adaptive mesh refinement and coarsening (AMR) has recently been demonstrated in the context of a number of large-scale PDE-based applications. Although linear octrees, which store only leaf octants, have an underlying tree structure by definition, it is not often exploited in previously published mesh-related algorithms. This is because the branches are not explicitly stored, and because the topological relationships in meshes, such as the adjacency between cells, introduce dependencies that do not respect the octree hierarchy. In this work we combine hierarchical and topological relationships between octree branches to design efficient recursive algorithms. We present three important algorithms with recursive implementations. The first is a parallel search for leaves matching any of a set of multiple search criteria. The second is a ghost layer construction algorithm that handles arbitrarily refined octrees that are not covered by previous algorithms, which require a 2:1 condition between neighboring leaves. The third is a universal mesh topology iterator. This iterator visits every cell in a domain partition, as well as every interface (face, edge and corner) between these cells. The iterator calculates the local topological information for every interface that it visits, taking into account the nonconforming interfaces that increase the complexity of describing the local topology. To demonstrate the utility of the topology iterator, we use it to compute the numbering and encoding of higher-order nodal basis functions. We analyze the complexity of the new recursive algorithms theoretically, and assess their performance, both in terms of single-processor efficiency and in terms of parallel scalability, demonstrating good weak and strong scaling up to 458k cores of the JUQUEEN supercomputer.',
	 'authors': u'Tobin Isaac, Carsten Burstedde, Lucas C. Wilcox, Omar Ghattas,',
	 'category': u'Computer Science ',
	 'date': '2014-5-31',
	 'pdflink': u'http://arxiv.org/pdf/1406.0089',
	 'subjects': u'Distributed, Parallel, and Cluster Computing (cs.DC)',
	 'title': u'\nRecursive Algorithms for Distributed Forests of Octrees',
	 'urllink': u'http://arxiv.org/abs/1406.0089'}
2015-03-23 20:11:48+0000 [xxu461000] INFO: Crawled 220 pages (at 12 pages/min), scraped 203 items (at 12 items/min)
2015-03-23 20:11:53+0000 [xxu461000] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1407.5410> (referer: http://arxiv.org/list/cs/14?skip=8000&show=1000)
2015-03-23 20:11:53+0000 [xxu461000] DEBUG: Scraped from <200 http://arxiv.org/abs/1407.5410>
	{'abstract': u"With millions of apps that can be downloaded from official or third-party market, Android has become one of the most popular mobile platforms today. These apps help people in all kinds of ways and thus have access to lots of user's data that in general fall into three categories: sensitive data, data to be shared with other apps, and non-sensitive data not to be shared with others. For the first and second type of data, Android has provided very good storage models: an app's private sensitive data are saved to its private folder that can only be access by the app itself, and the data to be shared are saved to public storage (either the external SD card or the emulated SD card area on internal FLASH memory). But for the last type, i.e., an app's non-sensitive and non-shared data, there is a big problem in Android's current storage model which essentially encourages an app to save its non-sensitive data to shared public storage that can be accessed by other apps. At first glance, it seems no problem to do so, as those data are non-sensitive after all, but it implicitly assumes that app developers could correctly identify all sensitive data and prevent all possible information leakage from private-but-non-sensitive data. In this paper, we will demonstrate that this is an invalid assumption with a thorough survey on information leaks of those apps that had followed Android's recommended storage model for non-sensitive data. Our studies showed that highly sensitive information from billions of users can be easily hacked by exploiting the mentioned problematic storage model. Although our empirical studies are based on a limited set of apps, the identified problems are never isolated or accidental bugs of those apps being investigated. On the contrary, the problem is rooted from the vulnerable storage model recommended by Android. To mitigate the threat, we also propose a defense framework.",
	 'authors': u'Xiangyu Liu, Zhe Zhou, Wenrui Diao, Zhou Li, Kehuan Zhang,',
	 'category': u'Computer Science ',
	 'date': '2014-7-21',
	 'pdflink': u'http://arxiv.org/pdf/1407.5410',
	 'subjects': u'Cryptography and Security (cs.CR)',
	 'title': u'\nAn Empirical Study on Android for Saving Non-shared Data on Public  Storage',
	 'urllink': u'http://arxiv.org/abs/1407.5410'}
2015-03-23 20:11:58+0000 [xxu461000] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1404.5525> (referer: http://arxiv.org/list/cs/14?skip=4000&show=1000)
2015-03-23 20:11:58+0000 [xxu461000] DEBUG: Scraped from <200 http://arxiv.org/abs/1404.5525>
	{'abstract': u'In this paper, we study iterative methods on the coefficients of the rational univariate representation (RUR) of a given algebraic set, called global Newton iteration. We compare two natural approaches to define locally quadratically convergent iterations: the first one involves Newton iteration applied to the approximate roots individually and then interpolation to find the RUR of these approximate roots; the second one considers the coefficients in the exact RUR as zeroes of a high dimensional map defined by polynomial reduction, and applies Newton iteration on this map. We prove that over fields with a p-adic valuation these two approaches give the same iteration function, but over fields equipped with the usual Archimedean absolute value, they are not equivalent. In the latter case, we give explicitly the iteration function for both approaches. Finally, we analyze the parallel complexity of the different versions of the global Newton iteration, compare them, and demonstrate that they can be efficiently computed. The motivation for this study comes from the certification of approximate roots of overdetermined and singular polynomial systems via the recovery of an exact RUR from approximate numerical data.',
	 'authors': u'Jonathan D. Hauenstein, Victor Pan, Agnes Szanto,',
	 'category': u'Computer Science ',
	 'date': '2014-4-17',
	 'pdflink': u'http://arxiv.org/pdf/1404.5525',
	 'subjects': u'Numerical Analysis (cs.NA)',
	 'title': u'\nGlobal Newton Iteration over Archimedean and non-Archimedean Fields',
	 'urllink': u'http://arxiv.org/abs/1404.5525'}
2015-03-23 20:12:05+0000 [xxu461000] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1405.2475> (referer: http://arxiv.org/list/cs/14?skip=5000&show=1000)
2015-03-23 20:12:05+0000 [xxu461000] DEBUG: Scraped from <200 http://arxiv.org/abs/1405.2475>
	{'abstract': u'In many radar scenarios, the radar target or the medium is assumed to possess randomly varying parts. The properties of a target are described by a random process known as the spreading function. Its second order statistics under the WSSUS assumption are given by the scattering function. Recent developments in operator sampling theory suggest novel channel sounding procedures that allow for the determination of the spreading function given complete statistical knowledge of the operator echo from a single sounding by a weighted pulse train. We construct and analyze a novel estimator for the scattering function based on these findings. Our results apply whenever the scattering function is supported on a compact subset of the time-frequency plane. We do not make any restrictions either on the geometry of this support set, or on its area. Our estimator can be seen as a generalization of an averaged periodogram estimator for the case of a non-rectangular geometry of the support set of the scattering function.',
	 'authors': u'G\xf6tz E. Pfander, Pavel Zheltov,',
	 'category': u'Computer Science ',
	 'date': '2014-5-10',
	 'pdflink': u'http://arxiv.org/pdf/1405.2475',
	 'subjects': u'Information Theory (cs.IT)',
	 'title': u'\nEstimation of Overspread Scattering Functions',
	 'urllink': u'http://arxiv.org/abs/1405.2475'}
2015-03-23 20:12:09+0000 [xxu461000] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1406.0086> (referer: http://arxiv.org/list/cs/14?skip=6000&show=1000)
2015-03-23 20:12:09+0000 [xxu461000] DEBUG: Scraped from <200 http://arxiv.org/abs/1406.0086>
	{'abstract': u'We study joint source-channel coding (JSCC) of compressed sensing (CS) measurements using vector quantizer (VQ). We develop a framework for realizing optimum JSCC schemes that enable encoding and transmitting CS measurements of a sparse source over discrete memoryless channels, and decoding the sparse source signal. For this purpose, the optimal design of encoder-decoder pair of a VQ is considered, where the optimality is addressed by minimizing end-to-end mean square error (MSE). We derive a theoretical lower-bound on the MSE performance, and propose a practical encoder-decoder design through an iterative algorithm. The resulting coding scheme is referred to as channel- optimized VQ for CS, coined COVQ-CS. In order to address the encoding complexity issue of the COVQ-CS, we propose to use a structured quantizer, namely low complexity multi-stage VQ (MSVQ). We derive new encoding and decoding conditions for the MSVQ, and then propose a practical encoder-decoder design algorithm referred to as channel-optimized MSVQ for CS, coined COMSVQ-CS. Through simulation studies, we compare the proposed schemes vis-a-vis relevant quantizers.',
	 'authors': u'Amirpasha Shirazinia, Saikat Chatterjee, Mikael Skoglund,',
	 'category': u'Computer Science ',
	 'date': '2014-5-31',
	 'pdflink': u'http://arxiv.org/pdf/1406.0086',
	 'subjects': u'Information Theory (cs.IT)',
	 'title': u'\nJoint Source-Channel Vector Quantization for Compressed Sensing',
	 'urllink': u'http://arxiv.org/abs/1406.0086'}
2015-03-23 20:12:15+0000 [xxu461000] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1407.5407> (referer: http://arxiv.org/list/cs/14?skip=8000&show=1000)
2015-03-23 20:12:15+0000 [xxu461000] DEBUG: Scraped from <200 http://arxiv.org/abs/1407.5407>
	{'abstract': u'The HANDE quantum Monte Carlo project offers accessible stochastic diagonalization algorithms for general use for scientists in the field of quantum chemistry. It is an ambitious and general high-performance code developed by a geographically-dispersed team with a variety of backgrounds in computational science. As we prepare for a public, open-source release, we take this as an opportunity to step back and look at what we have done and what we hope to do in the future. We pay particular attention to development processes and the approach taken to train students joining the project.',
	 'authors': u'J. S. Spencer, N. S. Blunt, W. A. Vigor, F. D. Malone, W. M. C. Foulkes, James J. Shepherd, A. J. W. Thom,',
	 'category': u'Computer Science ',
	 'date': '2014-7-21',
	 'pdflink': u'http://arxiv.org/pdf/1407.5407',
	 'subjects': u'Software Engineering (cs.SE)',
	 'title': u'\nThe Highly Accurate N-DEterminant (HANDE) quantum Monte Carlo project:  Open-source stochastic diagonalisation for quantum chemistry',
	 'urllink': u'http://arxiv.org/abs/1407.5407'}
2015-03-23 20:12:18+0000 [xxu461000] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1404.5521> (referer: http://arxiv.org/list/cs/14?skip=4000&show=1000)
2015-03-23 20:12:18+0000 [xxu461000] DEBUG: Scraped from <200 http://arxiv.org/abs/1404.5521>
	{'abstract': u'Massive Open Online Courses (MOOCs) offer a new scalable paradigm for e-learning by providing students with global exposure and opportunities for connecting and interacting with millions of people all around the world. Very often, students work as teams to effectively accomplish course related tasks. However, due to lack of face to face interaction, it becomes difficult for MOOC students to collaborate. Additionally, the instructor also faces challenges in manually organizing students into teams because students flock to these MOOCs in huge numbers. Thus, the proposed research is aimed at developing a robust methodology for dynamic team formation in MOOCs, the theoretical framework for which is grounded at the confluence of organizational team theory, social network analysis and machine learning. A prerequisite for such an undertaking is that we understand the fact that, each and every informal tie established among students offers the opportunities to influence and be influenced. Therefore, we aim to extract value from the inherent connectedness of students in the MOOC. These connections carry with them radical implications for the way students understand each other in the networked learning community. Our approach will enable course instructors to automatically group students in teams that have fairly balanced social connections with their peers, well defined in terms of appropriately selected qualitative and quantitative network metrics.',
	 'authors': u'Tanmay Sinha,',
	 'category': u'Computer Science ',
	 'date': '2014-4-22',
	 'pdflink': u'http://arxiv.org/pdf/1404.5521',
	 'subjects': u'Social and Information Networks (cs.SI)',
	 'title': u'\nTogether we stand, Together we fall, Together we win: Dynamic Team  Formation in Massive Open Online Courses',
	 'urllink': u'http://arxiv.org/abs/1404.5521'}
2015-03-23 20:12:23+0000 [xxu461000] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1405.2458> (referer: http://arxiv.org/list/cs/14?skip=5000&show=1000)
2015-03-23 20:12:23+0000 [xxu461000] DEBUG: Scraped from <200 http://arxiv.org/abs/1405.2458>
	{'abstract': u'We present a heuristic for designing vector non-linear network codes for non-multicast networks, which we call quasi-linear network codes. The method presented has two phases: finding an approximate linear network code over the reals, and then quantizing it to a vector non-linear network code using a fixed-point representation. Apart from describing the method, we draw some links between some network parameters and the rate of the resulting code.',
	 'authors': u'Moshe Schwartz, Muriel M\xe9dard,',
	 'category': u'Computer Science ',
	 'date': '2014-5-10',
	 'pdflink': u'http://arxiv.org/pdf/1405.2458',
	 'subjects': u'Information Theory (cs.IT)',
	 'title': u'\nQuasi-linear Network Coding',
	 'urllink': u'http://arxiv.org/abs/1405.2458'}
2015-03-23 20:12:27+0000 [xxu461000] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1406.0085> (referer: http://arxiv.org/list/cs/14?skip=6000&show=1000)
2015-03-23 20:12:27+0000 [xxu461000] DEBUG: Scraped from <200 http://arxiv.org/abs/1406.0085>
	{'abstract': u'A wide range of multi-agent coordination problems including reference tracking and disturbance rejection requirements can be formulated as a cooperative output regulation problem. The general framework captures typical problems such as output synchronization, leader-follower synchronization, and many more. In the present paper, we propose a novel distributed regulator for groups of identical and non-identical linear agents. We consider global external signals affecting all agents and local external signals affecting only individual agents in the group. Both signal types may contain references and disturbances. Our main contribution is a novel coupling among the agents based on their transient state components or estimates thereof in the output feedback case. This coupling achieves transient synchronization in order to improve the cooperative behavior of the group in transient phases and guarantee a desired decay rate of the synchronization error. This leads to a cooperative reaction of the group on local disturbances acting on individual agents. The effectiveness of the proposed distributed regulator is illustrated by a vehicle platooning example and a coordination example for a group of four non-identical 3-DoF helicopter models.',
	 'authors': u'Georg Seyboth, Wei Ren, Frank Allg\xf6wer,',
	 'category': u'Computer Science ',
	 'date': '2014-5-31',
	 'pdflink': u'http://arxiv.org/pdf/1406.0085',
	 'subjects': u'Systems and Control (cs.SY)',
	 'title': u'\nCooperative Control of Linear Multi-Agent Systems via Distributed Output  Regulation and Transient Synchronization',
	 'urllink': u'http://arxiv.org/abs/1406.0085'}
2015-03-23 20:12:32+0000 [xxu461000] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1407.5404> (referer: http://arxiv.org/list/cs/14?skip=8000&show=1000)
2015-03-23 20:12:32+0000 [xxu461000] DEBUG: Scraped from <200 http://arxiv.org/abs/1407.5404>
	{'abstract': u'The conventional design of real-time approaches depends heavily on the normal performance of systems and it often becomes incapacitated in dealing with catastrophic scenarios effectively. There are several investigations carried out to effectively tackle large scale catastrophe of a plant and how real-time systems must reorganize itself to respond optimally to changing scenarios to reduce catastrophe and aid human intervention. The study presented here is in this direction and the model accommodates catastrophe generated tasks while it tries to minimize the total number of deadline miss, by dynamically scheduling the unusual pattern of tasks. The problem is NP hard. We prove the methods for an optimal scheduling algorithm. We also derive a model to maintain the stability of the processes. Moreover, we study the problem of minimizing the number of processors required for scheduling with a set of periodic and sporadic hard real time tasks with primary/backup mechanism to achieve fault tolerance. EDF scheduling algorithms are used on each processor to manage scenario changes. Finally we present a simulation of super scheduler with small, medium and large real time tasks pattern for catastrophe management.',
	 'authors': u'A. Christy Persya, T.R. Gopalakrishnan Nair,',
	 'category': u'Computer Science ',
	 'date': '2014-7-21',
	 'pdflink': u'http://arxiv.org/pdf/1407.5404',
	 'subjects': u'Distributed, Parallel, and Cluster Computing (cs.DC)',
	 'title': u'\nModel based design of super schedulers managing catastrophic scenario in  hard real time systems',
	 'urllink': u'http://arxiv.org/abs/1407.5404'}
2015-03-23 20:12:38+0000 [xxu461000] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1404.5520> (referer: http://arxiv.org/list/cs/14?skip=4000&show=1000)
2015-03-23 20:12:38+0000 [xxu461000] DEBUG: Scraped from <200 http://arxiv.org/abs/1404.5520>
	{'abstract': u'We propose a computationally efficient limited memory Covariance Matrix Adaptation Evolution Strategy for large scale optimization, which we call the LM-CMA-ES. The LM-CMA-ES is a stochastic, derivative-free algorithm for numerical optimization of non-linear, non-convex optimization problems in continuous domain. Inspired by the limited memory BFGS method of Liu and Nocedal (1989), the LM-CMA-ES samples candidate solutions according to a covariance matrix reproduced from direction vectors selected during the optimization process. The decomposition of the covariance matrix into Cholesky factors allows to reduce the time and memory complexity of the sampling to , where is the number of decision variables. When is large (e.g., &gt; 1000), even relatively small values of (e.g., ) are sufficient to efficiently solve fully non-separable problems and to reduce the overall run-time.',
	 'authors': u'Ilya Loshchilov,',
	 'category': u'Computer Science ',
	 'date': '2014-4-21',
	 'pdflink': u'http://arxiv.org/pdf/1404.5520',
	 'subjects': u'Neural and Evolutionary Computing (cs.NE)',
	 'title': u'\nA Computationally Efficient Limited Memory CMA-ES for Large Scale  Optimization',
	 'urllink': u'http://arxiv.org/abs/1404.5520'}
2015-03-23 20:12:44+0000 [xxu461000] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1405.2452> (referer: http://arxiv.org/list/cs/14?skip=5000&show=1000)
2015-03-23 20:12:44+0000 [xxu461000] DEBUG: Scraped from <200 http://arxiv.org/abs/1405.2452>
	{'abstract': u"In this paper we consider a mechanism design problem in the context of large-scale crowdsourcing markets such as Amazon's Mechanical Turk, ClickWorker, CrowdFlower. In these markets, there is a requester who wants to hire workers to accomplish some tasks. Each worker is assumed to give some utility to the requester. Moreover each worker has a minimum cost that he wants to get paid for getting hired. This minimum cost is assumed to be private information of the workers. The question then is - if the requester has a limited budget, how to design a direct revelation mechanism that picks the right set of workers to hire in order to maximize the requester's utility. We note that although the previous work has studied this problem, a crucial difference in which we deviate from earlier work is the notion of large-scale markets that we introduce in our model. Without the large market assumption, it is known that no mechanism can achieve an approximation factor better than 0.414 and 0.5 for deterministic and randomized mechanisms respectively (while the best known deterministic and randomized mechanisms achieve an approximation ratio of 0.292 and 0.33 respectively). In this paper, we design a budget-feasible mechanism for large markets that achieves an approximation factor of 1-1/e (i.e. almost 0.63). Our mechanism can be seen as a generalization of an alternate way to look at the proportional share mechanism which is used in all the previous works so far on this problem. Interestingly, we also show that our mechanism is optimal by showing that no truthful mechanism can achieve a factor better than 1-1/e; thus, fully resolving this setting. Finally we consider the more general case of submodular utility functions and give new and improved mechanisms for the case when the markets are large.",
	 'authors': u'Nima Anari, Gagan Goel, Afshin Nikzad,',
	 'category': u'Computer Science ',
	 'date': '2014-5-10',
	 'pdflink': u'http://arxiv.org/pdf/1405.2452',
	 'subjects': u'Computer Science and Game Theory (cs.GT)',
	 'title': u'\nMechanism Design for Crowdsourcing: An Optimal 1-1/e Competitive  Budget-Feasible Mechanism for Large Markets',
	 'urllink': u'http://arxiv.org/abs/1405.2452'}
2015-03-23 20:12:48+0000 [xxu461000] INFO: Crawled 231 pages (at 11 pages/min), scraped 214 items (at 11 items/min)
2015-03-23 20:12:50+0000 [xxu461000] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1406.0080> (referer: http://arxiv.org/list/cs/14?skip=6000&show=1000)
2015-03-23 20:12:50+0000 [xxu461000] DEBUG: Scraped from <200 http://arxiv.org/abs/1406.0080>
	{'abstract': u'The Rank Minimization Problem asks to find a matrix of lowest rank inside a linear variety of the space of n x n matrices. The Low Rank Matrix Completion problem asks to complete a partially filled matrix such that the resulting matrix has smallest possible rank. The Tensor Rank Problem asks to determine the rank of a tensor. We show that these three problems are equivalent: each one of the problems can be reduced to the other two.',
	 'authors': u'Harm Derksen,',
	 'category': u'Computer Science ',
	 'date': '2014-5-31',
	 'pdflink': u'http://arxiv.org/pdf/1406.0080',
	 'subjects': u'Numerical Analysis (cs.NA)',
	 'title': u'\nOn the equivalence between low rank matrix completion and tensor rank',
	 'urllink': u'http://arxiv.org/abs/1406.0080'}
2015-03-23 20:12:53+0000 [xxu461000] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1407.5399> (referer: http://arxiv.org/list/cs/14?skip=8000&show=1000)
2015-03-23 20:12:53+0000 [xxu461000] DEBUG: Scraped from <200 http://arxiv.org/abs/1407.5399>
	{'abstract': u'Reactive synthesis deals with the automated construction of implementations of reactive systems from their specifications. To make the approach feasible in practice, systems engineers need effective and efficient means of debugging these specifications. In this paper, we provide techniques for report-based specification debugging, wherein salient properties of a specification are analyzed, and the result presented to the user in the form of a report. This provides a low-effort way to debug specifications, complementing high-effort techniques including the simulation of synthesized implementations. We demonstrate the usefulness of our report-based specification debugging toolkit by providing examples in the context of generalized reactivity(1) synthesis.',
	 'authors': u'R\xfcdiger Ehlers, Vasumathi Raman,',
	 'category': u'Computer Science ',
	 'date': '2014-7-21',
	 'pdflink': u'http://arxiv.org/pdf/1407.5399',
	 'subjects': u'Software Engineering (cs.SE)',
	 'title': u'\nLow-Effort Specification Debugging and Analysis',
	 'urllink': u'http://arxiv.org/abs/1407.5399'}
2015-03-23 20:12:58+0000 [xxu461000] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1404.5513> (referer: http://arxiv.org/list/cs/14?skip=4000&show=1000)
2015-03-23 20:12:58+0000 [xxu461000] DEBUG: Scraped from <200 http://arxiv.org/abs/1404.5513>
	{'abstract': u'Based on a non-rigorous formalism called the "cavity method", physicists have put forward intriguing predictions on phase transitions in discrete structures. One of the most remarkable ones is that in problems such as random -SAT or random graph -coloring, very shortly before the threshold for the existence of solutions there occurs another phase transition called "condensation" [Krzakala et al., PNAS 2007]. The existence of this phase transition appears to be intimately related to the difficulty of proving precise results on, e.g., the -colorability threshold as well as to the performance of message passing algorithms. In random graph -coloring, there is a precise conjecture as to the location of the condensation phase transition in terms of a distributional fixed point problem. In this paper we prove this conjecture for exceeding a certain constant .',
	 'authors': u'Victor Bapst, Amin Coja-Oghlan, Samuel Hetterich, Felicia Rassmann, Dan Vilenchik,',
	 'category': u'Computer Science ',
	 'date': '2014-4-19',
	 'pdflink': u'http://arxiv.org/pdf/1404.5513',
	 'subjects': u'Discrete Mathematics (cs.DM)',
	 'title': u'\nThe condensation phase transition in random graph coloring',
	 'urllink': u'http://arxiv.org/abs/1404.5513'}
2015-03-23 20:13:03+0000 [xxu461000] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1405.2447> (referer: http://arxiv.org/list/cs/14?skip=5000&show=1000)
2015-03-23 20:13:03+0000 [xxu461000] DEBUG: Scraped from <200 http://arxiv.org/abs/1405.2447>
	{'abstract': u'A vertex-subset graph problem defines which subsets of the vertices of an input graph are feasible solutions. The reconfiguration version of a vertex-subset problem asks whether it is possible to transform one feasible solution for into another in at most steps, where each step is a vertex addition or deletion, and each intermediate set is also a feasible solution for of size bounded by . Motivated by recent results establishing W[1]-hardness of the reconfiguration versions of most vertex-subset problems parameterized by , we investigate the complexity of such problems restricted to graphs of bounded treewidth. We show that the reconfiguration versions of most vertex-subset problems remain PSPACE-complete on graphs of treewidth at most but are fixed-parameter tractable parameterized by for all vertex-subset problems definable in monadic second-order logic (MSOL). To prove the latter result, we introduce a technique which allows us to circumvent cardinality constraints and define reconfiguration problems in MSOL.',
	 'authors': u'Amer E. Mouawad, Naomi Nishimura, Venkatesh Raman, Marcin Wrochna,',
	 'category': u'Computer Science ',
	 'date': '2014-5-10',
	 'pdflink': u'http://arxiv.org/pdf/1405.2447',
	 'subjects': u'Computational Complexity (cs.CC)',
	 'title': u'\nReconfiguration over tree decompositions',
	 'urllink': u'http://arxiv.org/abs/1405.2447'}
2015-03-23 20:13:08+0000 [xxu461000] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1406.0079> (referer: http://arxiv.org/list/cs/14?skip=6000&show=1000)
2015-03-23 20:13:08+0000 [xxu461000] DEBUG: Scraped from <200 http://arxiv.org/abs/1406.0079>
	{'abstract': u'The use of Structured English as a computation independent knowledge representation format for non-technical users in business rules representation has been proposed in OMGs Semantics and Business Vocabulary Representation (SBVR). In the legal domain we face a similar problem. Formal representation languages, such as OASIS LegalRuleML and legal ontologies (LKIF, legal OWL2 ontologies etc.) support the technical knowledge engineer and the automated reasoning. But, they can be hardly used directly by the legal domain experts who do not have a computer science background. In this paper we adapt the SBVR Structured English approach for the legal domain and implement a proof-of-concept, called KR4IPLaw, which enables legal domain experts to represent their knowledge in Structured English in a computational independent and hence, for them, more usable way. The benefit of this approach is that the underlying pre-defined semantics of the Structured English approach makes transformations into formal languages such as OASIS LegalRuleML and OWL2 ontologies possible. We exemplify our approach in the domain of patent law.',
	 'authors': u'Shashishekar Ramakrishna, Adrian Paschke,',
	 'category': u'Computer Science ',
	 'date': '2014-5-31',
	 'pdflink': u'http://arxiv.org/pdf/1406.0079',
	 'subjects': u'Computation and Language (cs.CL)',
	 'title': u'\nBridging the gap between Legal Practitioners and Knowledge Engineers  using semi-formal KR',
	 'urllink': u'http://arxiv.org/abs/1406.0079'}
2015-03-23 20:13:13+0000 [xxu461000] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1407.5397> (referer: http://arxiv.org/list/cs/14?skip=8000&show=1000)
2015-03-23 20:13:13+0000 [xxu461000] DEBUG: Scraped from <200 http://arxiv.org/abs/1407.5397>
	{'abstract': u'Counterexample-guided inductive synthesis CEGIS is used to synthesize programs from a candidate space of programs. The technique is guaranteed to terminate and synthesize the correct program if the space of candidate programs is finite. But the technique may or may not terminate with the correct program if the candidate space of programs is infinite. In this paper, we perform a theoretical analysis of counterexample-guided inductive synthesis technique. We investigate whether the set of candidate spaces for which the correct program can be synthesized using CEGIS depends on the counterexamples used in inductive synthesis, that is, whether there are good mistakes which would increase the synthesis power. We investigate whether the use of minimal counterexamples instead of arbitrary counterexamples expands the set of candidate spaces of programs for which inductive synthesis can successfully synthesize a correct program. We consider two kinds of counterexamples: minimal counterexamples and history bounded counterexamples. The history bounded counterexample used in any iteration of CEGIS is bounded by the examples used in previous iterations of inductive synthesis. We examine the relative change in power of inductive synthesis in both cases. We show that the synthesis technique using minimal counterexamples MinCEGIS has the same synthesis power as CEGIS but the synthesis technique using history bounded counterexamples HCEGIS has different power than that of CEGIS, but none dominates the other.',
	 'authors': u'Susmit Jha, Sanjit A. Seshia,',
	 'category': u'Computer Science ',
	 'date': '2014-7-21',
	 'pdflink': u'http://arxiv.org/pdf/1407.5397',
	 'subjects': u'Logic in Computer Science (cs.LO)',
	 'title': u'\nAre There Good Mistakes? A Theoretical Analysis of CEGIS',
	 'urllink': u'http://arxiv.org/abs/1407.5397'}
2015-03-23 20:13:18+0000 [xxu461000] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1404.5511> (referer: http://arxiv.org/list/cs/14?skip=4000&show=1000)
2015-03-23 20:13:18+0000 [xxu461000] DEBUG: Scraped from <200 http://arxiv.org/abs/1404.5511>
	{'abstract': u'Coactive learning is an online problem solving setting where the solutions provided by a solver are interactively improved by a domain expert, which in turn drives learning. In this paper we extend the study of coactive learning to problems where obtaining a globally optimal or near-optimal solution may be intractable or where an expert can only be expected to make small, local improvements to a candidate solution. The goal of learning in this new setting is to minimize the cost as measured by the expert effort over time. We first establish theoretical bounds on the average cost of the existing coactive Perceptron algorithm. In addition, we consider new online algorithms that use cost-sensitive and Passive-Aggressive (PA) updates, showing similar or improved theoretical bounds. We provide an empirical evaluation of the learners in various domains, which show that the Perceptron based algorithms are quite effective and that unlike the case for online classification, the PA algorithms do not yield significant performance gains.',
	 'authors': u'Robby Goetschalckx, Alan Fern, Prasad Tadepalli,',
	 'category': u'Computer Science ',
	 'date': '2014-4-18',
	 'pdflink': u'http://arxiv.org/pdf/1404.5511',
	 'subjects': u'Learning (cs.LG)',
	 'title': u'\nCoactive Learning for Locally Optimal Problem Solving',
	 'urllink': u'http://arxiv.org/abs/1404.5511'}
2015-03-23 20:13:22+0000 [xxu461000] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1405.2435> (referer: http://arxiv.org/list/cs/14?skip=5000&show=1000)
2015-03-23 20:13:22+0000 [xxu461000] DEBUG: Scraped from <200 http://arxiv.org/abs/1405.2435>
	{'abstract': u'A word w is called a synchronizing word of deterministic finite automaton (DFA) if sends all states of the automaton to a unique state. In 1964, Jan verny discovered a sequence of n-state complete DFA possessing a minimal synchronizing word of length . The verny conjecture claims that it is also the upper bound on the length of such a word for a complete DFA. The problem has motivated great and constantly growing number of investigations and generalizations and together with the Road Coloring problem is considered as a most fascinating old problem in the theory of finite automata. Some properties of synchronization are presented.',
	 'authors': u'A.N. Trahtman,',
	 'category': u'Computer Science ',
	 'date': '2014-5-10',
	 'pdflink': u'http://arxiv.org/pdf/1405.2435',
	 'subjects': u'Formal Languages and Automata Theory (cs.FL)',
	 'title': u'\nThe length of a minimal synchronizing word and the \u010cerny conjecture',
	 'urllink': u'http://arxiv.org/abs/1405.2435'}
2015-03-23 20:13:27+0000 [xxu461000] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1406.0074> (referer: http://arxiv.org/list/cs/14?skip=6000&show=1000)
2015-03-23 20:13:27+0000 [xxu461000] DEBUG: Scraped from <200 http://arxiv.org/abs/1406.0074>
	{'abstract': u'Many image segmentation techniques have been developed over the past two decades for segmenting the images, which help for object recognition, occlusion boundary estimation within motion or stereo systems, image compression, image editing. In this, there is a combined approach for segmenting the image. By using histogram equalization to the input image, from which it gives contrast enhancement output image .After that by applying median filtering,which will remove noise from contrast output image . At last I applied fuzzy c-mean clustering algorithm to denoising output image, which give segmented output image. In this way it produce better segmented image with less computation time.',
	 'authors': u'Shradha Dakhare, Harshal Chowhan, Manoj B.Chandak,',
	 'category': u'Computer Science ',
	 'date': '2014-5-31',
	 'pdflink': u'http://arxiv.org/pdf/1406.0074',
	 'subjects': u'Computer Vision and Pattern Recognition (cs.CV)',
	 'title': u'\nCombined Approach for Image Segmentation',
	 'urllink': u'http://arxiv.org/abs/1406.0074'}
2015-03-23 20:13:32+0000 [xxu461000] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1407.5396> (referer: http://arxiv.org/list/cs/14?skip=8000&show=1000)
2015-03-23 20:13:32+0000 [xxu461000] DEBUG: Scraped from <200 http://arxiv.org/abs/1407.5396>
	{'abstract': u'When treating Markov decision processes (MDPs) with large state spaces, using explicit representations quickly becomes unfeasible. Lately, Wimmer et al. have proposed a so-called symblicit algorithm for the synthesis of optimal strategies in MDPs, in the quantitative setting of expected mean-payoff. This algorithm, based on the strategy iteration algorithm of Howard and Veinott, efficiently combines symbolic and explicit data structures, and uses binary decision diagrams as symbolic representation. The aim of this paper is to show that the new data structure of pseudo-antichains (an extension of antichains) provides another interesting alternative, especially for the class of monotonic MDPs. We design efficient pseudo-antichain based symblicit algorithms (with open source implementations) for two quantitative settings: the expected mean-payoff and the stochastic shortest path. For two practical applications coming from automated planning and LTL synthesis, we report promising experimental results w.r.t. both the run time and the memory consumption.',
	 'authors': u'Aaron Bohy, V\xe9ronique Bruy\xe8re, Jean-Fran\xe7ois Raskin,',
	 'category': u'Computer Science ',
	 'date': '2014-7-21',
	 'pdflink': u'http://arxiv.org/pdf/1407.5396',
	 'subjects': u'Logic in Computer Science (cs.LO)',
	 'title': u'\nSymblicit algorithms for optimal strategy synthesis in monotonic Markov  decision processes',
	 'urllink': u'http://arxiv.org/abs/1407.5396'}
2015-03-23 20:13:38+0000 [xxu461000] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1404.5510> (referer: http://arxiv.org/list/cs/14?skip=4000&show=1000)
2015-03-23 20:13:38+0000 [xxu461000] DEBUG: Scraped from <200 http://arxiv.org/abs/1404.5510>
	{'abstract': u'We study an online problem in which a set of mobile resources or centers have to be moved in order to optimally serve a set of demands. There is a set of nodes and a set of resources that are placed at some of the nodes. Each node can potentially host several resources and the resources can be moved between the nodes. At the nodes, demands arrive in an online fashion, and the cost for serving the demands is a function of the number of demands and resources at the different nodes. An online algorithm has to move resources in order to keep the service cost low, however as moving resources is expensive, the objective of an online algorithm is to minimize the total number of movements. Specifically, we give a deterministic online algorithm, which for parameters and guarantees that at all times, the service cost is within a multiplicative factor and an additive term of the optimal service cost and where also the movement cost is within a small multiplicative factor and an additive term of the optimal overall cost. Roughly, for and , we show that the movement cost by time is upper bounded by the optimal service cost at time plus an additive term of order . For and arbitrary , we show that the movement cost by time is only linear in and logarithmic in the optimal service cost at time . We also show that both bounds are almost asymptotically tight.',
	 'authors': u'Abdolhamid Ghodselahi, Fabian Kuhn,',
	 'category': u'Computer Science ',
	 'date': '2014-4-22',
	 'pdflink': u'http://arxiv.org/pdf/1404.5510',
	 'subjects': u'Data Structures and Algorithms (cs.DS)',
	 'title': u'\nServing Online Demands with Movable Centers',
	 'urllink': u'http://arxiv.org/abs/1404.5510'}
2015-03-23 20:13:42+0000 [xxu461000] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1405.2434> (referer: http://arxiv.org/list/cs/14?skip=5000&show=1000)
2015-03-23 20:13:42+0000 [xxu461000] DEBUG: Scraped from <200 http://arxiv.org/abs/1405.2434>
	{'abstract': u'Minimum error rate training (MERT) is a widely used training procedure for statistical machine translation. A general problem of this approach is that the search space is easy to converge to a local optimum and the acquired weight set is not in accord with the real distribution of feature functions. This paper introduces coordinate system selection (RSS) into the search algorithm for MERT. Contrary to previous approaches in which every dimension only corresponds to one independent feature function, we create several coordinate systems by moving one of the dimensions to a new direction. The basic idea is quite simple but critical that the training procedure of MERT should be based on a coordinate system formed by search directions but not directly on feature functions. Experiments show that by selecting coordinate systems with tuning set results, better results can be obtained without any other language knowledge.',
	 'authors': u'Chen Lijiang,',
	 'category': u'Computer Science ',
	 'date': '2014-5-10',
	 'pdflink': u'http://arxiv.org/pdf/1405.2434',
	 'subjects': u'Computation and Language (cs.CL)',
	 'title': u'\nCoordinate System Selection for Minimum Error Rate Training in  Statistical Machine Translation',
	 'urllink': u'http://arxiv.org/abs/1405.2434'}
2015-03-23 20:13:47+0000 [xxu461000] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1406.0073> (referer: http://arxiv.org/list/cs/14?skip=6000&show=1000)
2015-03-23 20:13:47+0000 [xxu461000] DEBUG: Scraped from <200 http://arxiv.org/abs/1406.0073>
	{'abstract': u'We study the structure of sets with small sensitivity. The well-known Simon\'s lemma says that any of sensitivity must be of size at least . This result has been useful for proving lower bounds on sensitivity of Boolean functions, with applications to the theory of parallel computing and the "sensitivity vs. block sensitivity" conjecture. In this paper, we take a deeper look at the size of such sets and their structure. We show an unexpected "gap theorem": if has sensitivity , then we either have or . This is shown via classifying such sets into sets that can be constructed from low-sensitivity subsets of for and irreducible sets which cannot be constructed in such a way and then proving a lower bound on the size of irreducible sets. This provides new insights into the structure of low sensitivity subsets of the Boolean hypercube .',
	 'authors': u'Andris Ambainis, Jevg\u0113nijs Vihrovs,',
	 'category': u'Computer Science ',
	 'date': '2014-5-31',
	 'pdflink': u'http://arxiv.org/pdf/1406.0073',
	 'subjects': u'Computational Complexity (cs.CC)',
	 'title': u"\nSize of Sets with Small Sensitivity: a Generalization of Simon's Lemma",
	 'urllink': u'http://arxiv.org/abs/1406.0073'}
2015-03-23 20:13:48+0000 [xxu461000] INFO: Crawled 244 pages (at 13 pages/min), scraped 227 items (at 13 items/min)
2015-03-23 20:13:52+0000 [xxu461000] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1407.5395> (referer: http://arxiv.org/list/cs/14?skip=8000&show=1000)
2015-03-23 20:13:52+0000 [xxu461000] DEBUG: Scraped from <200 http://arxiv.org/abs/1407.5395>
	{'abstract': u'The increased interest in reactive synthesis over the last decade has led to many improved solutions but also to many new questions. In this paper, we discuss the question of how to deal with assumptions on environment behavior. We present four goals that we think should be met and review several different possibilities that have been proposed. We argue that each of them falls short in at least one aspect.',
	 'authors': u'Roderick Bloem, R\xfcdiger Ehlers, Swen Jacobs, Robert K\xf6nighofer,',
	 'category': u'Computer Science ',
	 'date': '2014-7-21',
	 'pdflink': u'http://arxiv.org/pdf/1407.5395',
	 'subjects': u'Logic in Computer Science (cs.LO)',
	 'title': u'\nHow to Handle Assumptions in Synthesis',
	 'urllink': u'http://arxiv.org/abs/1407.5395'}
2015-03-23 20:13:56+0000 [xxu461000] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1404.5507> (referer: http://arxiv.org/list/cs/14?skip=4000&show=1000)
2015-03-23 20:13:56+0000 [xxu461000] DEBUG: Scraped from <200 http://arxiv.org/abs/1404.5507>
	{'abstract': u'We study the problem of channel resolvability for fixed i.i.d. input distributions and discrete memoryless channels (DMCs), and derive the strong converse theorem for any DMCs that are not necessarily full rank. We also derive the optimal second-order rate under a condition. Furthermore, under the condition that a DMC has the unique capacity achieving input distribution, we derive the optimal second-order rate of channel resolvability for the worst input distribution.',
	 'authors': u'Shun Watanabe, Masahito Hayashi,',
	 'category': u'Computer Science ',
	 'date': '2014-4-22',
	 'pdflink': u'http://arxiv.org/pdf/1404.5507',
	 'subjects': u'Information Theory (cs.IT)',
	 'title': u'\nStrong Converse and Second-Order Asymptotics of Channel Resolvability',
	 'urllink': u'http://arxiv.org/abs/1404.5507'}
2015-03-23 20:14:02+0000 [xxu461000] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1405.2430> (referer: http://arxiv.org/list/cs/14?skip=5000&show=1000)
2015-03-23 20:14:02+0000 [xxu461000] DEBUG: Scraped from <200 http://arxiv.org/abs/1405.2430>
	{'abstract': u'Robots with lights is a model of autonomous mobile computational entities operating in the plane in Look-Compute-Move cycles: each agent has an externally visible light which can assume colors from a fixed set; the lights are persistent (i.e., the color is not erased at the end of a cycle), but otherwise the agents are oblivious. The investigation of computability in this model, initially suggested by Peleg, is under way, and several results have been recently established. In these investigations, however, an agent is assumed to be capable to see through another agent. In this paper we start the study of computing when visibility is obstructable, and investigate the most basic problem for this setting, Complete Visibility: The agents must reach within finite time a configuration where they can all see each other and terminate. We do not make any assumption on a-priori knowledge of the number of agents, on rigidity of movements nor on chirality. The local coordinate system of an agent may change at each activation. Also, by definition of lights, an agent can communicate and remember only a constant number of bits in each cycle. In spite of these weak conditions, we prove that Complete Visibility is always solvable, even in the asynchronous setting, without collisions and using a small constant number of colors. The proof is constructive. We also show how to extend our protocol for Complete Visibility so that, with the same number of colors, the agents solve the (non-uniform) Circle Formation problem with obstructed visibility.',
	 'authors': u'G. A. Di Luna, P. Flocchini, S. Gan Chaudhuri, N. Santoro, G. Viglietta,',
	 'category': u'Computer Science ',
	 'date': '2014-5-10',
	 'pdflink': u'http://arxiv.org/pdf/1405.2430',
	 'subjects': u'Distributed, Parallel, and Cluster Computing (cs.DC)',
	 'title': u'\nRobots with Lights: Overcoming Obstructed Visibility Without Colliding',
	 'urllink': u'http://arxiv.org/abs/1405.2430'}
2015-03-23 20:14:07+0000 [xxu461000] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1406.0062> (referer: http://arxiv.org/list/cs/14?skip=6000&show=1000)
2015-03-23 20:14:07+0000 [xxu461000] DEBUG: Scraped from <200 http://arxiv.org/abs/1406.0062>
	{'abstract': u'Crisis management is a complex problem raised by the scientific community currently. Decision support systems are a suitable solution for such issues, they are indeed able to help emergency managers to prevent and to manage crisis in emergency situations. However, they should be enough flexible and adaptive in order to be reliable to solve complex problems that are plunged in dynamic and unpredictable environments. The approach we propose in this paper addresses this challenge. We expose here a modelling of information for an emergency environment and an architecture of a multiagent decision support system that deals with these information in order to prevent and to manage the occur of a crisis in emergency situations. We focus on the first level of the system mechanism which intends to perceive and to reflect the evolution of the current situation. The general approach and experimentations are provided here.',
	 'authors': u'Fahem Kebair, Fr\xe9d\xe9ric Serin,',
	 'category': u'Computer Science ',
	 'date': '2014-5-31',
	 'pdflink': u'http://arxiv.org/pdf/1406.0062',
	 'subjects': u'Artificial Intelligence (cs.AI)',
	 'title': u'\nTowards a Multiagent Decision Support System for crisis Management',
	 'urllink': u'http://arxiv.org/abs/1406.0062'}
2015-03-23 20:14:13+0000 [xxu461000] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1407.5393> (referer: http://arxiv.org/list/cs/14?skip=8000&show=1000)
2015-03-23 20:14:13+0000 [xxu461000] DEBUG: Scraped from <200 http://arxiv.org/abs/1407.5393>
	{'abstract': u'For deterministic and probabilistic programs we investigate the problem of program synthesis and program optimisation (with respect to non-functional properties) in the general setting of global optimisation. This approach is based on the representation of the semantics of programs and program fragments in terms of linear operators, i.e. as matrices. We exploit in particular the fact that we can automatically generate the representation of the semantics of elementary blocks. These can then can be used in order to compositionally assemble the semantics of a whole program, i.e. the generator of the corresponding Discrete Time Markov Chain (DTMC). We also utilise a generalised version of Abstract Interpretation suitable for this linear algebraic or functional analytical framework in order to formulate semantical constraints (invariants) and optimisation objectives (for example performance requirements).',
	 'authors': u'Herbert Wiklicky,',
	 'category': u'Computer Science ',
	 'date': '2014-7-21',
	 'pdflink': u'http://arxiv.org/pdf/1407.5393',
	 'subjects': u'Programming Languages (cs.PL)',
	 'title': u'\nProgram Synthesis and Linear Operator Semantics',
	 'urllink': u'http://arxiv.org/abs/1407.5393'}
2015-03-23 20:14:19+0000 [xxu461000] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1404.5501> (referer: http://arxiv.org/list/cs/14?skip=4000&show=1000)
2015-03-23 20:14:19+0000 [xxu461000] DEBUG: Scraped from <200 http://arxiv.org/abs/1404.5501>
	{'abstract': u'We show that polar codes can be used to achieve the rate-distortion functions in the problem of hierarchical source coding also known as the successive refinement problem. We also analyze the distributed version of this problem, constructing a polar coding scheme that achieves the rate distortion functions for successive refinement with side information.',
	 'authors': u'Min Ye, Alexander Barg,',
	 'category': u'Computer Science ',
	 'date': '2014-4-22',
	 'pdflink': u'http://arxiv.org/pdf/1404.5501',
	 'subjects': u'Information Theory (cs.IT)',
	 'title': u'\nPolar Codes for Distributed Hierarchical Source Coding',
	 'urllink': u'http://arxiv.org/abs/1404.5501'}
2015-03-23 20:14:25+0000 [xxu461000] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1405.2424> (referer: http://arxiv.org/list/cs/14?skip=5000&show=1000)
2015-03-23 20:14:25+0000 [xxu461000] DEBUG: Scraped from <200 http://arxiv.org/abs/1405.2424>
	{'abstract': u'We consider the problems of finding optimal identifying codes, (open) locating-dominating sets and resolving sets (denoted IDENTIFYING CODE, (OPEN) LOCATING-DOMINATING SET and METRIC DIMENSION) of an interval or a permutation graph. In these problems, one asks to distinguish all vertices of a graph by a subset of the vertices, using either the neighbourhood within the solution set or the distances to the solution vertices. Using a general reduction for this class of problems, we prove that the decision problems associated to these four notions are NP-complete, even for graphs that are at the same time interval graphs and permutation graphs and have diameter 2. While IDENTIFYING CODE and (OPEN) LOCATING-DOMINATING SET are trivially fixed-parameter-tractable when parameterized by solution size, it is known that in the same setting METRIC DIMENSION is W[2]-hard. We show that for interval graphs, this parameterization of METRIC DIMENSION is fixed-parameter-tractable.',
	 'authors': u'Florent Foucaud, George B. Mertzios, Reza Naserasr, Aline Parreau, Petru Valicov,',
	 'category': u'Computer Science ',
	 'date': '2014-5-10',
	 'pdflink': u'http://arxiv.org/pdf/1405.2424',
	 'subjects': u'Discrete Mathematics (cs.DM)',
	 'title': u'\nIdentification, location-domination and metric dimension on interval and  permutation graphs. II. Algorithms and complexity',
	 'urllink': u'http://arxiv.org/abs/1405.2424'}
2015-03-23 20:14:28+0000 [xxu461000] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1406.0053> (referer: http://arxiv.org/list/cs/14?skip=6000&show=1000)
2015-03-23 20:14:28+0000 [xxu461000] DEBUG: Scraped from <200 http://arxiv.org/abs/1406.0053>
	{'abstract': u'The K "otter-Nielsen-Hholdt algorithm is a popular way to construct the bivariate interpolation polynomial in the Guruswami-Sudan decoding algorithm for Reed-Solomon codes. In this paper, we show how one can use Divide &amp; Conquer techniques to provide an asymptotic speed-up of the algorithm, rendering its complexity quasi-linear in n. Several of our observations can also provide a practical speed-up to the classical version of the algorithm.',
	 'authors': u'Johan S. R. Nielsen,',
	 'category': u'Computer Science ',
	 'date': '2014-5-31',
	 'pdflink': u'http://arxiv.org/pdf/1406.0053',
	 'subjects': u'Information Theory (cs.IT)',
	 'title': u'\nFast K\xf6tter-Nielsen-H\xf8holdt Interpolation in the Guruswami-Sudan  Algorithm',
	 'urllink': u'http://arxiv.org/abs/1406.0053'}
2015-03-23 20:14:33+0000 [xxu461000] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1407.5392> (referer: http://arxiv.org/list/cs/14?skip=8000&show=1000)
2015-03-23 20:14:33+0000 [xxu461000] DEBUG: Scraped from <200 http://arxiv.org/abs/1407.5392>
	{'abstract': u'With the increasing importance of distributed systems as a computing paradigm, a systematic approach to their design is needed. Although the area of formal verification has made enormous advances towards this goal, the resulting functionalities are limited to detecting problems in a particular design. By means of a classical example, we illustrate a simple template-based approach to computer-aided design of distributed systems based on leveraging the well-known technique of bounded model checking to the synthesis setting.',
	 'authors': u'Adri\xe0 Gasc\xf3n, Ashish Tiwari,',
	 'category': u'Computer Science ',
	 'date': '2014-7-21',
	 'pdflink': u'http://arxiv.org/pdf/1407.5392',
	 'subjects': u'Logic in Computer Science (cs.LO)',
	 'title': u'\nSynthesis of a simple self-stabilizing system',
	 'urllink': u'http://arxiv.org/abs/1407.5392'}
2015-03-23 20:14:37+0000 [xxu461000] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1404.5481> (referer: http://arxiv.org/list/cs/14?skip=4000&show=1000)
2015-03-23 20:14:37+0000 [xxu461000] DEBUG: Scraped from <200 http://arxiv.org/abs/1404.5481>
	{'abstract': u'The use of Internet in the every day life has pushed its evolution in a very fast way. The heterogeneity of the equipments supporting its networks, as well as the different devices from which it can be accessed, have participated in increasing the complexity of understanding its global behavior and performance. In our study we propose a new method for studying the performance of TCP protocol based on causal graphs. Causal graphs offer models easy to interpret and use. They highlight the structural model of the system they represent and give us access to the causal dependences between the different parameters of the system. One of the major contribution of causal graphs is their ability to predict the effects of an intervention from observations made before this intervention.',
	 'authors': u'Hadrien Hours, Ernst Biersack, Patrick Loiseau,',
	 'category': u'Computer Science ',
	 'date': '2014-4-22',
	 'pdflink': u'http://arxiv.org/pdf/1404.5481',
	 'subjects': u'Networking and Internet Architecture (cs.NI)',
	 'title': u'\nCausal study of Network Performance',
	 'urllink': u'http://arxiv.org/abs/1404.5481'}
2015-03-23 20:14:43+0000 [xxu461000] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1405.2421> (referer: http://arxiv.org/list/cs/14?skip=5000&show=1000)
2015-03-23 20:14:43+0000 [xxu461000] DEBUG: Scraped from <200 http://arxiv.org/abs/1405.2421>
	{'abstract': u'In this paper we propose a new decentralized control scheme for Islanded microGrids (ImGs) composed by the interconnection of Distributed Generation Units (DGUs). Local controllers regulate voltage and frequency at the Point of Common Coupling (PCC) of each DGU and they are able to guarantee stability of the overall ImG. The control design procedure is decentralized, since, besides two global scalar quantities, the synthesis of a local controller uses only information on the corresponding DGU and lines connected to it. Most important, our design procedure enables Plug-and-Play (PnP) operations: when a DGU is plugged in or out, only DGUs physically connected to it have to retune their local controllers. We study the performance of the proposed controllers simulating different scenarios in MatLab/Simulink and using performance indexes proposed in IEEE standards.',
	 'authors': u'Stefano Riverso, Fabio Sarzo, Giancarlo Ferrari-Trecate,',
	 'category': u'Computer Science ',
	 'date': '2014-5-10',
	 'pdflink': u'http://arxiv.org/pdf/1405.2421',
	 'subjects': u'Systems and Control (cs.SY)',
	 'title': u'\nPlug-and-play voltage and frequency control of islanded microgrids with  meshed topology',
	 'urllink': u'http://arxiv.org/abs/1405.2421'}
2015-03-23 20:14:48+0000 [xxu461000] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1406.0049> (referer: http://arxiv.org/list/cs/14?skip=6000&show=1000)
2015-03-23 20:14:48+0000 [xxu461000] DEBUG: Scraped from <200 http://arxiv.org/abs/1406.0049>
	{'abstract': u'In this paper, we analyze the ergodic capacity of a dual-hop amplify-and-forward relaying system where the relay is equipped with multiple antennas and subject to co-channel interference (CCI) and the additive white Gaussian noise. Specifically, we consider three heuristic precoding schemes, where the relay first applies the 1) maximal-ratio combining (MRC) 2) zero-forcing (ZF) 3) minimum mean-squared error (MMSE) principle to combine the signal from the source, and then steers the transformed signal towards the destination with the maximum ratio transmission (MRT) technique. For the MRC/MRT and MMSE/MRT schemes, we present new tight analytical upper and lower bounds for the ergodic capacity, while for the ZF/MRT scheme, we derive a new exact analytical ergodic capacity expression. Moreover, we make a comparison among all the three schemes, and our results reveal that, in terms of the ergodic capacity performance, the MMSE/MRT scheme always has the best performance and the ZF/MRT scheme is slightly inferior, while the MRC/MRT scheme is always the worst one. Finally, the asymptotic behavior of ergodic capacity for the three proposed schemes are characterized in large scenario, where is the number of relay antennas. Our results reveal that, in the large regime, both the ZF/MRT and MMSE/MRT schemes have perfect interference cancelation capability, which is not possible with the MRC/MRT scheme.',
	 'authors': u'Guangxu Zhu, Caijun Zhong, Himal A. Suraweera, Zhaoyang Zhang, Chau Yuen, Rui Yin,',
	 'category': u'Computer Science ',
	 'date': '2014-5-31',
	 'pdflink': u'http://arxiv.org/pdf/1406.0049',
	 'subjects': u'Information Theory (cs.IT)',
	 'title': u'\nErgodic Capacity Comparison of Different Relay Precoding Schemes in  Dual-Hop AF Systems with Co-Channel Interference',
	 'urllink': u'http://arxiv.org/abs/1406.0049'}
2015-03-23 20:14:48+0000 [xxu461000] INFO: Crawled 256 pages (at 12 pages/min), scraped 239 items (at 12 items/min)
2015-03-23 20:14:51+0000 [xxu461000] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1407.5385> (referer: http://arxiv.org/list/cs/14?skip=8000&show=1000)
2015-03-23 20:14:51+0000 [xxu461000] DEBUG: Scraped from <200 http://arxiv.org/abs/1407.5385>
	{'abstract': u'An ad hoc network is an infrastructureless network in which nodes perform terminal as well as routing functions. A routing protocol is the only substitute to complete the communications in the absence of an access point. In spite of that mobile nodes or so called routers uses some mechanism for calculating the best route when it has multiple routes for the same destination. On the basis of one or more metrics routes are ranked from best to worst. But, in an ad hoc network many factors can affect this decision, such as the delay, load, route lifetime etc. Thus measuring and finding routes on the basis of crisp mathematical model for all these attributes is complicated. That why, the fuzzy approach for best route determination is required for MANET because some of the metrics are fuzzy or vague and the classical ranking of routes and transitivity in the ranking does not hold. The proposed Nontransitive Route Ranking subjective comparison of one route with others and performs nontransitive ranking to rank routes from best to worst. The pairwise comparisons of each route with others give more accurate and fair comparison. The proposed ranking is easier than classical ranking in which metrics have assigned some value and these values are combined to obtain the ranking. Experimental result shows the efficiency of the proposed model. Keywords: Fuzzy, Rank, Nontransitive, Route, Ranking, Relativity',
	 'authors': u'Md. Amir Khusru Akhtar, Arshad Usmani, G. Sahoo,',
	 'category': u'Computer Science ',
	 'date': '2014-7-21',
	 'pdflink': u'http://arxiv.org/pdf/1407.5385',
	 'subjects': u'Networking and Internet Architecture (cs.NI)',
	 'title': u'\nNontransitive Ranking to Enhance Routing Decision in MANETS',
	 'urllink': u'http://arxiv.org/abs/1407.5385'}
2015-03-23 20:14:56+0000 [xxu461000] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1404.5479> (referer: http://arxiv.org/list/cs/14?skip=4000&show=1000)
2015-03-23 20:14:56+0000 [xxu461000] DEBUG: Scraped from <200 http://arxiv.org/abs/1404.5479>
	{'abstract': u"We investigate the monoid of transformations that are induced by sequences of writing to and reading from a queue storage. We describe this monoid by means of a confluent and terminating semi-Thue system and study some of its basic algebraic properties, e.g., conjugacy. Moreover, we show that while several properties concerning its rational subsets are undecidable, their uniform membership problem is NL-complete. Furthermore, we present an algebraic characterization of this monoid's recognizable subsets. Finally, we prove that it is not Thurston-automatic.",
	 'authors': u'Martin Huschenbett, Dietrich Kuske, Georg Zetzsche,',
	 'category': u'Computer Science ',
	 'date': '2014-4-22',
	 'pdflink': u'http://arxiv.org/pdf/1404.5479',
	 'subjects': u'Formal Languages and Automata Theory (cs.FL)',
	 'title': u'\nThe monoid of queue actions',
	 'urllink': u'http://arxiv.org/abs/1404.5479'}
2015-03-23 20:15:01+0000 [xxu461000] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1405.2420> (referer: http://arxiv.org/list/cs/14?skip=5000&show=1000)
2015-03-23 20:15:02+0000 [xxu461000] DEBUG: Scraped from <200 http://arxiv.org/abs/1405.2420>
	{'abstract': u'The fundamental theorem of statistical learning states that for binary classification problems, any Empirical Risk Minimization (ERM) learning rule has close to optimal sample complexity. In this paper we seek for a generic optimal learner for multiclass prediction. We start by proving a surprising result: a generic optimal multiclass learner must be improper, namely, it must have the ability to output hypotheses which do not belong to the hypothesis class, even though it knows that all the labels are generated by some hypothesis from the class. In particular, no ERM learner is optimal. This brings back the fundmamental question of "how to learn"? We give a complete answer to this question by giving a new analysis of the one-inclusion multiclass learner of Rubinstein et al (2006) showing that its sample complexity is essentially optimal. Then, we turn to study the popular hypothesis class of generalized linear classifiers. We derive optimal learners that, unlike the one-inclusion algorithm, are computationally efficient. Furthermore, we show that the sample complexity of these learners is better than the sample complexity of the ERM rule, thus settling in negative an open question due to Collins (2005).',
	 'authors': u'Amit Daniely, Shai Shalev-Shwartz,',
	 'category': u'Computer Science ',
	 'date': '2014-5-10',
	 'pdflink': u'http://arxiv.org/pdf/1405.2420',
	 'subjects': u'Learning (cs.LG)',
	 'title': u'\nOptimal Learners for Multiclass Problems',
	 'urllink': u'http://arxiv.org/abs/1405.2420'}
2015-03-23 20:15:06+0000 [xxu461000] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1406.0045> (referer: http://arxiv.org/list/cs/14?skip=6000&show=1000)
2015-03-23 20:15:06+0000 [xxu461000] DEBUG: Scraped from <200 http://arxiv.org/abs/1406.0045>
	{'abstract': u'As an equilibrium refinement of the Nash equilibrium, evolutionarily stable strategy (ESS) is a key concept in evolutionary game theory and has attracted growing interest. An ESS can be either a pure strategy or a mixed strategy. Even though the randomness is allowed in mixed strategy, the selection probability of pure strategy in a mixed strategy may fluctuate due to the impact of many factors. The fluctuation can lead to more uncertainty. In this paper, such uncertainty involved in mixed strategy has been further taken into consideration: a belief strategy is proposed in terms of Dempster-Shafer evidence theory. Furthermore, based on the proposed belief strategy, a belief-based ESS has been developed. The belief strategy and belief-based ESS can reduce to the mixed strategy and mixed ESS, which provide more realistic and powerful tools to describe interactions among agents.',
	 'authors': u'Xinyang Deng, Zhen Wang, Qi Liu, Yong Deng, Sankaran Mahadevan,',
	 'category': u'Computer Science ',
	 'date': '2014-5-31',
	 'pdflink': u'http://arxiv.org/pdf/1406.0045',
	 'subjects': u'Computer Science and Game Theory (cs.GT)',
	 'title': u'\nA belief-based evolutionarily stable strategy',
	 'urllink': u'http://arxiv.org/abs/1406.0045'}
2015-03-23 20:15:11+0000 [xxu461000] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1407.5383> (referer: http://arxiv.org/list/cs/14?skip=8000&show=1000)
2015-03-23 20:15:11+0000 [xxu461000] DEBUG: Scraped from <200 http://arxiv.org/abs/1407.5383>
	{'abstract': u'Exchangeable random partition processes are the basis for Bayesian approaches to statistical inference in large alphabet settings. On the other hand, the notion of the pattern of a sequence provides an information-theoretic framework for data compression in large alphabet scenarios. Because data compression and parameter estimation are intimately related, we study the redundancy of Bayes estimators coming from Poisson-Dirichlet priors (or "Chinese restaurant processes") and the Pitman-Yor prior. This provides an understanding of these estimators in the setting of unknown discrete alphabets from the perspective of universal compression. In particular, we identify relations between alphabet sizes and sample sizes where the redundancy is small, thereby characterizing useful regimes for these estimators.',
	 'authors': u'Narayana P. Santhanam, Anand D. Sarwate, Jae Oh Woo,',
	 'category': u'Computer Science ',
	 'date': '2014-7-21',
	 'pdflink': u'http://arxiv.org/pdf/1407.5383',
	 'subjects': u'Information Theory (cs.IT)',
	 'title': u'\nRedundancy of Exchangeable Estimators',
	 'urllink': u'http://arxiv.org/abs/1407.5383'}
2015-03-23 20:15:17+0000 [xxu461000] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1404.5478> (referer: http://arxiv.org/list/cs/14?skip=4000&show=1000)
2015-03-23 20:15:17+0000 [xxu461000] DEBUG: Scraped from <200 http://arxiv.org/abs/1404.5478>
	{'abstract': u"We model the spread of information in a homogeneously mixed population using the Maki Thompson rumor model. We formulate an optimal control problem, from the perspective of single campaigner, to maximize the spread of information when the campaign budget is fixed. Control signals, such as advertising in the mass media, attempt to convert ignorants and stiflers into spreaders. We show the existence of a solution to the optimal control problem when the campaigning incurs non-linear costs under the isoperimetric budget constraint. The solution employs Pontryagin's Minimum Principle and a modified version of forward backward sweep technique for numerical computation to accommodate the isoperimetric budget constraint. The techniques developed in this paper are general and can be applied to similar optimal control problems in other areas. We have allowed the spreading rate of the information epidemic to vary over the campaign duration to model practical situations when the interest level of the population in the subject of the campaign changes with time. The shape of the optimal control signal is studied for different model parameters and spreading rate profiles. We have also studied the variation of the optimal campaigning costs with respect to various model parameters. Results indicate that, for some model parameters, significant improvements can be achieved by the optimal strategy compared to the static control strategy. The static strategy respects the same budget constraint as the optimal strategy and has a constant value throughout the campaign horizon. This work finds application in election and social awareness campaigns, product advertising, movie promotion and crowdfunding campaigns.",
	 'authors': u'Kundan Kandhway, Joy Kuri,',
	 'category': u'Computer Science ',
	 'date': '2014-4-22',
	 'pdflink': u'http://arxiv.org/pdf/1404.5478',
	 'subjects': u'Systems and Control (cs.SY)',
	 'title': u'\nOptimal control of information epidemics modeled as Maki Thompson rumors',
	 'urllink': u'http://arxiv.org/abs/1404.5478'}
2015-03-23 20:15:23+0000 [xxu461000] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1405.2418> (referer: http://arxiv.org/list/cs/14?skip=5000&show=1000)
2015-03-23 20:15:23+0000 [xxu461000] DEBUG: Scraped from <200 http://arxiv.org/abs/1405.2418>
	{'abstract': u'The problem addressed concerns the determination of the average number of successive attempts of guessing a word of a certain length consisting of letters with given probabilities of occurrence. Both first- and second-order approximations to a natural language are considered. The guessing strategy used is guessing words in decreasing order of probability. When word and alphabet sizes are large, approximations are necessary in order to estimate the number of guesses. Several kinds of approximations are discussed demonstrating moderate requirements concerning both memory and CPU time. When considering realistic sizes of alphabets and words (100) the number of guesses can be estimated within minutes with reasonable accuracy (a few percent). For many probability distributions the density of the logarithm of probability products is close to a normal distribution. For those cases it is possible to derive an analytical expression for the average number of guesses. The proportion of guesses needed on average compared to the total number decreases almost exponentially with the word length. The leading term in an asymptotic expansion can be used to estimate the number of guesses for large word lengths. Comparisons with analytical lower bounds and entropy expressions are also provided.',
	 'authors': u'Kerstin Andersson,',
	 'category': u'Computer Science ',
	 'date': '2014-5-10',
	 'pdflink': u'http://arxiv.org/pdf/1405.2418',
	 'subjects': u'Information Theory (cs.IT)',
	 'title': u'\nExact Probability Distribution versus Entropy',
	 'urllink': u'http://arxiv.org/abs/1405.2418'}
2015-03-23 20:15:27+0000 [xxu461000] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1406.0043> (referer: http://arxiv.org/list/cs/14?skip=6000&show=1000)
2015-03-23 20:15:27+0000 [xxu461000] DEBUG: Scraped from <200 http://arxiv.org/abs/1406.0043>
	{'abstract': u'We define the concept of a monotonic theory and show how to build efficient SMT (SAT Modulo Theory) solvers, including effective theory propagation and clause learning, for such theories. We present examples showing that monotonic theories arise from many common problems, e.g., graph properties such as reachability, shortest paths, connected components, minimum spanning tree, and max-flow/min-cut, and then demonstrate our framework by building SMT solvers for each of these theories. We apply these solvers to procedural content generation problems, demonstrating major speed-ups over state-of-the-art approaches based on SAT or Answer Set Programming, and easily solving several instances that were previously impractical to solve.',
	 'authors': u'Sam Bayless, Noah Bayless, Holger H. Hoos, Alan J. Hu,',
	 'category': u'Computer Science ',
	 'date': '2014-5-31',
	 'pdflink': u'http://arxiv.org/pdf/1406.0043',
	 'subjects': u'Logic in Computer Science (cs.LO)',
	 'title': u'\nSAT Modulo Monotonic Theories',
	 'urllink': u'http://arxiv.org/abs/1406.0043'}
2015-03-23 20:15:32+0000 [xxu461000] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1407.5380> (referer: http://arxiv.org/list/cs/14?skip=8000&show=1000)
2015-03-23 20:15:32+0000 [xxu461000] DEBUG: Scraped from <200 http://arxiv.org/abs/1407.5380>
	{'abstract': u'As a contribution to the challenge of building game-playing AI systems, we develop and analyse a formal language for representing and reasoning about strategies. Our logical language builds on the existing general Game Description Language (GDL) and extends it by a standard modality for linear time along with two dual connectives to express preferences when combining strategies. The semantics of the language is provided by a standard state-transition model. As such, problems that require reasoning about games can be solved by the standard methods for reasoning about actions and change. We also endow the language with a specific semantics by which strategy formulas are understood as move recommendations for a player. To illustrate how our formalism supports automated reasoning about strategies, we demonstrate two example methods of implementation /: first, we formalise the semantic interpretation of our language in conjunction with game rules and strategy rules in the Situation Calculus; second, we show how the reasoning problem can be solved with Answer Set Programming.',
	 'authors': u'Dongmo Zhang, Michael Thielsher,',
	 'category': u'Computer Science ',
	 'date': '2014-7-21',
	 'pdflink': u'http://arxiv.org/pdf/1407.5380',
	 'subjects': u'Artificial Intelligence (cs.AI)',
	 'title': u'\nRepresenting and Reasoning about Game Strategies',
	 'urllink': u'http://arxiv.org/abs/1407.5380'}
2015-03-23 20:15:36+0000 [xxu461000] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1405.2417> (referer: http://arxiv.org/list/cs/14?skip=5000&show=1000)
2015-03-23 20:15:36+0000 [xxu461000] DEBUG: Scraped from <200 http://arxiv.org/abs/1405.2417>
	{'abstract': u'Vehicular safety applications intended for VANETs. It can be separated by inter-vehicle communication. It is needed for a vehicle can travel safety with high velocity and must interconnect quickly dependably. In this work, examined the impact of the IDM-IM and IDM-LC mobility model on AODV, AOMDV, DSDV and OLSR routing protocol using Nakagami propagation model and IEEE 802.11p MAC protocol in a particular urban scenario of Dhaka city. The periodic broadcast (PBC) agent is employed to transmit messages between vehicles in case of emergency or collision avoidance for vehicular safety communication. The simulation results recommend numerous concerns such as lower packet drop rate, delay, jitter, route cost and mean-hop is necessary to be measured before developing a robust safety application of VANET.',
	 'authors': u'Md Habibur Rahman, Mohammad Nasiruddin,',
	 'category': u'Computer Science ',
	 'date': '2014-5-10',
	 'pdflink': u'http://arxiv.org/pdf/1405.2417',
	 'subjects': u'Networking and Internet Architecture (cs.NI)',
	 'title': u'\nImpact of Two Realistic Mobility Models for Vehicular Safety  Applications',
	 'urllink': u'http://arxiv.org/abs/1405.2417'}
2015-03-23 20:15:41+0000 [xxu461000] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1406.0032> (referer: http://arxiv.org/list/cs/14?skip=6000&show=1000)
2015-03-23 20:15:41+0000 [xxu461000] DEBUG: Scraped from <200 http://arxiv.org/abs/1406.0032>
	{'abstract': u"Several messages express opinions about events, products, and services, political views or even their author's emotional state and mood. Sentiment analysis has been used in several applications including analysis of the repercussions of events in social networks, analysis of opinions about products and services, and simply to better understand aspects of social communication in Online Social Networks (OSNs). There are multiple methods for measuring sentiments, including lexical-based approaches and supervised machine learning methods. Despite the wide use and popularity of some methods, it is unclear which method is better for identifying the polarity (i.e., positive or negative) of a message as the current literature does not provide a method of comparison among existing methods. Such a comparison is crucial for understanding the potential limitations, advantages, and disadvantages of popular methods in analyzing the content of OSNs messages. Our study aims at filling this gap by presenting comparisons of eight popular sentiment analysis methods in terms of coverage (i.e., the fraction of messages whose sentiment is identified) and agreement (i.e., the fraction of identified sentiments that are in tune with ground truth). We develop a new method that combines existing approaches, providing the best coverage results and competitive agreement. We also present a free Web service called iFeel, which provides an open API for accessing and comparing results across different sentiment methods for a given text.",
	 'authors': u'Pollyanna Gon\xe7alves, Matheus Ara\xfajo, Fabr\xedcio Benevenuto, Meeyoung Cha,',
	 'category': u'Computer Science ',
	 'date': '2014-5-30',
	 'pdflink': u'http://arxiv.org/pdf/1406.0032',
	 'subjects': u'Computation and Language (cs.CL)',
	 'title': u'\nComparing and Combining Sentiment Analysis Methods',
	 'urllink': u'http://arxiv.org/abs/1406.0032'}
2015-03-23 20:15:46+0000 [xxu461000] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1407.5374> (referer: http://arxiv.org/list/cs/14?skip=8000&show=1000)
2015-03-23 20:15:46+0000 [xxu461000] DEBUG: Scraped from <200 http://arxiv.org/abs/1407.5374>
	{'abstract': u'The algorithm for Lov \'sz Local Lemma by Moser and Tardos gives a constructive way to prove the existence of combinatorial objects that satisfy a system of constraints. We present an alternative probabilistic analysis of the algorithm that does not involve reconstructing the history of the algorithm from the witness tree. We apply our technique to improve the best known upper bound to acyclic chromatic index. Specifically we show that a graph with maximum degree has an acyclic proper edge coloring with at most colors, whereas the previously known best bound was . The same technique is also applied to improve corresponding bounds for graphs with bounded girth. An interesting aspect of this application is that the probability of the "undesirable" events do not have a uniform upper bound, i.e. it constitutes a case of the asymmetric Lov \'sz Local Lemma.',
	 'authors': u'Ioannis Giotis, Lefteris Kirousis, Kostas I. Psaromiligkos, Dimitrios M. Thilikos,',
	 'category': u'Computer Science ',
	 'date': '2014-7-21',
	 'pdflink': u'http://arxiv.org/pdf/1407.5374',
	 'subjects': u'Discrete Mathematics (cs.DM)',
	 'title': u'\nOn the Algorithmic Lov\xe1sz Local Lemma and Acyclic Edge Coloring',
	 'urllink': u'http://arxiv.org/abs/1407.5374'}
2015-03-23 20:15:48+0000 [xxu461000] INFO: Crawled 268 pages (at 12 pages/min), scraped 251 items (at 12 items/min)
2015-03-23 20:15:51+0000 [xxu461000] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1404.5468> (referer: http://arxiv.org/list/cs/14?skip=4000&show=1000)
2015-03-23 20:15:51+0000 [xxu461000] DEBUG: Scraped from <200 http://arxiv.org/abs/1404.5468>
	{'abstract': u'Intersection graphs are very important in both theoretical as well as application point of view. Depending on the geometrical representation, different type of intersection graphs are defined. Among them interval, circular-arc, permutation, trapezoid, chordal, disk, circle graphs are more important. In this article, a brief introduction of each of these intersection graphs is given. Some basic properties and algorithmic status of few problems on these graphs are cited. This article will help to the beginners to start work in this direction. Since the article contains a lot of information in a compact form it is also useful for the expert researchers too.',
	 'authors': u'Madhumangal Pal,',
	 'category': u'Computer Science ',
	 'date': '2014-4-22',
	 'pdflink': u'http://arxiv.org/pdf/1404.5468',
	 'subjects': u'Discrete Mathematics (cs.DM)',
	 'title': u'\nIntersection Graphs: An Introduction',
	 'urllink': u'http://arxiv.org/abs/1404.5468'}
2015-03-23 20:15:56+0000 [xxu461000] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1405.2409> (referer: http://arxiv.org/list/cs/14?skip=5000&show=1000)
2015-03-23 20:15:56+0000 [xxu461000] DEBUG: Scraped from <200 http://arxiv.org/abs/1405.2409>
	{'abstract': u'G4LTL-ST automatically synthesizes control code for industrial Programmable Logic Controls (PLC) from timed behavioral specifications of input-output signals. These specifications are expressed in a linear temporal logic (LTL) extended with non-linear arithmetic constraints and timing constraints on signals. G4LTL-ST generates code in IEC 61131-3-compatible Structured Text, which is compiled into executable code for a large number of industrial field-level devices. The synthesis algorithm of G4LTL-ST implements pseudo-Boolean abstraction of data constraints and the compilation of timing constraints into LTL, together with a counterstrategy-guided abstraction refinement synthesis loop. Since temporal logic specifications are notoriously difficult to use in practice, G4LTL-ST supports engineers in specifying realizable control problems by suggesting suitable restrictions on the behavior of the control environment from failed synthesis attempts.',
	 'authors': u'Chih-Hong Cheng, Chung-Hao Huang, Harald Ruess, Stefan Stattelmann,',
	 'category': u'Computer Science ',
	 'date': '2014-5-10',
	 'pdflink': u'http://arxiv.org/pdf/1405.2409',
	 'subjects': u'Logic in Computer Science (cs.LO)',
	 'title': u'\nG4LTL-ST: Automatic Generation of PLC Programs',
	 'urllink': u'http://arxiv.org/abs/1405.2409'}
2015-03-23 20:15:59+0000 [xxu461000] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1406.0023> (referer: http://arxiv.org/list/cs/14?skip=6000&show=1000)
2015-03-23 20:15:59+0000 [xxu461000] DEBUG: Scraped from <200 http://arxiv.org/abs/1406.0023>
	{'abstract': u'This paper describes a circle detection method based on Electromagnetism-Like Optimization (EMO). Circle detection has received considerable attention over the last years thanks to its relevance for many computer vision tasks. EMO is a heuristic method for solving complex optimization problems inspired in electromagnetism principles. This algorithm searches a solution based in the attraction and repulsion among prototype candidates. In this paper the detection process is considered to be similar to an optimization problem, the algorithm uses the combination of three edge points (x, y, r) as parameters to determine circles candidates in the scene. An objective function determines if such circle candidates are actually present in the image. The EMO algorithm is used to find the circle candidate that is better related with the real circle present in the image according to the objective function. The final algorithm is a fast circle detector that locates circles with sub-pixel accuracy even considering complicated conditions and noisy images.',
	 'authors': u'Erik Cuevas, Diego Oliva, Daniel Zaldivar, Marco Perez-Cisneros, Humberto Sossa,',
	 'category': u'Computer Science ',
	 'date': '2014-5-30',
	 'pdflink': u'http://arxiv.org/pdf/1406.0023',
	 'subjects': u'Computer Vision and Pattern Recognition (cs.CV)',
	 'title': u'\nCircle detection using electro-magnetism optimization',
	 'urllink': u'http://arxiv.org/abs/1406.0023'}
2015-03-23 20:16:04+0000 [xxu461000] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1407.5373> (referer: http://arxiv.org/list/cs/14?skip=8000&show=1000)
2015-03-23 20:16:04+0000 [xxu461000] DEBUG: Scraped from <200 http://arxiv.org/abs/1407.5373>
	{'abstract': u"We introduce a dynamic mechanism design problem in which the designer wants to offer for sale an item to an agent, and another item to the same agent at some point in the future. The agent's joint distribution of valuations for the two items is known, and the agent knows the valuation for the current item (but not for the one in the future). The designer seeks to maximize expected revenue, and the auction must be deterministic, truthful, and ex post individually rational. The optimum mechanism involves a protocol whereby the seller elicits the buyer's current valuation, and based on the bid makes two take-it-or-leave-it offers, one for now and one for the future. We show that finding the optimum deterministic mechanism in this situation - arguably the simplest meaningful dynamic mechanism design problem imaginable - is NP-hard. We also prove several positive results, among them a polynomial linear programming-based algorithm for the optimum randomized auction (even for many bidders and periods), and we show strong separations in revenue between non-adaptive, adaptive, and randomized auctions, even when the valuations in the two periods are uncorrelated. Finally, for the same problem in an environment in which contracts cannot be enforced, and thus perfection of equilibrium is necessary, we show that the optimum randomized mechanism requires multiple rounds of cheap talk-like interactions.",
	 'authors': u'Christos Papadimitriou, George Pierrakos, Christos-Alexandros Psomas, Aviad Rubinstein,',
	 'category': u'Computer Science ',
	 'date': '2014-7-21',
	 'pdflink': u'http://arxiv.org/pdf/1407.5373',
	 'subjects': u'Computer Science and Game Theory (cs.GT)',
	 'title': u'\nOn the Complexity of Dynamic Mechanism Design',
	 'urllink': u'http://arxiv.org/abs/1407.5373'}
2015-03-23 20:16:09+0000 [xxu461000] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1404.5458> (referer: http://arxiv.org/list/cs/14?skip=4000&show=1000)
2015-03-23 20:16:09+0000 [xxu461000] DEBUG: Scraped from <200 http://arxiv.org/abs/1404.5458>
	{'abstract': u'The "IMP Science Gateway Portal" (this http URL) for complex workflow management and integration of distributed computing resources (like clusters, service grids, desktop grids, clouds) is presented. It is created on the basis of WS-PGRADE and gUSE technologies, where WS-PGRADE is designed for science workflow operation and gUSE - for smooth integration of available resources for parallel and distributed computing in various heterogeneous distributed computing infrastructures (DCI). The typical scientific workflow with possible scenarios of its preparation and usage is considered. Several typical science applications (scientific workflows) are considered for molecular dynamics (MD) simulations of complex behavior of various nanostructures (nanoindentation of graphene layers, defect system relaxation in metal nanocrystals, thermal stability of boron nitride nanotubes, etc.). The advantages and drawbacks of the solution are shortly analyzed in the context of its practical applications for MD simulations in materials science, physics and nanotechnologies with available heterogeneous DCIs.',
	 'authors': u'Yuri Gordienko, Lev Bekenov, Olexandr Gatsenko, Elena Zasimchuk, Valentin Tatarenko,',
	 'category': u'Computer Science ',
	 'date': '2014-4-22',
	 'pdflink': u'http://arxiv.org/pdf/1404.5458',
	 'subjects': u'Computational Engineering, Finance, and Science (cs.CE)',
	 'title': u'\nComplex Workflow Management and Integration of Distributed Computing  Resources by Science Gateway Portal for Molecular Dynamics Simulations in  Materials Science',
	 'urllink': u'http://arxiv.org/abs/1404.5458'}
2015-03-23 20:16:13+0000 [xxu461000] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1405.2407> (referer: http://arxiv.org/list/cs/14?skip=5000&show=1000)
2015-03-23 20:16:13+0000 [xxu461000] DEBUG: Scraped from <200 http://arxiv.org/abs/1405.2407>
	{'abstract': u'The European Holocaust Research Infrastructure (EHRI) has been set up by the European Union to create a sustainable complex of services for researchers. EHRI will bring together information about dispersed collections, based on currently more than 20 partner organisations in 13 countries and many other archives. EHRI, which brings together historians, archivists and specialists in digital humanities, strives to develop innovative on-line tools for finding, researching and sharing knowledge about the Holocaust. While connecting information about Holocaust collections, it strives to create tools and approaches applicable to other digital archival projects. The paper describes its current progress and collaboration across the disciplines involved.',
	 'authors': u'Reto Speck, Tobias Blanke, Cony Kristel, Michal Frankl, Kepa Rodriguez, Veerle Vanden Daelen,',
	 'category': u'Computer Science ',
	 'date': '2014-5-10',
	 'pdflink': u'http://arxiv.org/pdf/1405.2407',
	 'subjects': u'Digital Libraries (cs.DL)',
	 'title': u'\nThe Past and the Future of Holocaust Research: From Disparate Sources to  an Integrated European Holocaust Research Infrastructure',
	 'urllink': u'http://arxiv.org/abs/1405.2407'}
2015-03-23 20:16:19+0000 [xxu461000] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1406.0022> (referer: http://arxiv.org/list/cs/14?skip=6000&show=1000)
2015-03-23 20:16:19+0000 [xxu461000] DEBUG: Scraped from <200 http://arxiv.org/abs/1406.0022>
	{'abstract': u'This paper provides new error bounds on "consistent" reconstruction methods for signals observed from quantized random sensing. Those signal estimation techniques guarantee a perfect matching between the available quantized data and a reobservation of the estimated signal under the same sensing model. Focusing on dithered uniform scalar quantization of resolution , we prove first that, given a random Gaussian frame of with vectors, the worst case -error of consistent signal reconstruction decays with high probability as uniformly for all signals of the unit ball . Up to a log factor, this matches a known lower bound in . Equivalently, with a minimal number of frame coefficients behaving like , any vectors in with identical quantized projections are at most apart with high probability. Second, in the context of Quantized Compressed Sensing with random Gaussian measurements and under the same scalar quantization scheme, consistent reconstructions of -sparse signals of have a worst case error that decreases with high probability as uniformly for all such signals. Finally, we show that the strict consistency condition can be slightly relaxed, e.g., allowing for a bounded level of error in the quantization process, while still guaranteeing a proximity between the original and the estimated signal. In particular, if this quantization error is of order with respect to , similar worst case error decays are reached for reconstruction methods adjusted to such an approximate consistency.',
	 'authors': u'Laurent Jacques,',
	 'category': u'Computer Science ',
	 'date': '2014-5-30',
	 'pdflink': u'http://arxiv.org/pdf/1406.0022',
	 'subjects': u'Information Theory (cs.IT)',
	 'title': u'\nError Decay of (almost) Consistent Signal Estimations from Quantized  Random Gaussian Projections',
	 'urllink': u'http://arxiv.org/abs/1406.0022'}
2015-03-23 20:16:23+0000 [xxu461000] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1407.5367> (referer: http://arxiv.org/list/cs/14?skip=8000&show=1000)
2015-03-23 20:16:23+0000 [xxu461000] DEBUG: Scraped from <200 http://arxiv.org/abs/1407.5367>
	{'abstract': u'Given a set of point correspondences in two images, the existence of a fundamental matrix is a necessary condition for the points to be the images of a 3-dimensional scene imaged with two pinhole cameras. If the camera calibration is known then one requires the existence of an essential matrix. We present an efficient algorithm, using exact linear algebra, for testing the existence of a fundamental matrix. The input is any number of point correspondences. For essential matrices, we characterize the solvability of the Demazure polynomials. In both scenarios, we determine which linear subspaces intersect a fixed set defined by non-linear polynomials. The conditions we derive are polynomials stated purely in terms of image coordinates. They represent a new class of two-view invariants, free of fundamental (resp.~essential)~matrices.',
	 'authors': u'Sameer Agarwal, Hon-leung Lee, Bernd Sturmfels, Rekha R. Thomas,',
	 'category': u'Computer Science ',
	 'date': '2014-7-21',
	 'pdflink': u'http://arxiv.org/pdf/1407.5367',
	 'subjects': u'Computer Vision and Pattern Recognition (cs.CV)',
	 'title': u'\nCertifying the Existence of Epipolar Matrices',
	 'urllink': u'http://arxiv.org/abs/1407.5367'}
2015-03-23 20:16:28+0000 [xxu461000] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1404.5454> (referer: http://arxiv.org/list/cs/14?skip=4000&show=1000)
2015-03-23 20:16:28+0000 [xxu461000] DEBUG: Scraped from <200 http://arxiv.org/abs/1404.5454>
	{'abstract': u'Online services such as web search and e-commerce applications typically rely on the collection of data about users, including details of their activities on the web. Such personal data is used to enhance the quality of service via personalization of content and to maximize revenues via better targeting of advertisements and deeper engagement of users on sites. To date, service providers have largely followed the approach of either requiring or requesting consent for opting-in to share their data. Users may be willing to share private information in return for better quality of service or for incentives, or in return for assurances about the nature and extend of the logging of data. We introduce emph, a new approach to privacy centering on a simple concept: A guarantee is provided to users about the upper-bound on the probability that their personal data will be used. Such a probability, which we refer to as emph, can be assessed by users as a preference or communicated as a policy by a service provider. Service providers can work to personalize and to optimize revenues in accordance with preferences about privacy risk. We present procedures, proofs, and an overall system for maximizing the quality of services, while respecting bounds on allowable or communicated privacy risk. We demonstrate the methodology with a case study and evaluation of the procedures applied to web search personalization. We show how we can achieve near-optimal utility of accessing information with provable guarantees on the probability of sharing data.',
	 'authors': u'Adish Singla, Eric Horvitz, Ece Kamar, Ryen White,',
	 'category': u'Computer Science ',
	 'date': '2014-4-22',
	 'pdflink': u'http://arxiv.org/pdf/1404.5454',
	 'subjects': u'Artificial Intelligence (cs.AI)',
	 'title': u'\nStochastic Privacy',
	 'urllink': u'http://arxiv.org/abs/1404.5454'}
2015-03-23 20:16:30+0000 [xxu461000] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1406.0017> (referer: http://arxiv.org/list/cs/14?skip=6000&show=1000)
2015-03-23 20:16:30+0000 [xxu461000] DEBUG: Scraped from <200 http://arxiv.org/abs/1406.0017>
	{'abstract': u'We relate two complexity notions of bipartite graphs: the minimal weight biclique covering number and the minimal rectifier network size of a bipartite graph . We show that there exist graphs with . As a corollary, we establish that there exist nondeterministic finite automata (NFAs) with -transitions, having transitions total such that the smallest equivalent -free NFA has transitions. We also formulate a version of previous bounds for the weighted set cover problem and discuss its connections to giving upper bounds for the possible blow-up.',
	 'authors': u'Szabolcs Iv\xe1n, \xc1d\xe1m D\xe1niel Lelkes, Judit Nagy-Gy\xf6rgy, Bal\xe1zs Sz\xf6r\xe9nyi, Gy\xf6rgy Tur\xe1n,',
	 'category': u'Computer Science ',
	 'date': '2014-5-30',
	 'pdflink': u'http://arxiv.org/pdf/1406.0017',
	 'subjects': u'Formal Languages and Automata Theory (cs.FL)',
	 'title': u'\nBiclique coverings, rectifier networks and the cost of  $\\varepsilon$-removal',
	 'urllink': u'http://arxiv.org/abs/1406.0017'}
2015-03-23 20:16:36+0000 [xxu461000] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1407.5366> (referer: http://arxiv.org/list/cs/14?skip=8000&show=1000)
2015-03-23 20:16:36+0000 [xxu461000] DEBUG: Scraped from <200 http://arxiv.org/abs/1407.5366>
	{'abstract': u'In this paper, we construct protograph-based spatially coupled low-density parity-check (SC-LDPC) codes by coupling together a series of L disjoint, or uncoupled, LDPC code Tanner graphs into a single coupled chain. By varying L, we obtain a flexible family of code ensembles with varying rates and frame lengths that can share the same encoding and decoding architecture for arbitrary L. We demonstrate that the resulting codes combine the best features of optimized irregular and regular codes in one design: capacity approaching iterative belief propagation (BP) decoding thresholds and linear growth of minimum distance with block length. In particular, we show that, for sufficiently large L, the BP thresholds on both the binary erasure channel (BEC) and the binary-input additive white Gaussian noise channel (AWGNC) saturate to a particular value significantly better than the BP decoding threshold and numerically indistinguishable from the optimal maximum a-posteriori (MAP) decoding threshold of the uncoupled LDPC code. When all variable nodes in the coupled chain have degree greater than two, asymptotically the error probability converges at least doubly exponentially with decoding iterations and we obtain sequences of asymptotically good LDPC codes with fast convergence rates and BP thresholds close to the Shannon limit. Further, the gap to capacity decreases as the density of the graph increases, opening up a new way to construct capacity achieving codes on memoryless binary-input symmetric-output (MBS) channels with low-complexity BP decoding.',
	 'authors': u'David G. M. Mitchell, Michael Lentmaier, Daniel J. Costello Jr,',
	 'category': u'Computer Science ',
	 'date': '2014-7-21',
	 'pdflink': u'http://arxiv.org/pdf/1407.5366',
	 'subjects': u'Information Theory (cs.IT)',
	 'title': u'\nSpatially Coupled LDPC Codes Constructed from Protographs',
	 'urllink': u'http://arxiv.org/abs/1407.5366'}
2015-03-23 20:16:40+0000 [xxu461000] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1404.5453> (referer: http://arxiv.org/list/cs/14?skip=4000&show=1000)
2015-03-23 20:16:40+0000 [xxu461000] DEBUG: Scraped from <200 http://arxiv.org/abs/1404.5453>
	{'abstract': u'We consider multi-player graph games with partial-observation and parity objective. While the decision problem for three-player games with a coalition of the first and second players against the third player is undecidable, we present a decidability result for partial-observation games where the first and third player are in a coalition against the second player, thus where the second player is adversarial but weaker due to partial-observation. We establish tight complexity bounds in the case where player 1 is less informed than player 2, namely 2-EXPTIME-completeness for parity objectives. The symmetric case of player 1 more informed than player 2 is much more complicated, and we show that already in the case where player 1 has perfect observation, memory of size non-elementary is necessary in general for reachability objectives, and the problem is decidable for safety and reachability objectives. Our results have tight connections with partial-observation stochastic games for which we derive new complexity results.',
	 'authors': u'Krishnendu Chatterjee, Laurent Doyen,',
	 'category': u'Computer Science ',
	 'date': '2014-4-22',
	 'pdflink': u'http://arxiv.org/pdf/1404.5453',
	 'subjects': u'Logic in Computer Science (cs.LO)',
	 'title': u'\nGames with a Weak Adversary',
	 'urllink': u'http://arxiv.org/abs/1404.5453'}
2015-03-23 20:16:45+0000 [xxu461000] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1405.2403> (referer: http://arxiv.org/list/cs/14?skip=5000&show=1000)
2015-03-23 20:16:45+0000 [xxu461000] DEBUG: Scraped from <200 http://arxiv.org/abs/1405.2403>
	{'abstract': u'In this paper, we address the issue of hyperspectral pan-sharpening, which consists in fusing a (low spatial resolution) hyperspectral image HX and a (high spatial resolution) panchromatic image P to obtain a high spatial resolution hyperspectral image. The problem is addressed under a variational convex constrained formulation. The objective favors high resolution spectral bands with level lines parallel to those of the panchromatic image. This term is balanced with a total variation term as regularizer. Fit-to-P data and fit-to-HX data constraints are effectively considered as mathematical constraints, which depend on the statistics of the data noise measurements. The developed Alternating Direction Method of Multipliers (ADMM) optimization scheme enables us to solve this problem efficiently despite the non differentiabilities and the huge number of unknowns.',
	 'authors': u'Alexis Huck, Fran\xe7ois de Vieilleville, Pierre Weiss, Manuel Grizonnet,',
	 'category': u'Computer Science ',
	 'date': '2014-5-10',
	 'pdflink': u'http://arxiv.org/pdf/1405.2403',
	 'subjects': u'Computer Vision and Pattern Recognition (cs.CV)',
	 'title': u'\nHyperspectral pan-sharpening: a variational convex constrained  formulation to impose parallel level lines, solved with ADMM',
	 'urllink': u'http://arxiv.org/abs/1405.2403'}
2015-03-23 20:16:48+0000 [xxu461000] INFO: Crawled 281 pages (at 13 pages/min), scraped 264 items (at 13 items/min)
2015-03-23 20:16:51+0000 [xxu461000] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1405.4906> (referer: http://arxiv.org/list/cs/14?skip=6000&show=1000)
2015-03-23 20:16:51+0000 [xxu461000] DEBUG: Scraped from <200 http://arxiv.org/abs/1405.4906>
	{'abstract': u"The preparation of a mechanical oscillator driven by quantum back-action is a fundamental requirement to reach the standard quantum limit (SQL) for force measurement, in optomechanical systems. However, thermal fluctuating force generally dominates a disturbance on the oscillator. In the macroscopic scale, an optical linear cavity including a suspended mirror has been used for the weak force measurement, such as gravitational-wave detectors. This configuration has the advantages of reducing the dissipation of the pendulum (i.e., suspension thermal noise) due to a gravitational dilution by using a thin wire, and of increasing the circulating laser power. However, the use of the thin wire is weak for an optical torsional anti-spring effect in the cavity, due to the low mechanical restoring force of the wire. Thus, there is the trade-off between the stability of the system and the sensitivity. Here, we describe using a triangular optical cavity to overcome this limitation for reaching the SQL. The triangular cavity can provide a sensitive and stable system, because it can optically trap the mirror's motion of the yaw, through an optical positive torsional spring effect. To show this, we demonstrate a measurement of the torsional spring effect caused by radiation pressure forces.",
	 'authors': u'Nobuyuki Matsumoto, Yuta Michimura, Yoichi Aso, Kimio Tsubono,',
	 'category': u'Computer Science ',
	 'date': '2014-5-19',
	 'pdflink': u'http://arxiv.org/pdf/1405.4906',
	 'subjects': u'Optics (physics.optics)',
	 'title': u'\nAn optically trapped mirror for reaching the standard quantum limit',
	 'urllink': u'http://arxiv.org/abs/1405.4906'}
2015-03-23 20:16:56+0000 [xxu461000] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1407.5364> (referer: http://arxiv.org/list/cs/14?skip=8000&show=1000)
2015-03-23 20:16:56+0000 [xxu461000] DEBUG: Scraped from <200 http://arxiv.org/abs/1407.5364>
	{'abstract': u'Quasi-cyclic low-density parity-check (QC-LDPC) codes based on protographs are of great interest to code designers because analysis and implementation are facilitated by the protograph structure and the use of circulant permutation matrices for protograph lifting. However, these restrictions impose undesirable fixed upper limits on important code parameters, such as minimum distance and girth. In this paper, we consider an approach to constructing QC-LDPC codes that uses a two-step lifting procedure based on a protograph, and, by following this method instead of the usual one-step procedure, we obtain improved minimum distance and girth properties. We also present two new design rules for constructing good QC-LDPC codes using this two-step lifting procedure, and in each case we obtain a significant increase in minimum distance and achieve a certain guaranteed girth compared to one-step circulant-based liftings. The expected performance improvement is verified by simulation results.',
	 'authors': u'David G. M. Mitchell, Roxana Smarandache, Daniel J. Costello Jr,',
	 'category': u'Computer Science ',
	 'date': '2014-7-21',
	 'pdflink': u'http://arxiv.org/pdf/1407.5364',
	 'subjects': u'Information Theory (cs.IT)',
	 'title': u'\nQuasi-Cyclic LDPC Codes based on Pre-Lifted Protographs',
	 'urllink': u'http://arxiv.org/abs/1407.5364'}
2015-03-23 20:17:02+0000 [xxu461000] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1404.5448> (referer: http://arxiv.org/list/cs/14?skip=4000&show=1000)
2015-03-23 20:17:02+0000 [xxu461000] DEBUG: Scraped from <200 http://arxiv.org/abs/1404.5448>
	{'abstract': u'A dynamic path network is an undirected path with evacuees situated at each vertex. To evacuate the path, evacuees travel towards a designated sink (doorway) to exit. Each edge has a capacity, the number of evacuees that can enter the edge in unit time. Congestion occurs if an evacuee has to wait at a vertex for other evacuees to leave first. The basic problem is to place k sinks on the line, with an associated evacuation strategy, so as to minimize the total time needed to evacuate everyone. The minmax-regret version introduces uncertainty into the input, with the number of evacuees at vertices only being specified to within a range. The problem is to find a universal solution whose regret (difference from optimal for a given input) is minimized over all legal inputs. The previously best known algorithms for the minmax regret version problem ran in time exponential in k. In this paper, we derive new prop- erties of solutions that yield the first polynomial time algorithms for solving the problem.',
	 'authors': u'Guru Prakash Arumugam, John Augustine, Mordecai J. Golin, Prashanth Srikanthan,',
	 'category': u'Computer Science ',
	 'date': '2014-4-22',
	 'pdflink': u'http://arxiv.org/pdf/1404.5448',
	 'subjects': u'Data Structures and Algorithms (cs.DS)',
	 'title': u'\nA Polynomial Time Algorithm for Minimax-Regret Evacuation on a Dynamic  Path',
	 'urllink': u'http://arxiv.org/abs/1404.5448'}
2015-03-23 20:17:06+0000 [xxu461000] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1405.2387> (referer: http://arxiv.org/list/cs/14?skip=5000&show=1000)
2015-03-23 20:17:06+0000 [xxu461000] DEBUG: Scraped from <200 http://arxiv.org/abs/1405.2387>
	{'abstract': u'In this paper, we consider a multi-cell multi-user MISO broadcast channel. The system operates according to the opportunistic beamforming framework in a multi-cell environment with variable number of transmit beams (may alternatively be referred as the transmission rank) at each base station. The maximum number of co-scheduled users in a cell is equal to its transmission rank, thus increasing it will have the effect of increasing the multiplexing gain. However, this will simultaneously increase the amount of interference in the network, which will decrease the rate of communication. This paper focuses on optimally setting the transmission rank at each base station such that a set of Quality of Service (QoS) constraints, that will ensure a guaranteed minimum rate per beam at each base station, is not violated. Expressions representing the achievable region of transmission ranks are obtained considering different network settings. The achievable transmission rank region consists of all achievable transmission rank tuples that satisfy the QoS constraints. Numerical results are also presented to provide further insights on the feasibility problem.',
	 'authors': u'Meng Wang, Tharaka Samarasinghe, Jamie Evans,',
	 'category': u'Computer Science ',
	 'date': '2014-5-10',
	 'pdflink': u'http://arxiv.org/pdf/1405.2387',
	 'subjects': u'Information Theory (cs.IT)',
	 'title': u'\nTransmission Rank Selection for Opportunistic Beamforming with Quality  of Service Constraints',
	 'urllink': u'http://arxiv.org/abs/1405.2387'}
2015-03-23 20:17:10+0000 [xxu461000] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1405.2846> (referer: http://arxiv.org/list/cs/14?skip=6000&show=1000)
2015-03-23 20:17:10+0000 [xxu461000] DEBUG: Scraped from <200 http://arxiv.org/abs/1405.2846>
	{'abstract': u'Dynamic unary encoding takes unary encoding to the next level. Every n-bit binary string is an encoding of dynamic unary and every n-bit binary string is encodable by dynamic unary. By utilizing both forms of unary code and a single bit of parity information dynamic unary encoding partitions 2^n non-negative integers into n sets of disjoint cycles of n-bit elements. These cycles have been employed as virtual data sets, binary transforms and as a mathematical object. Characterization of both the cycles and of the cycle spectrum is given. Examples of encoding and decoding algorithms are given. Examples of other constructs utilizing the principles of dynamic unary encoding are presented. The cycle as a mathematical object is demonstrated.',
	 'authors': u'Ernst D. Berg,',
	 'category': u'Computer Science ',
	 'date': '2014-5-12',
	 'pdflink': u'http://arxiv.org/pdf/1405.2846',
	 'subjects': u'Information Theory (cs.IT)',
	 'title': u'\nIntroduction to Dynamic Unary Encoding',
	 'urllink': u'http://arxiv.org/abs/1405.2846'}
2015-03-23 20:17:14+0000 [xxu461000] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1407.5358> (referer: http://arxiv.org/list/cs/14?skip=8000&show=1000)
2015-03-23 20:17:14+0000 [xxu461000] DEBUG: Scraped from <200 http://arxiv.org/abs/1407.5358>
	{'abstract': u"Kernel-based reinforcement learning (KBRL) stands out among reinforcement learning algorithms for its strong theoretical guarantees. By casting the learning problem as a local kernel approximation, KBRL provides a way of computing a decision policy which is statistically consistent and converges to a unique solution. Unfortunately, the model constructed by KBRL grows with the number of sample transitions, resulting in a computational cost that precludes its application to large-scale or on-line domains. In this paper we introduce an algorithm that turns KBRL into a practical reinforcement learning tool. Kernel-based stochastic factorization (KBSF) builds on a simple idea: when a transition matrix is represented as the product of two stochastic matrices, one can swap the factors of the multiplication to obtain another transition matrix, potentially much smaller, which retains some fundamental properties of its precursor. KBSF exploits such an insight to compress the information contained in KBRL's model into an approximator of fixed size. This makes it possible to build an approximation that takes into account both the difficulty of the problem and the associated computational cost. KBSF's computational complexity is linear in the number of sample transitions, which is the best one can do without discarding data. Moreover, the algorithm's simple mechanics allow for a fully incremental implementation that makes the amount of memory used independent of the number of sample transitions. The result is a kernel-based reinforcement learning algorithm that can be applied to large-scale problems in both off-line and on-line regimes. We derive upper bounds for the distance between the value functions computed by KBRL and KBSF using the same data. We also illustrate the potential of our algorithm in an extensive empirical study in which KBSF is applied to difficult tasks based on real-world data.",
	 'authors': u'Andr\xe9 M. S. Barreto, Doina Precup, Joelle Pineau,',
	 'category': u'Computer Science ',
	 'date': '2014-7-21',
	 'pdflink': u'http://arxiv.org/pdf/1407.5358',
	 'subjects': u'Learning (cs.LG)',
	 'title': u'\nPractical Kernel-Based Reinforcement Learning',
	 'urllink': u'http://arxiv.org/abs/1407.5358'}
2015-03-23 20:17:18+0000 [xxu461000] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1404.5439> (referer: http://arxiv.org/list/cs/14?skip=4000&show=1000)
2015-03-23 20:17:18+0000 [xxu461000] DEBUG: Scraped from <200 http://arxiv.org/abs/1404.5439>
	{'abstract': u'We propose a novel approach for the formal verification of biological systems based on the use of a modal linear logic. We show how such a logic can be used, with worlds as instants of time, as an unified framework to encode both biological systems and temporal properties of their dynamic behaviour. To illustrate our methodology, we consider a model of the P53/Mdm2 DNA-damage repair mechanism. We prove several properties that are important for such a model to satisfy and serve to illustrate the promise of our approach. We formalize the proofs of these properties in the Coq Proof Assistant, with the help of a Lambda Prolog prover for partial automation of the proofs.',
	 'authors': u'Elisabetta De Maria, Joelle Despeyroux, Amy Felty,',
	 'category': u'Computer Science ',
	 'date': '2014-4-22',
	 'pdflink': u'http://arxiv.org/pdf/1404.5439',
	 'subjects': u'Logic in Computer Science (cs.LO)',
	 'title': u'\nA Logical Framework for Systems Biology',
	 'urllink': u'http://arxiv.org/abs/1404.5439'}
2015-03-23 20:17:23+0000 [xxu461000] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1405.2386> (referer: http://arxiv.org/list/cs/14?skip=5000&show=1000)
2015-03-23 20:17:23+0000 [xxu461000] DEBUG: Scraped from <200 http://arxiv.org/abs/1405.2386>
	{'abstract': u"In today's content-centric Internet, blogs are becoming increasingly popular and important from a data analysis perspective. According to Wikipedia, there were over 156 million public blogs on the Internet as of February 2011. Blogs are a reflection of our contemporary society. The contents of different blog posts are important from social, psychological, economical and political perspectives. Discovery of important topics in the blogosphere is an area which still needs much exploring. We try to come up with a procedure using probabilistic topic modeling and network centrality measures which identifies the central topics in a blog corpus.",
	 'authors': u'Srayan Datta,',
	 'category': u'Computer Science ',
	 'date': '2014-5-10',
	 'pdflink': u'http://arxiv.org/pdf/1405.2386',
	 'subjects': u'Information Retrieval (cs.IR)',
	 'title': u'\nPredicting Central Topics in a Blog Corpus from a Networks Perspective',
	 'urllink': u'http://arxiv.org/abs/1405.2386'}
2015-03-23 20:17:28+0000 [xxu461000] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1407.5355> (referer: http://arxiv.org/list/cs/14?skip=8000&show=1000)
2015-03-23 20:17:28+0000 [xxu461000] DEBUG: Scraped from <200 http://arxiv.org/abs/1407.5355>
	{'abstract': u"In this paper, we address the problem of secure wireless information and power transfer in a large-scale multiple-input multiple-output (LS-MIMO) amplify-and-forward (AF) relaying system. The advantage of LS-MIMO relay is exploited to enhance wireless security, transmission rate and energy efficiency. In particular, the challenging issues incurred by short interception distance and long transfer distance are well addressed simultaneously. Under very practical assumptions, i.e., no eavesdropper's channel state information (CSI) and imperfect legitimate channel CSI, this paper investigates the impact of imperfect CSI, and obtains an explicit expression of the secrecy outage capacity in terms of transmit power and channel condition. Then, we propose an optimal power splitting scheme at the relay to maximize the secrecy outage capacity. Finally, our theoretical claims are validated by simulation results.",
	 'authors': u'Xiaoming Chen, Jian Chen, Tao Liu,',
	 'category': u'Computer Science ',
	 'date': '2014-7-21',
	 'pdflink': u'http://arxiv.org/pdf/1407.5355',
	 'subjects': u'Information Theory (cs.IT)',
	 'title': u'\nSecure Wireless Information and Power Transfer in Large-Scale MIMO  Relaying Systems with Imperfect CSI',
	 'urllink': u'http://arxiv.org/abs/1407.5355'}
2015-03-23 20:17:34+0000 [xxu461000] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1404.5433> (referer: http://arxiv.org/list/cs/14?skip=4000&show=1000)
2015-03-23 20:17:34+0000 [xxu461000] DEBUG: Scraped from <200 http://arxiv.org/abs/1404.5433>
	{'abstract': u"We study voting games on possibly interconnected issues, where voters might hold a principled opinion about a subset of the issues at stake while willing to strike deals on the remaining ones, and can influence one another before casting their ballots in order to obtain an individually more favourable outcome. We analyse voters' rational behaviour in a two-phase game, allowing players to undergo a negotiation phase before their vote, and showing under what conditions undesirable equilibria can be removed as an effect of the pre-vote phase.",
	 'authors': u'Umberto Grandi, Davide Grossi, Paolo Turrini,',
	 'category': u'Computer Science ',
	 'date': '2014-4-22',
	 'pdflink': u'http://arxiv.org/pdf/1404.5433',
	 'subjects': u'Computer Science and Game Theory (cs.GT)',
	 'title': u'\nPre-vote negotiations and binary voting with constraints',
	 'urllink': u'http://arxiv.org/abs/1404.5433'}
2015-03-23 20:17:40+0000 [xxu461000] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1405.2378> (referer: http://arxiv.org/list/cs/14?skip=5000&show=1000)
2015-03-23 20:17:40+0000 [xxu461000] DEBUG: Scraped from <200 http://arxiv.org/abs/1405.2378>
	{'abstract': u"Can folding a piece of paper flat make it larger? We explore whether a shape must be scaled to cover a flat-folded copy of itself. We consider both single folds and arbitrary folds (continuous piecewise isometries ). The underlying problem is motivated by computational origami, and is related to other covering and fixturing problems, such as Lebesgue's universal cover problem and force closure grasps. In addition to considering special shapes (squares, equilateral triangles, polygons and disks), we give upper and lower bounds on scale factors for single folds of convex objects and arbitrary folds of simply connected objects.",
	 'authors': u'Oswin Aichholzer, Greg Aloupis, Erik D. Demaine, Martin L. Demaine, S\xe1ndor P. Fekete, Michael Hoffmann, Anna Lubiw, Jack Snoeyink, Andrew Winslow,',
	 'category': u'Computer Science ',
	 'date': '2014-5-10',
	 'pdflink': u'http://arxiv.org/pdf/1405.2378',
	 'subjects': u'Computational Geometry (cs.CG)',
	 'title': u'\nCovering Folded Shapes',
	 'urllink': u'http://arxiv.org/abs/1405.2378'}
2015-03-23 20:17:43+0000 [xxu461000] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1405.6095> (referer: http://arxiv.org/list/cs/14?skip=6000&show=1000)
2015-03-23 20:17:43+0000 [xxu461000] DEBUG: Scraped from <200 http://arxiv.org/abs/1405.6095>
	{'abstract': u'Zipper logic is a graph rewrite system, consisting in only local rewrites on a class of zipper graphs. Connections with the chemlambda artificial chemistry and with knot diagrammatics based computation are explored in the article.',
	 'authors': u'Marius Buliga,',
	 'category': u'Computer Science ',
	 'date': '2014-5-20',
	 'pdflink': u'http://arxiv.org/pdf/1405.6095',
	 'subjects': u'Combinatorics (math.CO)',
	 'title': u'\nZipper logic',
	 'urllink': u'http://arxiv.org/abs/1405.6095'}
2015-03-23 20:17:48+0000 [xxu461000] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1407.5336> (referer: http://arxiv.org/list/cs/14?skip=8000&show=1000)
2015-03-23 20:17:48+0000 [xxu461000] DEBUG: Scraped from <200 http://arxiv.org/abs/1407.5336>
	{'abstract': u'The Grundy number of a graph is the maximum number of colors used by the greedy coloring algorithm over all vertex orderings. In this paper, we study the computational complexity of GRUNDY COLORING, the problem of determining whether a given graph has Grundy number at least . We show that GRUNDY COLORING can be solved in time on graphs of order . While the problem is known to be solvable in time for graphs of treewidth , we prove that under the Exponential Time Hypothesis, it cannot be computed in time , for any constant . We also study the parameterized complexity of GRUNDY COLORING parameterized by the number of colors, showing that it is in FPT for graphs including chordal graphs, claw-free graphs, and graphs excluding a fixed minor. Finally, we consider two previously studied variants of GRUNDY COLORING, namely WEAK GRUNDY COLORING and CONNECTED GRUNDY COLORING. We show that WEAK GRUNDY COLORING is fixed-parameter tractable with respect to the weak Grundy number. In stark contrast, it turns out that checking whether a given graph has connected Grundy number at least is NP-complete already for .',
	 'authors': u'Edouard Bonnet, Florent Foucaud, Eun Jung Kim, Florian Sikora,',
	 'category': u'Computer Science ',
	 'date': '2014-7-20',
	 'pdflink': u'http://arxiv.org/pdf/1407.5336',
	 'subjects': u'Data Structures and Algorithms (cs.DS)',
	 'title': u'\nComplexity of Grundy coloring and its variants',
	 'urllink': u'http://arxiv.org/abs/1407.5336'}
2015-03-23 20:17:48+0000 [xxu461000] INFO: Crawled 294 pages (at 13 pages/min), scraped 277 items (at 13 items/min)
2015-03-23 20:17:54+0000 [xxu461000] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1404.5432> (referer: http://arxiv.org/list/cs/14?skip=4000&show=1000)
2015-03-23 20:17:54+0000 [xxu461000] DEBUG: Scraped from <200 http://arxiv.org/abs/1404.5432>
	{'abstract': u'We study the kernelizability of a class of NP-hard graph modification problems based on vertex degree properties. Our main positive results refer to NP-hard graph completion (that is, edge addition) cases while we show that there is no hope to achieve analogous results for the corresponding vertex or edge deletion versions. Our algorithms are based on a method that transforms graph completion problems into efficiently solvable number problems and exploits f-factor computations for translating the results back into the graph setting. Indeed, our core observation is that we encounter a win-win situation in the sense that either the number of edge additions is small (and thus faster to find) or the problem is polynomial- time solvable. This approach helps in answering an open question by Mathieson and Szeider [JCSS 2012].',
	 'authors': u'Vincent Froese, Andr\xe9 Nichterlein, Rolf Niedermeier,',
	 'category': u'Computer Science ',
	 'date': '2014-4-22',
	 'pdflink': u'http://arxiv.org/pdf/1404.5432',
	 'subjects': u'Discrete Mathematics (cs.DM)',
	 'title': u'\nWin-Win Kernelization for Degree Sequence Completion Problems',
	 'urllink': u'http://arxiv.org/abs/1404.5432'}
2015-03-23 20:17:58+0000 [xxu461000] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1405.2376> (referer: http://arxiv.org/list/cs/14?skip=5000&show=1000)
2015-03-23 20:17:58+0000 [xxu461000] DEBUG: Scraped from <200 http://arxiv.org/abs/1405.2376>
	{'abstract': u'Information flow analysis has largely ignored the setting where the analyst has neither control over nor a complete model of the analyzed system. We formalize such limited information flow analyses and study an instance of it: detecting the usage of data by websites. We prove that these problems are ones of causal inference. Leveraging this connection, we push beyond traditional information flow analysis to provide a systematic methodology based on experimental science and statistical analysis. Our methodology allows us to systematize prior works in the area viewing them as instances of a general approach. Our systematic study leads to practical advice for improving work on detecting data usage, a previously unformalized area. We illustrate these concepts with a series of experiments collecting data on the use of information by websites, which we statistically analyze.',
	 'authors': u'Michael Carl Tschantz, Amit Datta, Anupam Datta, Jeannette M. Wing,',
	 'category': u'Computer Science ',
	 'date': '2014-5-10',
	 'pdflink': u'http://arxiv.org/pdf/1405.2376',
	 'subjects': u'Cryptography and Security (cs.CR)',
	 'title': u'\nA Methodology for Information Flow Experiments',
	 'urllink': u'http://arxiv.org/abs/1405.2376'}
2015-03-23 20:18:04+0000 [xxu461000] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1405.7921> (referer: http://arxiv.org/list/cs/14?skip=6000&show=1000)
2015-03-23 20:18:04+0000 [xxu461000] DEBUG: Scraped from <200 http://arxiv.org/abs/1405.7921>
	{'abstract': u'In this paper we investigate when a parameterized controller, designed for a plant depending on unknown parameters, admits a realization which is independent of the parameters. It is argued that adaptation is unnecessary for this class of parameterized controllers. We prove that standard model reference controllers (state and output--feedback) for linear time invariant systems with a filter at the plant input admit a parameter independent realization. Although the addition of such a filter is of questionable interest, our result formally, and unquestionably, establishes the deleterious effect of such a modification, which has been widely publicized in the control literature under the name L1-adaptive control.',
	 'authors': u'Romeo Ortega, Elena Panteley,',
	 'category': u'Computer Science ',
	 'date': '2014-5-30',
	 'pdflink': u'http://arxiv.org/pdf/1405.7921',
	 'subjects': u'Optimization and Control (math.OC)',
	 'title': u'\nWhen is a Parameterized Controller Suitable for Adaptive Control?',
	 'urllink': u'http://arxiv.org/abs/1405.7921'}
2015-03-23 20:18:09+0000 [xxu461000] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1407.5327> (referer: http://arxiv.org/list/cs/14?skip=8000&show=1000)
2015-03-23 20:18:09+0000 [xxu461000] DEBUG: Scraped from <200 http://arxiv.org/abs/1407.5327>
	{'abstract': u'Significant research has been carried out recently to find the optimal path in network routing. Among them, the evolutionary algorithm approach is an area where work is carried out extensively. We in this paper have used particle swarm optimization (PSO) and genetic algorithm (GA) for finding the optimal path and the concept of region based network is introduced along with the use of indirect encoding. We demonstrate the advantage of fitness value and hop count in both PSO and GA. A comparative study of PSO and genetic algorithm (GA) is carried out, and it was found that PSO converged to arrive at the optimal path much faster than GA.',
	 'authors': u'Kavitha Sooda, T. R. Gopalakrishnan Nair,',
	 'category': u'Computer Science ',
	 'date': '2014-7-20',
	 'pdflink': u'http://arxiv.org/pdf/1407.5327',
	 'subjects': u'Networking and Internet Architecture (cs.NI)',
	 'title': u'\nA Comparative Analysis for Determining the Optimal Path using PSO and GA',
	 'urllink': u'http://arxiv.org/abs/1407.5327'}
2015-03-23 20:18:14+0000 [xxu461000] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1404.5428> (referer: http://arxiv.org/list/cs/14?skip=4000&show=1000)
2015-03-23 20:18:14+0000 [xxu461000] DEBUG: Scraped from <200 http://arxiv.org/abs/1404.5428>
	{'abstract': u'We consider the problem of minimizing the total cost to run a sequence of tasks in the given order by agents under the positional cost model. The cost to run a task not only depends on the intrinsic cost of the task itself, but also monotonically related to the position this task is in the working list of the agent assigned. Such a positional effect can naturally arise from the classic sum-of-completion-time minimization problems, and is also well motivated by the varying efficiency when an agent works in reality (such as due to the learning effects or deteriorating effects). Also, it can be seen as a deterministic variant of the classic Baysian sequential decision making problems. This paper presents a simple and practical algorithm that runs in time and minimizes the total cost of any problem instance consisting of two task types. The algorithm works by making greedy decision for each task sequentially based on some stopping thresholds in a "greedy-like" allocation simulation -- a working style coinciding with Gittins\' optimal-stopping based algorithm for the classic Baysian multi-armed bandit problem.',
	 'authors': u'Bojun Huang,',
	 'category': u'Computer Science ',
	 'date': '2014-4-22',
	 'pdflink': u'http://arxiv.org/pdf/1404.5428',
	 'subjects': u'Data Structures and Algorithms (cs.DS)',
	 'title': u'\nSequential Resource Allocation with Positional Costs',
	 'urllink': u'http://arxiv.org/abs/1404.5428'}
2015-03-23 20:18:16+0000 [xxu461000] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1405.2363> (referer: http://arxiv.org/list/cs/14?skip=5000&show=1000)
2015-03-23 20:18:16+0000 [xxu461000] DEBUG: Scraped from <200 http://arxiv.org/abs/1405.2363>
	{'abstract': u'Sampled-data (SD) systems, which are composed of both discrete- and continuous-time components, are arguably one of the most common classes of cyberphysical systems in practice; most modern controllers are implemented on digital platforms while the plant dynamics that are being controlled evolve continuously in time. As with all cyberphysical systems, ensuring hard constraint satisfaction is key in the safe operation of SD systems. A powerful analytical tool for guaranteeing such constraint satisfaction is the viability kernel: the set of all initial conditions for which a safety-preserving control law (that is, a control law that satisfies all input and state constraints) exists. In this paper we present a novel sampling-based algorithm that tightly approximates the viability kernel for high-dimensional sampled-data linear time-invariant (LTI) systems. Unlike prior work in this area, our algorithm formally handles both the discrete and continuous characteristics of SD systems. We prove the correctness and convergence of our approximation technique, provide discussions on heuristic methods to optimally bias the sampling process, and demonstrate the results on a twelve-dimensional flight envelope protection problem.',
	 'authors': u'Shahab Kaynama, Jeremy H. Gillula, Claire J. Tomlin,',
	 'category': u'Computer Science ',
	 'date': '2014-5-9',
	 'pdflink': u'http://arxiv.org/pdf/1405.2363',
	 'subjects': u'Systems and Control (cs.SY)',
	 'title': u'\nA sampling-based approach to scalable constraint satisfaction in linear  sampled-data systems---Part I: Computation',
	 'urllink': u'http://arxiv.org/abs/1405.2363'}
2015-03-23 20:18:22+0000 [xxu461000] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1405.7874> (referer: http://arxiv.org/list/cs/14?skip=6000&show=1000)
2015-03-23 20:18:22+0000 [xxu461000] DEBUG: Scraped from <200 http://arxiv.org/abs/1405.7874>
	{'abstract': u'A CIS graph is a graph in which every maximal stable set and every maximal clique intersect. A graph is well-covered if all its maximal stable sets are of the same size, co-well-covered if its complement is well-covered, and vertex-transitive if, for every pair of vertices, there exists an automorphism of the graph mapping one to the other. We show that a vertex-transitive graph is CIS if and only if it is well-covered, co-well-covered, and the product of its clique and stability numbers equals its order. A graph is irreducible if no two distinct vertices have the same neighborhood. We classify irreducible well-covered CIS graphs with clique number at most 3 and vertex-transitive CIS graphs of valency at most 7, which include an infinite family. We also exhibit an infinite family of vertex-transitive CIS graphs which are not Cayley.',
	 'authors': u'Edward Dobson, Ademir Hujdurovi\u0107, Martin Milani\u010d, Gabriel Verret,',
	 'category': u'Computer Science ',
	 'date': '2014-5-30',
	 'pdflink': u'http://arxiv.org/pdf/1405.7874',
	 'subjects': u'Combinatorics (math.CO)',
	 'title': u'\nVertex-transitive CIS graphs',
	 'urllink': u'http://arxiv.org/abs/1405.7874'}
2015-03-23 20:18:28+0000 [xxu461000] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1407.5324> (referer: http://arxiv.org/list/cs/14?skip=8000&show=1000)
2015-03-23 20:18:28+0000 [xxu461000] DEBUG: Scraped from <200 http://arxiv.org/abs/1407.5324>
	{'abstract': u'Road sign recognition is one of the core technologies in Intelligent Transport Systems. In the current study, a robust and real-time method is presented to identify and detect the roads speed signs in road image in different situations. In our proposed method, first, the connected components are created in the main image using the edge detection and mathematical morphology and the location of the road signs extracted by the geometric and color data; then the letters are segmented and recognized by Multiclass Support Vector Machine (SVMs) classifiers. Regarding that the geometric and color features ate properly used in detection the location of the road signs, so it is not sensitive to the distance and noise and has higher speed and efficiency. In the result part, the proposed approach is applied on Iranian road speed sign database and the detection and recognition accuracy rate achieved 98.66% and 100% respectively.',
	 'authors': u'Reza Azad, Babak Azad, Iman Tavakoli Kazerooni,',
	 'category': u'Computer Science ',
	 'date': '2014-7-20',
	 'pdflink': u'http://arxiv.org/pdf/1407.5324',
	 'subjects': u'Computer Vision and Pattern Recognition (cs.CV)',
	 'title': u'\nOptimized Method for Iranian Road Signs Detection and recognition system',
	 'urllink': u'http://arxiv.org/abs/1407.5324'}
2015-03-23 20:18:32+0000 [xxu461000] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1404.5424> (referer: http://arxiv.org/list/cs/14?skip=4000&show=1000)
2015-03-23 20:18:32+0000 [xxu461000] DEBUG: Scraped from <200 http://arxiv.org/abs/1404.5424>
	{'abstract': u'In the paper "The complexity of mean flow time scheduling problems with release times", by Baptiste, Brucker, Chrobak, D "urr, Kravchenko and Sourd, the authors claimed to prove strong NP-hardness of the scheduling problem , namely multiprocessor preemptive scheduling where the objective is to minimize the mean flow time. We point out a serious error in their proof and give a new proof of strong NP-hardness for this problem.',
	 'authors': u'Odile Bellenguez-Morineau, Marek Chrobak, Christoph D\xfcrr, Damien Prot,',
	 'category': u'Computer Science ',
	 'date': '2014-4-22',
	 'pdflink': u'http://arxiv.org/pdf/1404.5424',
	 'subjects': u'Computational Complexity (cs.CC)',
	 'title': u'\nA Note on NP-Hardness of Preemptive Mean Flow-Time Scheduling for  Parallel Machines',
	 'urllink': u'http://arxiv.org/abs/1404.5424'}
2015-03-23 20:18:38+0000 [xxu461000] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1405.2362> (referer: http://arxiv.org/list/cs/14?skip=5000&show=1000)
2015-03-23 20:18:38+0000 [xxu461000] DEBUG: Scraped from <200 http://arxiv.org/abs/1405.2362>
	{'abstract': u'Synchronization of coupled oscillators is observed at multiple levels of neural systems, and has been shown to play an important function in visual perception. We propose a computing system based on locally coupled oscillator networks for image segmentation. The system can serve as the preprocessing front-end of an image processing pipeline where the common frequencies of clusters of oscillators reflect the segmentation results. To demonstrate the feasibility of our design, the system is simulated and tested on a human face image dataset and its performance is compared with traditional intensity threshold based algorithms. Our system shows both better performance and higher noise tolerance than traditional methods.',
	 'authors': u'Yan Fang, Matthew J. Cotter, Donald M. Chiarulli, Steven P. Levitan,',
	 'category': u'Computer Science ',
	 'date': '2014-5-9',
	 'pdflink': u'http://arxiv.org/pdf/1405.2362',
	 'subjects': u'Computer Vision and Pattern Recognition (cs.CV)',
	 'title': u'\nImage Segmentation Using Frequency Locking of Coupled Oscillators',
	 'urllink': u'http://arxiv.org/abs/1405.2362'}
2015-03-23 20:18:43+0000 [xxu461000] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1405.7832> (referer: http://arxiv.org/list/cs/14?skip=6000&show=1000)
2015-03-23 20:18:43+0000 [xxu461000] DEBUG: Scraped from <200 http://arxiv.org/abs/1405.7832>
	{'abstract': u'As the number of scientific journals has multiplied, journal rankings have become increasingly important for scientific decisions. From submissions and subscriptions to grants and hirings, researchers, policy makers, and funding agencies make important decisions with influence from journal rankings such as the ISI journal impact factor. Typically, the rankings are derived from the citation network between a selection of journals and unavoidably depend on this selection. However, little is known about how robust rankings are to the selection of included journals. Here we compare the robustness of three journal rankings based on network flows induced on citation networks. They model pathways of researchers navigating scholarly literature, stepping between journals and remembering their previous steps to different degree: zero-step memory as impact factor, one-step memory as Eigenfactor, and two-step memory, corresponding to zero-, first-, and second-order Markov models of citation flow between journals. We conclude that a second-order Markov model is slightly more robust, because it combines the advantages of the lower-order models: perturbations that remain local and citation weights that depend on journal importance. However, the robustness gain comes at the cost of requiring more data, because the second-order Markov model requires citation data from twice as long a period.',
	 'authors': u'Ludvig Bohlin, Alcides Viamontes Esquivel, Andrea Lancichinetti, Martin Rosvall,',
	 'category': u'Computer Science ',
	 'date': '2014-5-30',
	 'pdflink': u'http://arxiv.org/pdf/1405.7832',
	 'subjects': u'Physics and Society (physics.soc-ph)',
	 'title': u'\nRobustness of journal rankings by network flows with different amounts  of memory',
	 'urllink': u'http://arxiv.org/abs/1405.7832'}
2015-03-23 20:18:48+0000 [xxu461000] INFO: Crawled 305 pages (at 11 pages/min), scraped 288 items (at 11 items/min)
2015-03-23 20:18:49+0000 [xxu461000] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1407.5323> (referer: http://arxiv.org/list/cs/14?skip=8000&show=1000)
2015-03-23 20:18:49+0000 [xxu461000] DEBUG: Scraped from <200 http://arxiv.org/abs/1407.5323>
	{'abstract': u'Project Management process plays a significant role in effective development of software projects. Key challenges in the project management process are the estimation of time, cost, defect count, and subsequently selection of apt developers. Therefore precise estimation of above stated factors decides the success level of a project. This paper provides an empirical study of several projects developed in a service oriented software company in order to comprehend the project management process. The analysis throws light on the existence of variation in the aforementioned factors between estimation and observed results. It further captures the need for betterment of project management process in estimation and allocation of resources in the realization of high quality software product. The paper therefore aims to bring in an improved awareness in software engineering personnel concerning the magnitude and significance of better estimation and accurate allocation of resources for developing successful project.',
	 'authors': u'Shashikumar N.R., T.R. Gopalakrishnan Nair, Suma V,',
	 'category': u'Computer Science ',
	 'date': '2014-7-20',
	 'pdflink': u'http://arxiv.org/pdf/1407.5323',
	 'subjects': u'Software Engineering (cs.SE)',
	 'title': u'\nA Parametric Analysis of Project Management Performance to Enhance  Software Development Process',
	 'urllink': u'http://arxiv.org/abs/1407.5323'}
2015-03-23 20:18:55+0000 [xxu461000] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1404.5421> (referer: http://arxiv.org/list/cs/14?skip=4000&show=1000)
2015-03-23 20:18:55+0000 [xxu461000] DEBUG: Scraped from <200 http://arxiv.org/abs/1404.5421>
	{'abstract': u'We consider the problem of multiple users targeting the arms of a single multi-armed stochastic bandit. The motivation for this problem comes from cognitive radio networks, where selfish users need to coexist without any side communication between them, implicit cooperation or common control. Even the number of users may be unknown and can vary as users join or leave the network. We propose an algorithm that combines an -greedy learning rule with a collision avoidance mechanism. We analyze its regret with respect to the system-wide optimum and show that sub-linear regret can be obtained in this setting. Experiments show dramatic improvement compared to other algorithms for this setting.',
	 'authors': u'Orly Avner, Shie Mannor,',
	 'category': u'Computer Science ',
	 'date': '2014-4-22',
	 'pdflink': u'http://arxiv.org/pdf/1404.5421',
	 'subjects': u'Learning (cs.LG)',
	 'title': u'\nConcurrent bandits and cognitive radio networks',
	 'urllink': u'http://arxiv.org/abs/1404.5421'}
2015-03-23 20:19:00+0000 [xxu461000] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1405.2349> (referer: http://arxiv.org/list/cs/14?skip=5000&show=1000)
2015-03-23 20:19:00+0000 [xxu461000] DEBUG: Scraped from <200 http://arxiv.org/abs/1405.2349>
	{'abstract': u'We study generalisations of a simple, combinatorial proof of a Chernoff bound similar to the one by Impagliazzo and Kabanets (RANDOM, 2010). In particular, we prove a randomized version of the hitting property of expander random walks and apply it to obtain a concentration bound for expander random walks which is essentially optimal for small deviations and a large number of steps. At the same time, we present a simpler proof that still yields a "right" bound settling a question asked by Impagliazzo and Kabanets. Next, we obtain a simple upper tail bound for polynomials with input variables in which are not necessarily independent, but obey a certain condition inspired by Impagliazzo and Kabanets. The resulting bound is used by Holenstein and Sinha (FOCS, 2012) in the proof of a lower bound for the number of calls in a black-box construction of a pseudorandom generator from a one-way function. We then show that the same technique yields the upper tail bound for the number of copies of a fixed graph in an Erd Hs-R \'enyi random graph, matching the one given by Janson, Oleszkiewicz and Ruci \'nski (Israel J. Math, 2002).',
	 'authors': u'Jan H\u0105z\u0142a, Thomas Holenstein,',
	 'category': u'Computer Science ',
	 'date': '2014-5-9',
	 'pdflink': u'http://arxiv.org/pdf/1405.2349',
	 'subjects': u'Discrete Mathematics (cs.DM)',
	 'title': u'\nUpper Tail Estimates with Combinatorial Proofs',
	 'urllink': u'http://arxiv.org/abs/1405.2349'}
2015-03-23 20:19:04+0000 [xxu461000] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1405.7811> (referer: http://arxiv.org/list/cs/14?skip=6000&show=1000)
2015-03-23 20:19:04+0000 [xxu461000] DEBUG: Scraped from <200 http://arxiv.org/abs/1405.7811>
	{'abstract': u'We study an opinion formation model by the means of a co-evolving complex network where the vertices represent the individuals, characterised by their evolving opinions, and the edges represent the interactions among them. The network adapts to the spreading of opinions in two ways: not only connected agents interact and eventually change their thinking but an agent may also rewire one of its links to a neighborhood holding the same opinion as his. The dynamics depends on an external parameter , which controls the plasticity of the network. We show how the information entropy associated to the distribution of group sizes, allows to locate the phase transition between full consensus and a society where different opinions coexist. We also determine the minimum size of the most informative sampling. At the transition the distribution of the sizes of groups holding the same opinion is scale free.',
	 'authors': u'Enrique Burgos, Laura Hernandez, Horacio Ceva, Roberto P. J. Perazzo,',
	 'category': u'Computer Science ',
	 'date': '2014-5-30',
	 'pdflink': u'http://arxiv.org/pdf/1405.7811',
	 'subjects': u'Physics and Society (physics.soc-ph)',
	 'title': u'\nEntropic determination of the phase transition in a coevolving  opinion-formation model',
	 'urllink': u'http://arxiv.org/abs/1405.7811'}
2015-03-23 20:19:10+0000 [xxu461000] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1407.5320> (referer: http://arxiv.org/list/cs/14?skip=8000&show=1000)
2015-03-23 20:19:10+0000 [xxu461000] DEBUG: Scraped from <200 http://arxiv.org/abs/1407.5320>
	{'abstract': u'Realizing an optimal task scheduling by taking into account the business importance of jobs has become a matter of interest in pay and use model of Cloud computing. Introduction of an appropriate model for an efficient task scheduling technique could derive benefit to the service providers as well as clients. In this paper, we have addressed two major challenges which has implications on the performance of the Cloud system. One of the major issues is handling technical aspects of distributing the tasks for targeted gains and the second issue is related to the handling of the business priority for concurrently resolving business complexity related to cloud consumers. A coordinated scheduling can be achieved by considering the weightage of both aspects viz. technical requirements and business requirements appropriately. It can be done in such a way that it meets the QoS requirements of technical domain as well as business domain. Along with the technical priority a business Bp is required in creating a resultant priority which could be given to stages of further processing, like task allocation and arbitration schemes. Here we consider a technical priority Tp that is governed by a semi-adaptive scheduling algorithm whereas the resultant priority is derived in which a Business Priority Bp layer encapsulates the Technical Priority Tp to achieve the overall priority of the incoming tasks. It results in a Hybrid priority creation, which is a combination of both technical priority Tp and business priority Bp. By taking into account the business priority of the jobs it is possible to achieve a higher service level satisfaction for the tasks which are submitted with their native technical priority. With this approach the waiting time of the tasks tends to get reduced and it gives a better service level satisfaction.',
	 'authors': u'Vivek Sharma, T. R. Gopalakrishnan Nair,',
	 'category': u'Computer Science ',
	 'date': '2014-7-20',
	 'pdflink': u'http://arxiv.org/pdf/1407.5320',
	 'subjects': u'Distributed, Parallel, and Cluster Computing (cs.DC)',
	 'title': u'\nAn Optimum Scheduling Approach for Creating Optimal Priority of Jobs  with Business Values in Cloud Computing',
	 'urllink': u'http://arxiv.org/abs/1407.5320'}
2015-03-23 20:19:13+0000 [xxu461000] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1404.5412> (referer: http://arxiv.org/list/cs/14?skip=4000&show=1000)
2015-03-23 20:19:13+0000 [xxu461000] DEBUG: Scraped from <200 http://arxiv.org/abs/1404.5412>
	{'abstract': u'In this paper, analytical assessment of overlay-inband device-to-device (D2D) communications is investigated, under cellular-network-assisted (coordinated) scheduling. To this end, a simple scheduling scheme is assumed that takes into account only local (per cell) topological information of the D2D links. Stochastic geometry tools are utilized in order to obtain analytical expressions for the interferers density as well as the D2D link signal-to-interference-ratio distribution. The analytical results accuracy is validated by comparison with simulations. In addition, the analytical expressions are employed for efficiently optimizing the parameters of a cellular system with overlay D2D communications. It is shown that coordinated scheduling of D2D transmissions enhances system performance both in terms of average user rate as well as maximum allowable D2D link distance.',
	 'authors': u'Stelios Stefanatos, Antonis G. Gotsis, Angeliki Alexiou,',
	 'category': u'Computer Science ',
	 'date': '2014-4-22',
	 'pdflink': u'http://arxiv.org/pdf/1404.5412',
	 'subjects': u'Information Theory (cs.IT)',
	 'title': u'\nAnalytical Assessment of Coordinated Overlay D2D Communications',
	 'urllink': u'http://arxiv.org/abs/1404.5412'}
2015-03-23 20:19:17+0000 [xxu461000] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1405.2330> (referer: http://arxiv.org/list/cs/14?skip=5000&show=1000)
2015-03-23 20:19:17+0000 [xxu461000] DEBUG: Scraped from <200 http://arxiv.org/abs/1405.2330>
	{'abstract': u'We examine how well various HTTP methods are supported by public web services. We sample 40,870 live URIs from the DMOZ collection (a curated directory of World Wide Web URIs) and found that about 55% URIs claim support (in the Allow header) for GET and POST methods, but less than 2% of the URIs claim support for one or more of PUT, PATCH, or DELETE methods.',
	 'authors': u'Sawood Alam, Charles L. Cartledge, Michael L. Nelson,',
	 'category': u'Computer Science ',
	 'date': '2014-5-8',
	 'pdflink': u'http://arxiv.org/pdf/1405.2330',
	 'subjects': u'Networking and Internet Architecture (cs.NI)',
	 'title': u'\nSupport for Various HTTP Methods on the Web',
	 'urllink': u'http://arxiv.org/abs/1405.2330'}
2015-03-23 20:19:20+0000 [xxu461000] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1405.7786> (referer: http://arxiv.org/list/cs/14?skip=6000&show=1000)
2015-03-23 20:19:20+0000 [xxu461000] DEBUG: Scraped from <200 http://arxiv.org/abs/1405.7786>
	{'abstract': u'We review and introduce new representations of tensor train decompositions for large-scale vectors, matrices, or low-order tensors. We provide extended definitions of mathematical multilinear operations such as Kronecker, Hadamard, and contracted products, with their properties for tensor calculus. Then we introduce an effective low-rank tensor approximation technique called the tensor train (TT) format with a number of mathematical and graphical representations. We also provide a brief review of mathematical properties of the TT format as a low-rank approximation technique. With the aim of breaking the curse-of-dimensionality in large-scale numerical analysis, we describe basic operations on large-scale vectors and matrices in TT format. The suggested representations can be used for describing numerical methods based on the TT format for solving large-scale optimization problems such as the system of linear equations and eigenvalue problems.',
	 'authors': u'Namgil Lee, Andrzej Cichocki,',
	 'category': u'Computer Science ',
	 'date': '2014-5-30',
	 'pdflink': u'http://arxiv.org/pdf/1405.7786',
	 'subjects': u'Numerical Analysis (math.NA)',
	 'title': u'\nFundamental Tensor Operations for Large-Scale Data Analysis in Tensor  Train Formats',
	 'urllink': u'http://arxiv.org/abs/1405.7786'}
2015-03-23 20:19:26+0000 [xxu461000] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1407.5319> (referer: http://arxiv.org/list/cs/14?skip=8000&show=1000)
2015-03-23 20:19:26+0000 [xxu461000] DEBUG: Scraped from <200 http://arxiv.org/abs/1407.5319>
	{'abstract': u'Generation Production of successful software project is one of the prime considerations of software industry. Engineering high quality software products is further influenced by several factors such as budget, schedule, resource constraints etc. A project manager is responsible for estimation and allocation of these resources in a project. Hence, role of project manager has a vital influence on success of the project. This research comprises of an empirical study of several projects developed in a product and service based CMMI Level 5 Software Company. The investigation result shows a significant impact of aforementioned factors on the success of software and on the company. The analysis further indicates the vital role of project managers in optimizing the resource allocation towards development of software. This paper brings in impact analysis of efficiency of project manager in effectively allocating resources such as time, cost, number of developers etc. An awareness of efficiency level of project manager in optimal allocation of resources enables one to realize the desired level of quality.',
	 'authors': u'T.R. Gopalakrishnan Nair, Suma V, Shashi Kumar N.R,',
	 'category': u'Computer Science ',
	 'date': '2014-7-20',
	 'pdflink': u'http://arxiv.org/pdf/1407.5319',
	 'subjects': u'Software Engineering (cs.SE)',
	 'title': u'\nImpact Analysis of Allocation of Resources by Project Manager on Success  of Software Projects',
	 'urllink': u'http://arxiv.org/abs/1407.5319'}
2015-03-23 20:19:31+0000 [xxu461000] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1404.5406> (referer: http://arxiv.org/list/cs/14?skip=4000&show=1000)
2015-03-23 20:19:31+0000 [xxu461000] DEBUG: Scraped from <200 http://arxiv.org/abs/1404.5406>
	{'abstract': u'Degradation analysis is used to analyze the useful lifetimes of systems, their failure rates, and various other system parameters like mean time to failure (MTTF), mean time between failures (MTBF), and the system failure rate (SFR). In many systems, certain possible parallel paths of execution that have greater chances of success are preferred over others. Thus we introduce here the concept of probabilistic parallel choice. We use binary and -ary probabilistic choice operators in describing the selections of parallel paths. These binary and -ary probabilistic choice operators are considered so as to represent the complete system (described as a series-parallel system) in terms of the probabilities of selection of parallel paths and their relevant parameters. Our approach allows us to derive new and generalized formulae for system parameters like MTTF, MTBF, and SFR. We use a generalized exponential distribution, allowing distinct installation times for individual components, and use this model to derive expressions for such system parameters.',
	 'authors': u'Avinash Saxena, Shrisha Rao,',
	 'category': u'Computer Science ',
	 'date': '2014-4-22',
	 'pdflink': u'http://arxiv.org/pdf/1404.5406',
	 'subjects': u'Performance (cs.PF)',
	 'title': u'\nDegradation Analysis of Probabilistic Parallel Choice Systems',
	 'urllink': u'http://arxiv.org/abs/1404.5406'}
2015-03-23 20:19:36+0000 [xxu461000] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1405.2329> (referer: http://arxiv.org/list/cs/14?skip=5000&show=1000)
2015-03-23 20:19:36+0000 [xxu461000] DEBUG: Scraped from <200 http://arxiv.org/abs/1405.2329>
	{'abstract': u'Concurrent Constraint Programming (CCP) is a simple and powerful model for concurrency where agents interact by telling and asking constraints. Since their inception, CCP-languages have been designed for having a strong connection to logic. In fact, the underlying constraint system can be built from a suitable fragment of intuitionistic (linear) logic --ILL-- and processes can be interpreted as formulas in ILL. Constraints as ILL formulas fail to represent accurately situations where "preferences" (called soft constraints) such as probabilities, uncertainty or fuzziness are present. In order to circumvent this problem, c-semirings have been proposed as algebraic structures for defining constraint systems where agents are allowed to tell and ask soft constraints. Nevertheless, in this case, the tight connection to logic and proof theory is lost. In this work, we give a proof theoretical interpretation to soft constraints: they can be defined as formulas in a suitable fragment of ILL with subexponentials (SELL) where subexponentials, ordered in a c-semiring structure, are interpreted as preferences. We hence achieve two goals: (1) obtain a CCP language where agents can tell and ask soft constraints and (2) prove that the language in (1) has a strong connection with logic. Hence we keep a declarative reading of processes as formulas while providing a logical framework for soft-CCP based systems. An interesting side effect of (1) is that one is also able to handle probabilities (and other modalities) in SELL, by restricting the use of the promotion rule for non-idempotent c-semirings.This finer way of controlling subexponentials allows for considering more interesting spaces and restrictions, and it opens the possibility of specifying more challenging computational systems.',
	 'authors': u'Elaine Pimentel, Carlos Olarte, Vivek Nigam,',
	 'category': u'Computer Science ',
	 'date': '2014-5-9',
	 'pdflink': u'http://arxiv.org/pdf/1405.2329',
	 'subjects': u'Logic in Computer Science (cs.LO)',
	 'title': u'\nA Proof Theoretic Study of Soft Concurrent Constraint Programming',
	 'urllink': u'http://arxiv.org/abs/1405.2329'}
2015-03-23 20:19:42+0000 [xxu461000] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1405.7769> (referer: http://arxiv.org/list/cs/14?skip=6000&show=1000)
2015-03-23 20:19:42+0000 [xxu461000] DEBUG: Scraped from <200 http://arxiv.org/abs/1405.7769>
	{'abstract': u'Uncovering urban mobility patterns is crucial for further predicting and controlling spatially embedded events. In this article, we analyze millions of geographical check-ins crawled from a Chinese leading location-based social networking service, Jiepang.com, which contains demographical information and thus allows the group-specific studies. We found distinguishable mobility patterns of natives and non-natives in all five large cities under consideration, and by assigning different algorithms onto natives and non-natives, the accuracy of location prediction can be largely improved compared with pure algorithms. We further propose the so-called indigenization coefficients to quantify to which extent an individual behaves like a native, which depend only on check-in behaviors, instead of any demographical information. To our surprise, a hybrid algorithm weighted by the indigenization coefficients outperforms the mixed algorithm accounting for additional demographical information.',
	 'authors': u'Zimo Yang, Nicholas Jing Yuan, Xing Xie, Defu Lian, Yong Rui, Tao Zhou,',
	 'category': u'Computer Science ',
	 'date': '2014-5-30',
	 'pdflink': u'http://arxiv.org/pdf/1405.7769',
	 'subjects': u'Physics and Society (physics.soc-ph)',
	 'title': u'\nIndigenization of Urban Mobility',
	 'urllink': u'http://arxiv.org/abs/1405.7769'}
2015-03-23 20:19:45+0000 [xxu461000] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1407.5298> (referer: http://arxiv.org/list/cs/14?skip=8000&show=1000)
2015-03-23 20:19:45+0000 [xxu461000] DEBUG: Scraped from <200 http://arxiv.org/abs/1407.5298>
	{'abstract': u'We consider the problem of solving packing/covering LPs online, when the columns of the constraint matrix are presented in random order. This problem has received much attention: the main open question is to figure out how large the right-hand sides of the LPs have to be (compared to the entries on the left-hand side of the constraint) to get -approximations online? It is known that the RHS has to be times the left-hand sides, where is the number of constraints. In this paper we give a primal-dual algorithm to achieve this bound for all packing LPs, and also for a class of mixed packing/covering LPs. Our algorithms construct dual solutions using a regret-minimizing online learning algorithm in a black-box fashion, and use them to construct primal solutions. The adversarial guarantee that holds for the constructed duals help us to take care of most of the correlations that arise in the algorithm; the remaining correlations are handled via martingale concentration and maximal inequalities. These ideas lead to conceptually simple and modular algorithms, which we hope will be useful in other contexts.',
	 'authors': u'Anupam Gupta, Marco Molinaro,',
	 'category': u'Computer Science ',
	 'date': '2014-7-20',
	 'pdflink': u'http://arxiv.org/pdf/1407.5298',
	 'subjects': u'Data Structures and Algorithms (cs.DS)',
	 'title': u'\nHow the Experts Algorithm Can Help Solve LPs Online',
	 'urllink': u'http://arxiv.org/abs/1407.5298'}
2015-03-23 20:19:48+0000 [xxu461000] INFO: Crawled 318 pages (at 13 pages/min), scraped 301 items (at 13 items/min)
2015-03-23 20:19:49+0000 [xxu461000] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1404.5398> (referer: http://arxiv.org/list/cs/14?skip=4000&show=1000)
2015-03-23 20:19:49+0000 [xxu461000] DEBUG: Scraped from <200 http://arxiv.org/abs/1404.5398>
	{'abstract': u'Given an input , and a search problem , local computation algorithms (LCAs) implement access to specified locations of in a legal output , using polylogarithmic time and space. Mansour et al., (2012), had previously shown how to convert certain online algorithms to LCAs. In this work, we expand on that line of work and develop new techniques for designing LCAs and bounding their space and time complexity. Our contributions are fourfold: (1) We significantly improve the running times and space requirements of LCAs for previous results, (2) we expand and better define the family of online algorithms which can be converted to LCAs using our techniques, (3) we show that our results apply to a larger family of graphs than that of previous results, and (4) our proofs are simpler and more concise than the previous proof methods. For example, we show how to construct LCAs that require space and time (and expected time ) for problems such as maximal matching on a large family of graphs, as opposed to the henceforth best results that required space and time, and applied to a smaller family of graphs.',
	 'authors': u'Omer Reingold, Shai Vardi,',
	 'category': u'Computer Science ',
	 'date': '2014-4-22',
	 'pdflink': u'http://arxiv.org/pdf/1404.5398',
	 'subjects': u'Data Structures and Algorithms (cs.DS)',
	 'title': u'\nNew Techniques and Tighter Bounds for Local Computation Algorithms',
	 'urllink': u'http://arxiv.org/abs/1404.5398'}
2015-03-23 20:19:53+0000 [xxu461000] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1405.2316> (referer: http://arxiv.org/list/cs/14?skip=5000&show=1000)
2015-03-23 20:19:53+0000 [xxu461000] DEBUG: Scraped from <200 http://arxiv.org/abs/1405.2316>
	{'abstract': u'Feature tracking in video is a crucial task in computer vision. Usually, the tracking problem is handled one feature at a time, using a single-feature tracker like the Kanade-Lucas-Tomasi algorithm, or one of its derivatives. While this approach works quite well when dealing with high-quality video and "strong" features, it often falters when faced with dark and noisy video containing low-quality features. We present a framework for jointly tracking a set of features, which enables sharing information between the different features in the scene. We show that our method can be employed to track features for both rigid and nonrigid motions (possibly of few moving bodies) even when some features are occluded. Furthermore, it can be used to significantly improve tracking results in poorly-lit scenes (where there is a mix of good and bad features). Our approach does not require direct modeling of the structure or the motion of the scene, and runs in real time on a single CPU core.',
	 'authors': u'Bryan Poling, Gilad Lerman, Arthur Szlam,',
	 'category': u'Computer Science ',
	 'date': '2014-5-9',
	 'pdflink': u'http://arxiv.org/pdf/1405.2316',
	 'subjects': u'Computer Vision and Pattern Recognition (cs.CV)',
	 'title': u'\nBetter Feature Tracking Through Subspace Constraints',
	 'urllink': u'http://arxiv.org/abs/1405.2316'}
2015-03-23 20:19:58+0000 [xxu461000] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1405.7764> (referer: http://arxiv.org/list/cs/14?skip=6000&show=1000)
2015-03-23 20:19:58+0000 [xxu461000] DEBUG: Scraped from <200 http://arxiv.org/abs/1405.7764>
	{'abstract': u'In this paper, we consider a supervised learning setting where side knowledge is provided about the labels of unlabeled examples. The side knowledge has the effect of reducing the hypothesis space, leading to tighter generalization bounds, and thus possibly better generalization. We consider several types of side knowledge, the first leading to linear and polygonal constraints on the hypothesis space, the second leading to quadratic constraints, and the last leading to conic constraints. We show how different types of domain knowledge can lead directly to these kinds of side knowledge. We prove bounds on complexity measures of the hypothesis space for quadratic and conic side knowledge, and show that these bounds are tight in a specific sense for the quadratic case.',
	 'authors': u'Theja Tulabandhula, Cynthia Rudin,',
	 'category': u'Computer Science ',
	 'date': '2014-5-30',
	 'pdflink': u'http://arxiv.org/pdf/1405.7764',
	 'subjects': u'Machine Learning (stat.ML)',
	 'title': u'\nGeneralization Bounds for Learning with Linear, Polygonal, Quadratic and  Conic Side Knowledge',
	 'urllink': u'http://arxiv.org/abs/1405.7764'}
2015-03-23 20:20:04+0000 [xxu461000] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1407.5286> (referer: http://arxiv.org/list/cs/14?skip=8000&show=1000)
2015-03-23 20:20:04+0000 [xxu461000] DEBUG: Scraped from <200 http://arxiv.org/abs/1407.5286>
	{'abstract': u'Verifiers that can prove programs correct against their full functional specification require, for programs with loops, additional annotations in the form of loop invariants---propeties that hold for every iteration of a loop. We show that significant loop invariant candidates can be generated by systematically mutating postconditions; then, dynamic checking (based on automatically generated tests) weeds out invalid candidates, and static checking selects provably valid ones. We present a framework that automatically applies these techniques to support a program prover, paving the way for fully automatic verification without manually written loop invariants: Applied to 28 methods (including 39 different loops) from various java.util classes (occasionally modified to avoid using Java features not fully supported by the static checker), our DYNAMATE prototype automatically discharged 97% of all proof obligations, resulting in automatic complete correctness proofs of 25 out of the 28 methods---outperforming several state-of-the-art tools for fully automatic verification.',
	 'authors': u'Juan P. Galeotti, Carlo A. Furia, Eva May, Gordon Fraser, Andreas Zeller,',
	 'category': u'Computer Science ',
	 'date': '2014-7-20',
	 'pdflink': u'http://arxiv.org/pdf/1407.5286',
	 'subjects': u'Software Engineering (cs.SE)',
	 'title': u'\nInferring Loop Invariants by Mutation, Dynamic Analysis, and Static  Checking',
	 'urllink': u'http://arxiv.org/abs/1407.5286'}
2015-03-23 20:20:10+0000 [xxu461000] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1404.5393> (referer: http://arxiv.org/list/cs/14?skip=4000&show=1000)
2015-03-23 20:20:10+0000 [xxu461000] DEBUG: Scraped from <200 http://arxiv.org/abs/1404.5393>
	{'abstract': u'In this paper we associate permitting symbols with rules of Grammars in the components of cooperating distributed context free hexagonal array grammar systems as a control mechanism and investigating the generative power of the resulting systems in the terminal mode. This feature of associating permitting symbols with rules when extended to patterns in the form of connected arrays also requires checking of symbols, but this is simpler than usual pattern matching. The benefit of allowing permitting symbols is that it enables us to reduce the number of components required in a cooperating distributed hexagonal array grammar system for generating a set of picture arrays.',
	 'authors': u'Sujathakumari K, Dersanambika K.S,',
	 'category': u'Computer Science ',
	 'date': '2014-4-22',
	 'pdflink': u'http://arxiv.org/pdf/1404.5393',
	 'subjects': u'Formal Languages and Automata Theory (cs.FL)',
	 'title': u'\nCooperating distributed context-free hexagonal array grammar systems  with permitting contexts',
	 'urllink': u'http://arxiv.org/abs/1404.5393'}
2015-03-23 20:20:15+0000 [xxu461000] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1405.2305> (referer: http://arxiv.org/list/cs/14?skip=5000&show=1000)
2015-03-23 20:20:15+0000 [xxu461000] DEBUG: Scraped from <200 http://arxiv.org/abs/1405.2305>
	{'abstract': u'3D printing has raised a lot of attention from fields outside the manufacturing one in the last years. In this paper, we will illustrate some recent advances of 3D printing technology, applied to the field of telemedicine and remote patient care. The potentiality of this technology will be detailed without lab examples. Some crucial aspect such as the regulation of these devices and the need of some standards will also be discussed. The purpose of this paper is to present some of the most promising applications of such technology.',
	 'authors': u'Piero Giacomelli, Asa Smedberg,',
	 'category': u'Computer Science ',
	 'date': '2014-5-6',
	 'pdflink': u'http://arxiv.org/pdf/1405.2305',
	 'subjects': u'Other Computer Science (cs.OH)',
	 'title': u'\nThe Eve of 3D Printing in Telemedicine: State of the Art and Future  Challenges',
	 'urllink': u'http://arxiv.org/abs/1405.2305'}
2015-03-23 20:20:20+0000 [xxu461000] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1405.7724> (referer: http://arxiv.org/list/cs/14?skip=6000&show=1000)
2015-03-23 20:20:20+0000 [xxu461000] DEBUG: Scraped from <200 http://arxiv.org/abs/1405.7724>
	{'abstract': u'Many real world systems can be expressed as complex networks of interconnected nodes. It is frequently important to be able to quantify the relative importance of the various nodes in the network, a task accomplished by defining some centrality measures, with different centrality definitions stressing different aspects of the network. It is interesting to know to what extent these different centrality definitions are related for different networks. In this work, we study the correlation between pairs of a set of centrality measures for different real world networks and two network models. We show that the centralities are in general correlated, but with stronger correlations for network models than for real networks. We also show that the strength of the correlation of each pair of centralities varies from network to network. Taking this fact into account, we propose the use of a centrality correlation profile, consisting of the values of the correlation coefficients between all pairs of centralities of interest, as a way to characterize networks. Using the yeast protein interaction network as an example we show also that the centrality correlation profile can be used to assess the adequacy of a network model as a representation of a given real network.',
	 'authors': u'Jos\xe9 Ricardo Furlan Ronqui, Gonzalo Travieso,',
	 'category': u'Computer Science ',
	 'date': '2014-5-29',
	 'pdflink': u'http://arxiv.org/pdf/1405.7724',
	 'subjects': u'Physics and Society (physics.soc-ph)',
	 'title': u'\nAnalyzing complex networks through correlations in centrality  measurements',
	 'urllink': u'http://arxiv.org/abs/1405.7724'}
2015-03-23 20:20:26+0000 [xxu461000] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1407.5245> (referer: http://arxiv.org/list/cs/14?skip=8000&show=1000)
2015-03-23 20:20:26+0000 [xxu461000] DEBUG: Scraped from <200 http://arxiv.org/abs/1407.5245>
	{'abstract': u'Visual learning problems such as object classification and action recognition are typically approached using extensions of the popular bag-of-words (BoW) model. Despite its great success, it is unclear what visual features the BoW model is learning: Which regions in the image or video are used to discriminate among classes? Which are the most discriminative visual words? Answering these questions is fundamental for understanding existing BoW models and inspiring better models for visual recognition. To answer these questions, this paper presents a method for feature selection and region selection in the visual BoW model. This allows for an intermediate visualization of the features and regions that are important for visual learning. The main idea is to assign latent weights to the features or regions, and jointly optimize these latent variables with the parameters of a classifier (e.g., SVM). There are four main benefits of our approach: (1) Our approach accommodates non-linear additive kernels such as the popular Chi-square and intersection kernel; (2) our approach is able to handle both regions in images and spatio-temporal regions in videos in a unified way; (3) the feature selection problem is convex, and both problems can be solved using a scalable reduced gradient method; (4) we point out strong connections with multiple kernel learning and multiple instance learning approaches. Experimental results in the PASCAL VOC 2007, MSR Action Dataset II and YouTube illustrate the benefits of our approach.',
	 'authors': u'Ji Zhao, Liantao Wang, Ricardo Cabral, Fernando De la Torre,',
	 'category': u'Computer Science ',
	 'date': '2014-7-20',
	 'pdflink': u'http://arxiv.org/pdf/1407.5245',
	 'subjects': u'Computer Vision and Pattern Recognition (cs.CV)',
	 'title': u'\nFeature and Region Selection for Visual Learning',
	 'urllink': u'http://arxiv.org/abs/1407.5245'}
2015-03-23 20:20:32+0000 [xxu461000] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1404.5385> (referer: http://arxiv.org/list/cs/14?skip=4000&show=1000)
2015-03-23 20:20:32+0000 [xxu461000] DEBUG: Scraped from <200 http://arxiv.org/abs/1404.5385>
	{'abstract': u'Cognitive Radio (CR) operates in different fields as varied, one of these is cognitive radio networks. In this paper, we propose a new approach used CR, which aims to manage potential failures of computer systems and applications through the introduction of two aspects of autonomous networks to make systems capable of managing themselves with minimum human intervention.',
	 'authors': u'Mohammed Zakarya Baba-Ahmed, Badr Benmammar, Fethi Tarik Bendimerad,',
	 'category': u'Computer Science ',
	 'date': '2014-4-22',
	 'pdflink': u'http://arxiv.org/pdf/1404.5385',
	 'subjects': u'Networking and Internet Architecture (cs.NI)',
	 'title': u"\nVers l'auto-gestion d'un r\xe9seau de radio cognitive",
	 'urllink': u'http://arxiv.org/abs/1404.5385'}
2015-03-23 20:20:38+0000 [xxu461000] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1407.5225> (referer: http://arxiv.org/list/cs/14?skip=8000&show=1000)
2015-03-23 20:20:38+0000 [xxu461000] DEBUG: Scraped from <200 http://arxiv.org/abs/1407.5225>
	{'abstract': u"The Turing test aimed to recognize the behavior of a human from that of a computer algorithm. Such challenge is more relevant than ever in today's social media context, where limited attention and technology constrain the expressive power of humans, while incentives abound to develop software agents mimicking humans. These social bots interact, often unnoticed, with real people in the social media ecosystems, but their abundance is uncertain. While many bots are benign, one can be design harmful bots with the goals of persuading, smearing, or deceiving. Here we discuss the characteristics of modern, sophisticated social bots, and how their presence can endanger online ecosystems and our society. We then review current efforts to detect social bots on Twitter. Features related to content, network, sentiment, and temporal patterns of activity are imitated by bots but at the same time can help discriminate synthetic behaviors from human ones, yielding signatures of engineered social tampering.",
	 'authors': u'Emilio Ferrara, Onur Varol, Clayton Davis, Filippo Menczer, Alessandro Flammini,',
	 'category': u'Computer Science ',
	 'date': '2014-7-19',
	 'pdflink': u'http://arxiv.org/pdf/1407.5225',
	 'subjects': u'Social and Information Networks (cs.SI)',
	 'title': u'\nThe Rise of Social Bots',
	 'urllink': u'http://arxiv.org/abs/1407.5225'}
2015-03-23 20:20:41+0000 [xxu461000] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1404.5356> (referer: http://arxiv.org/list/cs/14?skip=4000&show=1000)
2015-03-23 20:20:41+0000 [xxu461000] DEBUG: Scraped from <200 http://arxiv.org/abs/1404.5356>
	{'abstract': u'We study the two-player safe game of Competitive Diffusion, a game-theoretic model for the diffusion of technologies or influence through a social network. In game theory, safe strategies are mixed strategies with a minimal expected gain against unknown strategies of the opponents. Safe strategies for competitive diffusion lead to maximum spread of influence in the presence of uncertainty about the other players. We study the safe game on two specific classes of trees, spiders and complete trees, and give tight bounds on the minimal expected gain. We then use these results to give an algorithm which suggests a safe strategy for a player on any tree. We test this algorithm on randomly generated trees, and show that it finds strategies that are close to optimal.',
	 'authors': u'Jeannette Janssen, Celeste Vautour,',
	 'category': u'Computer Science ',
	 'date': '2014-4-22',
	 'pdflink': u'http://arxiv.org/pdf/1404.5356',
	 'subjects': u'Discrete Mathematics (cs.DM)',
	 'title': u'\nFinding safe strategies for competitive diffusion on trees',
	 'urllink': u'http://arxiv.org/abs/1404.5356'}
2015-03-23 20:20:47+0000 [xxu461000] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1409.2050> (referer: http://arxiv.org/list/cs/14?skip=10000&show=1000)
2015-03-23 20:20:47+0000 [xxu461000] DEBUG: Scraped from <200 http://arxiv.org/abs/1409.2050>
	{'abstract': u'We present the development and evaluation of a hand tracking algorithm based on single depth images captured from an overhead perspective for use in the COACH prompting system. We train a random decision forest body part classifier using approximately 5,000 manually labeled, unbalanced, partially labeled training images. The classifier represents a random subset of pixels in each depth image with a learned probability density function across all trained body parts. A local mode-find approach is used to search for clusters present in the underlying feature space sampled by the classified pixels. In each frame, body part positions are chosen as the mode with the highest confidence. User hand positions are translated into hand washing task actions based on proximity to environmental objects. We validate the performance of the classifier and task action proposals on a large set of approximately 24,000 manually labeled images.',
	 'authors': u'Stephen Czarnuch, Alex Mihailidis,',
	 'category': u'Computer Science ',
	 'date': '2014-9-6',
	 'pdflink': u'http://arxiv.org/pdf/1409.2050',
	 'subjects': u'Computer Vision and Pattern Recognition (cs.CV)',
	 'title': u'\nDepth image hand tracking from an overhead perspective using partially  labeled, unbalanced data: Development and real-world testing',
	 'urllink': u'http://arxiv.org/abs/1409.2050'}
2015-03-23 20:20:48+0000 [xxu461000] INFO: Crawled 330 pages (at 12 pages/min), scraped 313 items (at 12 items/min)
2015-03-23 20:20:53+0000 [xxu461000] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1405.2278> (referer: http://arxiv.org/list/cs/14?skip=5000&show=1000)
2015-03-23 20:20:53+0000 [xxu461000] DEBUG: Scraped from <200 http://arxiv.org/abs/1405.2278>
	{'abstract': u'Classifiers trained on data sets possessing an imbalanced class distribution are known to exhibit poor generalisation performance. This is known as the imbalanced learning problem. The problem becomes particularly acute when we consider incremental classifiers operating on imbalanced data streams, especially when the learning objective is rare class identification. As accuracy may provide a misleading impression of performance on imbalanced data, existing stream classifiers based on accuracy can suffer poor minority class performance on imbalanced streams, with the result being low minority class recall rates. In this paper we address this deficiency by proposing the use of the Hellinger distance measure, as a very fast decision tree split criterion. We demonstrate that by using Hellinger a statistically significant improvement in recall rates on imbalanced data streams can be achieved, with an acceptable increase in the false positive rate.',
	 'authors': u'R. J. Lyon, J. M. Brooke, J. D. Knowles, B. W. Stappers,',
	 'category': u'Computer Science ',
	 'date': '2014-5-9',
	 'pdflink': u'http://arxiv.org/pdf/1405.2278',
	 'subjects': u'Learning (cs.LG)',
	 'title': u'\nHellinger Distance Trees for Imbalanced Streams',
	 'urllink': u'http://arxiv.org/abs/1405.2278'}
2015-03-23 20:20:58+0000 [xxu461000] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1405.7136> (referer: http://arxiv.org/list/cs/14?skip=6000&show=1000)
2015-03-23 20:20:58+0000 [xxu461000] DEBUG: Scraped from <200 http://arxiv.org/abs/1405.7136>
	{'abstract': u'The time lag between the publication of a Nobel discovery and the conferment of the prize has been rapidly increasing for all disciplines, especially for Physics. Does this mean that fundamental science is running out of groundbreaking discoveries?',
	 'authors': u'Francesco Becattini, Arnab Chatterjee, Santo Fortunato, Marija Mitrovi\u0107, Raj Kumar Pan, Pietro Della Briotta Parolo,',
	 'category': u'Computer Science ',
	 'date': '2014-5-28',
	 'pdflink': u'http://arxiv.org/pdf/1405.7136',
	 'subjects': u'Physics and Society (physics.soc-ph)',
	 'title': u'\nThe Nobel Prize delay',
	 'urllink': u'http://arxiv.org/abs/1405.7136'}
2015-03-23 20:21:02+0000 [xxu461000] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1407.5212> (referer: http://arxiv.org/list/cs/14?skip=8000&show=1000)
2015-03-23 20:21:02+0000 [xxu461000] DEBUG: Scraped from <200 http://arxiv.org/abs/1407.5212>
	{'abstract': u'Conventional urban traffic control systems have been based on historical traffic data. Later advancements made use of detectors, which enabled the gathering of real time traffic data, in order to reorganize and calibrate traffic signalization programs. Further evolvement provided the ability to forecast traffic conditions, in order to develop traffic signalization programs and strategies precomputed and applied at the most appropriate time frame for the optimal control of the current traffic conditions. We, propose the next generation of traffic control systems based on principles of Artificial Intelligence and Context Awareness. Most of the existing algorithms use average waiting time or length of the queue to assess an algorithms performance. However, a low average waiting time may come at the cost of delaying other vehicles indefinitely. In our algorithm, besides the vehicle queue, we use fairness also as an important performance metric to assess an algorithms performance.',
	 'authors': u'Kandarp Khandwala, Rudra Sharma, Snehal Rao,',
	 'category': u'Computer Science ',
	 'date': '2014-7-19',
	 'pdflink': u'http://arxiv.org/pdf/1407.5212',
	 'subjects': u'Artificial Intelligence (cs.AI)',
	 'title': u'\nContext Aware Dynamic Traffic Signal Optimization',
	 'urllink': u'http://arxiv.org/abs/1407.5212'}
2015-03-23 20:21:07+0000 [xxu461000] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1404.5352> (referer: http://arxiv.org/list/cs/14?skip=4000&show=1000)
2015-03-23 20:21:07+0000 [xxu461000] DEBUG: Scraped from <200 http://arxiv.org/abs/1404.5352>
	{'abstract': u'This paper is a critique of version three of Joonmo Kim\'s paper entitled "P is not equal to NP by Modus Tollens. [arXiv:1403.4143v3]" After summarizing Kim\'s proof, we note that the logic that Kim uses is inconsistent, which provides evidence that the proof is invalid. To show this, we will consider two reasonable interpretations of Kim\'s definitions, and show that "P is not equal to NP" does not seem to follow in an obvious way using any of them.',
	 'authors': u'Dan Hassin, Adam Scrivener, Yibo Zhou,',
	 'category': u'Computer Science ',
	 'date': '2014-4-22',
	 'pdflink': u'http://arxiv.org/pdf/1404.5352',
	 'subjects': u'Computational Complexity (cs.CC)',
	 'title': u'\nCritique of J. Kim\'s "P is not equal to NP by Modus Tollens"',
	 'urllink': u'http://arxiv.org/abs/1404.5352'}
2015-03-23 20:21:13+0000 [xxu461000] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1409.2042> (referer: http://arxiv.org/list/cs/14?skip=10000&show=1000)
2015-03-23 20:21:13+0000 [xxu461000] DEBUG: Scraped from <200 http://arxiv.org/abs/1409.2042>
	{'abstract': u"Recommendations are central to the utility of many websites including YouTube, Quora as well as popular e-commerce stores. Such sites typically contain a set of recommendations on every product page that enables visitors to easily navigate the website. Choosing an appropriate set of recommendations at each page is one of the key features of backend engines that have been deployed at several e-commerce sites. Specifically at BloomReach, an engine consisting of several independent components analyzes and optimizes its clients' websites. This paper focuses on the structure optimizer component which improves the website navigation experience that enables the discovery of novel content. We begin by formalizing the concept of recommendations used for discovery. We formulate this as a natural graph optimization problem which in its simplest case, reduces to a bipartite matching problem. In practice, solving these matching problems requires superlinear time and is not scalable. Also, implementing simple algorithms is critical in practice because they are significantly easier to maintain in production. This motivated us to analyze three methods for solving the problem in increasing order of sophistication: a sampling algorithm, a greedy algorithm and a more involved partitioning based algorithm. We first theoretically analyze the performance of these three methods on random graph models characterizing when each method will yield a solution of sufficient quality and the parameter ranges when more sophistication is needed. We complement this by providing an empirical analysis of these algorithms on simulated and real-world production data. Our results confirm that it is not always necessary to implement complicated algorithms in the real-world and that very good practical results can be obtained by using heuristics that are backed by the confidence of concrete theoretical guarantees.",
	 'authors': u'Arda Antikacioglu, R. Ravi, Srinath Srihdar,',
	 'category': u'Computer Science ',
	 'date': '2014-9-6',
	 'pdflink': u'http://arxiv.org/pdf/1409.2042',
	 'subjects': u'Information Retrieval (cs.IR)',
	 'title': u'\nRecommendation Subgraphs for Web Discovery',
	 'urllink': u'http://arxiv.org/abs/1409.2042'}
2015-03-23 20:21:18+0000 [xxu461000] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1405.2271> (referer: http://arxiv.org/list/cs/14?skip=5000&show=1000)
2015-03-23 20:21:19+0000 [xxu461000] DEBUG: Scraped from <200 http://arxiv.org/abs/1405.2271>
	{'abstract': u'Boolean networks are used to model biological networks such as gene regulatory networks. Often Boolean networks show very chaotic behaviour which is sensitive to any small perturbations. In order to reduce the chaotic behaviour and to attain stability in the gene regulatory network, nested Canalizing Functions (NCFs) are best suited. NCFs and its variants have a wide range of applications in systems biology. Previously, many works were done on the application of canalizing functions, but there were fewer methods to check if any arbitrary Boolean function is canalizing or not. In this paper, by using Karnaugh Map this problem is solved and also it has been shown that when the canalizing functions of variable is given, all the canalizing functions of variable could be generated by the method of concatenation. In this paper we have uniquely identified the number of NCFs having a particular Hamming Distance (H.D) generated by each variable as starting canalizing input. Partially NCFs of 4 variables has also been studied in this paper.',
	 'authors': u'Camellia Ray, Jayanta Kumar Das, Pabitra Pal Choudhury,',
	 'category': u'Computer Science ',
	 'date': '2014-5-9',
	 'pdflink': u'http://arxiv.org/pdf/1405.2271',
	 'subjects': u'Systems and Control (cs.SY)',
	 'title': u'\nOn Analysis and Generation of some Biologically Important Boolean  Functions',
	 'urllink': u'http://arxiv.org/abs/1405.2271'}
2015-03-23 20:21:23+0000 [xxu461000] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1405.7115> (referer: http://arxiv.org/list/cs/14?skip=6000&show=1000)
2015-03-23 20:21:23+0000 [xxu461000] DEBUG: Scraped from <200 http://arxiv.org/abs/1405.7115>
	{'abstract': u'Geometric involutive bases for polynomial systems of equations have their origin in the prolongation and projection methods of the geometers Cartan and Kuranishi for systems of PDE. They are useful for numerical ideal membership testing and the solution of polynomial systems. In this paper we further develop our symbolic-numeric methods for such bases. We give methods to explicitly extract and decrease the degree of intermediate systems and the output basis. Algorithms for the numerical computation of involutivity criteria for positive dimensional ideals are also discussed. We were also motivated by some remarkable recent work by Lasserre and collaborators who employed our prolongation projection involutive criteria as a part of their semi-definite based programming (SDP) method for identifying the real radical of zero dimensional polynomial ideals. Consequently in this paper we begin an exploration of the interaction between geometric involutive bases and these methods particularly in the positive dimensional case. Motivated by the extension of these methods to the positive dimensional case we explore the interplay between geometric involutive bases and the new SDP methods.',
	 'authors': u'Greg Reid, Fei Wang, Wenyuan Wu,',
	 'category': u'Computer Science ',
	 'date': '2014-5-28',
	 'pdflink': u'http://arxiv.org/pdf/1405.7115',
	 'subjects': u'Algebraic Geometry (math.AG)',
	 'title': u'\nGeometric involutive bases for positive dimensional polynomial ideals  and SDP methods',
	 'urllink': u'http://arxiv.org/abs/1405.7115'}
2015-03-23 20:21:28+0000 [xxu461000] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1407.5197> (referer: http://arxiv.org/list/cs/14?skip=8000&show=1000)
2015-03-23 20:21:28+0000 [xxu461000] DEBUG: Scraped from <200 http://arxiv.org/abs/1407.5197>
	{'abstract': u'Semi or completely autonomous unmanned vehicles, remotely driven or controlled through artificial intelligence, are instrumental to foster space exploration. One of the most essential tasks of a rover is terrain traversing which requires the need of efficient suspension systems. This communication presents a suspension system giving degrees of freedom to every wheel with the help of linear actuators connected through bell crank levers. The actuation of linear actuators directly varies the height of every wheel from the chassis hence offering articulation to the rover. A control system is developed offering an algorithm for its autonomous actuation. This system proves instrumental for leveling of the chassis where any kind of slope, roll or pitch, may impute abstaining of payloads from efficient working. This was tried and tested successfully as a part of the rover developed by Team RUDRA from SRM University, INDIA (first Team from Asia and finishing at the fifth position) at University Rover Challenge 2013, held at UTAH, USA in May-June.',
	 'authors': u'Karan Vaish, Shah Mihir Rajesh, K. Pasupatheeswaran, Anubha Parashar, Jyoti Chaturvedi,',
	 'category': u'Computer Science ',
	 'date': '2014-7-19',
	 'pdflink': u'http://arxiv.org/pdf/1407.5197',
	 'subjects': u'Robotics (cs.RO)',
	 'title': u'\nDesign and Autonomous Control of the Active Adaptive Suspension System  Rudra Mars Rover',
	 'urllink': u'http://arxiv.org/abs/1407.5197'}
2015-03-23 20:21:34+0000 [xxu461000] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1404.5351> (referer: http://arxiv.org/list/cs/14?skip=4000&show=1000)
2015-03-23 20:21:34+0000 [xxu461000] DEBUG: Scraped from <200 http://arxiv.org/abs/1404.5351>
	{'abstract': u'We identify a novel instance of the background subtraction problem that focuses on extracting near-field foreground objects captured using handheld cameras. Given two user-generated videos of a scene, one with and the other without the foreground object(s), our goal is to efficiently generate an output video with only the foreground object(s) present in it. We cast this challenge as a spatio-temporal frame matching problem, and propose an efficient solution for it that exploits the temporal smoothness of the video sequences. We present theoretical analyses for the error bounds of our approach, and validate our findings using a detailed set of simulation experiments. Finally, we present the results of our approach tested on multiple real videos captured using handheld cameras, and compare them to several alternate foreground extraction approaches.',
	 'authors': u'Raffay Hamid, Atish Das Sarma, Dennis DeCoste, Neel Sundaresan,',
	 'category': u'Computer Science ',
	 'date': '2014-4-22',
	 'pdflink': u'http://arxiv.org/pdf/1404.5351',
	 'subjects': u'Computer Vision and Pattern Recognition (cs.CV)',
	 'title': u'\nFast Approximate Matching of Cell-Phone Videos for Robust Background  Subtraction',
	 'urllink': u'http://arxiv.org/abs/1404.5351'}
2015-03-23 20:21:38+0000 [xxu461000] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1409.2030> (referer: http://arxiv.org/list/cs/14?skip=10000&show=1000)
2015-03-23 20:21:38+0000 [xxu461000] DEBUG: Scraped from <200 http://arxiv.org/abs/1409.2030>
	{'abstract': u'Solving a quadratic equation with real coefficients is known to middle school students. Solving the equation over the quaternions is not straightforward. Huang and So cite give a complete set of formulas, breaking it into several cases depending on the coefficients. From a result of the second author in cite, zeros of can be expressed in terms of the zeros of a real quartic equation. This drastically simplifies solving a quadratic equation. Here we also consider solving iteratively via Newton and Halley methods developed in cite. We prove a property of the Jacobian of Newton and Halley methods and describe several 2D polynomiography based on these methods. The images not only encode the outcome of the iterative process, but by measuring the time taken to render them we find the relative speed of convergence for the methods.',
	 'authors': u'Fedor Andreev, Bahman Kalantari,',
	 'category': u'Computer Science ',
	 'date': '2014-9-6',
	 'pdflink': u'http://arxiv.org/pdf/1409.2030',
	 'subjects': u'Numerical Analysis (cs.NA)',
	 'title': u'\nAlgorithms and Polynomiography for Solving Quaternion Quadratic  Equations',
	 'urllink': u'http://arxiv.org/abs/1409.2030'}
2015-03-23 20:21:44+0000 [xxu461000] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1405.2262> (referer: http://arxiv.org/list/cs/14?skip=5000&show=1000)
2015-03-23 20:21:44+0000 [xxu461000] DEBUG: Scraped from <200 http://arxiv.org/abs/1405.2262>
	{'abstract': u'We present a method for training a deep neural network containing sinusoidal activation functions to fit to time-series data. Weights are initialized using a fast Fourier transform, then trained with regularization to improve generalization. A simple dynamic parameter tuning method is employed to adjust both the learning rate and regularization term, such that stability and efficient training are both achieved. We show how deeper layers can be utilized to model the observed sequence using a sparser set of sinusoid units, and how non-uniform regularization can improve generalization by promoting the shifting of weight toward simpler units. The method is demonstrated with time-series problems to show that it leads to effective extrapolation of nonlinear trends.',
	 'authors': u'Michael S. Gashler, Stephen C. Ashmore,',
	 'category': u'Computer Science ',
	 'date': '2014-5-9',
	 'pdflink': u'http://arxiv.org/pdf/1405.2262',
	 'subjects': u'Neural and Evolutionary Computing (cs.NE)',
	 'title': u'\nTraining Deep Fourier Neural Networks To Fit Time-Series Data',
	 'urllink': u'http://arxiv.org/abs/1405.2262'}
2015-03-23 20:21:48+0000 [xxu461000] INFO: Crawled 341 pages (at 11 pages/min), scraped 324 items (at 11 items/min)
2015-03-23 20:21:49+0000 [xxu461000] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1405.6989> (referer: http://arxiv.org/list/cs/14?skip=6000&show=1000)
2015-03-23 20:21:49+0000 [xxu461000] DEBUG: Scraped from <200 http://arxiv.org/abs/1405.6989>
	{'abstract': u'In this paper, we develop the themes presented at the 2003 Joint Complexity Conference at the London School of Economics and subsequently published in The Intelligencer (2004) and O Tempo Das Redes (2008). Following the data analysis of the 9/11 high-jacker network developed by Valdis Krebs from open sources, we apply social network theory to examine salient arguments regarding terrorism as seen from the standpoint of complex adaptive systems theory. In particular, we explore the concepts of group cohesion, adhesion and alternative network mappings derived from node removal.',
	 'authors': u'Philip Vos Fellman, Roxana Wright,',
	 'category': u'Computer Science ',
	 'date': '2014-5-20',
	 'pdflink': u'http://arxiv.org/pdf/1405.6989',
	 'subjects': u'Physics and Society (physics.soc-ph)',
	 'title': u'\nModeling Terrorist Networks, Complex Systems at the Mid-range',
	 'urllink': u'http://arxiv.org/abs/1405.6989'}
2015-03-23 20:21:54+0000 [xxu461000] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1407.5173> (referer: http://arxiv.org/list/cs/14?skip=8000&show=1000)
2015-03-23 20:21:54+0000 [xxu461000] DEBUG: Scraped from <200 http://arxiv.org/abs/1407.5173>
	{'abstract': u'This paper presents a low power ECG recording Sys-tem-on-Chip (SoC) with on-chip low complexity lossless ECG compression for data reduction in wireless/ambulatory ECG sensor devices. The proposed algorithm uses a linear slope predictor to estimate the ECG samples, and uses a novel low complexity dynamic coding-packaging scheme to frame the resulting estimation error into fixed-length 16-bit format. The proposed technique achieves an average compression ratio of 2.25x on MIT/BIH ECG database. Implemented in 0.35 m process, the compressor uses 0.565 K gates/channel occupying 0.4 mm2 for 4-channel, and consumes 535 nW/channel at 2.4V for ECG sampled at 512 Hz. Small size and ultra-low power consumption makes the proposed technique suitable for wearable ECG sensor application.',
	 'authors': u'C.J.Deepu,',
	 'category': u'Computer Science ',
	 'date': '2014-7-19',
	 'pdflink': u'http://arxiv.org/pdf/1407.5173',
	 'subjects': u'Hardware Architecture (cs.AR)',
	 'title': u'\nAn ECG-SoC with 535nW/channel lossless data compression for wearable  sensors',
	 'urllink': u'http://arxiv.org/abs/1407.5173'}
2015-03-23 20:22:00+0000 [xxu461000] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1404.5344> (referer: http://arxiv.org/list/cs/14?skip=4000&show=1000)
2015-03-23 20:22:00+0000 [xxu461000] DEBUG: Scraped from <200 http://arxiv.org/abs/1404.5344>
	{'abstract': u'The Fields of Experts (FoE) image prior model, a filter-based higher-order Markov Random Fields (MRF) model, has been shown to be effective for many image restoration problems. Motivated by the successes of FoE-based approaches, in this letter, we propose a novel variational model for multiplicative noise reduction based on the FoE image prior model. The resulted model corresponds to a non-convex minimization problem, which can be solved by a recently published non-convex optimization algorithm. Experimental results based on synthetic speckle noise and real synthetic aperture radar (SAR) images suggest that the performance of our proposed method is on par with the best published despeckling algorithm. Besides, our proposed model comes along with an additional advantage, that the inference is extremely efficient.',
	 'authors': u'Yunjin Chen, Wensen Feng, Ren\xe9 Ranftl, Hong Qiao, Thomas Pock,',
	 'category': u'Computer Science ',
	 'date': '2014-4-21',
	 'pdflink': u'http://arxiv.org/pdf/1404.5344',
	 'subjects': u'Computer Vision and Pattern Recognition (cs.CV)',
	 'title': u'\nA higher-order MRF based variational model for multiplicative noise  reduction',
	 'urllink': u'http://arxiv.org/abs/1404.5344'}
2015-03-23 20:22:05+0000 [xxu461000] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1409.2019> (referer: http://arxiv.org/list/cs/14?skip=10000&show=1000)
2015-03-23 20:22:05+0000 [xxu461000] DEBUG: Scraped from <200 http://arxiv.org/abs/1409.2019>
	{'abstract': u'A non-binary (2,v)-regular LDPC code is defined by a parity-check matrix with column weight 2 and row weight v. In this report, we give an ontology-based approach to the optimization for this class of codes. All possible inter-connected cycle patterns that lead to low symbol-weight codewords are identified to put together the ontology. The optimization goal is to improve the distance property of equivalent binary images. Using the proposed method, the estimation and optimization of bit-distance spectrum becomes easily handleable. Three codes in the CCSDS recommendation are analyzed and several codes with good minimum bit-distance are designed.',
	 'authors': u'Chao Chen,',
	 'category': u'Computer Science ',
	 'date': '2014-9-6',
	 'pdflink': u'http://arxiv.org/pdf/1409.2019',
	 'subjects': u'Information Theory (cs.IT)',
	 'title': u'\nAn ontology-based approach to the optimization of non-binary  (2,v)-regular LDPC codes',
	 'urllink': u'http://arxiv.org/abs/1409.2019'}
2015-03-23 20:22:09+0000 [xxu461000] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1405.2246> (referer: http://arxiv.org/list/cs/14?skip=5000&show=1000)
2015-03-23 20:22:09+0000 [xxu461000] DEBUG: Scraped from <200 http://arxiv.org/abs/1405.2246>
	{'abstract': u'Non-negative matrix factorization (NMF) has proved effective in many clustering and classification tasks. The classic ways to measure the errors between the original and the reconstructed matrix are distance or Kullback-Leibler (KL) divergence. However, nonlinear cases are not properly handled when we use these error measures. As a consequence, alternative measures based on nonlinear kernels, such as correntropy, are proposed. However, the current correntropy-based NMF only targets on the low-level features without considering the intrinsic geometrical distribution of data. In this paper, we propose a new NMF algorithm that preserves local invariance by adding graph regularization into the process of max-correntropy-based matrix factorization. Meanwhile, each feature can learn corresponding kernel from the data. The experiment results of Caltech101 and Caltech256 show the benefits of such combination against other NMF algorithms for the unsupervised image clustering.',
	 'authors': u'Le Li, Jianjun Yang, Kaili Zhao, Yang Xu, Honggang Zhang, Zhuoyi Fan,',
	 'category': u'Computer Science ',
	 'date': '2014-5-9',
	 'pdflink': u'http://arxiv.org/pdf/1405.2246',
	 'subjects': u'Computer Vision and Pattern Recognition (cs.CV)',
	 'title': u'\nGraph Regularized Non-negative Matrix Factorization By Maximizing  Correntropy',
	 'urllink': u'http://arxiv.org/abs/1405.2246'}
2015-03-23 20:22:14+0000 [xxu461000] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1405.6974> (referer: http://arxiv.org/list/cs/14?skip=6000&show=1000)
2015-03-23 20:22:14+0000 [xxu461000] DEBUG: Scraped from <200 http://arxiv.org/abs/1405.6974>
	{'abstract': u'Many machine learning models have important structural tuning parameters that cannot be directly estimated from the data. The common tactic for setting these parameters is to use resampling methods, such as cross--validation or the bootstrap, to evaluate a candidate set of values and choose the best based on some pre--defined criterion. Unfortunately, this process can be time consuming. However, the model tuning process can be streamlined by adaptively resampling candidate values so that settings that are clearly sub-optimal can be discarded. The notion of futility analysis is introduced in this context. An example is shown that illustrates how adaptive resampling can be used to reduce training time. Simulation studies are used to understand how the potential speed--up is affected by parallel processing techniques.',
	 'authors': u'Max Kuhn,',
	 'category': u'Computer Science ',
	 'date': '2014-5-27',
	 'pdflink': u'http://arxiv.org/pdf/1405.6974',
	 'subjects': u'Machine Learning (stat.ML)',
	 'title': u'\nFutility Analysis in the Cross-Validation of Machine Learning Models',
	 'urllink': u'http://arxiv.org/abs/1405.6974'}
2015-03-23 20:22:18+0000 [xxu461000] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1407.5166> (referer: http://arxiv.org/list/cs/14?skip=8000&show=1000)
2015-03-23 20:22:18+0000 [xxu461000] DEBUG: Scraped from <200 http://arxiv.org/abs/1407.5166>
	{'abstract': u'While the -calculus notoriously subsumes Alternating-time Temporal Logic (ATL), we show that the epistemic -calculus does not subsume ATL with imperfect information (ATL) for the synchronous perfect-recall semantics. To prove this we first establish that jumping parity tree automata (JTA), a recently introduced extension of alternating parity tree automata, are expressively equivalent to the epistemic -calculus, and this for any knowledge semantics. Using this result we also show that, for bounded-memory semantics, the epistemic -calculus is not more expressive than the standard -calculus, and that its satisfiability problem is EXPTIME-complete.',
	 'authors': u'C\u0103t\u0103lin Dima, Bastien Maubert, Sophie Pinchinat,',
	 'category': u'Computer Science ',
	 'date': '2014-7-19',
	 'pdflink': u'http://arxiv.org/pdf/1407.5166',
	 'subjects': u'Logic in Computer Science (cs.LO)',
	 'title': u'\nThe Expressive Power of Epistemic $\u03bc$-Calculus',
	 'urllink': u'http://arxiv.org/abs/1407.5166'}
2015-03-23 20:22:21+0000 [xxu461000] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1404.5331> (referer: http://arxiv.org/list/cs/14?skip=4000&show=1000)
2015-03-23 20:22:21+0000 [xxu461000] DEBUG: Scraped from <200 http://arxiv.org/abs/1404.5331>
	{'abstract': u'Spectrum sensing is a fundamental component of cognitive radio. How to promptly sense the presence of primary users is a key issue to a cognitive radio network. The time requirement is critical in that violating it will cause harmful interference to the primary user, leading to a system-wide failure. The motivation of our work is to provide an effective spectrum sensing method to detect primary users as soon as possible. In the language of streaming based real-time data processing, short-time means small-sized data. In this paper, we propose a cumulative spectrum sensing method dealing with limited sized data. A novel method of covariance matrix estimation is utilized to approximate the true covariance matrix. The theoretical analysis is derived based on concentration inequalities and random matrix theory to support the claims of detection performance. Comparisons between the proposed method and other traditional approaches, judged by the simulation using a captured digital TV signal, show that this proposed method can operate either using smaller-sized data or working under lower SNR environment.',
	 'authors': u'Feng Lin, Robert C. Qiu, James P. Browning,',
	 'category': u'Computer Science ',
	 'date': '2014-4-21',
	 'pdflink': u'http://arxiv.org/pdf/1404.5331',
	 'subjects': u'Networking and Internet Architecture (cs.NI)',
	 'title': u'\nSpectrum Sensing with Small-Sized Datasets in Cognitive Radio:  Algorithms and Analysis',
	 'urllink': u'http://arxiv.org/abs/1404.5331'}
2015-03-23 20:22:26+0000 [xxu461000] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1409.2017> (referer: http://arxiv.org/list/cs/14?skip=10000&show=1000)
2015-03-23 20:22:26+0000 [xxu461000] DEBUG: Scraped from <200 http://arxiv.org/abs/1409.2017>
	{'abstract': u'In this paper consensus in second-order multi-agent systems with a non-periodic sampled-data exchange among agents is investigated. The sampling is random with bounded inter-sampling intervals. It is assumed that each agent has exact knowledge of its own state at any time instant. The considered local interaction rule is PD-type. Sufficient conditions for stability of the consensus protocol to a time-invariant value are derived based on LMIs. Such conditions only require the knowledge of the connectivity of the graph modeling the network topology. Numerical simulations are presented to corroborate the theoretical results.',
	 'authors': u'Mehran Zareh, Dimos V. Dimarogonas, Mauro Franceschelli, Karl Henrik Johansson, Carla Seatzu,',
	 'category': u'Computer Science ',
	 'date': '2014-9-6',
	 'pdflink': u'http://arxiv.org/pdf/1409.2017',
	 'subjects': u'Systems and Control (cs.SY)',
	 'title': u'\nConsensus in multi-agent systems with non-periodic sampled-data exchange  and uncertain network topology',
	 'urllink': u'http://arxiv.org/abs/1409.2017'}
2015-03-23 20:22:30+0000 [xxu461000] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1405.2227> (referer: http://arxiv.org/list/cs/14?skip=5000&show=1000)
2015-03-23 20:22:30+0000 [xxu461000] DEBUG: Scraped from <200 http://arxiv.org/abs/1405.2227>
	{'abstract': u'Face recognition is a widely used biometric approach. Face recognition technology has developed rapidly in recent years and it is more direct, user friendly and convenient compared to other methods. But face recognition systems are vulnerable to spoof attacks made by non-real faces. It is an easy way to spoof face recognition systems by facial pictures such as portrait photographs. A secure system needs Liveness detection in order to guard against such spoofing. In this work, face liveness detection approaches are categorized based on the various types techniques used for liveness detection. This categorization helps understanding different spoof attacks scenarios and their relation to the developed solutions. A review of the latest works regarding face liveness detection works is presented. The main aim is to provide a simple path for the future development of novel and more secured face liveness detection approach.',
	 'authors': u'Saptarshi Chakraborty, Dhrubajyoti Das,',
	 'category': u'Computer Science ',
	 'date': '2014-5-9',
	 'pdflink': u'http://arxiv.org/pdf/1405.2227',
	 'subjects': u'Computer Vision and Pattern Recognition (cs.CV)',
	 'title': u'\nAn Overview of Face Liveness Detection',
	 'urllink': u'http://arxiv.org/abs/1405.2227'}
2015-03-23 20:22:34+0000 [xxu461000] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1405.6948> (referer: http://arxiv.org/list/cs/14?skip=6000&show=1000)
2015-03-23 20:22:34+0000 [xxu461000] DEBUG: Scraped from <200 http://arxiv.org/abs/1405.6948>
	{'abstract': u'We introduce the multidimensional manifold extraction for multicarrier continuous-variable (CV) quantum key distribution (QKD). The manifold extraction utilizes the resources that are injected into the transmission by the additional degrees of freedom of the multicarrier modulation. We demonstrate the results through the AMQD (adaptive multicarrier quadrature division) scheme, which granulates the information into Gaussian subcarrier CVs and divides the physical link into several Gaussian sub-channels for the transmission. We prove that the exploitable extra degree of freedom in a multicarrier CVQKD scenario significantly extends the possibilities of single-carrier CVQKD. The manifold extraction allows for the parties to reach decreased error probabilities by utilizing those extra resources of a multicarrier transmission that are not available in a single-carrier CVQKD setting. We define the multidimensional manifold space of multicarrier CVQKD and the optimal tradeoff between the available degrees of freedom of the multicarrier transmission. We also extend the manifold extraction for the multiple-access AMQD-MQA (multiuser quadrature allocation) multicarrier protocol. The additional resources of multicarrier CVQKD allow the achievement of significant performance improvements that are particularly crucial in an experimental scenario.',
	 'authors': u'Laszlo Gyongyosi,',
	 'category': u'Computer Science ',
	 'date': '2014-5-27',
	 'pdflink': u'http://arxiv.org/pdf/1405.6948',
	 'subjects': u'Quantum Physics (quant-ph)',
	 'title': u'\nMultidimensional Manifold Extraction for Multicarrier  Continuous-Variable Quantum Key Distribution',
	 'urllink': u'http://arxiv.org/abs/1405.6948'}
2015-03-23 20:22:40+0000 [xxu461000] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1407.5161> (referer: http://arxiv.org/list/cs/14?skip=8000&show=1000)
2015-03-23 20:22:40+0000 [xxu461000] DEBUG: Scraped from <200 http://arxiv.org/abs/1407.5161>
	{'abstract': u'In this paper, while considering the impact of antenna correlation and the interference from neighboring users, we analyze channel estimation and training sequence design for multi-input multi-output (MIMO) two-way relay (TWR) systems. To this end, we propose to decompose the bidirectional transmission links into two phases, i.e., the multiple access (MAC) phase and the broadcasting (BC) phase. By considering the Kronecker-structured channel model, we derive the optimal linear minimum mean-square-error (LMMSE) channel estimators. The corresponding training designs for the MAC and BC phases are then formulated and solved to improve channel estimation accuracy. For the general scenario of training sequence design for both phases, two iterative training design algorithms are proposed that are verified to produce training sequences that result in near optimal channel estimation performance. Furthermore, for specific practical scenarios, where the covariance matrices of the channel or disturbances are of particular structures, the optimal training sequence design guidelines are derived. In order to reduce training overhead, the minimum required training length for channel estimation in both the MAC and BC phases are also derived. Comprehensive simulations are carried out to demonstrate the effectiveness of the proposed training designs.',
	 'authors': u'Rui Wang, Meixia Tao, Hani Mehrpouyan, Yingbo Hua,',
	 'category': u'Computer Science ',
	 'date': '2014-7-19',
	 'pdflink': u'http://arxiv.org/pdf/1407.5161',
	 'subjects': u'Information Theory (cs.IT)',
	 'title': u'\nChannel Estimation and Optimal Training Design for Correlated MIMO  Two-Way Relay Systems in Colored Environment',
	 'urllink': u'http://arxiv.org/abs/1407.5161'}
2015-03-23 20:22:44+0000 [xxu461000] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1404.5322> (referer: http://arxiv.org/list/cs/14?skip=4000&show=1000)
2015-03-23 20:22:44+0000 [xxu461000] DEBUG: Scraped from <200 http://arxiv.org/abs/1404.5322>
	{'abstract': u'We present CitNetExplorer, a new software tool for analyzing and visualizing citation networks of scientific publications. CitNetExplorer can for instance be used to study the development of a research field, to delineate the literature on a research topic, and to support literature reviewing. We first introduce the main concepts that need to be understood when working with CitNetExplorer. We then demonstrate CitNetExplorer by using the tool to analyze the scientometric literature and the literature on community detection in networks. Finally, we discuss some technical details on the construction, visualization, and analysis of citation networks in CitNetExplorer.',
	 'authors': u'Nees Jan van Eck, Ludo Waltman,',
	 'category': u'Computer Science ',
	 'date': '2014-4-21',
	 'pdflink': u'http://arxiv.org/pdf/1404.5322',
	 'subjects': u'Digital Libraries (cs.DL)',
	 'title': u'\nCitNetExplorer: A new software tool for analyzing and visualizing  citation networks',
	 'urllink': u'http://arxiv.org/abs/1404.5322'}
2015-03-23 20:22:48+0000 [xxu461000] INFO: Crawled 354 pages (at 13 pages/min), scraped 337 items (at 13 items/min)
2015-03-23 20:22:50+0000 [xxu461000] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1409.2013> (referer: http://arxiv.org/list/cs/14?skip=10000&show=1000)
2015-03-23 20:22:50+0000 [xxu461000] DEBUG: Scraped from <200 http://arxiv.org/abs/1409.2013>
	{'abstract': u'We study a class of games which model the competition among agents to access some service provided by distributed service units and exhibit congestion and frustration phenomena when service units have limited service capacity. We propose a technique, based on the cavity method of statistical physics, to characterize the full spectrum of Nash equilibria of the game. The analysis reveals a large variety of equilibria, with very different statistical properties. Natural selfish dynamics, such as best-response, usually tend to large-utility equilibria, even though those of smaller utility are exponentially more numerous. Interestingly, the latter can be actually reached by selecting the initial conditions of the best-response dynamics close to the saturation limit of the service unit capacities. We also study a more realistic stochastic variant of the game by means of a simple and effective approximation of the (quenched) average over the random parameters, showing that the properties of the equilibrium space are qualitatively similar to the deterministic case.',
	 'authors': u"F. Altarelli, A. Braunstein, L. Dall'Asta,",
	 'category': u'Computer Science ',
	 'date': '2014-9-6',
	 'pdflink': u'http://arxiv.org/pdf/1409.2013',
	 'subjects': u'Computer Science and Game Theory (cs.GT)',
	 'title': u'\nStatics and dynamics of selfish interactions in distributed service  systems',
	 'urllink': u'http://arxiv.org/abs/1409.2013'}
2015-03-23 20:22:54+0000 [xxu461000] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1405.2226> (referer: http://arxiv.org/list/cs/14?skip=5000&show=1000)
2015-03-23 20:22:54+0000 [xxu461000] DEBUG: Scraped from <200 http://arxiv.org/abs/1405.2226>
	{'abstract': u'Quantum computers are considered as a future alternative to circumvent the heat dissipation problem of VLSI circuits. The synthesis of reversible circuits is a very promising area of study considering the expected further technological advances towards quantum computing. In this report, we propose a linear genetic programming system to design reversible circuits -RIMEP2-. The system has evolved reversible circuits starting from scratch without resorting to a pre-existing library. The results show that among the 26 considered benchmarks, RIMEP2 outperformed the best published solutions for 20 of them and matched the remaining 6. RIMEP2 is presented in this report as a promising method with a considerable potential for reversible circuit design. It will be considered as work reference for future studies based on this method.',
	 'authors': u'Fatima Hadjam, Claudio Moraga,',
	 'category': u'Computer Science ',
	 'date': '2014-5-9',
	 'pdflink': u'http://arxiv.org/pdf/1405.2226',
	 'subjects': u'Emerging Technologies (cs.ET)',
	 'title': u'\nIntroduction to RIMEP2: A Multi-Expression Programming System for the  Design of Reversible Digital Circuits',
	 'urllink': u'http://arxiv.org/abs/1405.2226'}
2015-03-23 20:22:56+0000 [xxu461000] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1405.6929> (referer: http://arxiv.org/list/cs/14?skip=6000&show=1000)
2015-03-23 20:22:56+0000 [xxu461000] DEBUG: Scraped from <200 http://arxiv.org/abs/1405.6929>
	{'abstract': u"A directed cycle double cover of a graph G is a family of cycles of G, each provided with an orientation, such that every edge of G is covered by exactly two oppositely directed cycles. Explicit obstacles to the existence of a directed cycle double cover in a graph are bridges. Jaeger conjectured that bridges are actually the only obstacles. One of the difficulties in proving the Jaeger's conjecture lies in discovering and avoiding obstructions to partial strategies that, if successful, create directed cycle double covers. In this work, we suggest a way to circumvent this difficulty. We formulate a conjecture on graph connections, whose validity follows by the successful avoidance of one cut-type obstruction that we call cut-obstacles. The main result of this work claims that our 'cut-obstacles avoidance conjecture' already implies Jaeger's directed cycle double cover conjecture.",
	 'authors': u'Andrea Jim\xe9nez, Martin Loebl,',
	 'category': u'Computer Science ',
	 'date': '2014-5-27',
	 'pdflink': u'http://arxiv.org/pdf/1405.6929',
	 'subjects': u'Combinatorics (math.CO)',
	 'title': u'\nDirected cycle double covers and cut-obstacles',
	 'urllink': u'http://arxiv.org/abs/1405.6929'}
2015-03-23 20:23:00+0000 [xxu461000] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1407.5155> (referer: http://arxiv.org/list/cs/14?skip=8000&show=1000)
2015-03-23 20:23:00+0000 [xxu461000] DEBUG: Scraped from <200 http://arxiv.org/abs/1407.5155>
	{'abstract': u'A popular approach within the signal processing and machine learning communities consists in modelling signals as sparse linear combinations of atoms selected from a learned dictionary. While this paradigm has led to numerous empirical successes in various fields ranging from image to audio processing, there have only been a few theoretical arguments supporting these evidences. In particular, sparse coding, or sparse dictionary learning, relies on a non-convex procedure whose local minima have not been fully analyzed yet. In this paper, we consider a probabilistic model of sparse signals, and show that, with high probability, sparse coding admits a local minimum around the reference dictionary generating the signals. Our study takes into account the case of over-complete dictionaries, noisy signals, and possible outliers, thus extending previous work limited to noiseless settings and/or under-complete dictionaries. The analysis we conduct is non-asymptotic and makes it possible to understand how the key quantities of the problem, such as the coherence or the level of noise, can scale with respect to the dimension of the signals, the number of atoms, the sparsity and the number of observations.',
	 'authors': u'R\xe9mi Gribonval, Rodolphe Jenatton, Francis Bach,',
	 'category': u'Computer Science ',
	 'date': '2014-7-19',
	 'pdflink': u'http://arxiv.org/pdf/1407.5155',
	 'subjects': u'Learning (cs.LG)',
	 'title': u'\nSparse and spurious: dictionary learning with noise and outliers',
	 'urllink': u'http://arxiv.org/abs/1407.5155'}
2015-03-23 20:23:04+0000 [xxu461000] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1404.5278> (referer: http://arxiv.org/list/cs/14?skip=4000&show=1000)
2015-03-23 20:23:04+0000 [xxu461000] DEBUG: Scraped from <200 http://arxiv.org/abs/1404.5278>
	{'abstract': u'This paper develops a compositional vector-based semantics of subject and object relative pronouns within a categorical framework. Frobenius algebras are used to formalise the operations required to model the semantics of relative pronouns, including passing information between the relative clause and the modified noun phrase, as well as copying, combining, and discarding parts of the relative clause. We develop two instantiations of the abstract semantics, one based on a truth-theoretic approach and one based on corpus statistics.',
	 'authors': u'Mehrnoosh Sadrzadeh, Stephen Clark, Bob Coecke,',
	 'category': u'Computer Science ',
	 'date': '2014-4-21',
	 'pdflink': u'http://arxiv.org/pdf/1404.5278',
	 'subjects': u'Computation and Language (cs.CL)',
	 'title': u'\nThe Frobenius anatomy of word meanings I: subject and object relative  pronouns',
	 'urllink': u'http://arxiv.org/abs/1404.5278'}
2015-03-23 20:23:10+0000 [xxu461000] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1409.2008> (referer: http://arxiv.org/list/cs/14?skip=10000&show=1000)
2015-03-23 20:23:10+0000 [xxu461000] DEBUG: Scraped from <200 http://arxiv.org/abs/1409.2008>
	{'abstract': u'It is shown how the Python library Sympy can be used to compute symbolically the coefficients of the power series solution of the Lane-Emden equation (LEE). Sympy is an open source Python library for symbolic mathematics. The power series solutions are compared to the numerically computed solutions using matplotlib. The results of a run time measurement of the implemented algorithm are discussed at the end.',
	 'authors': u'Klaus Rohe,',
	 'category': u'Computer Science ',
	 'date': '2014-9-6',
	 'pdflink': u'http://arxiv.org/pdf/1409.2008',
	 'subjects': u'Mathematical Software (cs.MS)',
	 'title': u'\nComputing the coefficients for the power series solution of the  Lane-Emden equation with the Python library SymPy',
	 'urllink': u'http://arxiv.org/abs/1409.2008'}
2015-03-23 20:23:15+0000 [xxu461000] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1405.2221> (referer: http://arxiv.org/list/cs/14?skip=5000&show=1000)
2015-03-23 20:23:15+0000 [xxu461000] DEBUG: Scraped from <200 http://arxiv.org/abs/1405.2221>
	{'abstract': u'The classical writing on dirty paper capacity result establishes that full interference pre-cancellation can be attained in Gelfand-Pinsker problem with additive state and additive white Gaussian noise. This result holds under the idealized assumption that perfect channel knowledge is available at both transmitter and receiver. While channel knowledge at the receiver can be obtained through pilot tones, transmitter channel knowledge is harder to acquire. For this reason, we are interested in characterizing the capacity under the more realistic assumption that only partial channel knowledge is available at the transmitter. We study, more specifically, the dirty paper channel in which the interference sequence in multiplied by fading value unknown to the transmitter but known at the receiver. For this model, we establish an approximate characterization of capacity for the case in which fading values vary greatly in between channel realizations. In this regime, which we term the strong fading regime, the capacity pre-log factor is equal to the inverse of the number of possible fading realizations.',
	 'authors': u'Stefano Rini, Shlomo Shamai,',
	 'category': u'Computer Science ',
	 'date': '2014-5-9',
	 'pdflink': u'http://arxiv.org/pdf/1405.2221',
	 'subjects': u'Information Theory (cs.IT)',
	 'title': u'\nOn Capacity of the Dirty Paper Channel with Fading Dirt in the Strong  Fading Regime',
	 'urllink': u'http://arxiv.org/abs/1405.2221'}
2015-03-23 20:23:20+0000 [xxu461000] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1405.6920> (referer: http://arxiv.org/list/cs/14?skip=6000&show=1000)
2015-03-23 20:23:20+0000 [xxu461000] DEBUG: Scraped from <200 http://arxiv.org/abs/1405.6920>
	{'abstract': u'In this note we compare the randomized extended Kaczmarz (EK) algorithm and randomized coordinate descent (CD) for solving the full-rank overdetermined linear least-squares problem and prove that CD needs less operations for satisfying the same residual-related termination criteria. For the general least-squares problems, we show that running first CD to compute the residual and then standard Kaczmarz on the resulting consistent system is more efficient than EK.',
	 'authors': u'Bogdan Dumitrescu,',
	 'category': u'Computer Science ',
	 'date': '2014-5-27',
	 'pdflink': u'http://arxiv.org/pdf/1405.6920',
	 'subjects': u'Numerical Analysis (math.NA)',
	 'title': u'\nOn the Relation Between the Randomized Extended Kaczmarz Algorithm and  Coordinate Descent',
	 'urllink': u'http://arxiv.org/abs/1405.6920'}
2015-03-23 20:23:25+0000 [xxu461000] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1407.5145> (referer: http://arxiv.org/list/cs/14?skip=8000&show=1000)
2015-03-23 20:23:25+0000 [xxu461000] DEBUG: Scraped from <200 http://arxiv.org/abs/1407.5145>
	{'abstract': u'We propose a new method for improving the presentation of subtitles in video (e.g. TV and movies). With conventional subtitles, the viewer has to constantly look away from the main viewing area to read the subtitles at the bottom of the screen, which disrupts the viewing experience and causes unnecessary eyestrain. Our method places on-screen subtitles next to the respective speakers to allow the viewer to follow the visual content while simultaneously reading the subtitles. We use novel identification algorithms to detect the speakers based on audio and visual information. Then the placement of the subtitles is determined using global optimization. A comprehensive usability study indicated that our subtitle placement method outperformed both conventional fixed-position subtitling and another previous dynamic subtitling method in terms of enhancing the overall viewing experience and reducing eyestrain.',
	 'authors': u'Yongtao Hu, Jan Kautz, Yizhou Yu, Wenping Wang,',
	 'category': u'Computer Science ',
	 'date': '2014-7-19',
	 'pdflink': u'http://arxiv.org/pdf/1407.5145',
	 'subjects': u'Human-Computer Interaction (cs.HC)',
	 'title': u'\nSpeaker-following Video Subtitles',
	 'urllink': u'http://arxiv.org/abs/1407.5145'}
2015-03-23 20:23:29+0000 [xxu461000] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1404.5267> (referer: http://arxiv.org/list/cs/14?skip=4000&show=1000)
2015-03-23 20:23:29+0000 [xxu461000] DEBUG: Scraped from <200 http://arxiv.org/abs/1404.5267>
	{'abstract': u"We generalize the work by P. Soboci 'nski on relational presheaves and their connection with weak (bi)simulation for labelled transistion systems to a coalgebraic setting. We show that the coalgebraic notion of saturation studied in our previous work can be expressed in the language of lax Kleisli-valued presheaves in terms of existence of a certain adjoint situation between presheaf categories. This observation allows us to generalize the notion of the coalgebraic (weak) bisimulation to lax Kleisli-valued presheaves. At this level of generality interesting properties of strong and weak bisimilarity emerge: in the family of -bisimilarities, which arises naturally in this setting, the former is the finest and the latter is the coarsest relation.",
	 'authors': u'Tomasz Brengos,',
	 'category': u'Computer Science ',
	 'date': '2014-4-21',
	 'pdflink': u'http://arxiv.org/pdf/1404.5267',
	 'subjects': u'Logic in Computer Science (cs.LO)',
	 'title': u'\nLax Kleisli-valued presheaves and coalgebraic weak bisimulation',
	 'urllink': u'http://arxiv.org/abs/1404.5267'}
2015-03-23 20:23:35+0000 [xxu461000] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1409.2003> (referer: http://arxiv.org/list/cs/14?skip=10000&show=1000)
2015-03-23 20:23:35+0000 [xxu461000] DEBUG: Scraped from <200 http://arxiv.org/abs/1409.2003>
	{'abstract': u'A word is called a reset word for a deterministic finite automaton if it maps all the states of the automaton to a unique state. Deciding about the existence of a reset word of a given maximum length for a given automaton is known to be an NP-complete problem. We prove that it remains NP-complete even if restricted to Eulerian automata with binary alphabets, as it has been conjectured by Martyugin (2011).',
	 'authors': u'Vojt\u011bch Vorel,',
	 'category': u'Computer Science ',
	 'date': '2014-9-6',
	 'pdflink': u'http://arxiv.org/pdf/1409.2003',
	 'subjects': u'Formal Languages and Automata Theory (cs.FL)',
	 'title': u'\nComplexity of a Problem Concerning Reset Words for Eulerian Binary  Automata',
	 'urllink': u'http://arxiv.org/abs/1409.2003'}
2015-03-23 20:23:38+0000 [xxu461000] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1405.2216> (referer: http://arxiv.org/list/cs/14?skip=5000&show=1000)
2015-03-23 20:23:38+0000 [xxu461000] DEBUG: Scraped from <200 http://arxiv.org/abs/1405.2216>
	{'abstract': u'Dynamic spectrum sharing approach is a paradigm shift from the conventional static and exclusive approach to spectrum allocation. The existing methodologies to define use of the spectrum and quantify its efficiency are based on the static spectrum assignment paradigm and not suitable for the dynamic spectrum sharing paradigm. There is a need to separately quantify the spectrum consumed by the individual transmitters and receivers when multiple heterogeneous wireless networks are sharing the spectrum in time, space, and frequency dimensions. By discretizing the spectrum dimensions, we define a methodology for quantifying the spectrum consumption spaces. This is an attempt to adopt the discretized signal processing principle and apply it to spectrum management functions that would bring in simplicity, flexibility, and precision among other advantages.',
	 'authors': u'Nilesh khambekar, Chad M. Spooner, Vipin Chaudhary,',
	 'category': u'Computer Science ',
	 'date': '2014-5-9',
	 'pdflink': u'http://arxiv.org/pdf/1405.2216',
	 'subjects': u'Networking and Internet Architecture (cs.NI)',
	 'title': u'\nGoing Towards Discretized Spectrum Space: Quantification of Spectrum  Consumption Spaces and a Quantified Spectrum Access Paradigm',
	 'urllink': u'http://arxiv.org/abs/1405.2216'}
2015-03-23 20:23:43+0000 [xxu461000] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1405.6879> (referer: http://arxiv.org/list/cs/14?skip=6000&show=1000)
2015-03-23 20:23:43+0000 [xxu461000] DEBUG: Scraped from <200 http://arxiv.org/abs/1405.6879>
	{'abstract': u"Diffusion of innovation can be interpreted as a social spreading phenomena governed by the impact of media and social interactions. Although these mechanisms have been identified by quantitative theories, their role and relative importance are not entirely understood, since empirical verification has so far been hindered by the lack of appropriate data. Here we analyse a dataset recording the spreading dynamics of the world's largest Voice over Internet Protocol service to empirically support the assumptions behind models of social contagion. We show that the rate of spontaneous service adoption is constant, the probability of adoption via social influence is linearly proportional to the fraction of adopting neighbours, and the rate of service termination is time-invariant and independent of the behaviour of peers. By implementing the detected diffusion mechanisms into a dynamical agent-based model, we are able to emulate the adoption dynamics of the service in several countries worldwide. This approach enables us to make medium-term predictions of service adoption and disclose dependencies between the dynamics of innovation spreading and the socioeconomic development of a country.",
	 'authors': u'M\xe1rton Karsai, Gerardo I\xf1iguez, Kimmo Kaski, J\xe1nos Kert\xe9sz,',
	 'category': u'Computer Science ',
	 'date': '2014-5-27',
	 'pdflink': u'http://arxiv.org/pdf/1405.6879',
	 'subjects': u'Physics and Society (physics.soc-ph)',
	 'title': u'\nComplex contagion process in spreading of online innovation',
	 'urllink': u'http://arxiv.org/abs/1405.6879'}
2015-03-23 20:23:46+0000 [xxu461000] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1407.5136> (referer: http://arxiv.org/list/cs/14?skip=8000&show=1000)
2015-03-23 20:23:46+0000 [xxu461000] DEBUG: Scraped from <200 http://arxiv.org/abs/1407.5136>
	{'abstract': u'In this paper, we investigate novel strategies for generating rate-compatible (RC) irregular low-density parity-check (LDPC) codes with short/moderate block lengths. We propose three puncturing and two extension schemes, which are designed to determine the puncturing positions that minimize the performance degradation and the extension that maximize the performance. The first puncturing scheme employs a counting cycle algorithm and a grouping strategy for variable nodes having short cycles of equal length in the Tanner Graph (TG). The second scheme relies on a metric called Extrinsic Message Degree (EMD) and the third scheme is a simulation-based exhaustive search to find the best puncturing pattern among several random ones. In addition, we devise two layer-structured extension schemes based on a counting cycle algorithm and an EMD metric which are applied to design RC-LDPC codes. Simulation results show that the proposed extension and puncturing techniques achieve greater rate flexibility and good performance over the additive white Gaussian noise (AWGN) channel, outperforming existing techniques.',
	 'authors': u'J. Liu, R. C. de Lamare,',
	 'category': u'Computer Science ',
	 'date': '2014-7-19',
	 'pdflink': u'http://arxiv.org/pdf/1407.5136',
	 'subjects': u'Information Theory (cs.IT)',
	 'title': u'\nRate-Compatible LDPC Codes Based on Puncturing and Extension Techniques  for Short Block Lengths',
	 'urllink': u'http://arxiv.org/abs/1407.5136'}
2015-03-23 20:23:48+0000 [xxu461000] INFO: Crawled 368 pages (at 14 pages/min), scraped 351 items (at 14 items/min)
2015-03-23 20:23:50+0000 [xxu461000] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1404.5254> (referer: http://arxiv.org/list/cs/14?skip=4000&show=1000)
2015-03-23 20:23:50+0000 [xxu461000] DEBUG: Scraped from <200 http://arxiv.org/abs/1404.5254>
	{'abstract': u'The use of simultaneous sources in geophysical inverse problems has revolutionized the ability to deal with large scale data sets that are obtained from multiple source experiments. However, the technique breaks when the data has non-uniform standard deviation or when some data are missing. In this paper we develop, study, and compare a number of techniques that enable to utilize advantages of the simultaneous source framework for these cases. We show that the inverse problem can still be solved efficiently by using these new techniques. We demonstrate our new approaches on the Direct Current Resistivity inverse problem.',
	 'authors': u'Eldad Haber, Mathias Chung,',
	 'category': u'Computer Science ',
	 'date': '2014-4-21',
	 'pdflink': u'http://arxiv.org/pdf/1404.5254',
	 'subjects': u'Computational Engineering, Finance, and Science (cs.CE)',
	 'title': u'\nSimultaneous Source for non-uniform data variance and missing data',
	 'urllink': u'http://arxiv.org/abs/1404.5254'}
2015-03-23 20:23:53+0000 [xxu461000] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1409.2002> (referer: http://arxiv.org/list/cs/14?skip=10000&show=1000)
2015-03-23 20:23:53+0000 [xxu461000] DEBUG: Scraped from <200 http://arxiv.org/abs/1409.2002>
	{'abstract': u"Nowadays, social medias such as Twitter, Memetracker and Blogs have become powerful tools to propagate information. They facilitate quick dissemination sequence of information such as news article, blog posts, user's interests and thoughts through large scale. Providing strong means to analyzing social networks structure and how information diffuse through them is essential. Many recent studies emphasize on modeling information diffusion and their patterns to gain some useful knowledge. In this paper, we propose a statistical approach to online detect peak points of news when spread over social networks, to the best of our knowledge has never investigated before. The proposed model use martingale approach to predict peak points when news reached the peak of its popularity. Experimental results on real datasets show good performance of our approach to online detect these peak points.",
	 'authors': u'Saba Babakhani, Niloofar Mozaffari, Ali Hamzeh,',
	 'category': u'Computer Science ',
	 'date': '2014-9-6',
	 'pdflink': u'http://arxiv.org/pdf/1409.2002',
	 'subjects': u'Social and Information Networks (cs.SI)',
	 'title': u'\nA Martingale Approach to Detect Peak of News in Social Network',
	 'urllink': u'http://arxiv.org/abs/1409.2002'}
2015-03-23 20:23:59+0000 [xxu461000] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1405.2212> (referer: http://arxiv.org/list/cs/14?skip=5000&show=1000)
2015-03-23 20:23:59+0000 [xxu461000] DEBUG: Scraped from <200 http://arxiv.org/abs/1405.2212>
	{'abstract': u'The path to greater diversity, as we have seen, cannot be achieved by merely hoping for a new search engine nor will government support for a single alternative achieve this goal. What is instead required is to create the conditions that will make establishing such a search engine possible in the first place. I describe how building and maintaining a proprietary index is the greatest deterrent to such an undertaking. We must first overcome this obstacle. Doing so will still not solve the problem of the lack of diversity in the search engine marketplace. But it may establish the conditions necessary to achieve that desired end.',
	 'authors': u'Dirk Lewandowski,',
	 'category': u'Computer Science ',
	 'date': '2014-5-9',
	 'pdflink': u'http://arxiv.org/pdf/1405.2212',
	 'subjects': u'Information Retrieval (cs.IR)',
	 'title': u'\nWhy we need an independent index of the Web',
	 'urllink': u'http://arxiv.org/abs/1405.2212'}
2015-03-23 20:24:04+0000 [xxu461000] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1405.6866> (referer: http://arxiv.org/list/cs/14?skip=6000&show=1000)
2015-03-23 20:24:04+0000 [xxu461000] DEBUG: Scraped from <200 http://arxiv.org/abs/1405.6866>
	{'abstract': u'We introduce the point spectrum of a represented spaces as a substructure of the Medvedev degrees. The point spectrum is closely linked to isomorphism type of a space w.r.t. countably continuous maps, and via this, also with the dimension. Through this new connection between descriptive set theory and degree theory (as part of computability theory) we can answer several open questions. As a result on the way, we prove that any admissible represented space with an effectively fiber-compact representation is already computably metrizable.',
	 'authors': u'Takayuki Kihara, Arno Pauly,',
	 'category': u'Computer Science ',
	 'date': '2014-5-27',
	 'pdflink': u'http://arxiv.org/pdf/1405.6866',
	 'subjects': u'General Topology (math.GN)',
	 'title': u'\nPoint degree spectra of represented spaces',
	 'urllink': u'http://arxiv.org/abs/1405.6866'}
2015-03-23 20:24:10+0000 [xxu461000] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1407.5128> (referer: http://arxiv.org/list/cs/14?skip=8000&show=1000)
2015-03-23 20:24:10+0000 [xxu461000] DEBUG: Scraped from <200 http://arxiv.org/abs/1407.5128>
	{'abstract': u'Let COLk be the set of all k-colorable graphs. It is easy to show that if a&lt;b then COLa le COLb (poly time reduction). Using the Cook-Levin theorem it is easy to show that if 3 le a&lt; b then COLb le COLa. However this proof is insane in that it translates a graph to a formula and then the formula to a graph. We give a simple proof that COLk le COL3.',
	 'authors': u'William Gasarch,',
	 'category': u'Computer Science ',
	 'date': '2014-7-18',
	 'pdflink': u'http://arxiv.org/pdf/1407.5128',
	 'subjects': u'Computational Complexity (cs.CC)',
	 'title': u'\nA Sane Proof that COLk \\le COL3',
	 'urllink': u'http://arxiv.org/abs/1407.5128'}
2015-03-23 20:24:14+0000 [xxu461000] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1404.5248> (referer: http://arxiv.org/list/cs/14?skip=4000&show=1000)
2015-03-23 20:24:14+0000 [xxu461000] DEBUG: Scraped from <200 http://arxiv.org/abs/1404.5248>
	{'abstract': u'Recommender systems for TV program have been studied for the realization of personalized TV Electronic Program Guides. In this paper, we propose automatic emotion Arabic speech recognition in order to achieve an intelligent remote control. In addition, the TV can estimate our interests and preferences by observing our behavior to watch and have a conversation on topics that might be interesting to us.',
	 'authors': u'M. Meddeb, H. Karray, Adel M. Alimi,',
	 'category': u'Computer Science ',
	 'date': '2014-4-21',
	 'pdflink': u'http://arxiv.org/pdf/1404.5248',
	 'subjects': u'Human-Computer Interaction (cs.HC)',
	 'title': u'\nIntelligent Remote Control for TV Program based on Emotion in Arabic  Speech',
	 'urllink': u'http://arxiv.org/abs/1404.5248'}
2015-03-23 20:24:20+0000 [xxu461000] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1409.1989> (referer: http://arxiv.org/list/cs/14?skip=10000&show=1000)
2015-03-23 20:24:20+0000 [xxu461000] DEBUG: Scraped from <200 http://arxiv.org/abs/1409.1989>
	{'abstract': u'Formula-based debugging techniques are becoming increasingly popular, as they provide a principled way to identify potentially faulty statements together with information that can help fix such statements. Although effective, these approaches are computationally expensive, which limits their practical applicability. Moreover, they tend to focus on failing test cases alone, thus ignoring the wealth of information provided by passing tests. To mitigate these issues, we propose two techniques: on-demand formula computation (OFC) and clause weighting (CW). OFC improves the overall efficiency of formula-based debugging by exploring all and only the parts of a program that are relevant to a failure. CW improves the accuracy of formula-based debugging by leveraging statistical fault-localization information that accounts for passing tests. Our empirical results show that both techniques are effective and can improve the state of the art in formula-based debugging.',
	 'authors': u'Wei Jin, Alessandro Orso,',
	 'category': u'Computer Science ',
	 'date': '2014-9-6',
	 'pdflink': u'http://arxiv.org/pdf/1409.1989',
	 'subjects': u'Software Engineering (cs.SE)',
	 'title': u'\nImproving Efficiency and Scalability of Formula-based Debugging',
	 'urllink': u'http://arxiv.org/abs/1409.1989'}
2015-03-23 20:24:25+0000 [xxu461000] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1405.2210> (referer: http://arxiv.org/list/cs/14?skip=5000&show=1000)
2015-03-23 20:24:25+0000 [xxu461000] DEBUG: Scraped from <200 http://arxiv.org/abs/1405.2210>
	{'abstract': u"Search engine retrieval effectiveness studies are usually small-scale, using only limited query samples. Furthermore, queries are selected by the researchers. We address these issues by taking a random representative sample of 1,000 informational and 1,000 navigational queries from a major German search engine and comparing Google's and Bing's results based on this sample. Jurors were found through crowdsourcing, data was collected using specialised software, the Relevance Assessment Tool (RAT). We found that while Google outperforms Bing in both query types, the difference in the performance for informational queries was rather low. However, for navigational queries, Google found the correct answer in 95.3 per cent of cases whereas Bing only found the correct answer 76.6 per cent of the time. We conclude that search engine performance on navigational queries is of great importance, as users in this case can clearly identify queries that have returned correct results. So, performance on this query type may contribute to explaining user satisfaction with search engines.",
	 'authors': u'Dirk Lewandowski,',
	 'category': u'Computer Science ',
	 'date': '2014-5-9',
	 'pdflink': u'http://arxiv.org/pdf/1405.2210',
	 'subjects': u'Information Retrieval (cs.IR)',
	 'title': u'\nEvaluating the retrieval effectiveness of Web search engines using a  representative query sample',
	 'urllink': u'http://arxiv.org/abs/1405.2210'}
2015-03-23 20:24:29+0000 [xxu461000] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1405.6804> (referer: http://arxiv.org/list/cs/14?skip=6000&show=1000)
2015-03-23 20:24:29+0000 [xxu461000] DEBUG: Scraped from <200 http://arxiv.org/abs/1405.6804>
	{'abstract': u"Designing effective and efficient classifier for pattern analysis is a key problem in machine learning and computer vision. Many the solutions to the problem require to perform logic operations such as `and', `or', and `not'. Classification and regression tree (CART) include these operations explicitly. Other methods such as neural networks, SVM, and boosting learn/compute a weighted sum on features (weak classifiers), which weakly perform the 'and' and 'or' operations. However, it is hard for these classifiers to deal with the 'xor' pattern directly. In this paper, we propose layered logic classifiers for patterns of complicated distributions by combining the `and', `or', and `not' operations. The proposed algorithm is very general and easy to implement. We test the classifiers on several typical datasets from the Irvine repository and two challenging vision applications, object segmentation and pedestrian detection. We observe significant improvements on all the datasets over the widely used decision stump based AdaBoost algorithm. The resulting classifiers have much less training complexity than decision tree based AdaBoost, and can be applied in a wide range of domains.",
	 'authors': u'Zhuowen Tu, Piotr Dollar, Yingnian Wu,',
	 'category': u'Computer Science ',
	 'date': '2014-5-27',
	 'pdflink': u'http://arxiv.org/pdf/1405.6804',
	 'subjects': u'Machine Learning (stat.ML)',
	 'title': u"\nLayered Logic Classifiers: Exploring the `And' and `Or' Relations",
	 'urllink': u'http://arxiv.org/abs/1405.6804'}
2015-03-23 20:24:35+0000 [xxu461000] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1407.5126> (referer: http://arxiv.org/list/cs/14?skip=8000&show=1000)
2015-03-23 20:24:35+0000 [xxu461000] DEBUG: Scraped from <200 http://arxiv.org/abs/1407.5126>
	{'abstract': u'In many embedded real-time systems, applications often interact with I/O devices via read/write operations, which may incur considerable suspension delays. Unfortunately, prior analysis methods for validating timing correctness in embedded systems become quite pessimistic when suspension delays are present. In this paper, we consider the problem of supporting two common types of I/O applications in a multiprocessor system, that is, write-only applications and read-write applications. For the write-only application model, we present a much improved analysis technique that results in only O(m) suspension-related utilization loss, where m is the number of processors. For the second application model, we present a flexible I/O placement strategy and a corresponding new scheduling algorithm, which can completely circumvent the negative impact due to read- and write-induced suspension delays. We illustrate the feasibility of the proposed I/O-placement-based schedule via a case study implementation. Furthermore, experiments presented herein show that the improvement with respect to system utilization over prior methods is often significant.',
	 'authors': u'Guangmo Tong, Cong Liu,',
	 'category': u'Computer Science ',
	 'date': '2014-7-18',
	 'pdflink': u'http://arxiv.org/pdf/1407.5126',
	 'subjects': u'Other Computer Science (cs.OH)',
	 'title': u'\nSupporting Read/Write Applications in Embedded Real-time Systems via  Suspension-aware Analysis',
	 'urllink': u'http://arxiv.org/abs/1407.5126'}
2015-03-23 20:24:40+0000 [xxu461000] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1404.5245> (referer: http://arxiv.org/list/cs/14?skip=4000&show=1000)
2015-03-23 20:24:40+0000 [xxu461000] DEBUG: Scraped from <200 http://arxiv.org/abs/1404.5245>
	{'abstract': u'We study the House Allocation problem (also known as the Assignment problem), i.e., the problem of allocating a set of objects among a set of agents, where each agent has ordinal preferences (possibly involving ties) over a subset of the objects. We focus on truthful mechanisms without monetary transfers for finding large Pareto optimal matchings. It is straightforward to show that no deterministic truthful mechanism can approximate a maximum cardinality Pareto optimal matching with ratio better than 2. We thus consider randomized mechanisms. We give a natural and explicit extension of the classical Random Serial Dictatorship Mechanism (RSDM) specifically for the House Allocation problem where preference lists can include ties. We thus obtain a universally truthful randomized mechanism for finding a Pareto optimal matching and show that it achieves an approximation ratio of . The same bound holds even when agents have priorities (weights) and our goal is to find a maximum weight (as opposed to maximum cardinality) Pareto optimal matching. On the other hand we give a lower bound of on the approximation ratio of any universally truthful Pareto optimal mechanism in settings with strict preferences. In the case that the mechanism must additionally be non-bossy, an improved lower bound of holds. This lower bound is tight given that RSDM for strict preference lists is non-bossy. We moreover interpret our problem in terms of the classical secretary problem and prove that our mechanism provides the best randomized strategy of the administrator who interviews the applicants.',
	 'authors': u'Piotr Krysta, David Manlove, Baharak Rastegari, Jinshan Zhang,',
	 'category': u'Computer Science ',
	 'date': '2014-4-21',
	 'pdflink': u'http://arxiv.org/pdf/1404.5245',
	 'subjects': u'Computer Science and Game Theory (cs.GT)',
	 'title': u'\nSize versus truthfulness in the House Allocation problem',
	 'urllink': u'http://arxiv.org/abs/1404.5245'}
2015-03-23 20:24:45+0000 [xxu461000] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1409.1987> (referer: http://arxiv.org/list/cs/14?skip=10000&show=1000)
2015-03-23 20:24:45+0000 [xxu461000] DEBUG: Scraped from <200 http://arxiv.org/abs/1409.1987>
	{'abstract': u'The Wiener index is one of the oldest graph parameter which is used to study molecular-graph-based structure. This parameter was first proposed by Harold Wiener in 1947 to determining the boiling point of paraffin. The Wiener index of a molecular graph measures the compactness of the underlying molecule. This parameter is wide studied area for molecular chemistry. It is used to study the physio-chemical properties of the underlying organic compounds. The Wiener index of a connected graph is denoted by W(G) and is defined as, that is W(G) is the sum of distances between all pairs (ordered) of vertices of G. In this paper, we give the algorithmic idea to find the Wiener index of some graphs, like cactus graphs and intersection graphs, viz. interval, circular-arc, permutation, trapezoid graphs.',
	 'authors': u'Kalyani Das,',
	 'category': u'Computer Science ',
	 'date': '2014-9-6',
	 'pdflink': u'http://arxiv.org/pdf/1409.1987',
	 'subjects': u'Discrete Mathematics (cs.DM)',
	 'title': u'\nComplexity to Find Wiener Index of Some Graphs',
	 'urllink': u'http://arxiv.org/abs/1409.1987'}
2015-03-23 20:24:48+0000 [xxu461000] INFO: Crawled 380 pages (at 12 pages/min), scraped 363 items (at 12 items/min)
2015-03-23 20:24:50+0000 [xxu461000] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1406.7650> (referer: http://arxiv.org/list/cs/14?skip=7000&show=1000)
2015-03-23 20:24:50+0000 [xxu461000] DEBUG: Scraped from <200 http://arxiv.org/abs/1406.7650>
	{'abstract': u'In this paper, we propose a new gossip-based signaling dissemination method for the Next Steps in Signaling protocol family. In more detail, we propose to extend the General Internet Signaling Transport (GIST) protocol, so as to leverage these new dissemination capabilities from all NSIS Signaling Layer Protocol applications using its transport capabilities. The new GIST extension consists of two main procedures: a bootstrap procedure, during which new GIST-enabled nodes discover each other, and a service dissemination procedure, which is used to effectively disseminate signaling messages within an Autonomous System. To this aim, we defined three dissemination models, bubble, balloon, and hose, so as to fulfill requirements of different network and/or service management scenarios. An experimental campaign carried out on the GENI testbed shows the effectiveness of the proposed solution.',
	 'authors': u'M. Femminella, R. Francescangeli, G. Reali, H. Schulzrinne,',
	 'category': u'Computer Science ',
	 'date': '2014-6-30',
	 'pdflink': u'http://arxiv.org/pdf/1406.7650',
	 'subjects': u'Networking and Internet Architecture (cs.NI)',
	 'title': u'\nGossip-based Signaling Dissemination Extension for Next Steps In  Signaling',
	 'urllink': u'http://arxiv.org/abs/1406.7650'}
2015-03-23 20:24:56+0000 [xxu461000] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1405.2202> (referer: http://arxiv.org/list/cs/14?skip=5000&show=1000)
2015-03-23 20:24:56+0000 [xxu461000] DEBUG: Scraped from <200 http://arxiv.org/abs/1405.2202>
	{'abstract': u'In this paper we propose and analyze a novel self-interference cancellation structure for in-band MIMO full-duplex transceivers. The proposed structure utilizes reference receiver chains to obtain reference signals for digital self-interference cancellation, which means that all the transmitter-induced nonidealities will be included in the digital cancellation signal. To the best of our knowledge, this type of a structure has not been discussed before in the context of full-duplex transceivers. First, we will analyze the overall achievable performance of the proposed cancellation scheme, while also providing some insight into the possible bottlenecks. We also provide a detailed formulation of the actual cancellation procedure, and perform an analysis into the effect of the received signal of interest on self-interference coupling channel estimation. The achieved performance of the proposed reference receiver based digital cancellation procedure is then assessed and verified with full waveform simulations. The analysis and waveform simulation results show that under practical transmitter RF/analog impairment levels, the proposed reference receiver based cancellation architecture can provide substantially better self-interference suppression than any existing solution, despite deploying only low-complexity linear digital processing.',
	 'authors': u'Dani Korpi, Lauri Anttila, Mikko Valkama,',
	 'category': u'Computer Science ',
	 'date': '2014-5-9',
	 'pdflink': u'http://arxiv.org/pdf/1405.2202',
	 'subjects': u'Information Theory (cs.IT)',
	 'title': u'\nReference Receiver Based Digital Self-Interference Cancellation in MIMO  Full-Duplex Transceivers',
	 'urllink': u'http://arxiv.org/abs/1405.2202'}
2015-03-23 20:25:02+0000 [xxu461000] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1405.6802> (referer: http://arxiv.org/list/cs/14?skip=6000&show=1000)
2015-03-23 20:25:02+0000 [xxu461000] DEBUG: Scraped from <200 http://arxiv.org/abs/1405.6802>
	{'abstract': u'We give an improved algorithm for counting the number of -avoiding permutations, resulting in 5 further terms of the generating function. We analyse the known coefficients and find compelling evidence that unlike other classical length-4 pattern-avoiding permutations, the generating function in this case does not have an algebraic singularity. Rather, the number of 1324-avoiding permutations of length behaves as B cdot mu^n cdot mu_1^ cdot n^g. We estimate and',
	 'authors': u'Andrew R Conway, Anthony J Guttmann,',
	 'category': u'Computer Science ',
	 'date': '2014-5-27',
	 'pdflink': u'http://arxiv.org/pdf/1405.6802',
	 'subjects': u'Combinatorics (math.CO)',
	 'title': u'\nOn the growth rate of 1324-avoiding permutations',
	 'urllink': u'http://arxiv.org/abs/1405.6802'}
2015-03-23 20:25:08+0000 [xxu461000] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1407.5117> (referer: http://arxiv.org/list/cs/14?skip=8000&show=1000)
2015-03-23 20:25:08+0000 [xxu461000] DEBUG: Scraped from <200 http://arxiv.org/abs/1407.5117>
	{'abstract': u'Science and engineering research increasingly relies on activities that facilitate research but are not currently rewarded or recognized, such as: data sharing; developing common data resources, software and methodologies; and annotating data and publications. To promote and advance these activities, we must develop mechanisms for assigning credit, facilitate the appropriate attribution of research outcomes, devise incentives for activities that facilitate research, and allocate funds to maximize return on investment. In this article, we focus on addressing the issue of assigning credit for both direct and indirect contributions, specifically by using JSON-LD to implement a prototype transitive credit system.',
	 'authors': u'Daniel S. Katz, Arfon M. Smith,',
	 'category': u'Computer Science ',
	 'date': '2014-7-18',
	 'pdflink': u'http://arxiv.org/pdf/1407.5117',
	 'subjects': u'Computers and Society (cs.CY)',
	 'title': u'\nImplementing Transitive Credit with JSON-LD',
	 'urllink': u'http://arxiv.org/abs/1407.5117'}
2015-03-23 20:25:13+0000 [xxu461000] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1404.5244> (referer: http://arxiv.org/list/cs/14?skip=4000&show=1000)
2015-03-23 20:25:13+0000 [xxu461000] DEBUG: Scraped from <200 http://arxiv.org/abs/1404.5244>
	{'abstract': u'Given a language that is online recognizable in linear time and space, we construct a linear time and space online recognition algorithm for the language , where is the language of all nonempty palindromes. Hence for every fixed positive , is online recognizable in linear time and space. Thus we solve an open problem posed by Galil and Seiferas in 1978.',
	 'authors': u'Dmitry Kosolobov, Mikhail Rubinchik, Arseny M. Shur,',
	 'category': u'Computer Science ',
	 'date': '2014-4-21',
	 'pdflink': u'http://arxiv.org/pdf/1404.5244',
	 'subjects': u'Formal Languages and Automata Theory (cs.FL)',
	 'title': u'\n$\\mathrm{Pal}^k$ Is Linear Recognizable Online',
	 'urllink': u'http://arxiv.org/abs/1404.5244'}
2015-03-23 20:25:18+0000 [xxu461000] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1409.1981> (referer: http://arxiv.org/list/cs/14?skip=10000&show=1000)
2015-03-23 20:25:18+0000 [xxu461000] DEBUG: Scraped from <200 http://arxiv.org/abs/1409.1981>
	{'abstract': u'A physiological signal monitoring system and alerting system using wireless technology is presented. The two types of physiological signal monitoring are captured from the body through leads and using the radio-frequency transmitting and receiving module the data are interfaced to computer systems. Furthering using a developed user interface module the captured signals are analyzed for checking abnormality. Any significant recordings are transmitted to the physicians hand phone by using external serial SMS modem. ECG signal de-noising is conducted by using low-pass and high-pass filters. EEG signals de-noising is conducted by using band-pass filters set. A comparative evaluation of the module with the manual recording shows encouraging results. The ECG and EEG pattern are presented in this paper.',
	 'authors': u'Ramesh.GP, Aravind.CV, Rajparthiban.R, N.Soysa,',
	 'category': u'Computer Science ',
	 'date': '2014-9-6',
	 'pdflink': u'http://arxiv.org/pdf/1409.1981',
	 'subjects': u'Human-Computer Interaction (cs.HC)',
	 'title': u'\nA Body Area Network through Wireless Technology',
	 'urllink': u'http://arxiv.org/abs/1409.1981'}
2015-03-23 20:25:21+0000 [xxu461000] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1406.7630> (referer: http://arxiv.org/list/cs/14?skip=7000&show=1000)
2015-03-23 20:25:21+0000 [xxu461000] DEBUG: Scraped from <200 http://arxiv.org/abs/1406.7630>
	{'abstract': u'This paper deals a continuous-time state-dependent jump linear system, a particular kind of stochastic switching system. In particular, we consider a situation when the transition rate of the random jump process depends on the state variable, and addressed the problem of stochastic stability and stabilization analysis for the proposed system. Numerically solvable ?sufficient conditions for the stochastic stability and stabilization of the proposed system is established in terms of linear matrix inequalities. The obtained results are illustrated in numerical examples.',
	 'authors': u'Shaikshavali Chitraganti, Samir Aberkane, Christophe Aubrun,',
	 'category': u'Computer Science ',
	 'date': '2014-6-30',
	 'pdflink': u'http://arxiv.org/pdf/1406.7630',
	 'subjects': u'Systems and Control (cs.SY)',
	 'title': u'\nStochastic stability and stabilization of a class of state-dependent  jump linear systems',
	 'urllink': u'http://arxiv.org/abs/1406.7630'}
2015-03-23 20:25:26+0000 [xxu461000] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1405.2199> (referer: http://arxiv.org/list/cs/14?skip=5000&show=1000)
2015-03-23 20:25:26+0000 [xxu461000] DEBUG: Scraped from <200 http://arxiv.org/abs/1405.2199>
	{'abstract': u'In this paper, it is shown that all programmes of all television channels can be modelled as an interval graph. The programme slots are taken as the vertices of the graph and if the time duration of two have non-empty intersection, the corresponding vertices are considered to be connected by an edge. The number of viewers of a programme is taken as the weight of the vertex. A set of programmes that are mutually exclusive in respect of time scheduling is called a session. We assume that a company sets the objective of selecting the popular programmes in parallel sessions among different channels so as to make its commercial advertisement reach the maximum number of viewers, that is, a company selects suitable programme slots simultaneously for advertisement. The aim of the paper is, therefore, to the companies to select the programme slots, which are mutually exclusive with respect to the time schedule of telecasting time, in such a way that the total number of viewers of the selected programme in parallel slots rises to the optimum level. It is shown that the solution of this problem is obtained by solving the maximum weight -colouring problem on an interval . An algorithm is designed to solve this just-in-time optimization problem using time, where and represent the total number of programmes of all channels and the upper bound of the viewers of all programmes of all channels respectively. The problem considered in this paper is a daily life problem which is modeled by -colouring problem on interval graph.',
	 'authors': u'Madhumangal Pal, Anita Pal,',
	 'category': u'Computer Science ',
	 'date': '2014-5-9',
	 'pdflink': u'http://arxiv.org/pdf/1405.2199',
	 'subjects': u'Discrete Mathematics (cs.DM)',
	 'title': u'\nScheduling algorithm to select $k$ optimal programme slots in television  channels: A graph theoretic approach',
	 'urllink': u'http://arxiv.org/abs/1405.2199'}
2015-03-23 20:25:29+0000 [xxu461000] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1405.6703> (referer: http://arxiv.org/list/cs/14?skip=6000&show=1000)
2015-03-23 20:25:29+0000 [xxu461000] DEBUG: Scraped from <200 http://arxiv.org/abs/1405.6703>
	{'abstract': u'Often mistakenly seen as a game, the online 3D immersive virtual world Second Life (SL) is itself a huge and sophisticated simulator of an entire Earthlike world. Differently from other metaverses where physical laws are not seriously taken into account, objects created in SL are automatically controlled by a powerful physics engine software. Despite that, it has been used mostly as a mere place for exploration and inquiry, with emphasis on group interaction. This article reports on a study conducted to evaluate the SL environment as a platform for physical simulations and microworlds. It begins by discussing a few relevant features of SL and a few differences found between it and traditional simulators e.g. Modellus. Finally, the SL environment as a platform for physical simulations and microworlds is evaluated. Some concrete examples of simulations in SL, including two of our own authorship, will be presented briefly in order to clarify and enrich both discussion and analysis. However, implementation of simulations in SL is not without drawbacks like the lack of experience many teachers have with programming and the differences found between SL Physics and Newtonian Physics. Despite of that, findings suggest it may possible for teachers to overcome these obstacles.',
	 'authors': u'Renato P. dos Santos,',
	 'category': u'Computer Science ',
	 'date': '2014-5-24',
	 'pdflink': u'http://arxiv.org/pdf/1405.6703',
	 'subjects': u'Physics Education (physics.ed-ph)',
	 'title': u'\nSecond Life as a Platform for Physics Simulations and Microworlds: An  Evaluation',
	 'urllink': u'http://arxiv.org/abs/1405.6703'}
2015-03-23 20:25:35+0000 [xxu461000] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1407.5111> (referer: http://arxiv.org/list/cs/14?skip=8000&show=1000)
2015-03-23 20:25:35+0000 [xxu461000] DEBUG: Scraped from <200 http://arxiv.org/abs/1407.5111>
	{'abstract': u'This paper introduces a new comparison base stable sorting algorithm, named RS sort. RS Sort involves only the comparison of pair of elements in an array which ultimately sorts the array and does not involve the comparison of each element with every other element. RS sort tries to build upon the relationship established between the elements in each pass. Suppose there is an array containing three elements a1, a2, a3 and if a relationship exist such that a1&lt;a2 and a2&lt;a3 then it can be established that a1&lt;a3 and so there is no need to compare a1 and a3. Sorting is a fundamental operation in computer science. RS sort is analyzed both theoretically and empirically. We have performed its Empirical analysis and compared its performance with the well-known quick sort for various input types.',
	 'authors': u'Harsh Ranjan, Sumit Agarwal, Niraj Kumar Singh,',
	 'category': u'Computer Science ',
	 'date': '2014-7-18',
	 'pdflink': u'http://arxiv.org/pdf/1407.5111',
	 'subjects': u'Data Structures and Algorithms (cs.DS)',
	 'title': u'\nDesign and Analysis of RS Sort',
	 'urllink': u'http://arxiv.org/abs/1407.5111'}
2015-03-23 20:25:40+0000 [xxu461000] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1404.5239> (referer: http://arxiv.org/list/cs/14?skip=4000&show=1000)
2015-03-23 20:25:40+0000 [xxu461000] DEBUG: Scraped from <200 http://arxiv.org/abs/1404.5239>
	{'abstract': u'We describe a methodology of rating the influence of a Twitter ac-count in this famous microblogging service. We then evaluate it over real ac-counts, under the belief that influence is not only a matter of quantity (amount of followers), but also a mixture of quality measures that reflect interaction, awareness, and visibility in the social sphere. The authors of this paper have created InfluenceTracker, a publicly available website where anyone can rate and compare the recent activity of any Twitter account.',
	 'authors': u'Gerasimos Razis, Ioannis Anagnostopoulos,',
	 'category': u'Computer Science ',
	 'date': '2014-4-21',
	 'pdflink': u'http://arxiv.org/pdf/1404.5239',
	 'subjects': u'Social and Information Networks (cs.SI)',
	 'title': u'\nInfluenceTracker: Rating the impact of a Twitter account',
	 'urllink': u'http://arxiv.org/abs/1404.5239'}
2015-03-23 20:25:44+0000 [xxu461000] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1409.1980> (referer: http://arxiv.org/list/cs/14?skip=10000&show=1000)
2015-03-23 20:25:44+0000 [xxu461000] DEBUG: Scraped from <200 http://arxiv.org/abs/1409.1980>
	{'abstract': u'In this paper, we study the statistical characterization of the sum of the squared shadowed random variables with correlated shadowing components. The probability density function (PDF) of this sum is obtained in the form of a power series. The derived PDF is utilized for obtaining the performance results of the maximal ratio combining (MRC) scheme over correlated shadowed fading channels. First, we derive the moment generating function (MGF) of the received signal-to-noise ratio of the MRC receiver. By using the derived MGF expression, the analytical diversity order is obtained; it is deduced on the basis of this analysis that the diversity of the MRC receiver over correlated shadowed channels depends upon the number of diversity branches and parameter. Further, the analytical average bit error rate of the MRC scheme is also derived, which is applicable for -PSK and -QAM constellations. The Shannon capacity of the correlated shadowed channels is also derived in the form of the Meijer-G function.',
	 'authors': u'Manav R. Bhatnagar,',
	 'category': u'Computer Science ',
	 'date': '2014-9-6',
	 'pdflink': u'http://arxiv.org/pdf/1409.1980',
	 'subjects': u'Information Theory (cs.IT)',
	 'title': u'\nOn the Sum of Correlated Squared $\u03ba-\u03bc$ Shadowed Random Variables  and its Application to Performance Analysis of MRC',
	 'urllink': u'http://arxiv.org/abs/1409.1980'}
2015-03-23 20:25:48+0000 [xxu461000] INFO: Crawled 392 pages (at 12 pages/min), scraped 375 items (at 12 items/min)
2015-03-23 20:25:49+0000 [xxu461000] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1406.7629> (referer: http://arxiv.org/list/cs/14?skip=7000&show=1000)
2015-03-23 20:25:49+0000 [xxu461000] DEBUG: Scraped from <200 http://arxiv.org/abs/1406.7629>
	{'abstract': u'In this article, we consider a receding horizon control of discrete-time state-dependent jump linear systems, particular kind of stochastic switching systems, subject to possibly unbounded random disturbances and probabilistic state constraints. Due to a nature of the dynamical system and the constraints, we consider a one-step receding horizon. Using inverse cumulative distribution function, we convert the probabilistic state constraints to deterministic constraints, and obtain a tractable deterministic receding horizon control problem. We consider the receding control law to have a linear state-feedback and an admissible offset term. We ensure mean square boundedness of the state variable via solving linear matrix inequalities off-line, and solve the receding horizon control problem on-line with control offset terms. We illustrate the overall approach applied on a macroeconomic system.',
	 'authors': u'Shaikshavali Chitraganti, Samir Aberkane, Christophe Aubrun, Guillermo Valencia-Palomo, Vasile Dragan,',
	 'category': u'Computer Science ',
	 'date': '2014-6-30',
	 'pdflink': u'http://arxiv.org/pdf/1406.7629',
	 'subjects': u'Systems and Control (cs.SY)',
	 'title': u'\nOn control of discrete-time state-dependent jump linear systems with  probabilistic constraints: A receding horizon approach',
	 'urllink': u'http://arxiv.org/abs/1406.7629'}
2015-03-23 20:25:55+0000 [xxu461000] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1405.2168> (referer: http://arxiv.org/list/cs/14?skip=5000&show=1000)
2015-03-23 20:25:55+0000 [xxu461000] DEBUG: Scraped from <200 http://arxiv.org/abs/1405.2168>
	{'abstract': u'In this paper a new evolutionary algorithm, for continuous nonlinear optimization problems, is surveyed. This method is inspired by the life of a bird, called Cuckoo. The Cuckoo Optimization Algorithm (COA) is evaluated by using the Rastrigin function. The problem is a non-linear continuous function which is used for evaluating optimization algorithms. The efficiency of the COA has been studied by obtaining optimal solution of various dimensions Rastrigin function in this paper. The mentioned function also was solved by FA and ABC algorithms. Comparing the results shows the COA has better performance than other algorithms. Application of algorithm to test function has proven its capability to deal with difficult optimization problems.',
	 'authors': u'Elham Shadkam, Mehdi Bijari,',
	 'category': u'Computer Science ',
	 'date': '2014-5-9',
	 'pdflink': u'http://arxiv.org/pdf/1405.2168',
	 'subjects': u'Neural and Evolutionary Computing (cs.NE)',
	 'title': u'\nEvaluation The Efficiency Of Cuckoo Optimization Algorithm',
	 'urllink': u'http://arxiv.org/abs/1405.2168'}
2015-03-23 20:25:59+0000 [xxu461000] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1405.6676> (referer: http://arxiv.org/list/cs/14?skip=6000&show=1000)
2015-03-23 20:25:59+0000 [xxu461000] DEBUG: Scraped from <200 http://arxiv.org/abs/1405.6676>
	{'abstract': u'This article assumes acquired the skills and expertise of a statistician in unsupervised (NMF, k-means, SVD) and supervised learning (regression, CART, random forest). What skills and knowledge do a statistician must acquire to reach the "Volume" scale of big data? After a quick overview of the different strategies available and especially of those imposed by Hadoop, the algorithms of some available learning methods are outlined in order to understand how they are adapted to the strong stresses of the Map-Reduce functionalities',
	 'authors': u'Philippe Besse, Nathalie Villa-Vialaneix,',
	 'category': u'Computer Science ',
	 'date': '2014-5-26',
	 'pdflink': u'http://arxiv.org/pdf/1405.6676',
	 'subjects': u'Other Statistics (stat.OT)',
	 'title': u"\nStatistique et Big Data Analytics; Volum\xe9trie, L'Attaque des Clones",
	 'urllink': u'http://arxiv.org/abs/1405.6676'}
2015-03-23 20:26:03+0000 [xxu461000] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1407.5107> (referer: http://arxiv.org/list/cs/14?skip=8000&show=1000)
2015-03-23 20:26:03+0000 [xxu461000] DEBUG: Scraped from <200 http://arxiv.org/abs/1407.5107>
	{'abstract': u"Google's PageRank method was developed to evaluate the importance of web-pages via their link structure. The mathematics of PageRank, however, are entirely general and apply to any graph or network in any domain. Thus, PageRank is now regularly used in bibliometrics, social and information network analysis, and for link prediction and recommendation. It's even used for systems analysis of road networks, as well as biology, chemistry, neuroscience, and physics. We'll see the mathematics and ideas that unite these diverse applications.",
	 'authors': u'David F. Gleich,',
	 'category': u'Computer Science ',
	 'date': '2014-7-18',
	 'pdflink': u'http://arxiv.org/pdf/1407.5107',
	 'subjects': u'Social and Information Networks (cs.SI)',
	 'title': u'\nPageRank beyond the Web',
	 'urllink': u'http://arxiv.org/abs/1407.5107'}
2015-03-23 20:26:08+0000 [xxu461000] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1404.5236> (referer: http://arxiv.org/list/cs/14?skip=4000&show=1000)
2015-03-23 20:26:08+0000 [xxu461000] DEBUG: Scraped from <200 http://arxiv.org/abs/1404.5236>
	{'abstract': u'In order to obtain the best-known guarantees, algorithms are traditionally tailored to the particular problem we want to solve. Two recent developments, the Unique Games Conjecture (UGC) and the Sum-of-Squares (SOS) method, surprisingly suggest that this tailoring is not necessary and that a single efficient algorithm could achieve best possible guarantees for a wide range of different problems. The Unique Games Conjecture (UGC) is a tantalizing conjecture in computational complexity, which, if true, will shed light on the complexity of a great many problems. In particular this conjecture predicts that a single concrete algorithm provides optimal guarantees among all efficient algorithms for a large class of computational problems. The Sum-of-Squares (SOS) method is a general approach for solving systems of polynomial constraints. This approach is studied in several scientific disciplines, including real algebraic geometry, proof complexity, control theory, and mathematical programming, and has found applications in fields as diverse as quantum information theory, formal verification, game theory and many others. We survey some connections that were recently uncovered between the Unique Games Conjecture and the Sum-of-Squares method. In particular, we discuss new tools to rigorously bound the running time of the SOS method for obtaining approximate solutions to hard optimization problems, and how these tools give the potential for the sum-of-squares method to provide new guarantees for many problems of interest, and possibly to even refute the UGC.',
	 'authors': u'Boaz Barak, David Steurer,',
	 'category': u'Computer Science ',
	 'date': '2014-4-21',
	 'pdflink': u'http://arxiv.org/pdf/1404.5236',
	 'subjects': u'Data Structures and Algorithms (cs.DS)',
	 'title': u'\nSum-of-squares proofs and the quest toward optimal algorithms',
	 'urllink': u'http://arxiv.org/abs/1404.5236'}
2015-03-23 20:26:14+0000 [xxu461000] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1409.1917> (referer: http://arxiv.org/list/cs/14?skip=10000&show=1000)
2015-03-23 20:26:14+0000 [xxu461000] DEBUG: Scraped from <200 http://arxiv.org/abs/1409.1917>
	{'abstract': u'Much of the energy consumption in buildings is due to HVAC systems, which has motivated several recent studies on making these systems more energy- efficient. Occupancy and activity are two important aspects, which need to be correctly estimated for optimal HVAC control. However, state-of-the-art methods to estimate occupancy and classify activity require infrastructure and/or wearable sensors which suffers from lower acceptability due to higher cost. Encouragingly, with the advancement of the smartphones, these are becoming more achievable. Most of the existing occupancy estimation tech- niques have the underlying assumption that the phone is always carried by its user. However, phones are often left at desk while attending meeting or other events, which generates estimation error for the existing phone based occupancy algorithms. Similarly, in the recent days the emerging theory of Sparse Random Classifier (SRC) has been applied for activity classification on smartphone, however, there are rooms to improve the on-phone process- ing. We propose a novel sensor fusion method which offers almost 100% accuracy for occupancy estimation. We also propose an activity classifica- tion algorithm, which offers similar accuracy as of the state-of-the-art SRC algorithms while offering 50% reduction in processing.',
	 'authors': u'Rajib Rana, Brano Kusy, Josh Wall, Wen Hu,',
	 'category': u'Computer Science ',
	 'date': '2014-9-5',
	 'pdflink': u'http://arxiv.org/pdf/1409.1917',
	 'subjects': u'Learning (cs.LG)',
	 'title': u'\nNovel Methods for Activity Classification and Occupany Prediction  Enabling Fine-grained HVAC Control',
	 'urllink': u'http://arxiv.org/abs/1409.1917'}
2015-03-23 20:26:19+0000 [xxu461000] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1406.7623> (referer: http://arxiv.org/list/cs/14?skip=7000&show=1000)
2015-03-23 20:26:19+0000 [xxu461000] DEBUG: Scraped from <200 http://arxiv.org/abs/1406.7623>
	{'abstract': u'We investigate the multiple-input multiple-output broadcast channel with statistical channel state information available at the transmitter. The so-called linear assignment operation is employed, and necessary conditions are derived for the optimal transmit design under general fading conditions. Based on this, we introduce an iterative algorithm to maximize the linear assignment weighted sum-rate by applying a gradient descent method. To reduce complexity, we derive an upper bound of the linear assignment achievable rate of each receiver, from which a simplified closed-form expression for a near-optimal linear assignment matrix is derived. This reveals an interesting construction analogous to that of dirty-paper coding. In light of this, a low complexity transmission scheme is provided. Numerical examples illustrate the significant performance of the proposed low complexity scheme.',
	 'authors': u'Yongpeng Wu, Shi Jin, Xiqi Gao, Matthew R. McKay, Chengshan Xiao,',
	 'category': u'Computer Science ',
	 'date': '2014-6-30',
	 'pdflink': u'http://arxiv.org/pdf/1406.7623',
	 'subjects': u'Information Theory (cs.IT)',
	 'title': u'\nTransmit Designs for the MIMO Broadcast Channel with Statistical CSI',
	 'urllink': u'http://arxiv.org/abs/1406.7623'}
2015-03-23 20:26:24+0000 [xxu461000] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1405.2163> (referer: http://arxiv.org/list/cs/14?skip=5000&show=1000)
2015-03-23 20:26:24+0000 [xxu461000] DEBUG: Scraped from <200 http://arxiv.org/abs/1405.2163>
	{'abstract': u'The study of degrees of freedom of signals observed within spatially diverse broadband multipath fields is an area of ongoing investigation and has a wide range of applications, including characterising broadband MIMO and cooperative networks. However, a fundamental question arises: given a size limitation on the observation region, what is the upper bound on the degrees of freedom of signals observed within a broadband multipath field over a finite time window? In order to address this question, we characterize the multipath field as a sum of a finite number of orthogonal waveforms or spatial modes. We show that (i) the "effective observation time" is independent of spatial modes and different from actual observation time, (ii) in wideband transmission regimes, the "effective bandwidth" is spatial mode dependent and varies from the given frequency bandwidth. These findings clearly indicate the strong coupling between space and time as well as space and frequency in spatially diverse wideband multipath fields. As a result, signal degrees of freedom does not agree with the well-established degrees of freedom result as a product of spatial degrees of freedom and time-frequency degrees of freedom. Instead, analogous to Shannon\'s communication model where signals are encoded in only one spatial mode, the available signal degrees of freedom in spatially diverse wideband multipath fields is the time-bandwidth product result extended from one spatial mode to finite modes. We also show that the degrees of freedom is affected by the acceptable signal to noise ratio (SNR) in each spatial mode.',
	 'authors': u'Farhana Bashar, S.M. Akramus Salehin, Thushara D. Abhayapala,',
	 'category': u'Computer Science ',
	 'date': '2014-5-9',
	 'pdflink': u'http://arxiv.org/pdf/1405.2163',
	 'subjects': u'Information Theory (cs.IT)',
	 'title': u'\nBand Limited Signals Observed Over Finite Spatial and Temporal Windows:  An Upper Bound to Signal Degrees of Freedom',
	 'urllink': u'http://arxiv.org/abs/1405.2163'}
2015-03-23 20:26:30+0000 [xxu461000] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1405.6642> (referer: http://arxiv.org/list/cs/14?skip=6000&show=1000)
2015-03-23 20:26:30+0000 [xxu461000] DEBUG: Scraped from <200 http://arxiv.org/abs/1405.6642>
	{'abstract': u'Stability has been of a great concern in statistics: similar statistical conclusions should be drawn based on different data sampled from the same population. In this article, we introduce a general measure of classification instability (CIS) to capture the sampling variability of the predictions made by a classification procedure. The minimax rate of CIS is established for general plug-in classifiers. As a concrete example, we consider the stability of the nearest neighbor classifier. In particular, we derive an asymptotically equivalent form for the CIS of a weighted nearest neighbor classifier. This allows us to develop a novel stabilized nearest neighbor classifier which well balances the trade-off between classification accuracy and stability. The resulting classification procedure is shown to possess the minimax optimal rates in both excess risk and CIS. Extensive experiments demonstrate a significant improvement of CIS over existing nearest neighbor classifiers at an ignorable cost of classification accuracy.',
	 'authors': u'Wei Sun, Xingye Qiao, Guang Cheng,',
	 'category': u'Computer Science ',
	 'date': '2014-5-26',
	 'pdflink': u'http://arxiv.org/pdf/1405.6642',
	 'subjects': u'Machine Learning (stat.ML)',
	 'title': u'\nNearest Neighbor Classifier with Optimal Stability',
	 'urllink': u'http://arxiv.org/abs/1405.6642'}
2015-03-23 20:26:34+0000 [xxu461000] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1407.5093> (referer: http://arxiv.org/list/cs/14?skip=8000&show=1000)
2015-03-23 20:26:34+0000 [xxu461000] DEBUG: Scraped from <200 http://arxiv.org/abs/1407.5093>
	{'abstract': u'A knowledgeable observer of a game of football (soccer) can make a subjective evaluation of the quality of passes made between players during the game. We investigate the problem of producing an automated system to make the same evaluation of passes. We present a model that constructs numerical predictor variables from spatiotemporal match data using feature functions based on methods from computational geometry, and then learns a classification function from labelled examples of the predictor variables. Furthermore, the learned classifiers are analysed to determine if there is a relationship between the complexity of the algorithm that computed the predictor variable and the importance of the variable to the classifier. Experimental results show that we are able to produce a classifier with 85.8% accuracy on classifying passes as Good, OK or Bad, and that the predictor variables computed using complex methods from computational geometry are of moderate importance to the learned classifiers. Finally, we show that the inter-rater agreement on pass classification between the machine classifier and a human observer is of similar magnitude to the agreement between two observers.',
	 'authors': u'Michael Horton, Joachim Gudmundsson, Sanjay Chawla, Jo\xebl Estephan,',
	 'category': u'Computer Science ',
	 'date': '2014-7-18',
	 'pdflink': u'http://arxiv.org/pdf/1407.5093',
	 'subjects': u'Learning (cs.LG)',
	 'title': u'\nClassification of Passes in Football Matches using Spatiotemporal Data',
	 'urllink': u'http://arxiv.org/abs/1407.5093'}
2015-03-23 20:26:40+0000 [xxu461000] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1404.5214> (referer: http://arxiv.org/list/cs/14?skip=4000&show=1000)
2015-03-23 20:26:40+0000 [xxu461000] DEBUG: Scraped from <200 http://arxiv.org/abs/1404.5214>
	{'abstract': u'We propose a representation of graph as a functional object derived from the power iteration of the underlying adjacency matrix. The proposed functional representation is a graph invariant, i.e., the functional remains unchanged under any reordering of the vertices. This property eliminates the difficulty of handling exponentially many isomorphic forms. Bhattacharyya kernel constructed between these functionals significantly outperforms the state-of-the-art graph kernels on 3 out of the 4 standard benchmark graph classification datasets, demonstrating the superiority of our approach. The proposed methodology is simple and runs in time linear in the number of edges, which makes our kernel more efficient and scalable compared to many widely adopted graph kernels with running time cubic in the number of vertices.',
	 'authors': u'Anshumali Shrivastava, Ping Li,',
	 'category': u'Computer Science ',
	 'date': '2014-4-21',
	 'pdflink': u'http://arxiv.org/pdf/1404.5214',
	 'subjects': u'Learning (cs.LG)',
	 'title': u'\nGraph Kernels via Functional Embedding',
	 'urllink': u'http://arxiv.org/abs/1404.5214'}
2015-03-23 20:26:42+0000 [xxu461000] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1409.1914> (referer: http://arxiv.org/list/cs/14?skip=10000&show=1000)
2015-03-23 20:26:42+0000 [xxu461000] DEBUG: Scraped from <200 http://arxiv.org/abs/1409.1914>
	{'abstract': u"This contribution discusses the automatic generation of event-driven, tuple-space based programs for task-oriented execution models from a sequential C specification. We developed a hierarchical mapping solution using auto-parallelizing compiler technology to target three different runtimes relying on event-driven tasks (EDTs). Our solution benefits from the important observation that loop types encode short, transitive relations among EDTs that are compact and efficiently evaluated at runtime. In this context, permutable loops are of particular importance as they translate immediately into conservative point-to-point synchronizations of distance 1. Our solution generates calls into a runtime-agnostic C++ layer, which we have retargeted to Intel's Concurrent Collections (CnC), ETI's SWARM, and the Open Community Runtime (OCR). Experience with other runtime systems motivates our introduction of support for hierarchical async-finishes in CnC. Experimental data is provided to show the benefit of automatically generated code for EDT-based runtimes as well as comparisons across runtimes.",
	 'authors': u'Nicolas Vasilache, Muthu Baskaran, Tom Henretty, Benoit Meister, M. Harper Langston, Sanket Tavarageri, Richard Lethin,',
	 'category': u'Computer Science ',
	 'date': '2014-9-5',
	 'pdflink': u'http://arxiv.org/pdf/1409.1914',
	 'subjects': u'Distributed, Parallel, and Cluster Computing (cs.DC)',
	 'title': u'\nA Tale of Three Runtimes',
	 'urllink': u'http://arxiv.org/abs/1409.1914'}
2015-03-23 20:26:47+0000 [xxu461000] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1406.7620> (referer: http://arxiv.org/list/cs/14?skip=7000&show=1000)
2015-03-23 20:26:47+0000 [xxu461000] DEBUG: Scraped from <200 http://arxiv.org/abs/1406.7620>
	{'abstract': u"In this paper, a joint spectrum sensing and accessing optimization framework for a multiuser cognitive network is proposed to significantly improve spectrum efficiency. For such a cognitive network, there are two important and limited resources that should be distributed in a comprehensive manner, namely feedback bits and time duration. First, regarding the feedback bits, there are two components: sensing component (used to convey various users' sensing results) and accessing component (used to feedback channel state information). A large sensing component can support more users to perform cooperative sensing, which results in high sensing precision. However, a large accessing component is preferred as well, as it has a direct impact on the performance in the multiuser cognitive network when multi-antenna technique, such as zero-forcing beamforming (ZFBF), is utilized. Second, the tradeoff of sensing and accessing duration in a transmission interval needs to be determined, so that the sum transmission rate is optimized while satisfying the interference constraint. In addition, the above two resources are interrelated and inversive under some conditions. Specifically, sensing time can be saved by utilizing more sensing feedback bits for a given performance objective. Hence, the resources should be allocation in a jointly manner. Based on the joint optimization framework and the intrinsic relationship between the two resources, we propose two joint resource allocation schemes by maximizing the average sum transmission rate in a multiuser multi-antenna cognitive network. Simulation results show that, by adopting the joint resource allocation schemes, obvious performance gain can be obtained over the traditional fixed strategies.",
	 'authors': u'Xiaoming Chen, Chau Yuen,',
	 'category': u'Computer Science ',
	 'date': '2014-6-30',
	 'pdflink': u'http://arxiv.org/pdf/1406.7620',
	 'subjects': u'Information Theory (cs.IT)',
	 'title': u'\nJoint Optimization of Spectrum Sensing and Accessing in Multiuser MISO  Cognitive Networks',
	 'urllink': u'http://arxiv.org/abs/1406.7620'}
2015-03-23 20:26:48+0000 [xxu461000] INFO: Crawled 405 pages (at 13 pages/min), scraped 388 items (at 13 items/min)
2015-03-23 20:26:53+0000 [xxu461000] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1405.2157> (referer: http://arxiv.org/list/cs/14?skip=5000&show=1000)
2015-03-23 20:26:53+0000 [xxu461000] DEBUG: Scraped from <200 http://arxiv.org/abs/1405.2157>
	{'abstract': u'Storing digital information, ensuring the accuracy, steady and uninterrupted access to the data are considered as fundamental challenges in enterprise-class organizations and companies. In recent years, new types of storage systems such as solid state disks (SSD) have been introduced. Unlike hard disks that have mechanical structure, SSDs are based on flash memory and thus have electronic structure. Generally a SSD consists of a number of flash memory chips, some buffers of the volatile memory type, and an embedded microprocessor, which have been interconnected by a port. This microprocessor run a small file system which called flash translation layer (FTL). This software controls and schedules buffers, data transfers and all flash memory tasks. SSDs have some advantages over hard disks such as high speed, low energy consumption, lower heat and noise, resistance against damage, and smaller size. Besides, some disadvantages such as limited endurance and high price are still challenging. In this study, the effort is to combine two common technologies - SLC and MLC chips - used in the manufacture of SSDs in a single SSD to decrease the side effects of current SSDs. The idea of using multi-layer SSD is regarded as an efficient solution in this field.',
	 'authors': u'Arash Batni, Farshad Safaei,',
	 'category': u'Computer Science ',
	 'date': '2014-5-9',
	 'pdflink': u'http://arxiv.org/pdf/1405.2157',
	 'subjects': u'Other Computer Science (cs.OH)',
	 'title': u'\nA New Multi-Tiered Solid State Disk Using Slc/Mlc Combined Flash Memory',
	 'urllink': u'http://arxiv.org/abs/1405.2157'}
2015-03-23 20:26:59+0000 [xxu461000] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1405.6623> (referer: http://arxiv.org/list/cs/14?skip=6000&show=1000)
2015-03-23 20:26:59+0000 [xxu461000] DEBUG: Scraped from <200 http://arxiv.org/abs/1405.6623>
	{'abstract': u'The scientific enterprise depends critically on the preservation of and open access to published data. This basic tenet applies acutely to phylogenies (estimates of evolutionary relationships among species). Increasingly, phylogenies are estimated from increasingly large, genome-scale datasets using increasingly complex statistical methods that require increasing levels of expertise and computational investment. Moreover, the resulting phylogenetic data provide an explicit historical perspective that critically informs research in a vast and growing number of scientific disciplines. One such use is the study of changes in rates of lineage diversification (speciation - extinction) through time. As part of a meta-analysis in this area, we sought to collect phylogenetic data (comprising nucleotide sequence alignment and tree files) from 217 studies published in 46 journals over a 13-year period. We document our attempts to procure those data (from online archives and by direct request to corresponding authors), and report results of analyses (using Bayesian logistic regression) to assess the impact of various factors on the success of our efforts. Overall, complete phylogenetic data for ~60% of these studies are effectively lost to science. Our study indicates that phylogenetic data are more likely to be deposited in online archives and/or shared upon request when: (1) the publishing journal has a strong data-sharing policy; (2) the publishing journal has a higher impact factor, and; (3) the data are requested from faculty rather than students. Although the situation appears dire, our analyses suggest that it is far from hopeless: recent initiatives by the scientific community -- including policy changes by journals and funding agencies -- are improving the state of affairs.',
	 'authors': u'Andrew F. Magee, Michael R. May, Brian R. Moore,',
	 'category': u'Computer Science ',
	 'date': '2014-5-23',
	 'pdflink': u'http://arxiv.org/pdf/1405.6623',
	 'subjects': u'Populations and Evolution (q-bio.PE)',
	 'title': u'\nThe Dawn of Open Access to Phylogenetic Data',
	 'urllink': u'http://arxiv.org/abs/1405.6623'}
2015-03-23 20:27:03+0000 [xxu461000] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1407.5080> (referer: http://arxiv.org/list/cs/14?skip=8000&show=1000)
2015-03-23 20:27:03+0000 [xxu461000] DEBUG: Scraped from <200 http://arxiv.org/abs/1407.5080>
	{'abstract': u'The Multiple Depot Ring-Star Problem (MDRSP) is an important combinatorial optimization problem that arises in the context of optical fiber network design, and in applications pertaining to collecting data using stationary sensing devices and autonomous vehicles. Given the locations of a set of customers and a set of depots, the goal is to (i) find a set of simple cycles such that each cycle (ring) passes through a subset of customers and exactly one depot, (ii) assign each non-visited customer to a visited customer or a depot, and (iii) minimize the sum of the routing costs, i.e., the cost of the cycles and the assignment costs. We present a mixed integer linear programming formulation for the MDRSP and propose valid inequalities to strengthen the linear programming relaxation. Furthermore, we present a polyhedral analysis and derive facet-inducing results for the MDRSP. All these results are then used to develop a branch-and-cut algorithm to obtain optimal solutions to the MDRSP. The performance of the branch-and-cut algorithm is evaluated through extensive computational experiments on several classes of test instances.',
	 'authors': u'Kaarthik Sundar, Sivakumar Rathinam,',
	 'category': u'Computer Science ',
	 'date': '2014-7-18',
	 'pdflink': u'http://arxiv.org/pdf/1407.5080',
	 'subjects': u'Data Structures and Algorithms (cs.DS)',
	 'title': u'\nMultiple Depot Ring Star Problem: A polyhedral study and exact algorithm',
	 'urllink': u'http://arxiv.org/abs/1407.5080'}
2015-03-23 20:27:07+0000 [xxu461000] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1404.5190> (referer: http://arxiv.org/list/cs/14?skip=4000&show=1000)
2015-03-23 20:27:07+0000 [xxu461000] DEBUG: Scraped from <200 http://arxiv.org/abs/1404.5190>
	{'abstract': u'We consider list versions of sparse approximation problems, where unlike the existing results in sparse approximation that consider situations with unique solutions, we are interested in multiple solutions. We introduce these problems and present the first combinatorial results on the output list size. These generalize and enhance some of the existing results on threshold phenomenon and uncertainty principles in sparse approximations. Our definitions and results are inspired by similar results in list decoding. We also present lower bound examples that bolster our results and show they are of the appropriate size.',
	 'authors': u'Mahmoud Abo Khamis, Anna C. Gilbert, Hung Q. Ngo, Atri Rudra,',
	 'category': u'Computer Science ',
	 'date': '2014-4-18',
	 'pdflink': u'http://arxiv.org/pdf/1404.5190',
	 'subjects': u'Information Theory (cs.IT)',
	 'title': u'\nSparse Approximation, List Decoding, and Uncertainty Principles',
	 'urllink': u'http://arxiv.org/abs/1404.5190'}
2015-03-23 20:27:11+0000 [xxu461000] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1409.1911> (referer: http://arxiv.org/list/cs/14?skip=10000&show=1000)
2015-03-23 20:27:11+0000 [xxu461000] DEBUG: Scraped from <200 http://arxiv.org/abs/1409.1911>
	{'abstract': u'Mapping Science across countries is a challenging task in the field of Scientometrics. A number of efforts trying to cope with this task has been discussed in the state of the art, addressing this challenge by processing collections of scientific digital libraries and visualizing author-based measures (for instance, the h-index) or document-based measures (for instance, the averaged number of citations per document). A major drawback of these approaches is related to the presence of bias. The bigger the country, the higher the measure value. We explore the use of an econometric index to tackle this limitation, known as the Revealed Comparative Advantage measure (RCA). Using RCA, the diversity and ubiquity of each field of knowledge is mapped across countries. Then, a RCA-based proximity function is explored to visualize citation and h-index ubiquity. Science maps relating 27 knowledge areas and 237 countries are introduced using data crawled from Scimago that ranges from 1996 to 2011. Our results shows that the proposal is feasible and can be extended to ellaborate a global scientific production characterization.',
	 'authors': u'Miguel Guevara, Marcelo Mendoza,',
	 'category': u'Computer Science ',
	 'date': '2014-9-5',
	 'pdflink': u'http://arxiv.org/pdf/1409.1911',
	 'subjects': u'Digital Libraries (cs.DL)',
	 'title': u'\nRevealing Comparative Advantages in the Backbone of Science',
	 'urllink': u'http://arxiv.org/abs/1409.1911'}
2015-03-23 20:27:16+0000 [xxu461000] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1406.7611> (referer: http://arxiv.org/list/cs/14?skip=7000&show=1000)
2015-03-23 20:27:16+0000 [xxu461000] DEBUG: Scraped from <200 http://arxiv.org/abs/1406.7611>
	{'abstract': u'Can altmetric data be validly used for the measurement of societal impact? The current study seeks to answer this question with a comprehensive dataset (about 100,000 records) from very disparate sources (F1000, Altmetric, and an in-house database based on Web of Science). In the F1000 peer review system, experts attach particular tags to scientific papers which indicate whether a paper could be of interest for science or rather for other segments of society. The results show that papers with the tag "good for teaching" do achieve higher altmetric counts than papers without this tag - if the quality of the papers is controlled. At the same time, a higher citation count is shown especially by papers with a tag that is specifically scientifically oriented ("new finding"). The findings indicate that papers tailored for a readership outside the area of research should lead to societal impact. If altmetric data is to be used for the measurement of societal impact, the question arises of its normalization. In bibliometrics, citations are normalized for the papers\' subject area and publication year. This study has taken a second analytic step involving a possible normalization of altmetric data. As the results show there are particular scientific topics which are of especial interest for a wide audience. Since these more or less interesting topics are not completely reflected in Thomson Reuters\' journal sets, a normalization of altmetric data should not be based on the level of subject categories, but on the level of topics.',
	 'authors': u'Lutz Bornmann,',
	 'category': u'Computer Science ',
	 'date': '2014-6-30',
	 'pdflink': u'http://arxiv.org/pdf/1406.7611',
	 'subjects': u'Digital Libraries (cs.DL)',
	 'title': u'\nValidity of altmetrics data for measuring societal impact: A study using  data from Altmetric and F1000Prime',
	 'urllink': u'http://arxiv.org/abs/1406.7611'}
2015-03-23 20:27:22+0000 [xxu461000] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1405.2128> (referer: http://arxiv.org/list/cs/14?skip=5000&show=1000)
2015-03-23 20:27:22+0000 [xxu461000] DEBUG: Scraped from <200 http://arxiv.org/abs/1405.2128>
	{'abstract': u'Image segmentation and image restoration are two important topics in image processing with great achievements. In this paper, we propose a new multiphase segmentation model by combining image restoration and image segmentation models. Utilizing image restoration aspects, the proposed segmentation model can effectively and robustly tackle high noisy images, blurry images, images with missing pixels, and vector-valued images. In particular, one of the most important segmentation models, the piecewise constant Mumford-Shah model, can be extended easily in this way to segment gray and vector-valued images corrupted for example by noise, blur or missing pixels after coupling a new data fidelity term which comes from image restoration topics. It can be solved efficiently using the alternating minimization algorithm, and we prove the convergence of this algorithm with three variables under mild condition. Experiments on many synthetic and real-world images demonstrate that our method gives better segmentation results in comparison to others state-of-the-art segmentation models especially for blurry images and images with missing pixels values.',
	 'authors': u'Xiaohao Cai,',
	 'category': u'Computer Science ',
	 'date': '2014-5-9',
	 'pdflink': u'http://arxiv.org/pdf/1405.2128',
	 'subjects': u'Computer Vision and Pattern Recognition (cs.CV)',
	 'title': u'\nVariational Image Segmentation Model Coupled with Image Restoration  Achievements',
	 'urllink': u'http://arxiv.org/abs/1405.2128'}
2015-03-23 20:27:26+0000 [xxu461000] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1405.6503> (referer: http://arxiv.org/list/cs/14?skip=6000&show=1000)
2015-03-23 20:27:26+0000 [xxu461000] DEBUG: Scraped from <200 http://arxiv.org/abs/1405.6503>
	{'abstract': u'We generalize a well-known algorithm for the generation of all subsets of a set in lexicographic order with respect to the sets as lists of elements (subset-lex order). We obtain algorithms for various combinatorial objects such as the subsets of a multiset, compositions and partitions represented as lists of parts, and for certain restricted growth strings. The algorithms are often loopless and require at most one extra variable for the computation of the next object. The performance of the algorithms is very competitive even when not loopless. A Gray code corresponding to the subset-lex order and a Gray code for compositions that was found during this work are described.',
	 'authors': u'J\xf6rg Arndt,',
	 'category': u'Computer Science ',
	 'date': '2014-5-26',
	 'pdflink': u'http://arxiv.org/pdf/1405.6503',
	 'subjects': u'Combinatorics (math.CO)',
	 'title': u'\nSubset-lex: did we miss an order?',
	 'urllink': u'http://arxiv.org/abs/1405.6503'}
2015-03-23 20:27:29+0000 [xxu461000] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1407.5074> (referer: http://arxiv.org/list/cs/14?skip=8000&show=1000)
2015-03-23 20:27:29+0000 [xxu461000] DEBUG: Scraped from <200 http://arxiv.org/abs/1407.5074>
	{'abstract': u'Max Consensus-based Auction (MCA) protocols are an elegant approach to establish conflict-free distributed allocations in a wide range of network utility maximization problems. A set of agents independently bid on a set of items, and exchange their bids with their first hop-neighbors for a distributed (max-consensus) winner determination. The use of MCA protocols was proposed, , to solve the task allocation problem for a fleet of unmanned aerial vehicles, in smart grids, or in distributed virtual network management applications. Misconfigured or malicious agents participating in a MCA, or an incorrect instantiation of policies can lead to oscillations of the protocol, causing, , Service Level Agreement (SLA) violations. In this paper, we propose a formal, machine-readable, Max-Consensus Auction model, encoded in the Alloy lightweight modeling language. The model consists of a network of agents applying the MCA mechanisms, instantiated with potentially different policies, and a set of predicates to analyze its convergence properties. We were able to verify that MCA is not resilient against rebidding attacks, and that the protocol fails (to achieve a conflict-free resource allocation) for some specific combinations of policies. Our model can be used to verify, with a "push-button" analysis, the convergence of the MCA mechanism to a conflict-free allocation of a wide range of policy instantiations.',
	 'authors': u'Saber Mirzaei, Flavio Esposito,',
	 'category': u'Computer Science ',
	 'date': '2014-7-14',
	 'pdflink': u'http://arxiv.org/pdf/1407.5074',
	 'subjects': u'Software Engineering (cs.SE)',
	 'title': u'\nAn Alloy Verification Model for Consensus-Based Auction Protocols',
	 'urllink': u'http://arxiv.org/abs/1407.5074'}
2015-03-23 20:27:35+0000 [xxu461000] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1404.5187> (referer: http://arxiv.org/list/cs/14?skip=4000&show=1000)
2015-03-23 20:27:35+0000 [xxu461000] DEBUG: Scraped from <200 http://arxiv.org/abs/1404.5187>
	{'abstract': u'We present fundamental limits on the reliable classification of linear and affine subspaces from noisy, linear features. Drawing an analogy between discrimination among subspaces and communication over vector wireless channels, we propose two Shannon-inspired measures to characterize asymptotic classifier performance. First, we define the classification capacity, which characterizes necessary and sufficient conditions for the misclassification probability to vanish as the signal dimension, the number of features, and the number of subspaces to be discerned all approach infinity. Second, we define the diversity-discrimination tradeoff which, by analogy with the diversity-multiplexing tradeoff of fading vector channels, characterizes relationships between the number of discernible subspaces and the misclassification probability as the noise power approaches zero. We derive upper and lower bounds on these measures which are tight in many regimes. Numerical results, including a face recognition application, validate the results in practice.',
	 'authors': u'Matthew Nokleby, Miguel Rodrigues, Robert Calderbank,',
	 'category': u'Computer Science ',
	 'date': '2014-4-21',
	 'pdflink': u'http://arxiv.org/pdf/1404.5187',
	 'subjects': u'Information Theory (cs.IT)',
	 'title': u'\nDiscrimination on the Grassmann Manifold: Fundamental Limits of Subspace  Classifiers',
	 'urllink': u'http://arxiv.org/abs/1404.5187'}
2015-03-23 20:27:41+0000 [xxu461000] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1409.1889> (referer: http://arxiv.org/list/cs/14?skip=10000&show=1000)
2015-03-23 20:27:41+0000 [xxu461000] DEBUG: Scraped from <200 http://arxiv.org/abs/1409.1889>
	{'abstract': u'Analysis of transient stability of strongly nonlinear post-fault dynamics is one of the most computationally challenging parts of Dynamic Security Assessment. This paper proposes a novel approach for assessment of transient stability of the system. The approach generalizes the idea of energy methods, and extends the concept of energy function to a more general Lyapunov Functions Family (LFF) constructed via Semi-Definite-Programming techniques. Unlike the traditional energy function and its variations, the constructed Lyapunov functions are proven to be decreasing only in a finite neighborhood of the equilibrium point. However, we show that they can still certify stability of a broader set of initial conditions in comparison to the traditional energy function in the closest-UEP method. Moreover, the certificates of stability can be constructed via a sequence of convex optimization problems that are tractable even for large scale systems. We also propose specific algorithms for adaptation of the Lyapunov functions to specific initial conditions and demonstrate the effectiveness of the approach on a number of IEEE test cases.',
	 'authors': u'Thanh Long Vu, Konstantin Turitsyn,',
	 'category': u'Computer Science ',
	 'date': '2014-9-5',
	 'pdflink': u'http://arxiv.org/pdf/1409.1889',
	 'subjects': u'Systems and Control (cs.SY)',
	 'title': u'\nLyapunov Functions Family Approach to Transient Stability Assessment',
	 'urllink': u'http://arxiv.org/abs/1409.1889'}
2015-03-23 20:27:44+0000 [xxu461000] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1406.7608> (referer: http://arxiv.org/list/cs/14?skip=7000&show=1000)
2015-03-23 20:27:44+0000 [xxu461000] DEBUG: Scraped from <200 http://arxiv.org/abs/1406.7608>
	{'abstract': u'We revisit the AMBA AHB case study that has been used as a benchmark for several reactive syn- thesis tools. Synthesizing AMBA AHB implementations that can serve a large number of masters is still a difficult problem. We demonstrate how to use parameterized synthesis in token rings to obtain an implementation for a component that serves a single master, and can be arranged in a ring of arbitrarily many components. We describe new tricks -- property decompositional synthesis, and direct encoding of simple GR(1) -- that together with previously described optimizations allowed us to synthesize the model with 14 states in 30 minutes.',
	 'authors': u'Roderick Bloem, Swen Jacobs, Ayrat Khalimov,',
	 'category': u'Computer Science ',
	 'date': '2014-6-30',
	 'pdflink': u'http://arxiv.org/pdf/1406.7608',
	 'subjects': u'Software Engineering (cs.SE)',
	 'title': u'\nParameterized Synthesis Case Study: AMBA AHB (extended version)',
	 'urllink': u'http://arxiv.org/abs/1406.7608'}
2015-03-23 20:27:48+0000 [xxu461000] INFO: Crawled 417 pages (at 12 pages/min), scraped 400 items (at 12 items/min)
2015-03-23 20:27:50+0000 [xxu461000] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1405.2113> (referer: http://arxiv.org/list/cs/14?skip=5000&show=1000)
2015-03-23 20:27:50+0000 [xxu461000] DEBUG: Scraped from <200 http://arxiv.org/abs/1405.2113>
	{'abstract': u'We consider signals that follow a parametric distribution where the parameter values are unknown. To estimate such signals from noisy measurements in scalar channels, we study the empirical performance of an empirical Bayes (EB) approach and a full Bayes (FB) approach. We then apply EB and FB to solve compressed sensing (CS) signal estimation problems by successively denoising a scalar Gaussian channel within an approximate message passing (AMP) framework. Our numerical results show that FB achieves better performance than EB in scalar channel denoising problems when the signal dimension is small. In the CS setting, the signal dimension must be large enough for AMP to work well; for large signal dimensions, AMP has similar performance with FB and EB.',
	 'authors': u'Yanting Ma, Jin Tan, Nikhil Krishnan, Dror Baron,',
	 'category': u'Computer Science ',
	 'date': '2014-5-8',
	 'pdflink': u'http://arxiv.org/pdf/1405.2113',
	 'subjects': u'Information Theory (cs.IT)',
	 'title': u'\nEmpirical Bayes and Full Bayes for Signal Estimation',
	 'urllink': u'http://arxiv.org/abs/1405.2113'}
2015-03-23 20:27:55+0000 [xxu461000] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1405.6477> (referer: http://arxiv.org/list/cs/14?skip=6000&show=1000)
2015-03-23 20:27:55+0000 [xxu461000] DEBUG: Scraped from <200 http://arxiv.org/abs/1405.6477>
	{'abstract': u'This paper examines synchronization of computer clocks connected via a data network and proposes a skewless algorithm to synchronize them. Unlike existing solutions, which either estimate and compensate the frequency difference (skew) among clocks or introduce offset corrections that can generate jitter and possibly even backward jumps, our solution achieves synchronization without these problems. We first analyze the convergence property of the algorithm and provide explicit necessary and sufficient conditions on the parameters to guarantee synchronization. We then study the effect of noisy measurements (jitter) and frequency drift (wander) on the offsets and synchronization frequency, and further optimize the parameter values to minimize their variance. Our study reveals a few insights, for example, we show that our algorithm can converge even in the presence of timing loops and noise, provided that there is a well defined leader. This marks a clear contrast with current standards such as NTP and PTP, where timing loops are specifically avoided. Furthermore, timing loops can even be beneficial in our scheme as it is demonstrated that highly connected subnetworks can collectively outperform individual clients when the time source has large jitter. The results are supported by experiments running on a cluster of IBM BladeCenter servers with Linux.',
	 'authors': u'Enrique Mallada, Xiaoqiao Meng, Michel Hack, Li Zhang, Ao Tang,',
	 'category': u'Computer Science ',
	 'date': '2014-5-26',
	 'pdflink': u'http://arxiv.org/pdf/1405.6477',
	 'subjects': u'Optimization and Control (math.OC)',
	 'title': u'\nSkewless Network Clock Synchronization Without Discontinuity:  Convergence and Performance',
	 'urllink': u'http://arxiv.org/abs/1405.6477'}
2015-03-23 20:27:59+0000 [xxu461000] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1407.5068> (referer: http://arxiv.org/list/cs/14?skip=8000&show=1000)
2015-03-23 20:27:59+0000 [xxu461000] DEBUG: Scraped from <200 http://arxiv.org/abs/1407.5068>
	{'abstract': u'An automaton is monotonic if its states can be arranged in a linear order that is preserved by the action of every letter. We prove that the problem of deciding whether a given automaton is monotonic is NP-complete. This also holds under the restriction to binary input alphabets. The same result is obtained for oriented automata, whose states can be arranged in a cyclic order.',
	 'authors': u'Marek Szyku\u0142a,',
	 'category': u'Computer Science ',
	 'date': '2014-7-18',
	 'pdflink': u'http://arxiv.org/pdf/1407.5068',
	 'subjects': u'Formal Languages and Automata Theory (cs.FL)',
	 'title': u'\nChecking whether an Automaton is Monotonic is NP-complete',
	 'urllink': u'http://arxiv.org/abs/1407.5068'}
2015-03-23 20:28:03+0000 [xxu461000] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1404.5173> (referer: http://arxiv.org/list/cs/14?skip=4000&show=1000)
2015-03-23 20:28:03+0000 [xxu461000] DEBUG: Scraped from <200 http://arxiv.org/abs/1404.5173>
	{'abstract': u'We study the problem of compression for the purpose of similarity identification, where similarity is measured by the mean square Euclidean distance between vectors. While the asymptotical fundamental limits of the problem - the minimal compression rate and the error exponent - were found in a previous work, in this paper we focus on the nonasymptotic domain and on practical, implementable schemes. We first present a finite blocklength achievability bound based on shape-gain quantization: The gain (amplitude) of the vector is compressed via scalar quantization and the shape (the projection on the unit sphere) is quantized using a spherical code. The results are numerically evaluated and they converge to the asymptotic values as predicted by the error exponent. We then give a nonasymptotic lower bound on the performance of any compression scheme, and compare to the upper (achievability) bound. For a practical implementation of such a scheme, we use wrapped spherical codes, studied by Hamkins and Zeger, and use the Leech lattice as an example for an underlying lattice. As a side result, we obtain a bound on the covering angle of any wrapped spherical code, as a function of the covering radius of the underlying lattice.',
	 'authors': u'Fabian Steiner, Steffen Dempfle, Amir Ingber, Tsachy Weissman,',
	 'category': u'Computer Science ',
	 'date': '2014-4-21',
	 'pdflink': u'http://arxiv.org/pdf/1404.5173',
	 'subjects': u'Information Theory (cs.IT)',
	 'title': u'\nCompression for Quadratic Similarity Queries: Finite Blocklength and  Practical Schemes',
	 'urllink': u'http://arxiv.org/abs/1404.5173'}
2015-03-23 20:28:09+0000 [xxu461000] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1409.1879> (referer: http://arxiv.org/list/cs/14?skip=10000&show=1000)
2015-03-23 20:28:09+0000 [xxu461000] DEBUG: Scraped from <200 http://arxiv.org/abs/1409.1879>
	{'abstract': u"Software systems situated in network environment may experience performance degradation, availability decrease and even crash during long time running, which is called software aging. This phenomenon has been studied for more than 15 years, but most of the literatures studied software as a black box, none of them uncovered the fundamental and widely accepted mechanism of software aging as far as we know. Through analyzing the characteristics between biological aging and software aging, we ?nd some interesting common points and bridge the gap between these two seemingly unrelated phenomena. The free radical aging theory in biological studies is also applicative to explore the mechanism and model of software aging. This paper ?nds an equivalent concept named `software free radical' in software aging to free radical in biological aging. In our study, the accumulation of `software free radical' is a root cause of software aging. Using the free radical modeling methodology in biological aging, we give a model for describing the kinetic of software aging based on feedback loops. Although this paper doesn't give enough theoretical proof of the modeling method, the practical results show that the feedback loop model can describe the kinetic of software aging precisely. To further validate the aging mechanism, we propose several software rejuvenation strategies focusing on cleaning the `software free radical'. The results show that software aging can be mitigated e?ectively by strengthening negative feedback loop or weakening positive feedback loop. This paper is the ?rst try to answer the question `How software ages' through interdisciplinary studies. Leveraging the conclusions in this paper, people can design better software systems or keep their systems at a high performance level during long time running.",
	 'authors': u'Pengfei Chen, Yong Qi, Di Hou, Jiankang Liu,',
	 'category': u'Computer Science ',
	 'date': '2014-8-27',
	 'pdflink': u'http://arxiv.org/pdf/1409.1879',
	 'subjects': u'Software Engineering (cs.SE)',
	 'title': u'\nBio-inspired Mechanism and Model Exploration of Software Aging',
	 'urllink': u'http://arxiv.org/abs/1409.1879'}
2015-03-23 20:28:12+0000 [xxu461000] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1406.7589> (referer: http://arxiv.org/list/cs/14?skip=7000&show=1000)
2015-03-23 20:28:12+0000 [xxu461000] DEBUG: Scraped from <200 http://arxiv.org/abs/1406.7589>
	{'abstract': u"Scholarship on teams has focused on the relationship between a team's performance, however defined, and the network structure among team members. For example, Uzzi and Spiro (2005) find that the creative performance of Broadway musical teams depends heavily on the internal cohesion of team members and their past collaborative experience with individuals outside their immediate teams. In other words, team members' internal cohesion and external ties are crucial to the team's success. How, then, do they interact to produce positive performance outcomes? In our work, we separate the proximal causes of tie formation from the proximal determinants of outcomes to determine the mechanism behind this interaction. To examine this puzzle, we examine the performance of national soccer squads over time as a function of changing levels and configurations of brokerage and closure ties formed by players working for professional soccer clubs.",
	 'authors': u'Jose Uribe, Dan Wang,',
	 'category': u'Computer Science ',
	 'date': '2014-6-30',
	 'pdflink': u'http://arxiv.org/pdf/1406.7589',
	 'subjects': u'Social and Information Networks (cs.SI)',
	 'title': u'\nLearning from Others, Together: Brokerage, Closure and Team Performance',
	 'urllink': u'http://arxiv.org/abs/1406.7589'}
2015-03-23 20:28:16+0000 [xxu461000] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1405.2106> (referer: http://arxiv.org/list/cs/14?skip=5000&show=1000)
2015-03-23 20:28:16+0000 [xxu461000] DEBUG: Scraped from <200 http://arxiv.org/abs/1405.2106>
	{'abstract': u'We present ITE (information theoretical estimators) a free and open source, multi-platform, Matlab/Octave toolbox that is capable of estimating many different variants of entropy, mutual information, divergence, association measures, cross quantities, and kernels on distributions. Thanks to its highly modular design, ITE supports additionally (i) the combinations of the estimation techniques, (ii) the easy construction and embedding of novel information theoretical estimators, and (iii) their immediate application in information theoretical optimization problems. ITE also includes a prototype application in a central problem class of signal processing, independent subspace analysis and its extensions.',
	 'authors': u'Zoltan Szabo,',
	 'category': u'Computer Science ',
	 'date': '2014-5-8',
	 'pdflink': u'http://arxiv.org/pdf/1405.2106',
	 'subjects': u'Information Theory (cs.IT)',
	 'title': u'\nInformation Theoretical Estimators Toolbox',
	 'urllink': u'http://arxiv.org/abs/1405.2106'}
2015-03-23 20:28:20+0000 [xxu461000] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1405.6467> (referer: http://arxiv.org/list/cs/14?skip=6000&show=1000)
2015-03-23 20:28:20+0000 [xxu461000] DEBUG: Scraped from <200 http://arxiv.org/abs/1405.6467>
	{'abstract': u'Many network applications rely on the synchronization of coupled oscillators. For example, such synchronization can provide networked devices with a common temporal reference necessary for coordinating actions or decoding transmitted messages. In this paper, we study the problem of using distributed control to achieve both phase and frequency synchronization of a network of coupled heterogeneous nonlinear oscillators. Not only do our controllers guarantee zero phase error in steady state under arbitrary frequency heterogeneity, but they also require little knowledge of the oscillator nonlinearities and network topology. Furthermore, we provide a global convergence analysis, in the absence of noise and propagation delay, for the resulting nonlinear system whose phase vector evolves on the n-torus.',
	 'authors': u'Enrique Mallada, Randy A. Freeman, Ao Tang,',
	 'category': u'Computer Science ',
	 'date': '2014-5-26',
	 'pdflink': u'http://arxiv.org/pdf/1405.6467',
	 'subjects': u'Optimization and Control (math.OC)',
	 'title': u'\nDistributed Synchronization of Heterogeneous Oscillators on Networks  with Arbitrary Topology',
	 'urllink': u'http://arxiv.org/abs/1405.6467'}
2015-03-23 20:28:26+0000 [xxu461000] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1407.5055> (referer: http://arxiv.org/list/cs/14?skip=8000&show=1000)
2015-03-23 20:28:26+0000 [xxu461000] DEBUG: Scraped from <200 http://arxiv.org/abs/1407.5055>
	{'abstract': u'We propose a data-dependent denoising procedure to restore noisy images. Different from existing denoising algorithms which search for patches from either the noisy image or a generic database, the new algorithm finds patches from a database that contains only relevant patches. We formulate the denoising problem as an optimal filter design problem and make two contributions. First, we determine the basis function of the denoising filter by solving a group sparsity minimization problem. The optimization formulation generalizes existing denoising algorithms and offers systematic analysis of the performance. Improvement methods are proposed to enhance the patch search process. Second, we determine the spectral coefficients of the denoising filter by considering a localized Bayesian prior. The localized prior leverages the similarity of the targeted database, alleviates the intensive Bayesian computation, and links the new method to the classical linear minimum mean squared error estimation. We demonstrate applications of the proposed method in a variety of scenarios, including text images, multiview images and face images. Experimental results show the superiority of the new algorithm over existing methods.',
	 'authors': u'Enming Luo, Stanley H. Chan, Truong Q. Nguyen,',
	 'category': u'Computer Science ',
	 'date': '2014-6-30',
	 'pdflink': u'http://arxiv.org/pdf/1407.5055',
	 'subjects': u'Computer Vision and Pattern Recognition (cs.CV)',
	 'title': u'\nAdaptive Image Denoising by Targeted Databases',
	 'urllink': u'http://arxiv.org/abs/1407.5055'}
2015-03-23 20:28:32+0000 [xxu461000] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1404.5169> (referer: http://arxiv.org/list/cs/14?skip=4000&show=1000)
2015-03-23 20:28:32+0000 [xxu461000] DEBUG: Scraped from <200 http://arxiv.org/abs/1404.5169>
	{'abstract': u'There is a close connection between Direct Product and XOR lemmas in the sense that in many settings, we can prove one given the other. The known reductions that are used for the above purpose are either in the non-uniform setting or give non-matching parameters. By non-matching parameter we mean that -wise Direct Product lemma implies -wise XOR lemma (and vice versa) for . In this work, we discuss reductions between -wise Direct Product and -wise XOR lemmas. That is, we show that if the -wise direct product lemma holds, then so does the -wise XOR lemma and vice versa. We show that even though there is a perfectly uniform reduction in one direction, the reduction in the other direction requires some amount of non-uniformity. We give reductions in both directions matching information-theoretic bounds up to polynomial factors. Our techniques also give a small quantitative improvement over the known results about proving -wise XOR lemma using -wise Direct Product lemma.',
	 'authors': u'Ragesh Jaiswal,',
	 'category': u'Computer Science ',
	 'date': '2014-4-21',
	 'pdflink': u'http://arxiv.org/pdf/1404.5169',
	 'subjects': u'Computational Complexity (cs.CC)',
	 'title': u'\nOn Uniform Reductions between Direct Product and XOR Lemmas',
	 'urllink': u'http://arxiv.org/abs/1404.5169'}
2015-03-23 20:28:37+0000 [xxu461000] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1409.1846> (referer: http://arxiv.org/list/cs/14?skip=10000&show=1000)
2015-03-23 20:28:37+0000 [xxu461000] DEBUG: Scraped from <200 http://arxiv.org/abs/1409.1846>
	{'abstract': u'We describe three in-field data collection efforts yielding a large database of RSSI values vs. time or distance from vehicles communicating with each other via DSRC. We show several data processing schemes we have devised to develop Vehicle-to-Vehicle (V2V) propagation models from such data. The database is limited in several important ways, not least, the presence of a high noise floor that limits the distance over which good modeling is feasible. Another is the presence of interference from multiple active transmitters. Our methodology makes it possible to obtain, despite these limitations, accurate models of median path loss vs. distance, shadow fading, and fast fading caused by multipath. We aim not to develop a new V2V model, but to show the methods enabling such a model to be obtained from in-field RSSI data.',
	 'authors': u'Silvija Kokalj-Filipovic, Larry Greenstein, Bin Cheng, Marco Gruteser,',
	 'category': u'Computer Science ',
	 'date': '2014-9-5',
	 'pdflink': u'http://arxiv.org/pdf/1409.1846',
	 'subjects': u'Other Computer Science (cs.OH)',
	 'title': u'\nV2V Propagation Modeling with Imperfect RSSI Samples',
	 'urllink': u'http://arxiv.org/abs/1409.1846'}
2015-03-23 20:28:42+0000 [xxu461000] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1406.7588> (referer: http://arxiv.org/list/cs/14?skip=7000&show=1000)
2015-03-23 20:28:42+0000 [xxu461000] DEBUG: Scraped from <200 http://arxiv.org/abs/1406.7588>
	{'abstract': u"We investigate the feasibility of obtaining highly trustworthy results using crowdsourcing on complex engineering tasks. Crowdsourcing is increasingly seen as a potentially powerful way of increasing the supply of labor for solving society's problems. While applications in domains such as citizen-science, citizen-journalism or knowledge organization (e.g., Wikipedia) have seen many successful applications, there have been fewer applications focused on solving engineering problems, especially those involving complex tasks. This may be in part because of concerns that low quality input into engineering analysis and design could result in failed structures leading to loss of life. We compared the quality of work of the anonymous workers of Amazon Mechanical Turk (AMT), an online crowdsourcing service, with the quality of work of expert engineers in solving the complex engineering task of evaluating virtual wind tunnel data graphs. On this representative complex engineering task, our results showed that there was little difference between expert engineers and crowdworkers in the quality of their work and explained reasons for these results. Along with showing that crowdworkers are effective at completing new complex tasks our paper supplies a number of important lessons that were learned in the process of collecting this data from AMT, which may be of value to other researchers.",
	 'authors': u'Matthew Staffelbach, Peter Sempolinski, David Hachen, Ahsan Kareem, Tracy Kijewski-Correa, Douglas Thain, Daniel Wei, Greg Madey,',
	 'category': u'Computer Science ',
	 'date': '2014-6-30',
	 'pdflink': u'http://arxiv.org/pdf/1406.7588',
	 'subjects': u'Social and Information Networks (cs.SI)',
	 'title': u'\nLessons Learned from an Experiment in Crowdsourcing Complex Citizen  Engineering Tasks with Amazon Mechanical Turk',
	 'urllink': u'http://arxiv.org/abs/1406.7588'}
2015-03-23 20:28:47+0000 [xxu461000] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1405.2102> (referer: http://arxiv.org/list/cs/14?skip=5000&show=1000)
2015-03-23 20:28:47+0000 [xxu461000] DEBUG: Scraped from <200 http://arxiv.org/abs/1405.2102>
	{'abstract': u'We propose a method to improve image clustering using sparse text and the wisdom of the crowds. In particular, we present a method to fuse two different kinds of document features, image and text features, and use a common dictionary or "wisdom of the crowds" as the connection between the two different kinds of documents. With the proposed fusion matrix, we use topic modeling via non-negative matrix factorization to cluster documents.',
	 'authors': u'Anna Ma, Arjuna Flenner, Deanna Needell, Allon G. Percus,',
	 'category': u'Computer Science ',
	 'date': '2014-5-8',
	 'pdflink': u'http://arxiv.org/pdf/1405.2102',
	 'subjects': u'Learning (cs.LG)',
	 'title': u'\nImproving Image Clustering using Sparse Text and the Wisdom of the  Crowds',
	 'urllink': u'http://arxiv.org/abs/1405.2102'}
2015-03-23 20:28:48+0000 [xxu461000] INFO: Crawled 430 pages (at 13 pages/min), scraped 413 items (at 13 items/min)
2015-03-23 20:28:53+0000 [xxu461000] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1405.6408> (referer: http://arxiv.org/list/cs/14?skip=6000&show=1000)
2015-03-23 20:28:53+0000 [xxu461000] DEBUG: Scraped from <200 http://arxiv.org/abs/1405.6408>
	{'abstract': u'We consider multiple-antenna signal detection of primary user transmission signals by a secondary user receiver in cognitive radio networks. The optimal detector is analyzed for the scenario where the number of primary user signals is no less than the number of receive antennas at the secondary user. We first derive exact expressions for the moments of the generalized likelihood ratio test (GLRT) statistic, yielding approximations for the false alarm and detection probabilities. We then show that the normalized GLRT statistic converges in distribution to a Gaussian random variable when the number of antennas and observations grow large at the same rate. Further, using results from large random matrix theory, we derive expressions to compute the detection probability without explicit knowledge of the channel, and then particularize these expressions for two scenarios of practical interest: 1) a single primary user sending spatially multiplexed signals, and 2) multiple spatially distributed primary users. Our analytical results are finally used to obtain simple design rules for the signal detection threshold.',
	 'authors': u'David Morales-Jimenez, Raymond H. Y. Louie, Matthew R. McKay, Yang Chen,',
	 'category': u'Computer Science ',
	 'date': '2014-5-25',
	 'pdflink': u'http://arxiv.org/pdf/1405.6408',
	 'subjects': u'Statistics Theory (math.ST)',
	 'title': u'\nMultiple-Antenna Signal Detection in Cognitive Radio Networks with  Multiple Primary User Signals',
	 'urllink': u'http://arxiv.org/abs/1405.6408'}
2015-03-23 20:28:56+0000 [xxu461000] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1407.5042> (referer: http://arxiv.org/list/cs/14?skip=8000&show=1000)
2015-03-23 20:28:56+0000 [xxu461000] DEBUG: Scraped from <200 http://arxiv.org/abs/1407.5042>
	{'abstract': u'Counterfactual Regret Minimization and variants (e.g. Public Chance Sampling CFR and Pure CFR) have been known as the best approaches for creating approximate Nash equilibrium solutions for imperfect information games such as poker. This paper introduces CFR, a new algorithm that typically outperforms the previously known algorithms by an order of magnitude or more in terms of computation time while also potentially requiring less memory.',
	 'authors': u'Oskari Tammelin,',
	 'category': u'Computer Science ',
	 'date': '2014-7-18',
	 'pdflink': u'http://arxiv.org/pdf/1407.5042',
	 'subjects': u'Computer Science and Game Theory (cs.GT)',
	 'title': u'\nSolving Large Imperfect Information Games Using CFR+',
	 'urllink': u'http://arxiv.org/abs/1407.5042'}
2015-03-23 20:29:02+0000 [xxu461000] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1404.5165> (referer: http://arxiv.org/list/cs/14?skip=4000&show=1000)
2015-03-23 20:29:02+0000 [xxu461000] DEBUG: Scraped from <200 http://arxiv.org/abs/1404.5165>
	{'abstract': u"Central to robot exploration and mapping is the task of persistent localization in environmental fields characterized by spatially correlated measurements. This paper presents a Gaussian process localization (GP-Localize) algorithm that, in contrast to existing works, can exploit the spatially correlated field measurements taken during a robot's exploration (instead of relying on prior training data) for efficiently and scalably learning the GP observation model online through our proposed novel online sparse GP. As a result, GP-Localize is capable of achieving constant time and memory (i.e., independent of the size of the data) per filtering step, which demonstrates the practical feasibility of using GPs for persistent robot localization and autonomy. Empirical evaluation via simulated experiments with real-world datasets and a real robot experiment shows that GP-Localize outperforms existing GP localization algorithms.",
	 'authors': u'Nuo Xu, Kian Hsiang Low, Jie Chen, Keng Kiat Lim, Etkin Baris Ozgul,',
	 'category': u'Computer Science ',
	 'date': '2014-4-21',
	 'pdflink': u'http://arxiv.org/pdf/1404.5165',
	 'subjects': u'Robotics (cs.RO)',
	 'title': u'\nGP-Localize: Persistent Mobile Robot Localization using Online Sparse  Gaussian Process Observation Model',
	 'urllink': u'http://arxiv.org/abs/1404.5165'}
2015-03-23 20:29:07+0000 [xxu461000] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1409.1831> (referer: http://arxiv.org/list/cs/14?skip=10000&show=1000)
2015-03-23 20:29:07+0000 [xxu461000] DEBUG: Scraped from <200 http://arxiv.org/abs/1409.1831>
	{'abstract': u'We make the case for mathematicians and statisticians to stake their claim in the fast-moving and high-impact research field that is becoming known as Future Cities. After assessing the Future Cities arena, we provide some illustrative challenges where mathematical scientists can make an impact.',
	 'authors': u'Peter Grindrod, Desmond J. Higham, Robert S. MacKay,',
	 'category': u'Computer Science ',
	 'date': '2014-9-5',
	 'pdflink': u'http://arxiv.org/pdf/1409.1831',
	 'subjects': u'Computers and Society (cs.CY)',
	 'title': u'\nOpportunities at the Mathematics/Future Cities Interface',
	 'urllink': u'http://arxiv.org/abs/1409.1831'}
2015-03-23 20:29:10+0000 [xxu461000] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1406.7586> (referer: http://arxiv.org/list/cs/14?skip=7000&show=1000)
2015-03-23 20:29:10+0000 [xxu461000] DEBUG: Scraped from <200 http://arxiv.org/abs/1406.7586>
	{'abstract': u"Using data from a large laboratory experiment on problem solving in which we varied the structure of 16-person networks we investigate how an organization's network structure may be constructed to optimize performance in complex problem-solving tasks. Problem solving involves both search for information and search for theories to make sense of that information. We show that the effect of network structure is opposite for these two equally important forms of search. Dense clustering encourages members of a network to generate more diverse information, but it also has the power to discourage the generation of diverse theories: clustering promotes exploration in information space, but decreases exploration in solution space. Previous research, tending to focus on only one of those two spaces, had produced inconsistent conclusions about the value of network clustering. By adopting an experimental platform on which information was measured separately from solutions, we were able to reconcile past contradictions and clarify the effects of network clustering on performance. The finding both provides a sharper tool for structuring organizations for knowledge work and reveals the challenges inherent in manipulating network structure to enhance performance, as the communication structure that helps one aspect of problem solving may harm the other.",
	 'authors': u'Jesse Shore, Ethan Bernstein, David Lazer,',
	 'category': u'Computer Science ',
	 'date': '2014-6-30',
	 'pdflink': u'http://arxiv.org/pdf/1406.7586',
	 'subjects': u'Social and Information Networks (cs.SI)',
	 'title': u'\nFacts and Figuring: An Experimental Investigation of Network Structure  and Performance in Information and Solution Spaces',
	 'urllink': u'http://arxiv.org/abs/1406.7586'}
2015-03-23 20:29:15+0000 [xxu461000] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1405.2092> (referer: http://arxiv.org/list/cs/14?skip=5000&show=1000)
2015-03-23 20:29:15+0000 [xxu461000] DEBUG: Scraped from <200 http://arxiv.org/abs/1405.2092>
	{'abstract': u'The conventional design of cellular systems prescribes the separation of uplink and downlink transmissions via time-division or frequency-division duplex. Recent advances in analog and digital domain self-interference interference cancellation challenge the need for this arrangement and open up the possibility to operate base stations, especially low-power ones, in a full-duplex mode. As a means to cope with the resulting downlink-to-uplink interference among base stations, this letter investigates the impact of the Cloud Radio Access Network (C-RAN) architecture. The analysis follows an information theoretic approach based on the classical Wyner model. The analytical results herein confirm the significant potential advantages of the C-RAN architecture in the presence of full-duplex base stations, as long as sufficient fronthaul capacity is available and appropriate mobile station scheduling, or successive interference cancellation at the mobile stations, is implemented.',
	 'authors': u'O. Simeone, E. Erkip, S. Shamai,',
	 'category': u'Computer Science ',
	 'date': '2014-5-8',
	 'pdflink': u'http://arxiv.org/pdf/1405.2092',
	 'subjects': u'Information Theory (cs.IT)',
	 'title': u'\nFull-Duplex Cloud Radio Access Networks: An Information-Theoretic  Viewpoint',
	 'urllink': u'http://arxiv.org/abs/1405.2092'}
2015-03-23 20:29:18+0000 [xxu461000] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1405.6400> (referer: http://arxiv.org/list/cs/14?skip=6000&show=1000)
2015-03-23 20:29:18+0000 [xxu461000] DEBUG: Scraped from <200 http://arxiv.org/abs/1405.6400>
	{'abstract': u"We investigate the role of networks of alliances in preventing (multilateral) interstate wars. We first show that, in the absence of international trade, no network of alliances is peaceful and stable. We then show that international trade induces peaceful and stable networks: trade increases the density of alliances so that countries are less vulnerable to attack and also reduces countries' incentives to attack an ally. We present historical data on wars and trade, noting that the dramatic drop in interstate wars since 1950, and accompanying densification and stabilization of alliances, are consistent with the model but not other prominent theories.",
	 'authors': u'Matthew O. Jackson, Stephen M. Nei,',
	 'category': u'Computer Science ',
	 'date': '2014-5-25',
	 'pdflink': u'http://arxiv.org/pdf/1405.6400',
	 'subjects': u'Physics and Society (physics.soc-ph)',
	 'title': u'\nNetworks of Military Alliances, Wars, and International Trade',
	 'urllink': u'http://arxiv.org/abs/1405.6400'}
2015-03-23 20:29:23+0000 [xxu461000] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1407.5040> (referer: http://arxiv.org/list/cs/14?skip=8000&show=1000)
2015-03-23 20:29:23+0000 [xxu461000] DEBUG: Scraped from <200 http://arxiv.org/abs/1407.5040>
	{'abstract': u'Wireless communication is the prerequisite for the highly desired in-situ and real-time monitoring capability in underground environments, including oil reservoirs, groundwater aquifers, volcanos, among others. However, existing wireless communication techniques do not work in such environments due to the harsh transmission medium with very high material absorption and the inaccessible nature of underground environment that requires extremely small device size. Although Magnetic Induction (MI) communication has been shown to be a promising technique in underground environments, the existing MI system utilizes very large coil antennas, which are not suitable for deployment in underground. In this paper, we propose a metamaterial enhanced magnetic induction communication mechanism that can achieve over meter scale communication range by using millimeter scale coil antennas in the harsh underground environment. An analytical channel model for the new mechanism is developed to explore the fundamentals of metamaterial enhanced MI communication in various underground environments. The effects of important system and environmental factors are quantitatively captured, including the operating frequency, bandwidth, and parameters of metamaterial antennas, as well as permittivity, permeability, and conductivity of underground medium. The theoretical model is validated through the finite element simulation software, COMSOL Multiphysics.',
	 'authors': u'Hongzhi Guo, Zhi Sun,',
	 'category': u'Computer Science ',
	 'date': '2014-7-17',
	 'pdflink': u'http://arxiv.org/pdf/1407.5040',
	 'subjects': u'Other Computer Science (cs.OH)',
	 'title': u'\nChannel Modeling for Metamaterial-Enhanced Underground Wireless  Communications',
	 'urllink': u'http://arxiv.org/abs/1407.5040'}
2015-03-23 20:29:29+0000 [xxu461000] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1404.5157> (referer: http://arxiv.org/list/cs/14?skip=4000&show=1000)
2015-03-23 20:29:29+0000 [xxu461000] DEBUG: Scraped from <200 http://arxiv.org/abs/1404.5157>
	{'abstract': u'One-Counter nets (OCN) consist of a nondeterministic finite control and a single integer counter that cannot be fully tested for zero. They form a natural subclass of both One-Counter Automata, which allow zero-tests and Petri Nets/VASS, which allow multiple such weak counters. The trace inclusion problem has recently been shown to be undecidable for OCN. In this paper, we contrast the complexity of two natural restrictions which imply decidability. First, we show that trace inclusion between an OCN and a deterministic OCN is NL-complete, even with arbitrary binary-encoded initial counter-values as part of the input. Secondly, we show Ackermannian completeness of for the trace universality problem of nondeterministic OCN. This problem is equivalent to checking trace inclusion between a finite and a OCN-process.',
	 'authors': u'Piotr Hofman, Patrick Totzke,',
	 'category': u'Computer Science ',
	 'date': '2014-4-21',
	 'pdflink': u'http://arxiv.org/pdf/1404.5157',
	 'subjects': u'Logic in Computer Science (cs.LO)',
	 'title': u'\nTrace Inclusion for One-Counter Nets Revisited',
	 'urllink': u'http://arxiv.org/abs/1404.5157'}
2015-03-23 20:29:34+0000 [xxu461000] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1409.1793> (referer: http://arxiv.org/list/cs/14?skip=10000&show=1000)
2015-03-23 20:29:34+0000 [xxu461000] DEBUG: Scraped from <200 http://arxiv.org/abs/1409.1793>
	{'abstract': u'When faced with the task of designing and implementing a new self-aware and self-expressive computing system, researchers and practitioners need a set of guidelines on how to use the concepts and foundations developed in the Engineering Proprioception in Computing Systems (EPiCS) project. This report provides such guidelines on how to design self-aware and self-expressive computing systems in a principled way. We have documented different categories of self-awareness and self-expression level using architectural patterns. We have also documented common architectural primitives, their possible candidate techniques and attributes for architecting self-aware and self-expressive systems. Drawing on the knowledge obtained from the previous investigations, we proposed a pattern driven methodology for engineering self-aware and self-expressive systems to assist in utilising the patterns and primitives during design. The methodology contains detailed guidance to make decisions with respect to the possible design alternatives, providing a systematic way to build self-aware and self-expressive systems. Then, we qualitatively and quantitatively evaluated the methodology using two case studies. The results reveal that our pattern driven methodology covers the main aspects of engineering self-aware and self-expressive systems, and that the resulted systems perform significantly better than the non-self-aware systems.',
	 'authors': u'Tao Chen, Funmilade Faniyi, Rami Bahsoon, Peter R. Lewis, Xin Yao, Leandro L. Minku, Lukas Esterle,',
	 'category': u'Computer Science ',
	 'date': '2014-9-5',
	 'pdflink': u'http://arxiv.org/pdf/1409.1793',
	 'subjects': u'Software Engineering (cs.SE)',
	 'title': u'\nThe Handbook of Engineering Self-Aware and Self-Expressive Systems',
	 'urllink': u'http://arxiv.org/abs/1409.1793'}
2015-03-23 20:29:39+0000 [xxu461000] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1406.7585> (referer: http://arxiv.org/list/cs/14?skip=7000&show=1000)
2015-03-23 20:29:39+0000 [xxu461000] DEBUG: Scraped from <200 http://arxiv.org/abs/1406.7585>
	{'abstract': u"Social contagion has been studied in various contexts. Many instances of social contagion can be modeled as an infection process where a specific state (adoption of product, fad, knowledge, behavior, etc.) spreads from individual to individual through links between them. In the meantime, other forms of social contagion may better be understood as a diffusion process where the state of an individual tends to assimilate with the social norm (i.e., local average state) within his/her neighborhood. Unlike infection scenarios where influence is nonlinear, unidirectional, fast, and potentially disruptive, social diffusion is linear, bidirectional, gradual, and converging. The distance between an individual's state and his/her neighbors' average state always decreases, and thus a homogeneous global state is guaranteed to be the network's stable equilibrium state in the long run. This does not sound as intriguing or exciting as infection dynamics, which might be why there are very few studies on mathematical models of social diffusion processes. Here, this study attempts to shed new light on an unrecognized characteristic of social diffusion, i.e., non-trivial drift it can cause to the network's global average state. Although somewhat counterintuitive, such global drift is indeed possible because, unlike physical diffusion processes, social diffusion processes are not conservational. In what follows, a mathematical model of social diffusion will be presented to explain the mechanism of this phenomenon, and some possible collective actions for influencing the direction of global drift will be proposed. The relevance of social diffusion to individual and collective improvement will be discussed briefly, with an emphasis on educational applications.",
	 'authors': u'Hiroki Sayama,',
	 'category': u'Computer Science ',
	 'date': '2014-6-30',
	 'pdflink': u'http://arxiv.org/pdf/1406.7585',
	 'subjects': u'Social and Information Networks (cs.SI)',
	 'title': u'\nSocial diffusion and global drift in adaptive social networks',
	 'urllink': u'http://arxiv.org/abs/1406.7585'}
2015-03-23 20:29:45+0000 [xxu461000] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1405.2088> (referer: http://arxiv.org/list/cs/14?skip=5000&show=1000)
2015-03-23 20:29:45+0000 [xxu461000] DEBUG: Scraped from <200 http://arxiv.org/abs/1405.2088>
	{'abstract': u'Computer-based information systems feature in almost every aspect of our lives, and yet most of us receive handwritten prescriptions when we visit our doctors and rely on paper-based medical records in our healthcare. Although electronic health record (EHR) systems have long been promoted as a cost-effective and efficient alternative to this situation, clear-cut evidence of their success has not been forthcoming. An examination of some of the underlying problems that prevent EHR systems from delivering the benefits that their proponents tout identifies four broad objectives - reducing cost, reducing errors, improving coordination and improving adherence to standards - and shows that they are not always met. The three possible causes for this failure to deliver involve problems with the codification of knowledge, group and tacit knowledge, and coordination and communication. There is, however, reason to be optimistic that EHR systems can fulfil a healthy part, if not all, of their potential.',
	 'authors': u'Chris Kimble,',
	 'category': u'Computer Science ',
	 'date': '2014-4-29',
	 'pdflink': u'http://arxiv.org/pdf/1405.2088',
	 'subjects': u'Computers and Society (cs.CY)',
	 'title': u'\nElectronic Health Records: Cure-all or Chronic Condition?',
	 'urllink': u'http://arxiv.org/abs/1405.2088'}
2015-03-23 20:29:48+0000 [xxu461000] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1405.6397> (referer: http://arxiv.org/list/cs/14?skip=6000&show=1000)
2015-03-23 20:29:48+0000 [xxu461000] DEBUG: Scraped from <200 http://arxiv.org/abs/1405.6397>
	{'abstract': u'The wrapped normal distribution arises when a the density of a one-dimensional normal distribution is wrapped around the circle infinitely many times. At first look, evaluation of its probability density function appears tedious as an infinite series is involved. In this paper, we investigate the evaluation of two truncated series representations. As one representation performs well for small uncertainties whereas the other performs well for large uncertainties, we show that in all cases a small number of summands is sufficient to achieve high accuracy.',
	 'authors': u'Gerhard Kurz, Igor Gilitschenski, Uwe D. Hanebeck,',
	 'category': u'Computer Science ',
	 'date': '2014-5-25',
	 'pdflink': u'http://arxiv.org/pdf/1405.6397',
	 'subjects': u'Computation (stat.CO)',
	 'title': u'\nEfficient Evaluation of the Probability Density Function of a Wrapped  Normal Distribution',
	 'urllink': u'http://arxiv.org/abs/1405.6397'}
2015-03-23 20:29:48+0000 [xxu461000] INFO: Crawled 443 pages (at 13 pages/min), scraped 426 items (at 13 items/min)
2015-03-23 20:29:52+0000 [xxu461000] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1407.5035> (referer: http://arxiv.org/list/cs/14?skip=8000&show=1000)
2015-03-23 20:29:52+0000 [xxu461000] DEBUG: Scraped from <200 http://arxiv.org/abs/1407.5035>
	{'abstract': u'A major challenge in scaling object detection is the difficulty of obtaining labeled images for large numbers of categories. Recently, deep convolutional neural networks (CNNs) have emerged as clear winners on object classification benchmarks, in part due to training with 1.2M+ labeled classification images. Unfortunately, only a small fraction of those labels are available for the detection task. It is much cheaper and easier to collect large quantities of image-level labels from search engines than it is to collect detection data and label it with precise bounding boxes. In this paper, we propose Large Scale Detection through Adaptation (LSDA), an algorithm which learns the difference between the two tasks and transfers this knowledge to classifiers for categories without bounding box annotated data, turning them into detectors. Our method has the potential to enable detection for the tens of thousands of categories that lack bounding box annotations, yet have plenty of classification data. Evaluation on the ImageNet LSVRC-2013 detection challenge demonstrates the efficacy of our approach. This algorithm enables us to produce a &gt;7.6K detector by using available classification data from leaf nodes in the ImageNet tree. We additionally demonstrate how to modify our architecture to produce a fast detector (running at 2fps for the 7.6K detector). Models and software are available at',
	 'authors': u'Judy Hoffman, Sergio Guadarrama, Eric Tzeng, Ronghang Hu, Jeff Donahue, Ross Girshick, Trevor Darrell, Kate Saenko,',
	 'category': u'Computer Science ',
	 'date': '2014-7-18',
	 'pdflink': u'http://arxiv.org/pdf/1407.5035',
	 'subjects': u'Computer Vision and Pattern Recognition (cs.CV)',
	 'title': u'\nLSDA: Large Scale Detection Through Adaptation',
	 'urllink': u'http://arxiv.org/abs/1407.5035'}
2015-03-23 20:29:57+0000 [xxu461000] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1404.5155> (referer: http://arxiv.org/list/cs/14?skip=4000&show=1000)
2015-03-23 20:29:57+0000 [xxu461000] DEBUG: Scraped from <200 http://arxiv.org/abs/1404.5155>
	{'abstract': u'We study the existence of pure Nash equilibrium (PNE) for the mechanisms used in Internet services (e.g., online reviews and question-answer websites) to incentivize users to generate high-quality content. Most existing work assumes that users are homogeneous and have the same ability. However, real-world users are heterogeneous and their abilities can be very different from each other due to their diverse background, culture, and profession. In this work, we consider heterogeneous users with the following framework: (1) the users are heterogeneous and each of them has a private type indicating the best quality of the content she can generate; (2) there is a fixed amount of reward to allocate to the participated users. Under this framework, we study the existence of pure Nash equilibrium of several mechanisms composed by different allocation rules, action spaces, and information settings. We prove the existence of PNE for some mechanisms and the non-existence of PNE for some mechanisms. We also discuss how to find a PNE for those mechanisms with PNE either through a constructive way or a search algorithm.',
	 'authors': u'Yingce Xia, Tao Qin, Nenghai Yu, Tie-Yan Liu,',
	 'category': u'Computer Science ',
	 'date': '2014-4-21',
	 'pdflink': u'http://arxiv.org/pdf/1404.5155',
	 'subjects': u'Computer Science and Game Theory (cs.GT)',
	 'title': u'\nIncentivizing High-quality Content from Heterogeneous Users: On the  Existence of Nash Equilibrium',
	 'urllink': u'http://arxiv.org/abs/1404.5155'}
2015-03-23 20:30:02+0000 [xxu461000] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1409.1789> (referer: http://arxiv.org/list/cs/14?skip=10000&show=1000)
2015-03-23 20:30:02+0000 [xxu461000] DEBUG: Scraped from <200 http://arxiv.org/abs/1409.1789>
	{'abstract': u'In this work, we propose a learning framework for identifying synapses using a deep and wide multi-scale recursive (DAWMR) network, previously considered in image segmentation applications. We apply this approach on electron microscopy data from invertebrate fly brain tissue. By learning features directly from the data, we are able to achieve considerable improvements over existing techniques that rely on a small set of hand-designed features. We show that this system can reduce the amount of manual annotation required, in both acquisition of training data as well as verification of inferred detections.',
	 'authors': u'Gary B. Huang, Stephen Plaza,',
	 'category': u'Computer Science ',
	 'date': '2014-9-5',
	 'pdflink': u'http://arxiv.org/pdf/1409.1789',
	 'subjects': u'Computer Vision and Pattern Recognition (cs.CV)',
	 'title': u'\nIdentifying Synapses Using Deep and Wide Multiscale Recursive Networks',
	 'urllink': u'http://arxiv.org/abs/1409.1789'}
2015-03-23 20:30:07+0000 [xxu461000] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1406.7584> (referer: http://arxiv.org/list/cs/14?skip=7000&show=1000)
2015-03-23 20:30:07+0000 [xxu461000] DEBUG: Scraped from <200 http://arxiv.org/abs/1406.7584>
	{'abstract': u'In an online prediction market, forecasters who could not see the current state of the market until they made their own separate estimates moved their estimates closer to the market forecast when the current state of the market became known. Their first edits to the market forecast were very similar to the first edits of forecasters who could always see the current state of the market, and forecasters in both conditions had similar accuracy. These results suggest that our more elaborate forecast elicitation method might not improve forecasts and that any anchoring on the state of the market does not constitute an error in judgment.',
	 'authors': u'Kenneth Olson, Kathryn Laskey, Charles Twardy,',
	 'category': u'Computer Science ',
	 'date': '2014-6-30',
	 'pdflink': u'http://arxiv.org/pdf/1406.7584',
	 'subjects': u'Social and Information Networks (cs.SI)',
	 'title': u'\nInterval Elicitation of Forecasts in a Prediction Market Reveals Lack of  Anchoring "Bias"',
	 'urllink': u'http://arxiv.org/abs/1406.7584'}
2015-03-23 20:30:13+0000 [xxu461000] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1405.2066> (referer: http://arxiv.org/list/cs/14?skip=5000&show=1000)
2015-03-23 20:30:13+0000 [xxu461000] DEBUG: Scraped from <200 http://arxiv.org/abs/1405.2066>
	{'abstract': u'Improving modularity and reusability are two key objectives in object-oriented programming. These objectives are achieved by applying several key concepts, such as data encapsulation and inheritance. A class in an object-oriented system is the basic unit of design. Assessing the quality of an object-oriented class may require flattening the class and representing it as it really is, including all accessible inherited class members. Thus, class flattening helps in exploring the impact of inheritance on improving code quality. This paper explains how to flatten Java classes and discusses the relationship between class flattening and some applications of interest to software practitioners, such as refactoring and indicating external quality attributes.',
	 'authors': u'Jehad Al Dallal,',
	 'category': u'Computer Science ',
	 'date': '2014-5-8',
	 'pdflink': u'http://arxiv.org/pdf/1405.2066',
	 'subjects': u'Software Engineering (cs.SE)',
	 'title': u'\nHow and When to Flatten Java Classes?',
	 'urllink': u'http://arxiv.org/abs/1405.2066'}
2015-03-23 20:30:19+0000 [xxu461000] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1405.6334> (referer: http://arxiv.org/list/cs/14?skip=6000&show=1000)
2015-03-23 20:30:19+0000 [xxu461000] DEBUG: Scraped from <200 http://arxiv.org/abs/1405.6334>
	{'abstract': u"Student difficulties in learning Physics have been thoroughly discussed in the scientific literature. Already in 1980, Papert complained that schools teach Newtonian motion by manipulating equations rather than by manipulating the Newtonian objects themselves, what would be possible in a 'physics microworld'. On the other hand, Second Life and its scripting language have a remarkable learning curve that discourages most teachers at using it as an environment for educational computer simulations and microworlds. The objective of this work is to describe TATI, a textual interface which, through TATILogo, an accessible Logo language extension, allows the generation of various physics microworlds in Second Life, containing different types of objects that follow different physical laws, providing a learning path into Newtonian Physics.",
	 'authors': u'Renato P. dos Santos,',
	 'category': u'Computer Science ',
	 'date': '2014-5-24',
	 'pdflink': u'http://arxiv.org/pdf/1405.6334',
	 'subjects': u'Physics Education (physics.ed-ph)',
	 'title': u'\nTATI -- A Logo-like interface for microworlds and simulations for  physics teaching in Second Life',
	 'urllink': u'http://arxiv.org/abs/1405.6334'}
2015-03-23 20:30:23+0000 [xxu461000] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1407.5030> (referer: http://arxiv.org/list/cs/14?skip=8000&show=1000)
2015-03-23 20:30:23+0000 [xxu461000] DEBUG: Scraped from <200 http://arxiv.org/abs/1407.5030>
	{'abstract': u'Quantitative games are two-player zero-sum games played on directed weighted graphs. Total-payoff games (that can be seen as a refinement of the well-studied mean-payoff games) are the variant where the payoff of a play is computed as the sum of the weights. Our aim is to describe the first pseudo-polynomial time algorithm for total-payoff games in the presence of arbitrary weights. It consists of a non-trivial application of the value iteration paradigm. Indeed, it requires to study, as a milestone, a refinement of these games, called min-cost reachability games, where we add a reachability objective to one of the players. For these games, we give an efficient value iteration algorithm to compute the values and optimal strategies (when they exist), that runs in pseudo-polynomial time. We also propose heuristics allowing one to possibly speed up the computations in both cases.',
	 'authors': u'Thomas Brihaye, Gilles Geeraerts, Axel Haddad, Benjamin Monmege,',
	 'category': u'Computer Science ',
	 'date': '2014-7-18',
	 'pdflink': u'http://arxiv.org/pdf/1407.5030',
	 'subjects': u'Computer Science and Game Theory (cs.GT)',
	 'title': u'\nTo Reach or not to Reach? Efficient Algorithms for Total-Payoff Games',
	 'urllink': u'http://arxiv.org/abs/1407.5030'}
2015-03-23 20:30:27+0000 [xxu461000] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1404.5144> (referer: http://arxiv.org/list/cs/14?skip=4000&show=1000)
2015-03-23 20:30:27+0000 [xxu461000] DEBUG: Scraped from <200 http://arxiv.org/abs/1404.5144>
	{'abstract': u'A method that allows us to give a different treatment to any neuron inside feedforward neural networks is presented. The algorithm has been implemented with two very different learning methods: a standard Back-propagation (BP) procedure and an evolutionary algorithm. First, we have demonstrated that the EA training method converges faster and gives more accurate results than BP. Then we have made a full analysis of the effects of turning off different combinations of neurons after the training phase. We demonstrate that EA is much more robust than BP for all the cases under study. Even in the case when two hidden neurons are lost, EA training is still able to give good average results. This difference implies that we must be very careful when pruning or redundancy effects are being studied since the network performance when losing neurons strongly depends on the training method. Moreover, the influence of the individual inputs will also depend on the training algorithm. Since EA keeps a good classification performance when units are lost, this method could be a good way to simulate biological learning systems since they must be robust against deficient neuron performance. Although biological systems are much more complex than the simulations shown in this article, we propose that a smart training strategy such as the one shown here could be considered as a first protection against the losing of a certain number of neurons.',
	 'authors': u'M. Konomi, G. M. Sacha,',
	 'category': u'Computer Science ',
	 'date': '2014-4-21',
	 'pdflink': u'http://arxiv.org/pdf/1404.5144',
	 'subjects': u'Neural and Evolutionary Computing (cs.NE)',
	 'title': u'\nInfluence of the learning method in the performance of feedforward  neural networks when the activity of neurons is modified',
	 'urllink': u'http://arxiv.org/abs/1404.5144'}
2015-03-23 20:30:33+0000 [xxu461000] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1409.1786> (referer: http://arxiv.org/list/cs/14?skip=10000&show=1000)
2015-03-23 20:30:33+0000 [xxu461000] DEBUG: Scraped from <200 http://arxiv.org/abs/1409.1786>
	{'abstract': u"Self-serving, rational agents sometimes cooperate to their mutual benefit. The two-player iterated prisoner's dilemma game is a model for including the emergence of cooperation. It is generally believed that there is no simple ultimatum strategy which a player can control the return of the other participants. The recent discovery of the powerful class of zero-determinant strategies in the iterated prisoner's dilemma dramatically expands our understanding of the classic game by uncovering strategies that provide a unilateral advantage to sentient players pitted against unwitting opponents. However, strategies in the prisoner's dilemma game are only two strategies. Are there these results for general multi-strategy games? To address this question, the paper develops a theory for zero-determinant strategies for multi-strategy games, with any number of strategies. The analytical results exhibit a similar yet different scenario to the case of two-strategy games. Zero-determinant strategies in iterated prisoner's dilemma can be seen as degenerate case of our results. The results are also applied to the snowdrift game, the hawk-dove game and the chicken game.",
	 'authors': u'Jin-Li Guo,',
	 'category': u'Computer Science ',
	 'date': '2014-9-5',
	 'pdflink': u'http://arxiv.org/pdf/1409.1786',
	 'subjects': u'Computer Science and Game Theory (cs.GT)',
	 'title': u'\nZero-determinant strategies in iterated multi-strategy games',
	 'urllink': u'http://arxiv.org/abs/1409.1786'}
2015-03-23 20:30:38+0000 [xxu461000] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1406.7583> (referer: http://arxiv.org/list/cs/14?skip=7000&show=1000)
2015-03-23 20:30:38+0000 [xxu461000] DEBUG: Scraped from <200 http://arxiv.org/abs/1406.7583>
	{'abstract': u"Prediction markets have demonstrated their value for aggregating collective expertise. Combinatorial prediction markets allow forecasts not only on base events, but also on conditional and/or Boolean combinations of events. We describe a trade-based combinatorial prediction market asset management system, called Dynamic Asset Cluster (DAC), that improves both time and space efficiency over the method of, which maintains parallel junction trees for assets and probabilities. The basic data structure is the asset block, which compactly represents a set of trades made by a user. A user's asset model consists of a set of asset blocks representing the user's entire trade history. A junction tree is created dynamically from the asset blocks to compute a user's minimum and expected assets.",
	 'authors': u'Wei Sun, Kathryn Laskey, Charles Twardy, Robin Hanson, Brandon Goldfedder,',
	 'category': u'Computer Science ',
	 'date': '2014-6-30',
	 'pdflink': u'http://arxiv.org/pdf/1406.7583',
	 'subjects': u'Computer Science and Game Theory (cs.GT)',
	 'title': u'\nTrade-based Asset Model using Dynamic Junction Tree for Combinatorial  Prediction Markets',
	 'urllink': u'http://arxiv.org/abs/1406.7583'}
2015-03-23 20:30:43+0000 [xxu461000] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1405.2063> (referer: http://arxiv.org/list/cs/14?skip=5000&show=1000)
2015-03-23 20:30:43+0000 [xxu461000] DEBUG: Scraped from <200 http://arxiv.org/abs/1405.2063>
	{'abstract': u'The development of 3-dimensional environments to be used within a biomechanical physics simulation framework, such as Articulated Total Body, can be laborious and time intensive. This brief article demonstrates how the ARAS 360 software package can aid the user by speeding up development time.',
	 'authors': u'Bob J. Scurlock,',
	 'category': u'Computer Science ',
	 'date': '2014-4-17',
	 'pdflink': u'http://arxiv.org/pdf/1405.2063',
	 'subjects': u'Other Computer Science (cs.OH)',
	 'title': u'\nUse of ARAS 360 to Facilitate Rapid Development of Articulated Total  Body Biomechanical Physics Simulations',
	 'urllink': u'http://arxiv.org/abs/1405.2063'}
2015-03-23 20:30:48+0000 [xxu461000] INFO: Crawled 454 pages (at 11 pages/min), scraped 437 items (at 11 items/min)
2015-03-23 20:30:49+0000 [xxu461000] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1405.6328> (referer: http://arxiv.org/list/cs/14?skip=6000&show=1000)
2015-03-23 20:30:49+0000 [xxu461000] DEBUG: Scraped from <200 http://arxiv.org/abs/1405.6328>
	{'abstract': u"We live in a digital world that, in 2010, crossed the mark of one zettabyte data. This huge amount of data processed on computers extremely fast with optimized techniques allows one to find insights in new and emerging types of data and content and to answer questions that were previously considered beyond reach. This is the idea of Big Data. Google now offers the Google Correlate analysis public tool that, from a search term or a series of temporal or regional data, provides a list of queries on Google whose frequencies follow patterns that best correlate with the data, according to the Pearson determination coefficient R2. Of course, correlation does not imply causation. We believe, however, that there is potential for these big data tools to find unexpected correlations that may serve as clues to interesting phenomena, from the pedagogical and even scientific point of view. As far as we know, this is the first proposal for the use of Big Data in Science Teaching, of constructionist character, taking as mediators the computer and the public and free tools such as Google Correlate. It also has an epistemological bias, not being merely a training in computational infrastructure or predictive analytics, but aiming at providing students a better understanding of physical concepts, such as phenomena, observation, measurement, physical laws, theory, and causality. With it, they would be able to become good Big Data specialists, the so needed 'data scientists' to solve the challenges of Big Data.",
	 'authors': u'Renato P. dos Santos,',
	 'category': u'Computer Science ',
	 'date': '2014-5-24',
	 'pdflink': u'http://arxiv.org/pdf/1405.6328',
	 'subjects': u'Physics Education (physics.ed-ph)',
	 'title': u'\nBig Data as a Mediator in Science Teaching: A Proposal',
	 'urllink': u'http://arxiv.org/abs/1405.6328'}
2015-03-23 20:30:54+0000 [xxu461000] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1407.5011> (referer: http://arxiv.org/list/cs/14?skip=8000&show=1000)
2015-03-23 20:30:54+0000 [xxu461000] DEBUG: Scraped from <200 http://arxiv.org/abs/1407.5011>
	{'abstract': u"We consider ancestry labeling schemes: Given a rooted tree T, assign a binary string (label) to each node, such that given the labels of any two nodes, one can determine whether the first is an ancestor of the second in T. Recently, Fraigniaud and Korman [STOC'10] showed that such labels can be assigned using log n + O(loglog n) bits per label, solving a long standing open question and matching a lower bound of log n + Omega(loglog n) due to Alstrup et al. [SODA'03]. In this paper we present an alternative ancestry labeling scheme using log n + 2loglog n + O(1) bits. Similar to the previously known schemes, our scheme relies on intervals and can encode any tree in O(n) time. Rather than using complicated decompositions, our scheme uses approximate interval sizes, which gives a clean algorithm that is simple enough to be taught and implemented.",
	 'authors': u'S\xf8ren Dahlgaard, Mathias B\xe6k Tejs Knudsen, Noy Rotbart,',
	 'category': u'Computer Science ',
	 'date': '2014-7-18',
	 'pdflink': u'http://arxiv.org/pdf/1407.5011',
	 'subjects': u'Data Structures and Algorithms (cs.DS)',
	 'title': u'\nImproved ancestry labeling scheme for trees',
	 'urllink': u'http://arxiv.org/abs/1407.5011'}
2015-03-23 20:30:58+0000 [xxu461000] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1404.5127> (referer: http://arxiv.org/list/cs/14?skip=4000&show=1000)
2015-03-23 20:30:58+0000 [xxu461000] DEBUG: Scraped from <200 http://arxiv.org/abs/1404.5127>
	{'abstract': u'We examine trade-offs among stakeholders in ad auctions. Our metrics are the revenue for the utility of the auctioneer, the number of clicks for the utility of the users and the welfare for the utility of the advertisers. We show how to optimize linear combinations of the stakeholder utilities, showing that these can be tackled through a GSP auction with a per-click reserve price. We then examine constrained optimization of stakeholder utilities. We use simulations and analysis of real-world sponsored search auction data to demonstrate the feasible trade-offs, examining the effect of changing the allowed number of ads on the utilities of the stakeholders. We investigate both short term effects, when the players do not have the time to modify their behavior, and long term equilibrium conditions. Finally, we examine a combinatorially richer constrained optimization problem, where there are several possible allowed configurations (templates) of ad formats. This model captures richer ad formats, which allow using the available screen real estate in various ways. We show that two natural generalizations of the GSP auction rules to this domain are poorly behaved, resulting in not having a symmetric Nash equilibrium or having one with poor welfare. We also provide positive results for restricted cases.',
	 'authors': u'Yoram Bachrach, Sofia Ceppi, Ian A. Kash, Peter Key, David Kurokawa,',
	 'category': u'Computer Science ',
	 'date': '2014-4-21',
	 'pdflink': u'http://arxiv.org/pdf/1404.5127',
	 'subjects': u'Computer Science and Game Theory (cs.GT)',
	 'title': u'\nOptimising Trade-offs Among Stakeholders in Ad Auctions',
	 'urllink': u'http://arxiv.org/abs/1404.5127'}
2015-03-23 20:31:04+0000 [xxu461000] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1409.1781> (referer: http://arxiv.org/list/cs/14?skip=10000&show=1000)
2015-03-23 20:31:04+0000 [xxu461000] DEBUG: Scraped from <200 http://arxiv.org/abs/1409.1781>
	{'abstract': u'We investigate the distribution of cells by dimension in cylindrical algebraic decompositions (CADs). We find that they follow a standard distribution which seems largely independent of the underlying problem or CAD algorithm used. Rather, the distribution is inherent to the cylindrical structure and determined mostly by the number of variables. This insight is then combined with an algorithm that produces only full-dimensional cells to give an accurate method of predicting the number of cells in a complete CAD. Since constructing only full-dimensional cells is relatively inexpensive (involving no costly algebraic number calculations) this leads to heuristics for helping with various questions of problem formulation for CAD, such as choosing an optimal variable ordering. Our experiments demonstrate that this approach can be highly effective.',
	 'authors': u'David Wilson, Matthew England, Russell Bradford, James H. Davenport,',
	 'category': u'Computer Science ',
	 'date': '2014-9-5',
	 'pdflink': u'http://arxiv.org/pdf/1409.1781',
	 'subjects': u'Symbolic Computation (cs.SC)',
	 'title': u'\nUsing the distribution of cells by dimension in a cylindrical algebraic  decomposition',
	 'urllink': u'http://arxiv.org/abs/1409.1781'}
2015-03-23 20:31:07+0000 [xxu461000] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1406.7582> (referer: http://arxiv.org/list/cs/14?skip=7000&show=1000)
2015-03-23 20:31:07+0000 [xxu461000] DEBUG: Scraped from <200 http://arxiv.org/abs/1406.7582>
	{'abstract': u'The goal of our research is to understand how ideas propagate, combine and are created in large social networks. In this work, we look at a sample of relevant scientific publications in the area of high-frequency analog circuit design and their citation distribution. A novel aspect of our work is the way in which we categorize citations based on the reason and place of it in a publication. We created seven citation categories from general domain references, references to specific methods used in the same domain problem, references to an analysis method, references for experimental comparison and so on. This added information allows us to define two new measures to characterize the creativity (novelty and usefulness) of a publication based on its pattern of citations clustered by reason, place and citing scientific group. We analyzed 30 publications in relevant journals since 2000 and their about 300 citations, all in the area of high-frequency analog circuit design. We observed that the number of citations a publication receives from different scientific groups matches a Levy type distribution: with a large number of groups citing a publication relatively few times, and a very small number of groups citing a publication a large number of times. We looked at the motifs a publication is cited differently by different scientific groups.',
	 'authors': u'Simona Doboli, Fanshu Zhao, Alex Doboli,',
	 'category': u'Computer Science ',
	 'date': '2014-6-30',
	 'pdflink': u'http://arxiv.org/pdf/1406.7582',
	 'subjects': u'Social and Information Networks (cs.SI)',
	 'title': u'\nNew measures for evaluating creativity in scientific publications',
	 'urllink': u'http://arxiv.org/abs/1406.7582'}
2015-03-23 20:31:12+0000 [xxu461000] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1405.2062> (referer: http://arxiv.org/list/cs/14?skip=5000&show=1000)
2015-03-23 20:31:12+0000 [xxu461000] DEBUG: Scraped from <200 http://arxiv.org/abs/1405.2062>
	{'abstract': u'In texture-plus-depth representation of a 3D scene, depth maps from different camera viewpoints are typically lossily compressed via the classical transform coding / coefficient quantization paradigm. In this paper we propose to reduce distortion of the decoded depth maps due to quantization. The key observation is that depth maps from different viewpoints constitute multiple descriptions (MD) of the same 3D scene. Considering the MD jointly, we perform a POCS-like iterative procedure to project a reconstructed signal from one depth map to the other and back, so that the converged depth maps have higher precision than the original quantized versions.',
	 'authors': u'Pengfei Wan, Gene Cheung, Philip A. Chou, Dinei Florencio, Cha Zhang, Oscar C. Au,',
	 'category': u'Computer Science ',
	 'date': '2014-2-25',
	 'pdflink': u'http://arxiv.org/pdf/1405.2062',
	 'subjects': u'Computer Vision and Pattern Recognition (cs.CV)',
	 'title': u'\nPrecision Enhancement of 3D Surfaces from Multiple Compressed Depth Maps',
	 'urllink': u'http://arxiv.org/abs/1405.2062'}
2015-03-23 20:31:15+0000 [xxu461000] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1405.6327> (referer: http://arxiv.org/list/cs/14?skip=6000&show=1000)
2015-03-23 20:31:15+0000 [xxu461000] DEBUG: Scraped from <200 http://arxiv.org/abs/1405.6327>
	{'abstract': u"Science teaching detached itself from reality and became restricted to the classrooms and textbooks with their overreliance on standardized and repetitive exercises, while students keep their own alternative conceptions. Papert, displeased with this inefficient learning process, championed physics microworlds, where students could experience a variety of laws of motion, from Aristotle to Newton and Einstein or even new laws invented by the students themselves. While often mistakenly seen as a game, Second Life (SL), the online 3-D virtual world hosted by Linden Lab, imposes essentially no rules on the residents beyond reasonable restrictions on improper behavior and the physical rules that guarantee its similitude to the real world. As a consequence, SL qualifies itself as an environment for personal discovery and exploration as proposed by constructivist theories. The physical laws are implemented through the well-known physics engine Havok, whose design aims to provide game-players a consistent, realistic environment. The Havok User Guide (2008) explicitly encourages developers to use several tricks to cheat the simulator in order to make games funnier or easier to play. As it is shown in this study, SL physics is unexpectedly neither the Newtonian idealized physics nor a real world physics virtualization, intentionally diverging from reality in such a way that it could be called hyper-real. As a matter of fact, if some of its features make objects behave more realistically than real ones, certain quantities like energy have a totally different meaning in SL as compared to physics. Far from considering it as a problem, however, the author argues that its hyper-reality may be a golden teaching opportunity, allowing surreal physics simulations and epistemologically rich classroom discussions around the what is a physical law? issue, in accordance with Papert's never-implemented proposal.",
	 'authors': u'Renato P. dos Santos,',
	 'category': u'Computer Science ',
	 'date': '2014-5-24',
	 'pdflink': u'http://arxiv.org/pdf/1405.6327',
	 'subjects': u'Physics Education (physics.ed-ph)',
	 'title': u'\nSecond Life Physics: Virtual, real, or surreal?',
	 'urllink': u'http://arxiv.org/abs/1405.6327'}
2015-03-23 20:31:21+0000 [xxu461000] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1407.5008> (referer: http://arxiv.org/list/cs/14?skip=8000&show=1000)
2015-03-23 20:31:21+0000 [xxu461000] DEBUG: Scraped from <200 http://arxiv.org/abs/1407.5008>
	{'abstract': u'Under normal circumstances, as an intermediate device, if we want to move or copy data from one mass storage device to another, we use a computer in the form of desktops, laptops, etc. We need a device which can be used as an intermediate device, also which is a complete blend of hardware &amp; software. This device is a gadget that can be used to transfer data between two flash SCSI devices via a touch screen. This is a user friendly device which uses the most popular bus USB (Universal Serial Bus) with Type-A connector. It is governed by the USB 2.0 Protocol. One of the major advantage of this device is its portability.',
	 'authors': u'Anurag A. Chakravorty, Raghwendra J. Suryawanshi,',
	 'category': u'Computer Science ',
	 'date': '2014-7-17',
	 'pdflink': u'http://arxiv.org/pdf/1407.5008',
	 'subjects': u'Other Computer Science (cs.OH)',
	 'title': u'\nData Transfer between Two USB Flash SCSI Disks using a Touch Screen',
	 'urllink': u'http://arxiv.org/abs/1407.5008'}
2015-03-23 20:31:25+0000 [xxu461000] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1404.5123> (referer: http://arxiv.org/list/cs/14?skip=4000&show=1000)
2015-03-23 20:31:25+0000 [xxu461000] DEBUG: Scraped from <200 http://arxiv.org/abs/1404.5123>
	{'abstract': u'P2P networking has become a promising technology and has achieved popularity as a mechanism for users to share files without the need for centralized servers. The rapid growth of P2P networks beginning with Kaza, Lime wire, Napsters, E-donkey, Gnutella etc makes them an attractive target to the creators of viruses and other security threats. This paper describes the major security issues on P2P networks (Viruses and worms) and presents the study of propagation mechanisms. In particular, the paper explores different P2P viruses and worms, their propagation methodology, outlines the challenges, and evaluates how P2P worms affect the network. The experimental results obtained will provide new direction in surmounting the security concerns in P2P Networks',
	 'authors': u'Mansoor Ebrahim, Shujaat Khan, UmerBin Khalid,',
	 'category': u'Computer Science ',
	 'date': '2014-4-21',
	 'pdflink': u'http://arxiv.org/pdf/1404.5123',
	 'subjects': u'Cryptography and Security (cs.CR)',
	 'title': u'\nSecurity Risk Analysis in Peer 2 Peer System; An Approach towards  Surmounting Security Challenges',
	 'urllink': u'http://arxiv.org/abs/1404.5123'}
2015-03-23 20:31:31+0000 [xxu461000] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1409.1752> (referer: http://arxiv.org/list/cs/14?skip=10000&show=1000)
2015-03-23 20:31:31+0000 [xxu461000] DEBUG: Scraped from <200 http://arxiv.org/abs/1409.1752>
	{'abstract': u'We use recent results on the Fourier analysis of the zero sets of Brownian motion to explore the diophantine properties of an algorithmically random Brownian motion (also known as a complex oscillation). We discuss the construction and definability of perfect sets which are linearly independent over the rationals directly from Martin-L "of random reals. Finally we explore the recent work of Tsirelson on countable dense sets to study the diophantine properties of local minimisers of Brownian motion.',
	 'authors': u'Willem L. Fouche,',
	 'category': u'Computer Science ',
	 'date': '2014-9-3',
	 'pdflink': u'http://arxiv.org/pdf/1409.1752',
	 'subjects': u'Logic in Computer Science (cs.LO)',
	 'title': u'\nDiophantine properties of Brownian motion: recursive aspects',
	 'urllink': u'http://arxiv.org/abs/1409.1752'}
2015-03-23 20:31:37+0000 [xxu461000] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1406.7581> (referer: http://arxiv.org/list/cs/14?skip=7000&show=1000)
2015-03-23 20:31:37+0000 [xxu461000] DEBUG: Scraped from <200 http://arxiv.org/abs/1406.7581>
	{'abstract': u'The only acceptable form of polling in the multi-billion dollar survey research field utilizes representative samples. We argue that with proper statistical adjustment, non-representative polling can provide accurate predictions, and often in a much more timely and cost-effective fashion. We demonstrate this by applying multilevel regression and post-stratification (MRP) to a 2012 election survey on the Xbox gaming platform. Not only do the transformed top-line projections from this data closely trend standard indicators, but we use the unique nature of the data\'s size and panel to answer a meaningful political puzzle. We find that reported swings in public opinion polls are generally not due to actual shifts in vote intention, but rather are the result of temporary periods of relatively low response rates among supporters of the reportedly slumping candidate. This work shows great promise for using non-representative polling to measure public opinion and the first product of this new polling technique raises the possibility that decades of large, reported swings in public opinion-including the perennial "convention bounce"-are mostly artifacts of sampling bias.',
	 'authors': u'David Rothschild, Sharad Goel, Andrew Gelman, Doug Rivers,',
	 'category': u'Computer Science ',
	 'date': '2014-6-30',
	 'pdflink': u'http://arxiv.org/pdf/1406.7581',
	 'subjects': u'Social and Information Networks (cs.SI)',
	 'title': u'\nThe Mythical Swing Voter',
	 'urllink': u'http://arxiv.org/abs/1406.7581'}
2015-03-23 20:31:42+0000 [xxu461000] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1405.2061> (referer: http://arxiv.org/list/cs/14?skip=5000&show=1000)
2015-03-23 20:31:42+0000 [xxu461000] DEBUG: Scraped from <200 http://arxiv.org/abs/1405.2061>
	{'abstract': u'Shannon\'s metric of "Entropy" of information is a foundational concept of information theory. This article is a primer for novices that presents an intuitive way of understanding, remembering, and/or reconstructing Shannon\'s Entropy metric for information.',
	 'authors': u'Sriram Vajapeyam,',
	 'category': u'Computer Science ',
	 'date': '2014-3-24',
	 'pdflink': u'http://arxiv.org/pdf/1405.2061',
	 'subjects': u'Information Theory (cs.IT)',
	 'title': u"\nUnderstanding Shannon's Entropy metric for Information",
	 'urllink': u'http://arxiv.org/abs/1405.2061'}
2015-03-23 20:31:48+0000 [xxu461000] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1405.6317> (referer: http://arxiv.org/list/cs/14?skip=6000&show=1000)
2015-03-23 20:31:48+0000 [xxu461000] DEBUG: Scraped from <200 http://arxiv.org/abs/1405.6317>
	{'abstract': u'Using Heijenoort\'s unpublished generalized rules of quantification, we discuss the proof of Herbrand\'s Fundamental Theorem in the form of Heijenoort\'s correction of Herbrand\'s "False Lemma" and present a didactic example. Although we are mainly concerned with the inner structure of Herbrand\'s Fundamental Theorem and the questions of its quality and its depth, we also discuss the outer questions of its historical context and why Bernays called it "the central theorem of predicate logic" and considered the form of its expression to be "concise and felicitous".',
	 'authors': u'Claus-Peter Wirth,',
	 'category': u'Computer Science ',
	 'date': '2014-5-24',
	 'pdflink': u'http://arxiv.org/pdf/1405.6317',
	 'subjects': u'Logic (math.LO)',
	 'title': u"\nHerbrand's Fundamental Theorem: The Historical Facts and their  Streamlining",
	 'urllink': u'http://arxiv.org/abs/1405.6317'}
2015-03-23 20:31:48+0000 [xxu461000] INFO: Crawled 467 pages (at 13 pages/min), scraped 450 items (at 13 items/min)
2015-03-23 20:31:53+0000 [xxu461000] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1407.4999> (referer: http://arxiv.org/list/cs/14?skip=8000&show=1000)
2015-03-23 20:31:53+0000 [xxu461000] DEBUG: Scraped from <200 http://arxiv.org/abs/1407.4999>
	{'abstract': u'In this paper we fix 7 types of undirected graphs: paths, paths with prescribed endvertices, circuits, forests, spanning trees, (not necessarily spanning) trees and cuts. Given an undirected graph and two "object types" and chosen from the alternatives above, we consider the following questions. textbf can we find an object of type and one of type in the edge set of , so that they are edge-disjoint? textbf can we partition into an object of type and one of type ? textbf can we cover with an object of type , and an object of type ? This framework includes 44 natural graph theoretic questions. Some of these problems were well-known before, for example covering the edge-set of a graph with two spanning trees, or finding an - path and an - path that are edge-disjoint. However, many others were not, for example can we find an - path and a spanning tree that are edge-disjoint? Most of these previously unknown problems turned out to be NP-complete, many of them even in planar graphs. This paper determines the status of these 44 problems. For the NP-complete problems we also investigate the planar version, for the polynomial problems we consider the matroidal generalization (wherever this makes sense).',
	 'authors': u'Attila Bern\xe1th, Zolt\xe1n Kir\xe1ly,',
	 'category': u'Computer Science ',
	 'date': '2014-7-18',
	 'pdflink': u'http://arxiv.org/pdf/1407.4999',
	 'subjects': u'Computational Complexity (cs.CC)',
	 'title': u'\nOn the tractability of some natural packing, covering and partitioning  problems',
	 'urllink': u'http://arxiv.org/abs/1407.4999'}
2015-03-23 20:31:59+0000 [xxu461000] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1404.5122> (referer: http://arxiv.org/list/cs/14?skip=4000&show=1000)
2015-03-23 20:31:59+0000 [xxu461000] DEBUG: Scraped from <200 http://arxiv.org/abs/1404.5122>
	{'abstract': u"Energy consumption is an important issue in continuous wireless telemonitoring of physiological signals. Compressed sensing (CS) is a promising framework to address it, due to its energy-efficient data compression procedure. However, most CS algorithms have difficulty in data recovery due to non-sparsity characteristic of many physiological signals. Block sparse Bayesian learning (BSBL) is an effective approach to recover such signals with satisfactory recovery quality. However, it is time-consuming in recovering multichannel signals, since its computational load almost linearly increases with the number of channels. This work proposes a spatiotemporal sparse Bayesian learning algorithm to recover multichannel signals simultaneously. It not only exploits temporal correlation within each channel signal, but also exploits inter-channel correlation among different channel signals. Furthermore, its computational load is not significantly affected by the number of channels. The proposed algorithm was applied to brain computer interface (BCI) and EEG-based driver's drowsiness estimation. Results showed that the algorithm had both better recovery performance and much higher speed than BSBL. Particularly, the proposed algorithm ensured that the BCI classification and the drowsiness estimation had little degradation even when data were compressed by 80%, making it very suitable for continuous wireless telemonitoring of multichannel signals.",
	 'authors': u'Zhilin Zhang, Tzyy-Ping Jung, Scott Makeig, Zhouyue Pi, Bhaskar D. Rao,',
	 'category': u'Computer Science ',
	 'date': '2014-4-21',
	 'pdflink': u'http://arxiv.org/pdf/1404.5122',
	 'subjects': u'Information Theory (cs.IT)',
	 'title': u'\nSpatiotemporal Sparse Bayesian Learning with Applications to Compressed  Sensing of Multichannel Physiological Signals',
	 'urllink': u'http://arxiv.org/abs/1404.5122'}
2015-03-23 20:32:03+0000 [xxu461000] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1409.1749> (referer: http://arxiv.org/list/cs/14?skip=10000&show=1000)
2015-03-23 20:32:03+0000 [xxu461000] DEBUG: Scraped from <200 http://arxiv.org/abs/1409.1749>
	{'abstract': u"We consider the problem of merging two sorted sequences on a comparator network that is used repeatedly, that is, if the output is not sorted, the network is applied again using the output as input. The challenging task is to construct such networks of small depth (called a period in this context). In our previous paper entitled Faster 3-Periodic Merging Network we reduced twice the time of merging on -periodic networks, i.e. from to , compared to the first construction given by Kutyowski, Lory 's and Oesterdikhoff. Note that merging on -periodic networks require linear time. In this paper we extend our construction, which is based on Canfield and Williamson -periodic sorter, and the analysis from that paper to any period . For our -periodic network merges two sorted sequences of length in at most rounds. The previous bound given by Kutyowski at al. was . That means, for example, that our -periodic merging networks work in time upper-bounded by and our -periodic ones in time upper-bounded by compared to the corresponding and previous bounds. Our construction is regular and follows the same periodification schema, whereas some additional techniques were used previously to tune the construction for . Moreover, our networks are also periodic sorters and tests on random permutations show that average sorting time is closed to .",
	 'authors': u'Marek Piotr\xf3w,',
	 'category': u'Computer Science ',
	 'date': '2014-9-5',
	 'pdflink': u'http://arxiv.org/pdf/1409.1749',
	 'subjects': u'Data Structures and Algorithms (cs.DS)',
	 'title': u'\nFaster Small-Constant-Periodic Merging Networks',
	 'urllink': u'http://arxiv.org/abs/1409.1749'}
2015-03-23 20:32:09+0000 [xxu461000] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1406.7579> (referer: http://arxiv.org/list/cs/14?skip=7000&show=1000)
2015-03-23 20:32:09+0000 [xxu461000] DEBUG: Scraped from <200 http://arxiv.org/abs/1406.7579>
	{'abstract': u"The propagation of online memes is initially influenced by meme creators and secondarily by meme consumers, whose individual sharing decisions accumulate to determine total meme propagation. We characterize this as a sender/receiver sequence in which the first sender is also the creator. This sequence consists of two distinct processes, the creation process and the sharing process. We investigated these processes separately to determine their individual influence on sharing outcomes. Our study observed participants creating memes in the lab. We then tracked the sharing of those memes, derived a model of sharing behavior, and implemented our sharing model in a contagion simulation. Although we assume meme consumers typically have little or no information about a meme's creator when making a decision about whether to share a meme (and vice versa), we nevertheless ask whether consumer re-sharing behavior can be predicted based on features of the creator. Using human participants, web log monitoring, and statistical model fitting, the resulting Creator Model of Re-sharing Behavior predicts 11.5% of the variance in the behavior of consumers. Even when we know nothing about re-sharers of a meme, we can predict something about their behavior by observing the creation process. To investigate the individual re-sharing decisions that, together, constitute a meme's total consumer response, we built a statistical model from human observation. Receivers make their decision to share as a function of the meme's content and their reaction to it, which we model as a consumer's decision to share. The resulting Consumer Model of Sharing Decisions describes 37.5% of the variance in this decision making process.",
	 'authors': u'Ian Miller, Gerald Cupchik,',
	 'category': u'Computer Science ',
	 'date': '2014-6-30',
	 'pdflink': u'http://arxiv.org/pdf/1406.7579',
	 'subjects': u'Social and Information Networks (cs.SI)',
	 'title': u'\nMeme creation and sharing processes: individuals shaping the masses',
	 'urllink': u'http://arxiv.org/abs/1406.7579'}
2015-03-23 20:32:13+0000 [xxu461000] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1405.2058> (referer: http://arxiv.org/list/cs/14?skip=5000&show=1000)
2015-03-23 20:32:13+0000 [xxu461000] DEBUG: Scraped from <200 http://arxiv.org/abs/1405.2058>
	{'abstract': u'Abductive logic programs offer a formalism to declaratively represent and reason about problems in a variety of areas: diagnosis, decision making, hypothetical reasoning, etc. On the other hand, logic program updates allow us to express knowledge changes, be they internal (or self) and external (or world) changes. Abductive logic programs and logic program updates thus naturally coexist in problems that are susceptible to hypothetical reasoning about change. Taking this as a motivation, in this paper we integrate abductive logic programs and logic program updates by jointly exploiting tabling features of logic programming. The integration is based on and benefits from the two implementation techniques we separately devised previously, viz., tabled abduction and incremental tabling for query-driven propagation of logic program updates. A prototype of the integrated system is implemented in XSB Prolog.',
	 'authors': u'Ari Saptawijaya, Lu\xeds Moniz Pereira,',
	 'category': u'Computer Science ',
	 'date': '2014-5-8',
	 'pdflink': u'http://arxiv.org/pdf/1405.2058',
	 'subjects': u'Artificial Intelligence (cs.AI)',
	 'title': u'\nJoint Tabling of Logic Program Abductions and Updates',
	 'urllink': u'http://arxiv.org/abs/1405.2058'}
2015-03-23 20:32:18+0000 [xxu461000] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1405.6296> (referer: http://arxiv.org/list/cs/14?skip=6000&show=1000)
2015-03-23 20:32:18+0000 [xxu461000] DEBUG: Scraped from <200 http://arxiv.org/abs/1405.6296>
	{'abstract': u'We studied the roles of morphogenetic principles---heterogeneity of components, dynamic differentiation/re-differentiation of components, and local information sharing among components---in the self-organization of morphogenetic collective systems. By incrementally introducing these principles to collectives, we defined four distinct classes of morphogenetic collective systems. Monte Carlo simulations were conducted using an extended version of the Swarm Chemistry model that was equipped with dynamic differentiation/re-differentiation and local information sharing capabilities. Self-organization of swarms was characterized by several kinetic and topological measurements, the latter of which were facilitated by a newly developed network-based method. Results of simulations revealed that, while heterogeneity of components had a strong impact on the structure and behavior of the swarms, dynamic differentiation/re-differentiation of components and local information sharing helped the swarms maintain spatially adjacent, coherent organization.',
	 'authors': u'Hiroki Sayama,',
	 'category': u'Computer Science ',
	 'date': '2014-5-24',
	 'pdflink': u'http://arxiv.org/pdf/1405.6296',
	 'subjects': u'Adaptation and Self-Organizing Systems (nlin.AO)',
	 'title': u'\nFour Classes of Morphogenetic Collective Systems',
	 'urllink': u'http://arxiv.org/abs/1405.6296'}
2015-03-23 20:32:24+0000 [xxu461000] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1407.4992> (referer: http://arxiv.org/list/cs/14?skip=8000&show=1000)
2015-03-23 20:32:24+0000 [xxu461000] DEBUG: Scraped from <200 http://arxiv.org/abs/1407.4992>
	{'abstract': u'Background: With the ever increasing use of computational models in the biosciences, the need to share models and reproduce the results of published studies efficiently and easily is becoming more important. To this end, various standards have been proposed that can be used to describe models, simulations, data or other essential information in a consistent fashion. These constitute various separate components required to reproduce a given published scientific result. Results: We describe the Open Modeling EXchange format (OMEX). Together with the use of other standard formats from the Computational Modeling in Biology Network (COMBINE), OMEX is the basis of the COMBINE Archive, a single file that supports the exchange of all the information necessary for a modeling and simulation experiment in biology. An OMEX file is a ZIP container that includes a manifest file, listing the content of the archive, an optional metadata file adding information about the archive and its content, and the files describing the model. The content of a COMBINE Archive consists of files encoded in COMBINE standards whenever possible, but may include additional files defined by an Internet Media Type. Several tools that support the COMBINE Archive are available, either as independent libraries or embedded in modeling software. Conclusions: The COMBINE Archive facilitates the reproduction of modeling and simulation experiments in biology by embedding all the relevant information in one file. Having all the information stored and exchanged at once also helps in building activity logs and audit trails. We anticipate that the COMBINE Archive will become a significant help for modellers, as the domain moves to larger, more complex experiments such as multi-scale models of organs, digital organisms, and bioengineering.',
	 'authors': u'Frank T. Bergmann, Richard Adams, Stuart Moodie, Jonathan Cooper, Mihai Glont, Martin Golebiewski, Michael Hucka, Camille Laibe, Andrew K. Miller, David P. Nickerson, Brett G. Olivier, Nicolas Rodriguez, Herbert M. Sauro, Martin Scharm, Stian Soiland-Reyes, Dagmar Waltemath, Florent Yvon, Nicolas Le Nov\xe8re,',
	 'category': u'Computer Science ',
	 'date': '2014-7-18',
	 'pdflink': u'http://arxiv.org/pdf/1407.4992',
	 'subjects': u'Digital Libraries (cs.DL)',
	 'title': u'\nOne file to share them all: Using the COMBINE Archive and the OMEX  format to share all information about a modeling project',
	 'urllink': u'http://arxiv.org/abs/1407.4992'}
2015-03-23 20:32:29+0000 [xxu461000] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1404.5121> (referer: http://arxiv.org/list/cs/14?skip=4000&show=1000)
2015-03-23 20:32:29+0000 [xxu461000] DEBUG: Scraped from <200 http://arxiv.org/abs/1404.5121>
	{'abstract': u'Power consumption in data centers has been growing significantly in recent years. To reduce power, servers are being equipped with increasingly sophisticated power management mechanisms. Different mechanisms offer dramatically different trade-offs between power savings and performance penalties. Considering the complexity, variety, and temporally varying nature of the applications hosted in a typical data center, intelligently determining which power management policy to use and when is a complicated task. In this paper we analyze a system model featuring both performance scaling and low-power states. We reveal the interplay between performance scaling and low-power states via intensive simulation and analytic verification. Based on the observations, we present SleepScale, a runtime power management tool designed to efficiently exploit existing power control mechanisms. At run time, SleepScale characterizes power consumption and quality-of-service (QoS) for each low-power state and frequency setting, and selects the best policy for a given QoS constraint. We evaluate SleepScale using workload traces from data centers and achieve significant power savings relative to conventional power management strategies.',
	 'authors': u'Yanpei Liu, Stark C. Draper, Nam Sung Kim,',
	 'category': u'Computer Science ',
	 'date': '2014-4-21',
	 'pdflink': u'http://arxiv.org/pdf/1404.5121',
	 'subjects': u'Performance (cs.PF)',
	 'title': u'\nSleepScale: Runtime Joint Speed Scaling and Sleep States Management for  Power Efficient Data Centers',
	 'urllink': u'http://arxiv.org/abs/1404.5121'}
2015-03-23 20:32:35+0000 [xxu461000] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1409.1739> (referer: http://arxiv.org/list/cs/14?skip=10000&show=1000)
2015-03-23 20:32:35+0000 [xxu461000] DEBUG: Scraped from <200 http://arxiv.org/abs/1409.1739>
	{'abstract': u'Maximum throughput requires path diversity enabled by bifurcating traffic at different network nodes. In this work, we consider a network where traffic bifurcation is allowed only at a subset of nodes called emph, while the rest nodes (called emph) cannot bifurcate traffic and hence only forward packets on specified paths. This implements an overlay network of routers where each overlay link corresponds to a path in the physical network. We study dynamic routing implemented at the overlay. We develop a queue-based policy, which is shown to be maximally stable (throughput optimal) for a restricted class of network scenarios where overlay links do not correspond to overlapping physical paths. Simulation results show that our policy yields better delay over dynamic policies that allow bifurcation at all nodes, such as the backpressure policy. Additionally, we provide a heuristic extension of our proposed overlay routing scheme for the unrestricted class of networks.',
	 'authors': u'Georgios Paschos, Eytan Modiano,',
	 'category': u'Computer Science ',
	 'date': '2014-9-5',
	 'pdflink': u'http://arxiv.org/pdf/1409.1739',
	 'subjects': u'Networking and Internet Architecture (cs.NI)',
	 'title': u'\nThroughput Optimal Routing in Overlay Networks',
	 'urllink': u'http://arxiv.org/abs/1409.1739'}
2015-03-23 20:32:40+0000 [xxu461000] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1406.7578> (referer: http://arxiv.org/list/cs/14?skip=7000&show=1000)
2015-03-23 20:32:40+0000 [xxu461000] DEBUG: Scraped from <200 http://arxiv.org/abs/1406.7578>
	{'abstract': u'Human groups can perform extraordinary accurate estimations compared to individuals by simply using the mean, median or geometric mean of the individual estimations [Galton 1907, Surowiecki 2005, Page 2008]. However, this is true only for some tasks and in general these collective estimations show strong biases. The method fails also when allowing for social interactions, which makes the collective estimation worse as individuals tend to converge to the biased result [Lorenz et al. 2011]. Here we show that there is a bright side of this apparently negative impact of social interactions into collective intelligence. We found that some individuals resist the social influence and, when using the median of this subgroup, we can eliminate the bias of the wisdom of the full crowd. To find this subgroup of individuals more confident in their private estimations than in the social influence, we model individuals as estimators that combine private and social information with different relative weights [Perez-Escudero &amp; de Polavieja 2011, Arganda et al. 2012]. We then computed the geometric mean for increasingly smaller groups by eliminating those using in their estimations higher values of the social influence weight. The trend obtained in this procedure gives unbiased results, in contrast to the simpler method of computing the median of the complete group. Our results show that, while a simple operation like the mean, median or geometric mean of a group may not allow groups to make good estimations, a more complex operation taking into account individuality in the social dynamics can lead to a better collective intelligence.',
	 'authors': u'Gonzalo De Polavieja, Gabriel Madirolas,',
	 'category': u'Computer Science ',
	 'date': '2014-6-30',
	 'pdflink': u'http://arxiv.org/pdf/1406.7578',
	 'subjects': u'Social and Information Networks (cs.SI)',
	 'title': u'\nWisdom of the Confident: Using Social Interactions to Eliminate the Bias  in Wisdom of the Crowds',
	 'urllink': u'http://arxiv.org/abs/1406.7578'}
2015-03-23 20:32:45+0000 [xxu461000] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1405.2049> (referer: http://arxiv.org/list/cs/14?skip=5000&show=1000)
2015-03-23 20:32:45+0000 [xxu461000] DEBUG: Scraped from <200 http://arxiv.org/abs/1405.2049>
	{'abstract': u"We derive a new upper bound on the string oblivious transfer capacity of discrete memoryless channels. The main tool we use is the tension region of a pair of random variables introduced in Prabhakaran and Prabhakaran (2014) where it was used to derive upper bounds on rates of secure sampling in the source model. In this paper, we consider secure computation of string oblivious transfer in the channel model. Our bound is based on a monotonicity property of the tension region in the channel model. We show that our bound strictly improves upon the upper bound of Ahlswede and Csisz 'ar (2013).",
	 'authors': u'K. Sankeerth Rao, Vinod M. Prabhakaran,',
	 'category': u'Computer Science ',
	 'date': '2014-5-8',
	 'pdflink': u'http://arxiv.org/pdf/1405.2049',
	 'subjects': u'Information Theory (cs.IT)',
	 'title': u'\nA New Upperbound for the Oblivious Transfer Capacity of Discrete  Memoryless Channels',
	 'urllink': u'http://arxiv.org/abs/1405.2049'}
2015-03-23 20:32:48+0000 [xxu461000] INFO: Crawled 478 pages (at 11 pages/min), scraped 461 items (at 11 items/min)
2015-03-23 20:32:51+0000 [xxu461000] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1405.6267> (referer: http://arxiv.org/list/cs/14?skip=6000&show=1000)
2015-03-23 20:32:51+0000 [xxu461000] DEBUG: Scraped from <200 http://arxiv.org/abs/1405.6267>
	{'abstract': u"We present a nonintrusive method for reliably estimating the noise level during quantum computation and quantum communication protected by quantum error-correcting codes. As preprocessing of quantum error correction, our scheme estimates the current noise level through a negligible amount of classical computation using error syndromes and updates the decoder's knowledge on the spot before inferring the locations of errors. This preprocessing requires no additional quantum interaction or modification in the system. The estimate can be of higher quality than the maximum likelihood estimate based on perfect knowledge of channel parameters, thereby eliminating the need of the unrealistic assumption that the decoder accurately knows channel parameters a priori. Simulations demonstrate that not only can the decoder pick up on a change of channel parameters, but even if the channel stays the same, a quantum low-density parity-check code can perform better when the decoder exploits the on-the-spot estimates instead of the true error probabilities of the quantum channel.",
	 'authors': u'Yuichiro Fujiwara,',
	 'category': u'Computer Science ',
	 'date': '2014-5-24',
	 'pdflink': u'http://arxiv.org/pdf/1405.6267',
	 'subjects': u'Quantum Physics (quant-ph)',
	 'title': u'\nInstantaneous Quantum Channel Estimation during Quantum Information  Processing',
	 'urllink': u'http://arxiv.org/abs/1405.6267'}
2015-03-23 20:32:56+0000 [xxu461000] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1407.4990> (referer: http://arxiv.org/list/cs/14?skip=8000&show=1000)
2015-03-23 20:32:56+0000 [xxu461000] DEBUG: Scraped from <200 http://arxiv.org/abs/1407.4990>
	{'abstract': u'In network science, assortativity refers to the tendency of links to exist between nodes with similar attributes. In social networks, for example, links tend to exist between individuals of similar age, nationality, location, race, income, educational level, religious belief, and language. Thus, various attributes jointly affect the network topology. An interesting problem is to detect community structure beyond some specific assortativity-related attributes , i.e., to take out the effect of on network topology and reveal the hidden community structure which are due to other attributes. An approach to this problem is to redefine the null model of the modularity measure, so as to simulate the effect of on network topology. However, a challenge is that we do not know to what extent the network topology is affected by and by other attributes. In this paper, we propose Dist-Modularity which allows us to freely choose any suitable function to simulate the effect of . Such freedom can help us probe the effect of and detect the hidden communities which are due to other attributes. We test the effectiveness of Dist-Modularity on synthetic benchmarks and two real-world networks.',
	 'authors': u'Xin Liu, Tsuyoshi Murata, Ken Wakita,',
	 'category': u'Computer Science ',
	 'date': '2014-7-15',
	 'pdflink': u'http://arxiv.org/pdf/1407.4990',
	 'subjects': u'Social and Information Networks (cs.SI)',
	 'title': u'\nDetecting network communities beyond assortativity-related attributes',
	 'urllink': u'http://arxiv.org/abs/1407.4990'}
2015-03-23 20:33:01+0000 [xxu461000] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1404.5084> (referer: http://arxiv.org/list/cs/14?skip=4000&show=1000)
2015-03-23 20:33:01+0000 [xxu461000] DEBUG: Scraped from <200 http://arxiv.org/abs/1404.5084>
	{'abstract': u'In contrast to the usual understanding of probabilistic systems as stochastic processes, recently these systems have also been regarded as transformers of probabilities. In this paper, we give a natural definition of strong bisimulation for probabilistic systems corresponding to this view that treats probability distributions as first-class citizens. Our definition applies in the same way to discrete systems as well as to systems with uncountable state and action spaces. Several examples demonstrate that our definition refines the understanding of behavioural equivalences of probabilistic systems. In particular, it solves a long-standing open problem concerning the representation of memoryless continuous time by memory-full continuous time. Finally, we give algorithms for computing this bisimulation not only for finite but also for classes of uncountably infinite systems.',
	 'authors': u'Holger Hermanns, Jan Kr\u010d\xe1l, Jan K\u0159et\xednsk\xfd,',
	 'category': u'Computer Science ',
	 'date': '2014-4-21',
	 'pdflink': u'http://arxiv.org/pdf/1404.5084',
	 'subjects': u'Logic in Computer Science (cs.LO)',
	 'title': u'\nProbabilistic Bisimulation: Naturally on Distributions',
	 'urllink': u'http://arxiv.org/abs/1404.5084'}
2015-03-23 20:33:06+0000 [xxu461000] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1409.1730> (referer: http://arxiv.org/list/cs/14?skip=10000&show=1000)
2015-03-23 20:33:06+0000 [xxu461000] DEBUG: Scraped from <200 http://arxiv.org/abs/1409.1730>
	{'abstract': u'Defining an optimal protection strategy against viruses, spam propagation or any other kind of contamination process is an important feature for designing new networks and architectures. In this work, we consider decentralized optimal protection strategies when a virus is propagating over a network through a SIS epidemic process. We assume that each node in the network can fully protect itself from infection at a constant cost, or the node can use recovery software, once it is infected. We model our system using a game theoretic framework and find pure, mixed equilibria, and the Price of Anarchy (PoA) in several network topologies. Further, we propose both a decentralized algorithm and an iterative procedure to compute a pure equilibrium in the general case of a multiple communities network. Finally, we evaluate the algorithms and give numerical illustrations of all our results.',
	 'authors': u'Stojan Trajanovski, Yezekael Hayel, Eitan Altman, Huijuan Wang, Piet Van Mieghem,',
	 'category': u'Computer Science ',
	 'date': '2014-9-5',
	 'pdflink': u'http://arxiv.org/pdf/1409.1730',
	 'subjects': u'Systems and Control (cs.SY)',
	 'title': u'\nDecentralized Protection Strategies against SIS Epidemics in Networks',
	 'urllink': u'http://arxiv.org/abs/1409.1730'}
2015-03-23 20:33:10+0000 [xxu461000] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1406.7577> (referer: http://arxiv.org/list/cs/14?skip=7000&show=1000)
2015-03-23 20:33:10+0000 [xxu461000] DEBUG: Scraped from <200 http://arxiv.org/abs/1406.7577>
	{'abstract': u'This paper describes early work on the correlation between Bitcoin price indicators and Bitcoin-related Twitter posts. Within a timeframe of 104 days, a total of 161,200 Twitter posts containing "bitcoin" in combination with positive or negative sentiments and signals of uncertainty has been collected. Positive and negative sentiments were ex-ante filtered with the help of API queries, by using identifiers such as the words "happy, love, fun, good" or "bad, sad, unhappy" respectively. Additionally, tweets on "hope, fear" and "worry" have been collected to derive insights on market dynamics and uncertainties. Current results show a significant positive correlation of emotional tweets with intraday close prices and trading volumes of Bitcoin. Data suggests: When Bitcoin transactions fly high, emotions fly high on Twitter, too. Another observation is that a balanced ratio of negative and positive sentiments positively correlates with the trading volume and thus may designate Twitter as place that reflects the "speculative momentum" within the market. Summarizing, the microblogging platform Twitter may be interpreted as a virtual trading floor that heats up with virtual emotions and reflects market movements.',
	 'authors': u'Jermain Kaminski, Peter Gloor,',
	 'category': u'Computer Science ',
	 'date': '2014-6-30',
	 'pdflink': u'http://arxiv.org/pdf/1406.7577',
	 'subjects': u'Social and Information Networks (cs.SI)',
	 'title': u'\nNowcasting the Bitcoin Market with Twitter Signals',
	 'urllink': u'http://arxiv.org/abs/1406.7577'}
2015-03-23 20:33:16+0000 [xxu461000] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1405.2048> (referer: http://arxiv.org/list/cs/14?skip=5000&show=1000)
2015-03-23 20:33:16+0000 [xxu461000] DEBUG: Scraped from <200 http://arxiv.org/abs/1405.2048>
	{'abstract': u'Name matching is a key component of systems for entity resolution or record linkage. Alternative spellings of the same names are a com- mon occurrence in many applications. We use the largest collection of genealogy person records in the world together with user search query logs to build name matching models. The procedure for building a crowd-sourced training set is outlined together with the presentation of our method. We cast the problem of learning alternative spellings as a machine translation problem at the character level. We use in- formation retrieval evaluation methodology to show that this method substantially outperforms on our data a number of standard well known phonetic and string similarity methods in terms of precision and re- call. Additionally, we rigorously compare the performance of standard methods when compared with each other. Our result can lead to a significant practical impact in entity resolution applications.',
	 'authors': u'Jeffrey Sukharev, Leonid Zhukov, Alexandrin Popescul,',
	 'category': u'Computer Science ',
	 'date': '2014-5-7',
	 'pdflink': u'http://arxiv.org/pdf/1405.2048',
	 'subjects': u'Information Retrieval (cs.IR)',
	 'title': u'\nLearning Alternative Name Spellings',
	 'urllink': u'http://arxiv.org/abs/1405.2048'}
2015-03-23 20:33:22+0000 [xxu461000] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1405.6256> (referer: http://arxiv.org/list/cs/14?skip=6000&show=1000)
2015-03-23 20:33:22+0000 [xxu461000] DEBUG: Scraped from <200 http://arxiv.org/abs/1405.6256>
	{'abstract': u'Cyclic codes are an important class of linear codes, whose weight distribution have been extensively studied. So far, most of previous results obtained were for cyclic codes with no more than three zeros. Recently, cite constructed a class of cyclic codes with arbitrary number of zeros, and computed the weight distributions for several cases. In this paper, we determine the weight distribution for a new family of such codes. This is achieved by certain new methods, such as the theory of Jacobi sums over finite fields and subtle treatment of some complicated combinatorial identities.',
	 'authors': u'Jing Yang, Lingli Xia, Maosheng Xiong,',
	 'category': u'Computer Science ',
	 'date': '2014-5-24',
	 'pdflink': u'http://arxiv.org/pdf/1405.6256',
	 'subjects': u'Number Theory (math.NT)',
	 'title': u'\nWeight Distributions of a Class of Cyclic Codes with Arbitrary Number of  Zeros II',
	 'urllink': u'http://arxiv.org/abs/1405.6256'}
2015-03-23 20:33:24+0000 [xxu461000] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1407.4989> (referer: http://arxiv.org/list/cs/14?skip=8000&show=1000)
2015-03-23 20:33:24+0000 [xxu461000] DEBUG: Scraped from <200 http://arxiv.org/abs/1407.4989>
	{'abstract': u'There has been a surge of interest in community detection in homogeneous single-relational networks which contain only one type of nodes and edges. However, many real-world systems are naturally described as heterogeneous multi-relational networks which contain multiple types of nodes and edges. In this paper, we propose a new method for detecting communities in such networks. Our method is based on optimizing the composite modularity, which is a new modularity proposed for evaluating partitions of a heterogeneous multi-relational network into communities. Our method is parameter-free, scalable, and suitable for various networks with general structure. We demonstrate that it outperforms the state-of-the-art techniques in detecting pre-planted communities in synthetic networks. Applied to a real-world Digg network, it successfully detects meaningful communities.',
	 'authors': u'Xin Liu, Weichu Liu, Tsuyoshi Murata, Ken Wakita,',
	 'category': u'Computer Science ',
	 'date': '2014-7-15',
	 'pdflink': u'http://arxiv.org/pdf/1407.4989',
	 'subjects': u'Social and Information Networks (cs.SI)',
	 'title': u'\nA framework for community detection in heterogeneous multi-relational  networks',
	 'urllink': u'http://arxiv.org/abs/1407.4989'}
2015-03-23 20:33:27+0000 [xxu461000] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1404.5083> (referer: http://arxiv.org/list/cs/14?skip=4000&show=1000)
2015-03-23 20:33:27+0000 [xxu461000] DEBUG: Scraped from <200 http://arxiv.org/abs/1404.5083>
	{'abstract': u'Cognitive radio (CR) technology addresses the problem of spectrum under-utilization. In underlay CR mode, the secondary users are allowed to communicate provided that their transmission is not detrimental to primary user communication. Transmit antenna selection is one of the low-complexity methods to increase the capacity of wireless communication systems. In this article, we propose and analyze the performance benefit of a transmit antenna selection scheme for underlay secondary system that ensures the instantaneous interference caused by the secondary transmitter to the primary receiver is below a predetermined level. Closed-form expressions of the outage probability, amount of fading, and ergodic capacity for the secondary network are derived. Monte-carlo simulations are also carried out to confirm various mathematical results presented in this article.',
	 'authors': u'Muhammad Hanif, Hong-Chuan Yang, Mohamed-Slim Alouini,',
	 'category': u'Computer Science ',
	 'date': '2014-4-20',
	 'pdflink': u'http://arxiv.org/pdf/1404.5083',
	 'subjects': u'Information Theory (cs.IT)',
	 'title': u'\nTransmit Antenna Selection in Underlay Cognitive Radio Environment',
	 'urllink': u'http://arxiv.org/abs/1404.5083'}
2015-03-23 20:33:32+0000 [xxu461000] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1409.1726> (referer: http://arxiv.org/list/cs/14?skip=10000&show=1000)
2015-03-23 20:33:32+0000 [xxu461000] DEBUG: Scraped from <200 http://arxiv.org/abs/1409.1726>
	{'abstract': u'We analyze the data about works (papers, books) from the time period 1990-2010 that are collected in Zentralblatt MATH database. The data were converted into four 2-mode networks (works authors, works journals, works keywords and works MSCs) and into a partition of works by publication year. The networks were analyzed using Pajek -- a program for analysis and visualization of large networks. We explore the distributions of some properties of works and the collaborations among mathematicians. We also take a closer look at the characteristics of the field of graph theory as were realized with the publications.',
	 'authors': u'Monika Cerin\u0161ek, Vladimir Batagelj,',
	 'category': u'Computer Science ',
	 'date': '2014-9-5',
	 'pdflink': u'http://arxiv.org/pdf/1409.1726',
	 'subjects': u'Social and Information Networks (cs.SI)',
	 'title': u'\nNetwork analysis of Zentralblatt MATH data',
	 'urllink': u'http://arxiv.org/abs/1409.1726'}
2015-03-23 20:33:38+0000 [xxu461000] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1406.7572> (referer: http://arxiv.org/list/cs/14?skip=7000&show=1000)
2015-03-23 20:33:38+0000 [xxu461000] DEBUG: Scraped from <200 http://arxiv.org/abs/1406.7572>
	{'abstract': u'This paper analyzes the performance of clustered decode-and-forward multi-hop relaying (CDFMR) wireless Rayleigh fading networks, and sheds light on their design principles for energy and spectral efficiency. The focus is on a general performance analysis (over all SNR range) of heterogeneous wireless networks with possibly different numbers of relays in clusters of various separations. For clustered multi-hop relaying systems, ad-hoc routing is known as an efficient decentralized routing algorithm which selects the best relay node on a hop-by-hop basis using local channel state information. In this article, we combine ad-hoc routing and cooperative diversity in CDFMR systems, and we derive (i) a closed-form expression for the probability distribution of the end-to-end SNR at the destination node; (ii) the system symbol error rate (SER) performance for a wide class of modulation schemes; and (iii) exact analytical expressions for the system ergodic capacity, the outage probability and the achievable probability of the SNR (power) gain. We also provide simple analytical asymptotic expressions for SER and the outage probability in high SNR regime. Simulation results are provided to validate the correctness of the presented analyses.',
	 'authors': u'Amin Azari, Jalil Seifali Harsini, Farshad Lahouti,',
	 'category': u'Computer Science ',
	 'date': '2014-6-30',
	 'pdflink': u'http://arxiv.org/pdf/1406.7572',
	 'subjects': u'Information Theory (cs.IT)',
	 'title': u'\nPerformance Analysis of Ad-Hoc Routing in Clustered Multi-hop Wireless  Networks',
	 'urllink': u'http://arxiv.org/abs/1406.7572'}
2015-03-23 20:33:42+0000 [xxu461000] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1405.2034> (referer: http://arxiv.org/list/cs/14?skip=5000&show=1000)
2015-03-23 20:33:42+0000 [xxu461000] DEBUG: Scraped from <200 http://arxiv.org/abs/1405.2034>
	{'abstract': u"Recently, Gunn, Allison and Abbott (GAA) [this http URL] proposed a new scheme to utilize electromagnetic waves for eavesdropping on the Kirchhoff-law-Johnson-noise (KLJN) secure key distribution. We proved in a former paper [Fluct. Noise Lett. 13 (2014) 1450016] that GAA's mathematical model is unphysical. Here we analyze GAA's cracking scheme and show that, in the case of a loss-free cable, it provides less eavesdropping information than in the earlier (Bergou)-Scheuer-Yariv mean-square-based attack [Kish LB, Scheuer J, Phys. Lett. A 374 (2010) 2140-2142], while it offers no information in the case of a lossy cable. We also investigate GAA's claim to be experimentally capable of distinguishing - using statistics over a few correlation times only - the distributions of two Gaussian noises with a relative variance difference of less than 10^-8. Normally such distinctions would require hundreds of millions of correlations times to be observable. We identify several potential experimental artifacts as results of poor KLJN design, which can lead to GAA's assertions: deterministic currents due to spurious harmonic components caused by ground loops, DC offset, aliasing, non-Gaussian features including non-linearities and other non-idealities in generators, and the time-derivative nature of GAA's scheme which tends to enhance all of these artifacts.",
	 'authors': u'Hsien-Pu Chen, Laszlo B. Kish, Claes-Goran Granqvist, Gabor Schmera,',
	 'category': u'Computer Science ',
	 'date': '2014-4-11',
	 'pdflink': u'http://arxiv.org/pdf/1405.2034',
	 'subjects': u'Cryptography and Security (cs.CR)',
	 'title': u'\nOn the "cracking" scheme in the paper "A directional coupler attack  against the Kish key distribution system" by Gunn, Allison and Abbott',
	 'urllink': u'http://arxiv.org/abs/1405.2034'}
2015-03-23 20:33:48+0000 [xxu461000] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1405.6224> (referer: http://arxiv.org/list/cs/14?skip=6000&show=1000)
2015-03-23 20:33:48+0000 [xxu461000] DEBUG: Scraped from <200 http://arxiv.org/abs/1405.6224>
	{'abstract': u'Research on human social interactions has traditionally relied on self-reports. Despite their widespread use, self-reported accounts of behaviour are prone to biases and necessarily reduce the range of behaviours, and the number of subjects, that may be studied simultaneously. The development of ever smaller sensors makes it possible to study group-level human behaviour in naturalistic settings outside research laboratories. We used such sensors, sociometers, to examine gender, talkativeness and interaction style in two different contexts. Here, we find that in the collaborative context, women were much more likely to be physically proximate to other women and were also significantly more talkative than men, especially in small groups. In contrast, there were no gender-based differences in the non-collaborative setting. Our results highlight the importance of objective measurement in the study of human behaviour, here enabling us to discern context specific, gender-based differences in interaction style.',
	 'authors': u'Jukka-Pekka Onnela, Benjamin N. Waber, Alex, Pentland, Sebastian Schnorf, David Lazer,',
	 'category': u'Computer Science ',
	 'date': '2014-5-23',
	 'pdflink': u'http://arxiv.org/pdf/1405.6224',
	 'subjects': u'Physics and Society (physics.soc-ph)',
	 'title': u'\nUsing sociometers to quantify social interaction patterns',
	 'urllink': u'http://arxiv.org/abs/1405.6224'}
2015-03-23 20:33:48+0000 [xxu461000] INFO: Crawled 491 pages (at 13 pages/min), scraped 474 items (at 13 items/min)
2015-03-23 20:33:54+0000 [xxu461000] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1407.4979> (referer: http://arxiv.org/list/cs/14?skip=8000&show=1000)
2015-03-23 20:33:54+0000 [xxu461000] DEBUG: Scraped from <200 http://arxiv.org/abs/1407.4979>
	{'abstract': u'Various hand-crafted features and metric learning methods prevail in the field of person re-identification. Compared to these methods, this paper proposes a more general way that can learn a similarity metric from image pixels directly. By using a "siamese" deep neural network, the proposed method can jointly learn the color feature, texture feature and metric in a unified framework. The network has a symmetry structure with two sub-networks which are connected by Cosine function. To deal with the big variations of person images, binomial deviance is used to evaluate the cost between similarities and labels, which is proved to be robust to outliers. Compared to existing researches, a more practical setting is studied in the experiments that is training and test on different datasets (cross dataset person re-identification). Both in "intra dataset" and "cross dataset" settings, the superiorities of the proposed method are illustrated on VIPeR and PRID.',
	 'authors': u'Dong Yi, Zhen Lei, Stan Z. Li,',
	 'category': u'Computer Science ',
	 'date': '2014-7-18',
	 'pdflink': u'http://arxiv.org/pdf/1407.4979',
	 'subjects': u'Computer Vision and Pattern Recognition (cs.CV)',
	 'title': u'\nDeep Metric Learning for Practical Person Re-Identification',
	 'urllink': u'http://arxiv.org/abs/1407.4979'}
2015-03-23 20:33:59+0000 [xxu461000] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1404.5078> (referer: http://arxiv.org/list/cs/14?skip=4000&show=1000)
2015-03-23 20:33:59+0000 [xxu461000] DEBUG: Scraped from <200 http://arxiv.org/abs/1404.5078>
	{'abstract': u'TurKontrol, and algorithm presented in (Dai et al. 2010), uses a POMDP to model and control an iterative workflow for crowdsourced work. Here, TurKontrol is re-implemented as "TurKPF," which uses a Particle Filter to reduce computation time &amp; memory usage. Most importantly, in our experimental environment with default parameter settings, the action is chosen nearly instantaneously. Through a series of experiments we see that TurKPF and TurKontrol perform similarly.',
	 'authors': u'Ethan Petuchowski, Matthew Lease,',
	 'category': u'Computer Science ',
	 'date': '2014-4-20',
	 'pdflink': u'http://arxiv.org/pdf/1404.5078',
	 'subjects': u'Artificial Intelligence (cs.AI)',
	 'title': u'\nTurKPF: TurKontrol as a Particle Filter',
	 'urllink': u'http://arxiv.org/abs/1404.5078'}
2015-03-23 20:34:05+0000 [xxu461000] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1409.1722> (referer: http://arxiv.org/list/cs/14?skip=10000&show=1000)
2015-03-23 20:34:05+0000 [xxu461000] DEBUG: Scraped from <200 http://arxiv.org/abs/1409.1722>
	{'abstract': u'We consider the problem of online graph multi-coloring with advice. Multi-coloring is often used to model frequency allocation in cellular networks. We give several nearly tight upper and lower bounds for the most standard topologies of cellular networks, paths and hexagonal graphs. For the path, negative results trivially carry over to bipartite graphs, and our positive results are also valid for bipartite graphs. The advice given represents information that is likely to be available, studying for instance the data from earlier similar periods of time.',
	 'authors': u'Marie G. Christ, Lene M. Favrholdt, Kim S. Larsen,',
	 'category': u'Computer Science ',
	 'date': '2014-9-5',
	 'pdflink': u'http://arxiv.org/pdf/1409.1722',
	 'subjects': u'Data Structures and Algorithms (cs.DS)',
	 'title': u'\nOnline Multi-Coloring with Advice',
	 'urllink': u'http://arxiv.org/abs/1409.1722'}
2015-03-23 20:34:09+0000 [xxu461000] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1406.7570> (referer: http://arxiv.org/list/cs/14?skip=7000&show=1000)
2015-03-23 20:34:09+0000 [xxu461000] DEBUG: Scraped from <200 http://arxiv.org/abs/1406.7570>
	{'abstract': u'The sheer increase in the size of graph data has created a lot of interest into developing efficient distributed graph processing frameworks. Popular existing frameworks such as Graphlab and Pregel rely on balanced graph partitioning in order to minimize communication and achieve work balance. In this work we contribute to the recent research line of streaming graph partitioning cite which computes an approximately balanced -partitioning of the vertex set of a graph using a single pass over the graph stream using degree-based criteria. This graph partitioning framework is well tailored to processing large-scale and dynamic graphs. In this work we introduce the use of higher length walks for streaming graph partitioning and show that their use incurs a minor computational cost which can significantly improve the quality of the graph partition. We perform an average case analysis of our algorithm using the planted partition model cite. We complement the recent results of Stanton cite by showing that our proposed method recovers the true partition with high probability even when the gap of the model tends to zero as the size of the graph grows. Furthermore, among the wide number of choices for the length of the walks we show that the proposed length is optimal. Finally, we conduct experiments which verify the value of the proposed method.',
	 'authors': u'Charalampos E. Tsourakakis,',
	 'category': u'Computer Science ',
	 'date': '2014-6-30',
	 'pdflink': u'http://arxiv.org/pdf/1406.7570',
	 'subjects': u'Data Structures and Algorithms (cs.DS)',
	 'title': u'\nStreaming Graph Partitioning in the Planted Partition Model',
	 'urllink': u'http://arxiv.org/abs/1406.7570'}
2015-03-23 20:34:13+0000 [xxu461000] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1405.2031> (referer: http://arxiv.org/list/cs/14?skip=5000&show=1000)
2015-03-23 20:34:13+0000 [xxu461000] DEBUG: Scraped from <200 http://arxiv.org/abs/1405.2031>
	{'abstract': u'The Substrate Integrated Waveguide (SIW) technology is a very promising technique with which we can take the advantages of both waveguides and planar transmission lines. Therefore, in [2.1-3] GHz band various microwave components and devices have been designed successfully using Ansoft HFSS software. We then proceeded to the realization of the coupler and then made measurements of the frequency response in the range [2.1-3] GHz using a network analyzer. Thus, results of this modeling are presented, discussed and allow to integrate these devices in planar circuits.',
	 'authors': u'Rahali Bochra, Feham Mohammed, Junwu Tao,',
	 'category': u'Computer Science ',
	 'date': '2014-5-8',
	 'pdflink': u'http://arxiv.org/pdf/1405.2031',
	 'subjects': u'Emerging Technologies (cs.ET)',
	 'title': u'\nAnalysis of S Band Substrate Integrated Waveguide Power Divider,  Circulator and Coupler',
	 'urllink': u'http://arxiv.org/abs/1405.2031'}
2015-03-23 20:34:18+0000 [xxu461000] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1405.6077> (referer: http://arxiv.org/list/cs/14?skip=6000&show=1000)
2015-03-23 20:34:18+0000 [xxu461000] DEBUG: Scraped from <200 http://arxiv.org/abs/1405.6077>
	{'abstract': u'We analyze the game of go from the point of view of complex networks. We construct three different directed networks of increasing complexity, defining nodes as local patterns on plaquettes of increasing sizes, and links as actual successions of these patterns in databases of real games. We discuss the peculiarities of these networks compared to other types of networks. We explore the ranking vectors and community structure of the networks and show that this approach enables to extract groups of moves with common strategic properties. We also investigate different networks built from games with players of different levels or from different phases of the game. We discuss how the study of the community structure of these networks may help to improve the computer simulations of the game. More generally, we believe such studies may help to improve the understanding of human decision process.',
	 'authors': u'V. Kandiah, B. Georgeot, O. Giraud,',
	 'category': u'Computer Science ',
	 'date': '2014-5-23',
	 'pdflink': u'http://arxiv.org/pdf/1405.6077',
	 'subjects': u'Physics and Society (physics.soc-ph)',
	 'title': u'\nMove ordering and communities in complex networks describing the game of  go',
	 'urllink': u'http://arxiv.org/abs/1405.6077'}
2015-03-23 20:34:24+0000 [xxu461000] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1407.4972> (referer: http://arxiv.org/list/cs/14?skip=8000&show=1000)
2015-03-23 20:34:24+0000 [xxu461000] DEBUG: Scraped from <200 http://arxiv.org/abs/1407.4972>
	{'abstract': u'This paper will analyze several quadratic-time solvable problems, and will classify them into two classes: problems that are solvable in truly subquadratic time (that is, in time for some ) and problems that are not, unless the well known Strong Exponential Time Hypothesis (SETH) is false. In particular, we will prove that some quadratic-time solvable problems are indeed easier than expected. We will provide an algorithm that computes the transitive closure of a directed graph in time , where denotes the number of edges in the transitive closure and is the exponent for matrix multiplication. As a side effect, we will prove that our algorithm runs in time if the transitive closure is sparse. The same time bounds hold if we want to check whether a graph is transitive, by replacing m with the number of edges in the graph itself. As far as we know, this is the fastest algorithm for sparse transitive digraph recognition. Finally, we will apply our algorithm to the comparability graph recognition problem (dating back to 1941), obtaining the first truly subquadratic algorithm. The second part of the paper deals with hardness results. Starting from an artificial quadratic-time solvable variation of the k-SAT problem, we will construct a graph of Karp reductions, proving that a truly subquadratic-time algorithm for any of the problems in the graph falsifies SETH. The analyzed problems are the following: computing the subset graph, finding dominating sets, computing the betweenness centrality of a vertex, computing the minimum closeness centrality, and computing the hyperbolicity of a pair of vertices. We will also be able to include in our framework three proofs already appeared in the literature, concerning the graph diameter computation, local alignment of strings and orthogonality of vectors.',
	 'authors': u'Michele Borassi, Pierluigi Crescenzi, Michel Habib,',
	 'category': u'Computer Science ',
	 'date': '2014-7-18',
	 'pdflink': u'http://arxiv.org/pdf/1407.4972',
	 'subjects': u'Computational Complexity (cs.CC)',
	 'title': u'\nInto the Square - On the Complexity of Quadratic-Time Solvable Problems',
	 'urllink': u'http://arxiv.org/abs/1407.4972'}
2015-03-23 20:34:28+0000 [xxu461000] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1404.5069> (referer: http://arxiv.org/list/cs/14?skip=4000&show=1000)
2015-03-23 20:34:28+0000 [xxu461000] DEBUG: Scraped from <200 http://arxiv.org/abs/1404.5069>
	{'abstract': u'A period of a rational integral is the result of integrating, with respect to one or several variables, a rational function over a closed path. This work focuses particularly on periods depending on a parameter: in this case the period under consideration satisfies a linear differential equation, the Picard-Fuchs equation. I give a reduction algorithm that extends the Griffiths-Dwork reduction and apply it to the computation of Picard-Fuchs equations. The resulting algorithm is elementary and has been successfully applied to problems that were previously out of reach.',
	 'authors': u'Pierre Lairez,',
	 'category': u'Computer Science ',
	 'date': '2014-4-20',
	 'pdflink': u'http://arxiv.org/pdf/1404.5069',
	 'subjects': u'Symbolic Computation (cs.SC)',
	 'title': u'\nComputing periods of rational integrals',
	 'urllink': u'http://arxiv.org/abs/1404.5069'}
2015-03-23 20:34:32+0000 [xxu461000] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1409.1716> (referer: http://arxiv.org/list/cs/14?skip=10000&show=1000)
2015-03-23 20:34:32+0000 [xxu461000] DEBUG: Scraped from <200 http://arxiv.org/abs/1409.1716>
	{'abstract': u"Human mobility is highly predictable. Individuals tend to only visit a few locations with high frequency, and to move among them in a certain sequence reflecting their habits and daily routine. This predictability has to be taken into account in the design of location privacy preserving mechanisms (LPPMs) in order to effectively protect users when they continuously expose their position to location-based services (LBSs). In this paper, we describe a method for creating LPPMs that are customized for a user's mobility profile taking into account privacy and quality of service requirements. By construction, our LPPMs take into account the sequential correlation across the user's exposed locations, providing the maximum possible trajectory privacy, i.e., privacy for the user's present location, as well as past and expected future locations. Moreover, our LPPMs are optimal against a strategic adversary, i.e., an attacker that implements the strongest inference attack knowing both the LPPM operation and the user's mobility profile. The optimality of the LPPMs in the context of trajectory privacy is a novel contribution, and it is achieved by formulating the LPPM design problem as a Bayesian Stackelberg game between the user and the adversary. An additional benefit of our formal approach is that the design parameters of the LPPM are chosen by the optimization algorithm.",
	 'authors': u'George Theodorakopoulos, Reza Shokri, Carmela Troncoso, Jean-Pierre Hubaux, Jean-Yves Le Boudec,',
	 'category': u'Computer Science ',
	 'date': '2014-9-5',
	 'pdflink': u'http://arxiv.org/pdf/1409.1716',
	 'subjects': u'Cryptography and Security (cs.CR)',
	 'title': u'\nProlonging the Hide-and-Seek Game: Optimal Trajectory Privacy for  Location-Based Services',
	 'urllink': u'http://arxiv.org/abs/1409.1716'}
2015-03-23 20:34:38+0000 [xxu461000] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1406.7564> (referer: http://arxiv.org/list/cs/14?skip=7000&show=1000)
2015-03-23 20:34:38+0000 [xxu461000] DEBUG: Scraped from <200 http://arxiv.org/abs/1406.7564>
	{'abstract': u"Social learning -by observing and copying others- is a highly successful cultural mechanism for adaptation, outperforming individual information acquisition and experience. Here, we investigate social learning in the context of the uniquely human capacity for reflective, analytical reasoning. A hallmark of the human mind is our ability to engage analytical reasoning, and suppress false associative intuitions. Through a set of lab-based network experiments, we find that social learning fails to propagate this cognitive strategy. When people make false intuitive conclusions, and are exposed to the analytic output of their peers, they recognize and adopt this correct output. But they fail to engage analytical reasoning in similar subsequent tasks. Thus, humans exhibit an 'unreflective copying bias,' which limits their social learning to the output, rather than the process, of their peers' reasoning -even when doing so requires minimal effort and no technical skill. In contrast to much recent work on observation-based social learning, which emphasizes the propagation of successful behavior through copying, our findings identify a limit on the power of social networks in situations that require analytical reasoning.",
	 'authors': u'Iyad Rahwan, Dmytro Krasnoshtan, Azim Shariff, Jean-Francois Bonnefon,',
	 'category': u'Computer Science ',
	 'date': '2014-6-29',
	 'pdflink': u'http://arxiv.org/pdf/1406.7564',
	 'subjects': u'Social and Information Networks (cs.SI)',
	 'title': u'\nAnalytical reasoning task reveals limits of social learning in networks',
	 'urllink': u'http://arxiv.org/abs/1406.7564'}
2015-03-23 20:34:42+0000 [xxu461000] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1405.2029> (referer: http://arxiv.org/list/cs/14?skip=5000&show=1000)
2015-03-23 20:34:42+0000 [xxu461000] DEBUG: Scraped from <200 http://arxiv.org/abs/1405.2029>
	{'abstract': u'Advanced channel decoders rely on soft-decision decoder inputs for which mutual information (MI) is the natural figure of merit. In this paper, we analyze an optical fiber system by evaluating MI as the maximum achievable rate of transmission of such a system. MI is estimated by means of histograms for which the correct bin number is determined in a blind way. The MI estimate obtained this way shows excellent accuracy in comparison with the true MI of 16-state quadrature amplitude modulation (QAM) over an additive white Gaussian noise channel with additional phase noise, which is a simplified model of a nonlinear optical fiber channel. We thereby justify to use the MI estimation method to accurately estimate the MI of an optical fiber system. In the second part of this work, a transoceanic fiber system with 6000 km of standard single-mode fiber is simulated and its MI determined. Among rectangular QAMs, 16-QAM is found to be the optimal modulation scheme for this link as to performance in terms of MI and requirements on components and digital signal processing. For the reported MI of 3.1 bits/symbol, a minimum coding overhead of 29% is required when the channel memory is not taken into account. By employing ideal single-channel digital back-propagation, an increase in MI by 0.25 bits/symbol and 0.28 bits/symbol is reported for 16-QAM and 64-QAM, respectively, lowering the required overhead to 19% and 16%. When the channel spacing is decreased to be close to the Nyquist rate, the dual-polarization spectral efficiency is 5.7 bits/s/Hz, an increase of more than 2 bits/symbol compared to a 50 GHz spacing.',
	 'authors': u'Tobias Fehenberger, Norbert Hanik,',
	 'category': u'Computer Science ',
	 'date': '2014-4-10',
	 'pdflink': u'http://arxiv.org/pdf/1405.2029',
	 'subjects': u'Information Theory (cs.IT)',
	 'title': u'\nMutual Information as a Figure of Merit for Optical Fiber Systems',
	 'urllink': u'http://arxiv.org/abs/1405.2029'}
2015-03-23 20:34:44+0000 [xxu461000] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1405.6015> (referer: http://arxiv.org/list/cs/14?skip=6000&show=1000)
2015-03-23 20:34:44+0000 [xxu461000] DEBUG: Scraped from <200 http://arxiv.org/abs/1405.6015>
	{'abstract': u'The concepts of quantum correlation complexity and quantum communication complexity were recently proposed to quantify the minimum amount of resources needed in generating bipartite classical or quantum states in the single-shot setting. The former is the minimum size of the initially shared state on which local operations by the two parties (without communication) can generate the target state , and the latter is the minimum amount of communication needed when initially sharing nothing. In this paper, we generalize these two concepts to multipartite cases, for both exact and approximate state generation. Our results are summarized as follows. (1) For multipartite pure states, the correlation complexity can be completely characterized by local ranks of sybsystems. (2) We extend the notion of PSD-rank of matrices to that of tensors, and use it to bound the quantum correlation complexity for generating multipartite classical distributions. (3) For generating multipartite mixed quantum states, communication complexity is not always equal to correlation complexity (as opposed to bipartite case). But they differ by at most a factor of 2. Generating a multipartite mixed quantum state has the same communication complexity as generating its optimal purification. But for correlation complexity of these two tasks can be different (though still related by less than a factor of 2). (4) To generate a bipartite classical distribution approximately, the quantum communication complexity is completely characterized by the approximate PSD-rank of . The quantum correlation complexity of approximately generating multipartite pure states is bounded by approximate local ranks.',
	 'authors': u'Rahul Jain, Zhaohui Wei, Penghui Yao, Shengyu Zhang,',
	 'category': u'Computer Science ',
	 'date': '2014-5-23',
	 'pdflink': u'http://arxiv.org/pdf/1405.6015',
	 'subjects': u'Quantum Physics (quant-ph)',
	 'title': u'\nMultipartite Quantum Correlation and Communication Complexities',
	 'urllink': u'http://arxiv.org/abs/1405.6015'}
2015-03-23 20:34:48+0000 [xxu461000] INFO: Crawled 503 pages (at 12 pages/min), scraped 486 items (at 12 items/min)
2015-03-23 20:34:49+0000 [xxu461000] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1407.4945> (referer: http://arxiv.org/list/cs/14?skip=8000&show=1000)
2015-03-23 20:34:49+0000 [xxu461000] DEBUG: Scraped from <200 http://arxiv.org/abs/1407.4945>
	{'abstract': u'Viral marketing is becoming important due to the popularity of online social networks (OSNs). Companies may provide incentives (e.g., via free samples of a product) to a small group of users in an OSN, and these users provide recommendations to their friends, which eventually increases the overall sales of a given product. Nevertheless, this also opens a door for "malicious behaviors": dishonest users may intentionally give misleading recommendations to their friends so as to distort the normal sales distribution. In this paper, we propose a detection framework to identify dishonest users in OSNs. In particular, we present a set of fully distributed and randomized algorithms, and also quantify the performance of the algorithms by deriving probability of false positive, probability of false negative, and the distribution of number of detection rounds. Extensive simulations are also carried out to illustrate the impact of misleading recommendations and the effectiveness of our detection algorithms. The methodology we present here will enhance the security level of viral marketing in OSNs.',
	 'authors': u'Yongkun Li, John C. S. Lui,',
	 'category': u'Computer Science ',
	 'date': '2014-7-18',
	 'pdflink': u'http://arxiv.org/pdf/1407.4945',
	 'subjects': u'Social and Information Networks (cs.SI)',
	 'title': u'\nFriends or Foes: Distributed and Randomized Algorithms to Determine  Dishonest Recommenders in Online Social Networks',
	 'urllink': u'http://arxiv.org/abs/1407.4945'}
2015-03-23 20:34:55+0000 [xxu461000] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1404.5068> (referer: http://arxiv.org/list/cs/14?skip=4000&show=1000)
2015-03-23 20:34:55+0000 [xxu461000] DEBUG: Scraped from <200 http://arxiv.org/abs/1404.5068>
	{'abstract': u'The acute disparity between increasing bandwidth demand and available spectrum, has brought millimeter wave (mmW) bands to the forefront of candidate solutions for the next-generation cellular networks. Highly directional transmissions are essential for cellular communication in these frequencies to compensate for high isotropic path loss. This reliance on directional beamforming, however, complicates initial cell search since the mobile and base station must jointly search over a potentially large angular directional space to locate a suitable path to initiate communication. To address this problem, this paper proposes a directional cell discovery procedure where base stations periodically transmit synchronization signals, potentially in time-varying random directions, to scan the angular space. Detectors for these signals are derived based on a Generalized Likelihood Ratio Test (GLRT) under various signal and receiver assumptions. The detectors are then simulated under realistic design parameters and channels based on actual experimental measurements at 28~GHz in New York City. The study reveals two key findings: (i) digital beamforming can significantly outperform analog beamforming even when the digital beamforming uses very low quantization to compensate for the additional power requirements; and (ii) omni-directional transmissions of the synchronization signals from the base station generally outperforms random directional scanning.',
	 'authors': u'C. Nicolas Barati, S. Amir Hosseini, Sundeep Rangan, Pei Liu, Thanasis Korakis, Shivendra S. Panwar, Theodore S. Rappaport,',
	 'category': u'Computer Science ',
	 'date': '2014-4-20',
	 'pdflink': u'http://arxiv.org/pdf/1404.5068',
	 'subjects': u'Information Theory (cs.IT)',
	 'title': u'\nDirectional Cell Discovery in Millimeter Wave Cellular Networks',
	 'urllink': u'http://arxiv.org/abs/1404.5068'}
2015-03-23 20:34:59+0000 [xxu461000] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1409.1715> (referer: http://arxiv.org/list/cs/14?skip=10000&show=1000)
2015-03-23 20:34:59+0000 [xxu461000] DEBUG: Scraped from <200 http://arxiv.org/abs/1409.1715>
	{'abstract': u'The balance of exploration versus exploitation (EvE) is a key issue on evolutionary computation. In this paper we will investigate how an adaptive controller aimed to perform Operator Selection can be used to dynamically manage the EvE balance required by the search, showing that the search strategies determined by this control paradigm lead to an improvement of solution quality found by the evolutionary algorithm.',
	 'authors': u'Giacomo di Tollo, Fr\xe9d\xe9ric Lardeux, Jorge Maturana, Fr\xe9d\xe9ric Saubion,',
	 'category': u'Computer Science ',
	 'date': '2014-9-5',
	 'pdflink': u'http://arxiv.org/pdf/1409.1715',
	 'subjects': u'Neural and Evolutionary Computing (cs.NE)',
	 'title': u'\nAn Experimental Study of Adaptive Control for Evolutionary Algorithms',
	 'urllink': u'http://arxiv.org/abs/1409.1715'}
2015-03-23 20:35:04+0000 [xxu461000] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1406.7563> (referer: http://arxiv.org/list/cs/14?skip=7000&show=1000)
2015-03-23 20:35:04+0000 [xxu461000] DEBUG: Scraped from <200 http://arxiv.org/abs/1406.7563>
	{'abstract': u'Numerous studies and anecdotes demonstrate the "wisdom of the crowd," the surprising accuracy of a group\'s aggregated judgments. Less is known, however, about the generality of crowd wisdom. For example, are crowds wise even if their members have systematic judgmental biases, or can influence each other before members render their judgments? If so, are there situations in which we can expect a crowd to be less accurate than skilled individuals? We provide a precise but general definition of crowd wisdom: A crowd is wise if a linear aggregate, for example a mean, of its members\' judgments is closer to the target value than a randomly, but not necessarily uniformly, sampled member of the crowd. Building on this definition, we develop a theoretical framework for examining, a priori, when and to what degree a crowd will be wise. We systematically investigate the boundary conditions for crowd wisdom within this framework and determine conditions under which the accuracy advantage for crowds is maximized. Our results demonstrate that crowd wisdom is highly robust: Even if judgments are biased and correlated, one would need to nearly deterministically select only a highly skilled judge before an individual\'s judgment could be expected to be more accurate than a simple averaging of the crowd. Our results also provide an accuracy rationale behind the need for diversity of judgments among group members. Contrary to folk explanations of crowd wisdom which hold that judgments should ideally be independent so that errors cancel out, we find that crowd wisdom is maximized when judgments systematically differ as much as possible. We re-analyze data from two published studies that confirm our theoretical results.',
	 'authors': u'Clintin Davis-Stober, David Budescu, Jason Dana, Stephen Broomell,',
	 'category': u'Computer Science ',
	 'date': '2014-6-29',
	 'pdflink': u'http://arxiv.org/pdf/1406.7563',
	 'subjects': u'Social and Information Networks (cs.SI)',
	 'title': u'\nWhen is a crowd wise?',
	 'urllink': u'http://arxiv.org/abs/1406.7563'}
2015-03-23 20:35:09+0000 [xxu461000] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1405.2017> (referer: http://arxiv.org/list/cs/14?skip=5000&show=1000)
2015-03-23 20:35:09+0000 [xxu461000] DEBUG: Scraped from <200 http://arxiv.org/abs/1405.2017>
	{'abstract': u'Device-to-device (D2D) communication enables the user equipments (UEs) located in close proximity to bypass the cellular base stations (BSs) and directly connect to each other, and thereby, offload traffic from the cellular infrastructure. D2D communication can improve spatial frequency reuse and energy efficiency in cellular networks. This paper presents a comprehensive and tractable analytical framework for D2D-enabled uplink cellular networks with a flexible mode selection scheme along with truncated channel inversion power control. Different from the existing mode selection schemes where the decision on mode selection is made based only on the D2D link distance (i.e., distance between two UEs using D2D mode of communication), the proposed mode selection scheme for a UE accounts for both the D2D link distance and cellular link distance (i.e., distance between the UE and the BS). The developed framework is used to analyze and understand how the underlaying D2D communication affects the cellular network performance. Through comprehensive numerical analysis, we investigate the expected performance gains and provide guidelines for selecting the network parameters.',
	 'authors': u'Hesham ElSawy, Ekram Hossain,',
	 'category': u'Computer Science ',
	 'date': '2014-5-8',
	 'pdflink': u'http://arxiv.org/pdf/1405.2017',
	 'subjects': u'Networking and Internet Architecture (cs.NI)',
	 'title': u'\nAnalytical Modeling of Mode Selection and Power Control for Underlay D2D  Communication in Cellular Networks',
	 'urllink': u'http://arxiv.org/abs/1405.2017'}
2015-03-23 20:35:13+0000 [xxu461000] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1405.6009> (referer: http://arxiv.org/list/cs/14?skip=6000&show=1000)
2015-03-23 20:35:13+0000 [xxu461000] DEBUG: Scraped from <200 http://arxiv.org/abs/1405.6009>
	{'abstract': u'Although a number of models have been developed to investigate the emergence of culture and evolutionary phases in social systems, one important aspect has not yet been sufficiently emphasized. This is the structure of the underlaying network of social relations serving as channels in transmitting cultural traits, which is expected to play a crucial role in the evolutionary processes in social systems. In this paper we contribute to the understanding of the role of the network structure by developing a layered ego-centric network structure based model, inspired by the social brain hypothesis, to study transmission of cultural traits and their evolution in social network. For this model we first find analytical results in the spirit of mean-field approximation and then to validate the results we compare them with the results of extensive numerical simulations.',
	 'authors': u'Vasyl Palchykov, Kimmo Kaski, Janos Kert\xe9sz,',
	 'category': u'Computer Science ',
	 'date': '2014-5-23',
	 'pdflink': u'http://arxiv.org/pdf/1405.6009',
	 'subjects': u'Physics and Society (physics.soc-ph)',
	 'title': u'\nTransmission of cultural traits in layered ego-centric networks',
	 'urllink': u'http://arxiv.org/abs/1405.6009'}
2015-03-23 20:35:18+0000 [xxu461000] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1407.4937> (referer: http://arxiv.org/list/cs/14?skip=8000&show=1000)
2015-03-23 20:35:18+0000 [xxu461000] DEBUG: Scraped from <200 http://arxiv.org/abs/1407.4937>
	{'abstract': u'The idea of synthesis, i.e., the process of automatically computing implementations from their specifications, has recently gained a lot of momentum in the contexts of software engineering and reactive system design. While it is widely believed that, due to complexity/undecidability issues, synthesis cannot completely replace manual engineering, it can assist the process of designing the intricate pieces of code that most programmers find challenging, or help with orchestrating tasks in reactive environments. The SYNT workshop aims to bring together researchers interested in synthesis to discuss and present ongoing and mature work on all aspects of automated synthesis and its applications. The third iteration of the workshop took place in Vienna, Austria, and was co-located with the 26th International Conference on Computer Aided Verification, held in the context of the Vienna Summer of Logic in July 2014. The workshop included eight contributed talks and four invited talks. In addition, it featured a special session about the Syntax-Guided Synthesis Competition (SyGuS) and the SyntComp Synthesis competition.',
	 'authors': u'Krishnendu Chatterjee, R\xfcdiger Ehlers, Susmit Jha,',
	 'category': u'Computer Science ',
	 'date': '2014-7-18',
	 'pdflink': u'http://arxiv.org/html/1407.4937',
	 'subjects': u'Logic in Computer Science (cs.LO)',
	 'title': u'\nProceedings 3rd Workshop on Synthesis',
	 'urllink': u'http://arxiv.org/abs/1407.4937'}
2015-03-23 20:35:22+0000 [xxu461000] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1404.5065> (referer: http://arxiv.org/list/cs/14?skip=4000&show=1000)
2015-03-23 20:35:22+0000 [xxu461000] DEBUG: Scraped from <200 http://arxiv.org/abs/1404.5065>
	{'abstract': u'Multi-target regression is concerned with the simultaneous prediction of multiple continuous target variables based on the same set of input variables. It arises in several interesting industrial and environmental application domains, such as ecological modelling and energy forecasting. This paper presents an ensemble method for multi-target regression that constructs new target variables via random linear combinations of existing targets. We discuss the connection of our approach with multi-label classification algorithms, in particular RAEL, which originally inspired this work, and a family of recent multi-label classification algorithms that involve output coding. Experimental results on 12 multi-target datasets show that it performs significantly better than a strong baseline that learns a single model for each target using gradient boosting and compares favourably to multi-objective random forest approach, which is a state-of-the-art approach. The experiments further show that our approach improves more when stronger unconditional dependencies exist among the targets.',
	 'authors': u'Grigorios Tsoumakas, Eleftherios Spyromitros-Xioufis, Aikaterini Vrekou, Ioannis Vlahavas,',
	 'category': u'Computer Science ',
	 'date': '2014-4-20',
	 'pdflink': u'http://arxiv.org/pdf/1404.5065',
	 'subjects': u'Learning (cs.LG)',
	 'title': u'\nMulti-Target Regression via Random Linear Target Combinations',
	 'urllink': u'http://arxiv.org/abs/1404.5065'}
2015-03-23 20:35:28+0000 [xxu461000] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1409.1699> (referer: http://arxiv.org/list/cs/14?skip=10000&show=1000)
2015-03-23 20:35:28+0000 [xxu461000] DEBUG: Scraped from <200 http://arxiv.org/abs/1409.1699>
	{'abstract': u'In order to help children with dyslalia we created a set of software exercises. This set has a unitary software block (data base, programming language, programming philosophy). In this paper we present this software infrastructure with its advantage and disadvantage. The exercises are part of a software system named LOGOMON. Therefore, besides horizontal compatibilities (between exercises) vertical compatibilities are also presented (with LOGOMON system). Concerning database tables used for modulus of exercises, a part of them is "inherited" from LOGOMON application and another are specific for exercises application. We also need to specify that there were necessary minor changes of database tables used by LOGOMON. As programming language we used C#, implemented in Visual Studio 2005. We developed specific interfaces elements and classes. We also used multimedia resources that were necessary for exercises (images, correct pronouncing obtained from speech therapist recording, video clips). Another section of this application is related to loading of exercises on mobile devices (Pocket PC). A part of code has been imported directly, but there were a lot of files that need to be rewritten. Anyway, the multimedia resources were used without any processing.',
	 'authors': u'Cristian-Eduard Belciug, Ovidiu-Andrei Schipor, Mirela Danubianu,',
	 'category': u'Computer Science ',
	 'date': '2014-9-5',
	 'pdflink': u'http://arxiv.org/pdf/1409.1699',
	 'subjects': u'Other Computer Science (cs.OH)',
	 'title': u'\nExercises for Children with Dyslalia-Software Infrastructure',
	 'urllink': u'http://arxiv.org/abs/1409.1699'}
2015-03-23 20:35:32+0000 [xxu461000] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1406.7562> (referer: http://arxiv.org/list/cs/14?skip=7000&show=1000)
2015-03-23 20:35:32+0000 [xxu461000] DEBUG: Scraped from <200 http://arxiv.org/abs/1406.7562>
	{'abstract': u'During social interactions, groups develop collective competencies that (ideally) should assist groups to outperform average standalone individual members (weak cognitive synergy) or the best performing member in the group (strong cognitive synergy). In two experimental studies we manipulate the type of decision rule used in group decision-making (identify the best vs. collaborative), and the way in which the decision rules are induced (direct vs. analogical) and we test the effect of these two manipulations on the emergence of strong and weak cognitive synergy. Our most important results indicate that an analogically induced decision rule (imitate-the-successful heuristic) in which groups have to identify the best member and build on his/her performance (take-the-best heuristic) is the most conducive for strong cognitive synergy. Our studies bring evidence for the role of analogy-making in groups as well as the role of fast-and-frugal heuristics for group decision-making.',
	 'authors': u'Nicoleta Meslec, Petru Curseu, Marius Meeus, Oana Fodor,',
	 'category': u'Computer Science ',
	 'date': '2014-6-29',
	 'pdflink': u'http://arxiv.org/pdf/1406.7562',
	 'subjects': u'Social and Information Networks (cs.SI)',
	 'title': u'\nWhen none of us perform better than all of us together: the role of  analogical decision rules in groups',
	 'urllink': u'http://arxiv.org/abs/1406.7562'}
2015-03-23 20:35:37+0000 [xxu461000] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1405.2013> (referer: http://arxiv.org/list/cs/14?skip=5000&show=1000)
2015-03-23 20:35:37+0000 [xxu461000] DEBUG: Scraped from <200 http://arxiv.org/abs/1405.2013>
	{'abstract': u'While cognitive radio enables spectrum-efficient wireless communication, radio frequency (RF) energy harvesting from ambient interference is an enabler for energy-efficient wireless communication. In this paper, we model and analyze cognitive and energy harvesting-based device-to-device (D2D) communication in cellular networks.',
	 'authors': u'Ahmed Hamdi Sakr, Ekram Hossain,',
	 'category': u'Computer Science ',
	 'date': '2014-5-8',
	 'pdflink': u'http://arxiv.org/pdf/1405.2013',
	 'subjects': u'Networking and Internet Architecture (cs.NI)',
	 'title': u'\nCognitive and Energy Harvesting-Based D2D Communication in Cellular  Networks: Stochastic Geometry Modeling and Analysis',
	 'urllink': u'http://arxiv.org/abs/1405.2013'}
2015-03-23 20:35:41+0000 [xxu461000] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1405.5978> (referer: http://arxiv.org/list/cs/14?skip=6000&show=1000)
2015-03-23 20:35:41+0000 [xxu461000] DEBUG: Scraped from <200 http://arxiv.org/abs/1405.5978>
	{'abstract': u'The article presents several approaches to the blockmodeling of multilevel network data. Multilevel network data consist of networks that are measured on at least two levels (e.g. between organizations and people) and information on ties between those levels (e.g. information on which people are members of which organizations). Several approaches will be considered: a separate analysis of the levels; transforming all networks to one level and blockmodeling on this level using information from all levels; and a truly multilevel approach where all levels and ties among them are modeled at the same time. Advantages and disadvantages of these approaches will be discussed.',
	 'authors': u'Ale\u0161 \u017diberna,',
	 'category': u'Computer Science ',
	 'date': '2014-5-23',
	 'pdflink': u'http://arxiv.org/pdf/1405.5978',
	 'subjects': u'Methodology (stat.ME)',
	 'title': u'\nBlockmodeling of multilevel networks',
	 'urllink': u'http://arxiv.org/abs/1405.5978'}
2015-03-23 20:35:44+0000 [xxu461000] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1407.4923> (referer: http://arxiv.org/list/cs/14?skip=8000&show=1000)
2015-03-23 20:35:44+0000 [xxu461000] DEBUG: Scraped from <200 http://arxiv.org/abs/1407.4923>
	{'abstract': u'Previous research about sensor based attacks on Android platform focused mainly on accessing or controlling over sensitive device components, such as camera, microphone and GPS. These approaches get data from sensors directly and need corresponding sensor invoking permissions. This paper presents a novel approach (GVS-Attack) to launch permission bypassing attacks from a zero permission Android application (VoicEmployer) through the speaker. The idea of GVS-Attack utilizes an Android system built-in voice assistant module -- Google Voice Search. Through Android Intent mechanism, VoicEmployer triggers Google Voice Search to the foreground, and then plays prepared audio files (like "call number 1234 5678") in the background. Google Voice Search can recognize this voice command and execute corresponding operations. With ingenious designs, our GVS-Attack can forge SMS/Email, access privacy information, transmit sensitive data and achieve remote control without any permission. Also we found a vulnerability of status checking in Google Search app, which can be utilized by GVS-Attack to dial arbitrary numbers even when the phone is securely locked with password. A prototype of VoicEmployer has been implemented to demonstrate the feasibility of GVS-Attack in real world. In theory, nearly all Android devices equipped with Google Services Framework can be affected by GVS-Attack. This study may inspire application developers and researchers rethink that zero permission doesn\'t mean safety and the speaker can be treated as a new attack surface.',
	 'authors': u'Wenrui Diao, Xiangyu Liu, Zhe Zhou, Kehuan Zhang,',
	 'category': u'Computer Science ',
	 'date': '2014-7-18',
	 'pdflink': u'http://arxiv.org/pdf/1407.4923',
	 'subjects': u'Cryptography and Security (cs.CR)',
	 'title': u'\nYour Voice Assistant is Mine: How to Abuse Speakers to Steal Information  and Control Your Phone',
	 'urllink': u'http://arxiv.org/abs/1407.4923'}
2015-03-23 20:35:48+0000 [xxu461000] INFO: Crawled 516 pages (at 13 pages/min), scraped 499 items (at 13 items/min)
2015-03-23 20:35:49+0000 [xxu461000] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1404.5062> (referer: http://arxiv.org/list/cs/14?skip=4000&show=1000)
2015-03-23 20:35:49+0000 [xxu461000] DEBUG: Scraped from <200 http://arxiv.org/abs/1404.5062>
	{'abstract': u'This paper deals with combination of two modern engineering methods in order to optimise the shape of a representative casting product. The product being analysed is a sling, which is used to attach pulling rope in timber transportation. The first step was 3D modelling and static stress/strain analysis using CAD/CAE software NX4. The slinger shape optimization was performed using Traction method, by means of software Optishape-TS. To define constraints for shape optimization, FEA software FEMAP was used. The mould pattern with optimized 3D shape was then prepared using Fused Deposition Modelling (FDM) Rapid prototyping method. The sling mass decreased by 20%, while signifficantly better stress distribution was achieved, with maximum stress 3.5 times less than initial value. The future researches should use 3D scanning technology in order to provide more accurate 3D model of initial part. Results of this research can be used by toolmakers in order to engage FEA/RP technology to design and manufacture lighter products with acceptable stress distribution.',
	 'authors': u'Nermina Zaimovic-Uzunovic, Samir Lemes, Damir Curic, Alan Topcic,',
	 'category': u'Computer Science ',
	 'date': '2014-4-20',
	 'pdflink': u'http://arxiv.org/pdf/1404.5062',
	 'subjects': u'Computational Engineering, Finance, and Science (cs.CE)',
	 'title': u'\nRapid prototyping for sling design optimization',
	 'urllink': u'http://arxiv.org/abs/1404.5062'}
2015-03-23 20:35:55+0000 [xxu461000] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1409.1695> (referer: http://arxiv.org/list/cs/14?skip=10000&show=1000)
2015-03-23 20:35:55+0000 [xxu461000] DEBUG: Scraped from <200 http://arxiv.org/abs/1409.1695>
	{'abstract': u'We introduce a new concept called scalability to adaptive control in this paper. In particular, we analyze how to scale learning rates of adaptive weight update laws of various adaptive control schemes with respect to given command profiles to achieve a predictable closed-loop response. An illustrative numerical example is provided to demonstrate the proposed concept, which emphasize that it can be an effective tool for validation and verification of adaptive controllers.',
	 'authors': u'Simon P. Schatz, Tansel Yucelen,',
	 'category': u'Computer Science ',
	 'date': '2014-9-5',
	 'pdflink': u'http://arxiv.org/pdf/1409.1695',
	 'subjects': u'Systems and Control (cs.SY)',
	 'title': u'\nScalability Concept for Predictable Closed-Loop Response of Adaptive  Controllers',
	 'urllink': u'http://arxiv.org/abs/1409.1695'}
2015-03-23 20:36:00+0000 [xxu461000] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1406.7561> (referer: http://arxiv.org/list/cs/14?skip=7000&show=1000)
2015-03-23 20:36:00+0000 [xxu461000] DEBUG: Scraped from <200 http://arxiv.org/abs/1406.7561>
	{'abstract': u'The emergence and ongoing development of Web 2.0 technologies have enabled new and advanced forms of collective intelligence at unprecedented scales, allowing large numbers of individuals to act collectively and create high quality intellectual artifacts. However, little is known about how and when they indeed promote collective intelligence. In this manuscript, we provide a survey of the automated tools developed to analyze discourse-centric collective intelligence. By conducting a thematic analysis of the current research direction, a set of gaps and limitations are identified.',
	 'authors': u'Taraneh Khazaei, Lu Xiao,',
	 'category': u'Computer Science ',
	 'date': '2014-6-29',
	 'pdflink': u'http://arxiv.org/pdf/1406.7561',
	 'subjects': u'Social and Information Networks (cs.SI)',
	 'title': u'\nCollective intelligence in Massive Online Dialogues',
	 'urllink': u'http://arxiv.org/abs/1406.7561'}
2015-03-23 20:36:06+0000 [xxu461000] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1405.2011> (referer: http://arxiv.org/list/cs/14?skip=5000&show=1000)
2015-03-23 20:36:06+0000 [xxu461000] DEBUG: Scraped from <200 http://arxiv.org/abs/1405.2011>
	{'abstract': u'We present new distributed algorithms for constructing a Steiner Forest in the CONGEST model. Our deterministic algorithm finds, for any given constant , a -approximation in rounds, where is the shortest path diameter, is the number of terminals, is the number of terminal components in the input, and is the number of nodes. Our randomized algorithm finds, with high probability, an - approximation in time , where is the unweighted diameter of the network. We also prove a matching lower bound of on the running time of any distributed approximation algorithm for the Steiner Forest problem. Previous algorithms were randomized, and obtained either an -approximation in time, or an -approximation in time.',
	 'authors': u'Christoph Lenzen, Boaz Patt-Shamir,',
	 'category': u'Computer Science ',
	 'date': '2014-5-8',
	 'pdflink': u'http://arxiv.org/pdf/1405.2011',
	 'subjects': u'Distributed, Parallel, and Cluster Computing (cs.DC)',
	 'title': u'\nImproved Distributed Steiner Forest Construction',
	 'urllink': u'http://arxiv.org/abs/1405.2011'}
2015-03-23 20:36:11+0000 [xxu461000] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1405.5873> (referer: http://arxiv.org/list/cs/14?skip=6000&show=1000)
2015-03-23 20:36:11+0000 [xxu461000] DEBUG: Scraped from <200 http://arxiv.org/abs/1405.5873>
	{'abstract': u'Real-world data typically contain repeated and periodic patterns. This suggests that they can be effectively represented and compressed using only a few coefficients of an appropriate basis (e.g., Fourier, Wavelets, etc.). However, distance estimation when the data are represented using different sets of coefficients is still a largely unexplored area. This work studies the optimization problems related to obtaining the emph lower/upper bound on Euclidean distances when each data object is potentially compressed using a different set of orthonormal coefficients. Our technique leads to tighter distance estimates, which translates into more accurate search, learning and mining operations textit in the compressed domain. We formulate the problem of estimating lower/upper distance bounds as an optimization problem. We establish the properties of optimal solutions, and leverage the theoretical analysis to develop a fast algorithm to obtain an emph solution to the problem. The suggested solution provides the tightest estimation of the -norm or the correlation. We show that typical data-analysis operations, such as k-NN search or k-Means clustering, can operate more accurately using the proposed compression and distance reconstruction technique. We compare it with many other prevalent compression and reconstruction techniques, including random projections and PCA-based techniques. We highlight a surprising result, namely that when the data are highly sparse in some basis, our technique may even outperform PCA-based compression. The contributions of this work are generic as our methodology is applicable to any sequential or high-dimensional data as well as to any orthogonal data transformation used for the underlying data compression scheme.',
	 'authors': u'Michail Vlachos, Nikolaos Freris, Anastasios Kyrillidis,',
	 'category': u'Computer Science ',
	 'date': '2014-5-22',
	 'pdflink': u'http://arxiv.org/pdf/1405.5873',
	 'subjects': u'Machine Learning (stat.ML)',
	 'title': u'\nCompressive Mining: Fast and Optimal Data Mining in the Compressed  Domain',
	 'urllink': u'http://arxiv.org/abs/1405.5873'}
2015-03-23 20:36:15+0000 [xxu461000] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1407.4917> (referer: http://arxiv.org/list/cs/14?skip=8000&show=1000)
2015-03-23 20:36:15+0000 [xxu461000] DEBUG: Scraped from <200 http://arxiv.org/abs/1407.4917>
	{'abstract': u'Backward slicing has been used extensively in program understanding, debugging and scaling up of program analysis. For large programs, the size of the conventional backward slice is about 25% of the program size. This may be too large to be useful. Our investigations reveal that in general, the size of a slice is influenced more by computations governing the control flow reaching the slicing criterion than by the computations governing the values relevant to the slicing criterion. We distinguish between the two by defining data slices and control slices both of which are smaller than the conventional slices which can be obtained by combining the two. This is useful because for many applications, the individual data or control slices are sufficient. Our experiments show that for more than 50% of cases, the data slice is smaller than 10% of the program in size. Besides, the time to compute data or control slice is comparable to that for computing the conventional slice.',
	 'authors': u'Shrawan Kumar, Amitabha Sanyal, Uday Khedker,',
	 'category': u'Computer Science ',
	 'date': '2014-7-18',
	 'pdflink': u'http://arxiv.org/pdf/1407.4917',
	 'subjects': u'Software Engineering (cs.SE)',
	 'title': u'\nSliced Slices: Separating Data and Control Influences',
	 'urllink': u'http://arxiv.org/abs/1407.4917'}
2015-03-23 20:36:19+0000 [xxu461000] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1404.5060> (referer: http://arxiv.org/list/cs/14?skip=4000&show=1000)
2015-03-23 20:36:19+0000 [xxu461000] DEBUG: Scraped from <200 http://arxiv.org/abs/1404.5060>
	{'abstract': u'In this paper, the problem of writing on a dirty paper in the presence of jamming is examined. We consider an AWGN channel with an additive white Gaussian state and an additive adversarial jammer. The state is assumed to be known non-causally to the encoder and the jammer but not to the decoder. The capacity of the channel in the presence of a jammer is determined. A surprising result that this capacity is equal to the capacity of a relaxed version of the problem, where the state is also known non-causally to the decoder, is proved.',
	 'authors': u'Amitalok J. Budkuley, Bikash Kumar Dey, Vinod M. Prabhakaran,',
	 'category': u'Computer Science ',
	 'date': '2014-4-20',
	 'pdflink': u'http://arxiv.org/pdf/1404.5060',
	 'subjects': u'Information Theory (cs.IT)',
	 'title': u'\nWriting on a Dirty Paper in the presence of Jamming',
	 'urllink': u'http://arxiv.org/abs/1404.5060'}
2015-03-23 20:36:24+0000 [xxu461000] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1409.1694> (referer: http://arxiv.org/list/cs/14?skip=10000&show=1000)
2015-03-23 20:36:24+0000 [xxu461000] DEBUG: Scraped from <200 http://arxiv.org/abs/1409.1694>
	{'abstract': u'The longest common substring with -mismatches problem is to find, given two strings and , a longest substring of and of such that the Hamming distance between and is . We introduce a practical time and space solution for this problem, where and are the lengths of and , respectively. This algorithm can also be used to compute the matching statistics with -mismatches of and in time and space. Moreover, we also present a theoretical solution for the case which runs in time, assuming , and uses space, improving over the existing time and space bound of Babenko and Starikovskaya.',
	 'authors': u'Tomas Flouri, Emanuele Giaquinta, Kassian Kobert, Esko Ukkonen,',
	 'category': u'Computer Science ',
	 'date': '2014-9-5',
	 'pdflink': u'http://arxiv.org/pdf/1409.1694',
	 'subjects': u'Data Structures and Algorithms (cs.DS)',
	 'title': u'\nLongest common substrings with k mismatches',
	 'urllink': u'http://arxiv.org/abs/1409.1694'}
2015-03-23 20:36:29+0000 [xxu461000] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1406.7560> (referer: http://arxiv.org/list/cs/14?skip=7000&show=1000)
2015-03-23 20:36:29+0000 [xxu461000] DEBUG: Scraped from <200 http://arxiv.org/abs/1406.7560>
	{'abstract': u'Given the rapid proliferation of advanced information technologies, including the Internet, modern humans can easily access vast amount of socially transmitted information. Intuitively, this situation is isomorphic to some eusocial insects that are known to solve the exploration-exploitation dilemma collectively through information transfer (e.g., honeybees [Seeley et al., 1991]; and ants [Shaffer, Sasaki &amp; Pratt, 2013]). Yet, in contrast from the eusocial insects, whose colonies are composed of kin, human collective performance may be affected by an inherent free-rider problem [Bolton &amp; Harris, 1999; Kameda, Tsukasaki, Hastie &amp; Berg, 2011]. Specifically, in groups involving non-kin members, it is expected that free-riders, who allow others to search for better alternatives and then exploit their findings through social learning ("information scroungers"), will frequently appear, and consequently undermine the advantage of collective intelligence [Rogers, 1998; Kameda &amp; Nakanishi, 2003].',
	 'authors': u'Wataru Toyokawa, Hye-rin Kim, Tatsuya Kameda,',
	 'category': u'Computer Science ',
	 'date': '2014-6-29',
	 'pdflink': u'http://arxiv.org/pdf/1406.7560',
	 'subjects': u'Social and Information Networks (cs.SI)',
	 'title': u'\nLess-is-more in a 5-star rating system: an experimental study of human  combined decisions in a multi-armed bandit problem',
	 'urllink': u'http://arxiv.org/abs/1406.7560'}
2015-03-23 20:36:34+0000 [xxu461000] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1405.2000> (referer: http://arxiv.org/list/cs/14?skip=5000&show=1000)
2015-03-23 20:36:34+0000 [xxu461000] DEBUG: Scraped from <200 http://arxiv.org/abs/1405.2000>
	{'abstract': u'We present a joint sub-channel and power allocation framework for downlink transmission an orthogonal frequency-division multiple access (OFDMA)-based cellular network composed of a macrocell overlaid by small cells. In this framework, the resource allocation (RA) problems for both the macrocell and small cells are formulated as optimization problems. Numerical results confirm the performance gains of our proposed RA formulation for the macrocell over the traditional resource allocation based on minimizing the transmission power. Besides, it is shown that the formulation based on convex relaxation yields a similar behavior to the MINLP formulation. Also, the distributed solution converges to the same solution obtained by solving the corresponding convex optimization problem in a centralized fashion.',
	 'authors': u'Amr Abdelnasser, Ekram Hossain, Dong In Kim,',
	 'category': u'Computer Science ',
	 'date': '2014-5-8',
	 'pdflink': u'http://arxiv.org/pdf/1405.2000',
	 'subjects': u'Networking and Internet Architecture (cs.NI)',
	 'title': u'\nTier-Aware Resource Allocation in OFDMA Macrocell-Small Cell Networks',
	 'urllink': u'http://arxiv.org/abs/1405.2000'}
2015-03-23 20:36:39+0000 [xxu461000] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1405.5869> (referer: http://arxiv.org/list/cs/14?skip=6000&show=1000)
2015-03-23 20:36:39+0000 [xxu461000] DEBUG: Scraped from <200 http://arxiv.org/abs/1405.5869>
	{'abstract': u'We present the first provably sublinear time algorithm for approximate emph (MIPS). Our proposal is also the first hashing algorithm for searching with (un-normalized) inner product as the underlying similarity measure. Finding hashing schemes for MIPS was considered hard. We formally show that the existing Locality Sensitive Hashing (LSH) framework is insufficient for solving MIPS, and then we extend the existing LSH framework to allow asymmetric hashing schemes. Our proposal is based on an interesting mathematical phenomenon in which inner products, after independent asymmetric transformations, can be converted into the problem of approximate near neighbor search. This key observation makes efficient sublinear hashing scheme for MIPS possible. In the extended asymmetric LSH (ALSH) framework, we provide an explicit construction of provably fast hashing scheme for MIPS. The proposed construction and the extended LSH framework could be of independent theoretical interest. Our proposed algorithm is simple and easy to implement. We evaluate the method, for retrieving inner products, in the collaborative filtering task of item recommendations on Netflix and Movielens datasets.',
	 'authors': u'Anshumali Shrivastava, Ping Li,',
	 'category': u'Computer Science ',
	 'date': '2014-5-22',
	 'pdflink': u'http://arxiv.org/pdf/1405.5869',
	 'subjects': u'Machine Learning (stat.ML)',
	 'title': u'\nAsymmetric LSH (ALSH) for Sublinear Time Maximum Inner Product Search  (MIPS)',
	 'urllink': u'http://arxiv.org/abs/1405.5869'}
2015-03-23 20:36:44+0000 [xxu461000] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1407.4908> (referer: http://arxiv.org/list/cs/14?skip=8000&show=1000)
2015-03-23 20:36:44+0000 [xxu461000] DEBUG: Scraped from <200 http://arxiv.org/abs/1407.4908>
	{'abstract': u'Analyzing and working with big data could be very diffi cult using classical means like relational database management systems or desktop software packages for statistics and visualization. Instead, big data requires large clusters with hundreds or even thousands of computing nodes. Offi cial statistics is increasingly considering big data for deriving new statistics because big data sources could produce more relevant and timely statistics than traditional sources. One of the software tools successfully and wide spread used for storage and processing of big data sets on clusters of commodity hardware is Hadoop. Hadoop framework contains libraries, a distributed fi le-system (HDFS), a resource-management platform and implements a version of the MapReduce programming model for large scale data processing. In this paper we investigate the possibilities of integrating Hadoop with R which is a popular software used for statistical computing and data visualization. We present three ways of integrating them: R with Streaming, Rhipe and RHadoop and we emphasize the advantages and disadvantages of each solution.',
	 'authors': u'Bogdan Oancea, Raluca Mariana Dragoescu,',
	 'category': u'Computer Science ',
	 'date': '2014-7-18',
	 'pdflink': u'http://arxiv.org/pdf/1407.4908',
	 'subjects': u'Distributed, Parallel, and Cluster Computing (cs.DC)',
	 'title': u'\nIntegrating R and Hadoop for Big Data Analysis',
	 'urllink': u'http://arxiv.org/abs/1407.4908'}
2015-03-23 20:36:48+0000 [xxu461000] INFO: Crawled 528 pages (at 12 pages/min), scraped 511 items (at 12 items/min)
2015-03-23 20:36:50+0000 [xxu461000] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1404.5055> (referer: http://arxiv.org/list/cs/14?skip=4000&show=1000)
2015-03-23 20:36:50+0000 [xxu461000] DEBUG: Scraped from <200 http://arxiv.org/abs/1404.5055>
	{'abstract': u'We study correlated jamming in joint source-channel communication systems. An i.i.d. source is to be communicated over a memoryless channel in the presence of a correlated jammer with non-causal knowledge of user transmission. This user-jammer interaction is modeled as a zero sum game. A set of conditions on the source and the channel is provided for the existence of a Nash equilibrium for this game, where the user strategy is uncoded transmission and the jammer strategy is i.i.d jamming. This generalizes a well-known example of uncoded communication of a Gaussian sources over Gaussian channels with additive jamming. Another example, of a Binary Symmetric source over a Binary Symmetric channel with jamming, is provided as a validation of this result.',
	 'authors': u'Amitalok J. Budkuley, Bikash Kumar Dey, Vinod M. Prabhakaran,',
	 'category': u'Computer Science ',
	 'date': '2014-4-20',
	 'pdflink': u'http://arxiv.org/pdf/1404.5055',
	 'subjects': u'Information Theory (cs.IT)',
	 'title': u'\nCorrelated Jamming in a Joint Source Channel Communication System',
	 'urllink': u'http://arxiv.org/abs/1404.5055'}
2015-03-23 20:36:55+0000 [xxu461000] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1409.1693> (referer: http://arxiv.org/list/cs/14?skip=10000&show=1000)
2015-03-23 20:36:55+0000 [xxu461000] DEBUG: Scraped from <200 http://arxiv.org/abs/1409.1693>
	{'abstract': u'The development of ICT has emerged a new way of learning using electronic platforms: E-learning. In addition, pedagogical approaches have been adopted in teaching based on group learning, such as the project-based teaching. The project-based teaching is an active learning method, based on group work to develop skills and acquire knowledge. However, the group of students is facing several challenges throughout the project, such as the decision-making group. The group decision generates convergences and divergences among members. Our approach in this article relates to the calculation of the homogeneity of a group of learners during decision making in an educational project.',
	 'authors': u'Benjelloun Touimi Yassine,',
	 'category': u'Computer Science ',
	 'date': '2014-9-5',
	 'pdflink': u'http://arxiv.org/pdf/1409.1693',
	 'subjects': u'Computers and Society (cs.CY)',
	 'title': u'\nThe Homogeneity Indicator of Learners in Project-based Learning',
	 'urllink': u'http://arxiv.org/abs/1409.1693'}
2015-03-23 20:37:00+0000 [xxu461000] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1406.7558> (referer: http://arxiv.org/list/cs/14?skip=7000&show=1000)
2015-03-23 20:37:00+0000 [xxu461000] DEBUG: Scraped from <200 http://arxiv.org/abs/1406.7558>
	{'abstract': u"Human communication systems, such as language, evolve culturally; their components undergo reproduction and variation. However, a role for selection in cultural evolutionary dynamics is less clear. Often neutral evolution (also known as 'drift') models, are used to explain the evolution of human communication systems, and cultural evolution more generally. Under this account, cultural change is unbiased: for instance, vocabulary, baby names and pottery designs have been found to spread through random copying. While drift is the null hypothesis for models of cultural evolution it does not always adequately explain empirical results. Alternative models include cultural selection, which assumes variant adoption is biased. Theoretical models of human communication argue that during conversation interlocutors are biased to adopt the same labels and other aspects of linguistic representation (including prosody and syntax). This basic alignment mechanism has been extended by computer simulation to account for the emergence of linguistic conventions. When agents are biased to match the linguistic behavior of their interlocutor, a single variant can propagate across an entire population of interacting computer agents. This behavior-matching account operates at the level of the individual. We call it the Conformity-biased model. Under a different selection account, called content-biased selection, functional selection or replicator selection, variant adoption depends upon the intrinsic value of the particular variant (e.g., ease of learning or use). This second alternative account operates at the level of the cultural variant. Following Boyd and Richerson we call it the Content-biased model. The present paper tests the drift model and the two biased selection models' ability to explain the spread of communicative signal variants in an experimental micro-society.",
	 'authors': u'Nicolas Fay, Monica Tamariz, T Mark Ellison, Dale Barr,',
	 'category': u'Computer Science ',
	 'date': '2014-6-29',
	 'pdflink': u'http://arxiv.org/pdf/1406.7558',
	 'subjects': u'Social and Information Networks (cs.SI)',
	 'title': u'\nHuman Communication Systems Evolve by Cultural Selection',
	 'urllink': u'http://arxiv.org/abs/1406.7558'}
2015-03-23 20:37:04+0000 [xxu461000] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1405.1999> (referer: http://arxiv.org/list/cs/14?skip=5000&show=1000)
2015-03-23 20:37:04+0000 [xxu461000] DEBUG: Scraped from <200 http://arxiv.org/abs/1405.1999>
	{'abstract': u"Fractional order derivatives and integrals (differintegrals) are viewed from a frequency-domain perspective using the formalism of Riesz, providing a computational tool as well as a way to interpret the operations in the frequency domain. Differintegrals provide a logical extension of current techniques, generalizing the notion of integral and differential operators and acting as kind of frequency-domain filtering that has many of the advantages of a nonlocal linear operator. Several important properties of differintegrals are presented, and sample applications are given to one- and two-dimensional signals. Computer code to carry out the computations is made available on the author's website.",
	 'authors': u'William A. Sethares, Sel\xe7uk \u015e. Bay\u0131n,',
	 'category': u'Computer Science ',
	 'date': '2014-3-21',
	 'pdflink': u'http://arxiv.org/pdf/1405.1999',
	 'subjects': u'Computer Vision and Pattern Recognition (cs.CV)',
	 'title': u'\nModel-Driven Applications of Fractional Derivatives and Integrals',
	 'urllink': u'http://arxiv.org/abs/1405.1999'}
2015-03-23 20:37:09+0000 [xxu461000] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1405.5860> (referer: http://arxiv.org/list/cs/14?skip=6000&show=1000)
2015-03-23 20:37:09+0000 [xxu461000] DEBUG: Scraped from <200 http://arxiv.org/abs/1405.5860>
	{'abstract': u"The von Neumann and Morgenstern theory postulates that rational choice under uncertainty is equivalent to maximization of expected utility (EU). This view is mathematically appealing and natural because of the affine structure of the space of probability measures. Behavioural economists and psychologists, on the other hand, have demonstrated that humans consistently violate the EU postulate by switching from risk-averse to risk-taking behaviour. This paradox has led to the development of descriptive theories of decisions, such as the celebrated prospect theory, which uses an -shaped value function with concave and convex branches explaining the observed asymmetry. Although successful in modelling human behaviour, these theories appear to contradict the natural set of axioms behind the EU postulate. Here we show that the observed asymmetry in behaviour can be explained if, apart from utilities of the outcomes, rational agents also value information communicated by random events. We review the main ideas of the classical value of information theory and its generalizations. Then we prove that the value of information is an -shaped function, and that its asymmetry does not depend on how the concept of information is defined, but follows only from linearity of the expected utility. Thus, unlike many descriptive and `non-expected' utility theories that abandon the linearity (i.e. the `independence' axiom), we formulate a rigorous argument that the von Neumann and Morgenstern rational agents should be both risk-averse and risk-taking if they are not indifferent to information.",
	 'authors': u'Roman V. Belavkin,',
	 'category': u'Computer Science ',
	 'date': '2014-5-19',
	 'pdflink': u'http://arxiv.org/pdf/1405.5860',
	 'subjects': u'Optimization and Control (math.OC)',
	 'title': u'\nAsymmetry of Risk and Value of Information',
	 'urllink': u'http://arxiv.org/abs/1405.5860'}
2015-03-23 20:37:15+0000 [xxu461000] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1407.4903> (referer: http://arxiv.org/list/cs/14?skip=8000&show=1000)
2015-03-23 20:37:15+0000 [xxu461000] DEBUG: Scraped from <200 http://arxiv.org/abs/1407.4903>
	{'abstract': u'Glass published the first report on the assessment of systems and software engineering scholars and institutions two decades ago. The ongoing, annual survey of publications in this field provides fund managers, young scholars, graduate students, etc. with useful information for different purposes. However, the studies have been questioned by some critics because of a few shortcomings of the evaluation method. It is actually very hard to reach a widely recognized consensus on such an assessment of scholars and institutions. This paper presents a module and automated method for assessment and trends analysis in software engineering compared with the prior studies. To achieve a more reasonable evaluation result, we take into consideration more high-quality publications, the rank of each publication analyzed, and the different roles of authors named on each paper in question. According to the 7638 papers published in 36 publications from 2008 to 2013, the statistics of research subjects roughly follow power laws, implying the interesting Matthew Effect. We then identify the Top 20 scholars, institutions and countries or regions in terms of a new evaluation rule based on the frequently-used one. The top-ranked scholar is Mark Harman of the University College London, UK, the top-ranked institution is the University of California, USA, and the top-ranked country is the USA. Besides, we also show two levels of trend changes based on the EI classification system and user-defined uncontrolled keywords, as well as noteworthy scholars and institutions in a specific research area. We believe that our results would provide a valuable insight for young scholars and graduate students to seek possible potential collaborators and grasp the popular research topics in software engineering.',
	 'authors': u'Zhi Wang, Bing Li, Yutao Ma,',
	 'category': u'Computer Science ',
	 'date': '2014-7-18',
	 'pdflink': u'http://arxiv.org/pdf/1407.4903',
	 'subjects': u'Software Engineering (cs.SE)',
	 'title': u'\nAn Analysis of Research in Software Engineering: Assessment and Trends',
	 'urllink': u'http://arxiv.org/abs/1407.4903'}
2015-03-23 20:37:21+0000 [xxu461000] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1404.5043> (referer: http://arxiv.org/list/cs/14?skip=4000&show=1000)
2015-03-23 20:37:21+0000 [xxu461000] DEBUG: Scraped from <200 http://arxiv.org/abs/1404.5043>
	{'abstract': u"Higher-dimensional analogs of the predictable degree property and column reducedness are defined, and it is proved that the two properties are equivalent. It is shown that every multidimensional convolutional code has, what is called, a minimal reduced polynomial resolution. It is uniquely determined (up to isomorphism) and leads to a number of important integer invariants of the code generalizing classical Forney's indices.",
	 'authors': u'Vakhtang Lomadze,',
	 'category': u'Computer Science ',
	 'date': '2014-4-20',
	 'pdflink': u'http://arxiv.org/pdf/1404.5043',
	 'subjects': u'Information Theory (cs.IT)',
	 'title': u'\nThe predictable degree property, column reducedness, and minimality in  multidimensional convolutional coding',
	 'urllink': u'http://arxiv.org/abs/1404.5043'}
2015-03-23 20:37:26+0000 [xxu461000] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1409.1686> (referer: http://arxiv.org/list/cs/14?skip=10000&show=1000)
2015-03-23 20:37:26+0000 [xxu461000] DEBUG: Scraped from <200 http://arxiv.org/abs/1409.1686>
	{'abstract': u'In this paper, we propose a model for simulating search operators whose behaviour often changes continuously during the search. In these scenarios, the performance of the operators decreases when they are applied. This is motivated by the fact that operators for optimization problems are often roughly classified into exploitation operators and exploration operators. Our simulation model is used to compare the different performances of operator selection policies and clearly identify their ability to adapt to such specific operators behaviours. The experimental study provides interesting results on the respective behaviours of operator selection policies when faced to such non stationary search scenarios.',
	 'authors': u'Adrien Go\xebffon, Fr\xe9d\xe9ric Lardeux, Fr\xe9d\xe9ric Saubion,',
	 'category': u'Computer Science ',
	 'date': '2014-9-5',
	 'pdflink': u'http://arxiv.org/pdf/1409.1686',
	 'subjects': u'Artificial Intelligence (cs.AI)',
	 'title': u'\nSimulating Non Stationary Operators in Search Algorithms',
	 'urllink': u'http://arxiv.org/abs/1409.1686'}
2015-03-23 20:37:31+0000 [xxu461000] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1406.7557> (referer: http://arxiv.org/list/cs/14?skip=7000&show=1000)
2015-03-23 20:37:31+0000 [xxu461000] DEBUG: Scraped from <200 http://arxiv.org/abs/1406.7557>
	{'abstract': u'A multi-antenna transmitter that conveys independent sets of common data to distinct groups of users is considered. This model is known as physical layer multicasting to multiple co-channel groups. In this context, the practical constraint of a maximum permitted power level radiated by each antenna is addressed. The per-antenna power constrained system is optimized in a maximum fairness sense with respect to predetermined quality of service weights. In other words, the worst scaled user is boosted by maximizing its weighted signal-to-interference plus noise ratio. A detailed solution to tackle the weighted max-min fair multigroup multicast problem under per-antenna power constraints is therefore derived. The implications of the novel constraints are investigated via prominent applications and paradigms. What is more, robust per-antenna constrained multigroup multicast beamforming solutions are proposed. Finally, an extensive performance evaluation quantifies the gains of the proposed algorithm over existing solutions and exhibits its accuracy over per-antenna power constrained systems.',
	 'authors': u'Dimitrios Christopoulos, Symeon Chatzinotas, Bjorn Ottersten,',
	 'category': u'Computer Science ',
	 'date': '2014-6-29',
	 'pdflink': u'http://arxiv.org/pdf/1406.7557',
	 'subjects': u'Information Theory (cs.IT)',
	 'title': u'\nWeighted Fair Multicast Multigroup Beamforming under Per-antenna Power  Constraints',
	 'urllink': u'http://arxiv.org/abs/1406.7557'}
2015-03-23 20:37:37+0000 [xxu461000] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1405.1993> (referer: http://arxiv.org/list/cs/14?skip=5000&show=1000)
2015-03-23 20:37:37+0000 [xxu461000] DEBUG: Scraped from <200 http://arxiv.org/abs/1405.1993>
	{'abstract': u'Recently, Wireless Sensor Networks (WSNs) grow to be one of the dominant technology trends; new needs are continuously emerging and demanding more complex constraints in a duty cycle, such as extend the life time communication . The MAC layer plays a crucial role in these networks; it controls the communication module and manages the medium sharing. In this work we use OSC-MAC tackles combining with the performance of cooperative transmission (CT) in multi-hop WSN and the Wi-Lem technology',
	 'authors': u'Mbida Mohamed, Ezzati Abdellah,',
	 'category': u'Computer Science ',
	 'date': '2014-5-8',
	 'pdflink': u'http://arxiv.org/pdf/1405.1993',
	 'subjects': u'Networking and Internet Architecture (cs.NI)',
	 'title': u'\nOSCMAC_Duty_Cycle_with_Multi_Helpers_CT_Mode_WILEM_Technology_in_Wireless_Sensor_Networks',
	 'urllink': u'http://arxiv.org/abs/1405.1993'}
2015-03-23 20:37:42+0000 [xxu461000] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1405.5756> (referer: http://arxiv.org/list/cs/14?skip=6000&show=1000)
2015-03-23 20:37:42+0000 [xxu461000] DEBUG: Scraped from <200 http://arxiv.org/abs/1405.5756>
	{'abstract': u"The sampling method has been paid much attention in the field of complex network in general and statistical physics in particular. This paper presents two new sampling methods based on the perspective that a small part of vertices with high node degree can possess the most structure information of a network. The two proposed sampling methods are efficient in sampling the nodes with high degree. The first new sampling method is improved on the basis of the stratified random sampling method and selects the high degree nodes with higher probability by classifying the nodes according to their degree distribution. The second sampling method improves the existing snowball sampling method so that it enables to sample the targeted nodes selectively in every sampling step. Besides, the two proposed sampling methods not only sample the nodes but also pick the edges directly connected to these nodes. In order to demonstrate the two methods' availability and accuracy, we compare them with the existing sampling methods in three commonly used simulation networks that are scale-free network, random network, small-world network, and two real networks. The experimental results show that the two proposed sampling methods perform much better than the compared existing sampling methods in terms of sampling cost and obtaining the true network structural characteristics.",
	 'authors': u'Luo Peng, Li Yongli, Wu Chong,',
	 'category': u'Computer Science ',
	 'date': '2014-5-22',
	 'pdflink': u'http://arxiv.org/pdf/1405.5756',
	 'subjects': u'Physics and Society (physics.soc-ph)',
	 'title': u'\nTowards Cost-efficient Sampling Methods',
	 'urllink': u'http://arxiv.org/abs/1405.5756'}
2015-03-23 20:37:45+0000 [xxu461000] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1407.4898> (referer: http://arxiv.org/list/cs/14?skip=8000&show=1000)
2015-03-23 20:37:45+0000 [xxu461000] DEBUG: Scraped from <200 http://arxiv.org/abs/1407.4898>
	{'abstract': u'Hand pointing detection has multiple applications in many fields such as virtual reality and control devices in smart homes. In this paper, we proposed a novel approach to detect pointing vector in 2D space of a room. After background subtraction, face and forehead is detected. In the second step, forehead skin H-S plane histograms in HSV space is calculated. By using these histogram templates of users skin, and back projection method, skin areas are detected. The contours of hand are extracted using Freeman chain code algorithm. Next step is finding fingertips. Points in hand contour which are candidates for the fingertip can be found in convex defects of convex hull and contour. We introduced a novel method for finding the fingertip based on the special points on the contour and their relationships. Our approach detects hand-pointing vectors in live video from a common webcam with 94%TP and 85%TN.',
	 'authors': u'Ghassem Tofighi, Nasser Ali Afarin, Kamraan Raahemifar, Anastasios N. Venetsanopoulos,',
	 'category': u'Computer Science ',
	 'date': '2014-7-18',
	 'pdflink': u'http://arxiv.org/pdf/1407.4898',
	 'subjects': u'Computer Vision and Pattern Recognition (cs.CV)',
	 'title': u'\nHand Pointing Detection Using Live Histogram Template of Forehead Skin',
	 'urllink': u'http://arxiv.org/abs/1407.4898'}
2015-03-23 20:37:48+0000 [xxu461000] INFO: Crawled 540 pages (at 12 pages/min), scraped 523 items (at 12 items/min)
2015-03-23 20:37:48+0000 [xxu461000] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1404.5037> (referer: http://arxiv.org/list/cs/14?skip=4000&show=1000)
2015-03-23 20:37:48+0000 [xxu461000] DEBUG: Scraped from <200 http://arxiv.org/abs/1404.5037>
	{'abstract': u'In the chapter "Multiresolution Analysis on Compact Riemannian Manifolds" Isaac Pesenson describes multiscale analysis, sampling, interpolation and approximation of functions defined on manifolds. His main achievements are: construction on manifolds of bandlimited and space-localized frames which have Parseval property and construction of variational splines on manifolds. Such frames and splines enable multiscale analysis on arbitrary compact manifolds, and they already found a number of important applications (statistics, CMB, crystallography) related to such manifolds as two-dimensional sphere and group of its rotations.',
	 'authors': u'Isaac Z. Pesenson,',
	 'category': u'Computer Science ',
	 'date': '2014-4-20',
	 'pdflink': u'http://arxiv.org/pdf/1404.5037',
	 'subjects': u'Information Theory (cs.IT)',
	 'title': u'\nMultiresolution analysis on compact Riemannian manifolds',
	 'urllink': u'http://arxiv.org/abs/1404.5037'}
2015-03-23 20:37:54+0000 [xxu461000] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1409.1676> (referer: http://arxiv.org/list/cs/14?skip=10000&show=1000)
2015-03-23 20:37:54+0000 [xxu461000] DEBUG: Scraped from <200 http://arxiv.org/abs/1409.1676>
	{'abstract': u'In a graph , an is a subset of vertices such that is an independent set and each vertex outside has exactly one neighbor in . The problem (EDS) asks for the existence of an efficient dominating set in a given graph . The EDS is known to be -complete for -free graphs, and is known to be polynomial time solvable for -free graphs. However, the computational complexity of the EDS problem is unknown for -free graphs. In this paper, we show that the EDS problem can be solved in polynomial time for a subclass of -free graphs, namely (, banner)-free graphs.',
	 'authors': u'T. Karthick,',
	 'category': u'Computer Science ',
	 'date': '2014-9-5',
	 'pdflink': u'http://arxiv.org/pdf/1409.1676',
	 'subjects': u'Discrete Mathematics (cs.DM)',
	 'title': u'\nNew polynomial case for efficient domination in $P_6$-free graphs',
	 'urllink': u'http://arxiv.org/abs/1409.1676'}
2015-03-23 20:37:59+0000 [xxu461000] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1406.7551> (referer: http://arxiv.org/list/cs/14?skip=7000&show=1000)
2015-03-23 20:37:59+0000 [xxu461000] DEBUG: Scraped from <200 http://arxiv.org/abs/1406.7551>
	{'abstract': u"The recent emergence of online citizen science is illustrative of an efficient and effective means to harness the crowd in order to achieve a range of scientific discoveries. Fundamentally, citizen science projects draw upon crowds of non-expert volunteers to complete short Tasks, which can vary in domain and complexity. However, unlike most human-computational systems, participants in these systems, the `citizen scientists' are volunteers, whereby no incentives, financial or otherwise, are offered. Furthermore, encouraged by citizen science platforms such as Zooniverse, online communities have emerged, providing them with an environment to discuss, share ideas, and solve problems. In fact, it is the result of these forums that has enabled a number of scientific discoveries to be made. In this paper we explore the phenomenon of collective intelligence via the relationship between the activities of online citizen science communities and the discovery of scientific knowledge. We perform a cross-project analysis of ten Zooniverse citizen science projects and analyse the behaviour of users with regards to their Task completion activity and participation in discussion and discover collective behaviour amongst highly active users. Whilst our findings have implications for future citizen science design, we also consider the wider implications for understanding collective intelligence research in general.",
	 'authors': u'Ramine Tinati, Elena Simperl, Markus Luczak-Roesch, Max Van Kleek, Nigel Shadbolt,',
	 'category': u'Computer Science ',
	 'date': '2014-6-29',
	 'pdflink': u'http://arxiv.org/pdf/1406.7551',
	 'subjects': u'Social and Information Networks (cs.SI)',
	 'title': u'\nCollective Intelligence in Citizen Science -- A Study of Performers and  Talkers',
	 'urllink': u'http://arxiv.org/abs/1406.7551'}
2015-03-23 20:38:02+0000 [xxu461000] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1405.1992> (referer: http://arxiv.org/list/cs/14?skip=5000&show=1000)
2015-03-23 20:38:02+0000 [xxu461000] DEBUG: Scraped from <200 http://arxiv.org/abs/1405.1992>
	{'abstract': u"In today's world of Web application development, programmers are commonly called upon to use the Hypertext Markup Language (HTML) as a programming language, something for which it was never intended and for which it is woefully inadequate. HTML is a data language, nothing more. It lacks high level programming constructions like procedures, conditions, and loops. Moreover it provides no intrinsic mechanism to insert or associate dynamic application data. Lastly, despite the visibly apparent structure of a web page when viewed in a browser, the responsible HTML code bears little to no discernible corresponding structure, making it very difficult to read, augment, and maintain. This paper examines the various drawbacks inherent in HTML when used in Web development and examines the various augmenting technologies available in the industry today and their drawbacks. It then proposes an alternative, complete with the necessary constructs, structure, and data associating facilities based upon server-side, Extensible Stylesheet Language Transforms (XSLT). This alternative approach gives rise to an entirely new, higher level, markup language that can be readily used in web development.",
	 'authors': u'John Francisco, Victor Sadikov,',
	 'category': u'Computer Science ',
	 'date': '2014-5-8',
	 'pdflink': u'http://arxiv.org/pdf/1405.1992',
	 'subjects': u'Software Engineering (cs.SE)',
	 'title': u'\nStructured Approach to Web Development',
	 'urllink': u'http://arxiv.org/abs/1405.1992'}
2015-03-23 20:38:07+0000 [xxu461000] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1405.5755> (referer: http://arxiv.org/list/cs/14?skip=6000&show=1000)
2015-03-23 20:38:07+0000 [xxu461000] DEBUG: Scraped from <200 http://arxiv.org/abs/1405.5755>
	{'abstract': u'We present an efficient endomorphism for the Jacobian of a curve of genus 2 (hyperelliptic) for divisors having a Non disjoint support. This extends the work of Costello and Lauter in [12] who calculated explicit formulae for divisor doubling and addition of divisors with disjoint support in using only base field operations. Explicit formulae is presented for this third case and a slightly different approach for divisor doubling.',
	 'authors': u'Eduardo Ruiz Duarte, Octavio P\xe1ez Osuna,',
	 'category': u'Computer Science ',
	 'date': '2014-5-22',
	 'pdflink': u'http://arxiv.org/pdf/1405.5755',
	 'subjects': u'Algebraic Geometry (math.AG)',
	 'title': u'\nExplicit endomorphism of the Jacobian of a hyperelliptic function field  of genus 2 using base field operations',
	 'urllink': u'http://arxiv.org/abs/1405.5755'}
2015-03-23 20:38:13+0000 [xxu461000] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1407.4885> (referer: http://arxiv.org/list/cs/14?skip=8000&show=1000)
2015-03-23 20:38:13+0000 [xxu461000] DEBUG: Scraped from <200 http://arxiv.org/abs/1407.4885>
	{'abstract': u"The D4D-Senegal challenge is an open innovation data challenge on anonymous call patterns of Orange's mobile phone users in Senegal. The goal of the challenge is to help address society development questions in novel ways by contributing to the socio-economic development and well-being of the Senegalese population. Participants to the challenge are given access to three mobile phone datasets. This paper describes the three datasets. The datasets are based on Call Detail Records (CDR) of phone calls and text exchanges between more than 9 million of Orange's customers in Senegal between January 1, 2013 to December 31, 2013. The datasets are: (1) antenna-to-antenna traffic for 1666 antennas on an hourly basis, (2) fine-grained mobility data on a rolling 2-week basis for a year with bandicoot behavioral indicators at individual level for about 300,000 randomly sampled users, (3) one year of coarse-grained mobility data at arrondissement level with bandicoot behavioral indicators at individual level for about 150,000 randomly sampled users",
	 'authors': u'Yves-Alexandre de Montjoye, Zbigniew Smoreda, Romain Trinquart, Cezary Ziemlicki, Vincent D. Blondel,',
	 'category': u'Computer Science ',
	 'date': '2014-7-18',
	 'pdflink': u'http://arxiv.org/pdf/1407.4885',
	 'subjects': u'Computers and Society (cs.CY)',
	 'title': u'\nD4D-Senegal: The Second Mobile Phone Data for Development Challenge',
	 'urllink': u'http://arxiv.org/abs/1407.4885'}
2015-03-23 20:38:19+0000 [xxu461000] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1404.5034> (referer: http://arxiv.org/list/cs/14?skip=4000&show=1000)
2015-03-23 20:38:19+0000 [xxu461000] DEBUG: Scraped from <200 http://arxiv.org/abs/1404.5034>
	{'abstract': u'Growth in the number of PMOs established by the industry over last decade and ever growing body of literature on PMO related research in academia is a clear indication that there is very clear interest of researchers, practitioners and industries across the globe to understand and explore value propositions of PMO. However, there is still a lack of consensus on many critical aspects of PMOs. While there are many PMOs being established, but there are also many being closed and disbanded, which is definitely a matter of concern. In industry environment, a narrow majority of PMOs are well-regarded by their organizations and are seen as contributing business value, many of the others are still struggling to show value for money and some are failing, causing a high mortality rate among PMOs. This paper is the result of a study undertaken to get a deeper understanding of factors that may be causing mortality and failure of PMOs. Post Implementation Reviews of 4-failed &amp; 3-challenged PMOs in IT-Industry were carried out with concerned Project Managers &amp; PMO-staff, using grounded theory research method, with support from the concerned enterprise from IT-Industry.',
	 'authors': u'Parvez Mahmood Khan, M M Sufyan Beg, Musheer Ahmad,',
	 'category': u'Computer Science ',
	 'date': '2014-4-20',
	 'pdflink': u'http://arxiv.org/pdf/1404.5034',
	 'subjects': u'Software Engineering (cs.SE)',
	 'title': u'\nSustaining IT PMOs during Cycles of Global Recession',
	 'urllink': u'http://arxiv.org/abs/1404.5034'}
2015-03-23 20:38:23+0000 [xxu461000] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1409.1673> (referer: http://arxiv.org/list/cs/14?skip=10000&show=1000)
2015-03-23 20:38:23+0000 [xxu461000] DEBUG: Scraped from <200 http://arxiv.org/abs/1409.1673>
	{'abstract': u'We address the problem of super-resolution frequency recovery using prior knowledge of the structure of a spectrally sparse, undersampled signal. In many applications of interest, some structure information about the signal spectrum is often known. The prior information might be simply knowing precisely some signal frequencies or the likelihood of a particular frequency component in the signal. We devise a general semidefinite program to recover these frequencies using theories of positive trigonometric polynomials. Our theoretical analysis shows that, given sufficient prior information, perfect signal reconstruction is possible using signal samples no more than thrice the number of signal frequencies. Numerical experiments demonstrate great performance enhancements using our method. We show that the nominal resolution necessary for the grid-free results can be improved if prior information is suitably employed.',
	 'authors': u'Kumar Vijay Mishra, Myung Cho, Anton Kruger, Weiyu Xu,',
	 'category': u'Computer Science ',
	 'date': '2014-9-5',
	 'pdflink': u'http://arxiv.org/pdf/1409.1673',
	 'subjects': u'Information Theory (cs.IT)',
	 'title': u'\nSpectral Super-resolution With Prior Knowledge',
	 'urllink': u'http://arxiv.org/abs/1409.1673'}
2015-03-23 20:38:28+0000 [xxu461000] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1406.7550> (referer: http://arxiv.org/list/cs/14?skip=7000&show=1000)
2015-03-23 20:38:28+0000 [xxu461000] DEBUG: Scraped from <200 http://arxiv.org/abs/1406.7550>
	{'abstract': u"The increasing relevance of Internet-based markets requires a sustained investigation into the relationship between design and user behavior. This research begins within the sociology of quantification and markets to investigate the impacts of basic design decisions on user behavior and individual success on a widely used crowdfunding website. This study looks at one common design feature, publishing recipients' sex, on the probability of receiving funding. Following research in the sociology of gender, these effects are defined along individual, behavioral, and structural dimensions. The results reveal that before teachers' sex was published, gender discrimination was weak and inconsistent. However, afterward gender discrimination increases by an order of magnitude and becomes systematized. Contrary to expectation, donors did not discriminate by sex category, but by teachers' structural position and the kinds of language they used. Implications for research on gender discrimination, priming, and online behavior are discussed.",
	 'authors': u'Jason Radford,',
	 'category': u'Computer Science ',
	 'date': '2014-6-29',
	 'pdflink': u'http://arxiv.org/pdf/1406.7550',
	 'subjects': u'Social and Information Networks (cs.SI)',
	 'title': u'\nArchitectures of Virtual Decision-Making: The Emergence of Gender  Discrimination on a Crowdfunding Website',
	 'urllink': u'http://arxiv.org/abs/1406.7550'}
2015-03-23 20:38:34+0000 [xxu461000] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1405.1980> (referer: http://arxiv.org/list/cs/14?skip=5000&show=1000)
2015-03-23 20:38:34+0000 [xxu461000] DEBUG: Scraped from <200 http://arxiv.org/abs/1405.1980>
	{'abstract': u'This paper studies constructive heuristics for the minimum labelling spanning tree (MLST) problem. The purpose is to find a spanning tree that uses edges that are as similar as possible. Given an undirected labeled connected graph (i.e., with a label or color for each edge), the minimum labeling spanning tree problem seeks a spanning tree whose edges have the smallest possible number of distinct labels. The model can represent many real-world problems in telecommunication networks, electric networks, and multimodal transportation networks, among others, and the problem has been shown to be NP-complete even for complete graphs. A primary heuristic, named the maximum vertex covering algorithm has been proposed. Several versions of this constructive heuristic have been proposed to improve its efficiency. Here we describe the problem, review the literature and compare some variants of this algorithm.',
	 'authors': u'Sergio Consoli, Jose Andres Moreno-Perez, Kenneth Darby-Dowman, Nenad Mladenovic,',
	 'category': u'Computer Science ',
	 'date': '2014-4-16',
	 'pdflink': u'http://arxiv.org/pdf/1405.1980',
	 'subjects': u'Discrete Mathematics (cs.DM)',
	 'title': u'\nMejora de la exploracion y la explotacion de las heuristicas  constructivas para el MLSTP',
	 'urllink': u'http://arxiv.org/abs/1405.1980'}
2015-03-23 20:38:40+0000 [xxu461000] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1405.5726> (referer: http://arxiv.org/list/cs/14?skip=6000&show=1000)
2015-03-23 20:38:40+0000 [xxu461000] DEBUG: Scraped from <200 http://arxiv.org/abs/1405.5726>
	{'abstract': u"We investigate the response function of human agents as demonstrated by written correspondence, uncovering a new universal pattern for how the reactive dynamics of individuals is distributed across the set of each agent's contacts. In long-term empirical data on email, we find that the set of response times considered separately for the messages to each different correspondent of a given writer, generate a family of heavy-tailed distributions, which have largely the same features for all agents, and whose characteristic times grow exponentially with the rank of each correspondent. We show this universal behavioral pattern emerges robustly by considering weighted moving averages of the priority-conditioned response-time probabilities generated by a basic prioritization model. Our findings clarify how the range of priorities in the inputs from one's environment underpin and shape the dynamics of agents embedded in a net of reactive relations. These newly revealed activity patterns constrain future models of communication and interaction networks, affecting their architecture and evolution.",
	 'authors': u'Marco Formentin, Alberto Lovison, Amos Maritan, Giovanni Zanzotto,',
	 'category': u'Computer Science ',
	 'date': '2014-5-22',
	 'pdflink': u'http://arxiv.org/pdf/1405.5726',
	 'subjects': u'Physics and Society (physics.soc-ph)',
	 'title': u'\nUniversal activity pattern in human interactive dynamics',
	 'urllink': u'http://arxiv.org/abs/1405.5726'}
2015-03-23 20:38:46+0000 [xxu461000] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1407.4884> (referer: http://arxiv.org/list/cs/14?skip=8000&show=1000)
2015-03-23 20:38:46+0000 [xxu461000] DEBUG: Scraped from <200 http://arxiv.org/abs/1407.4884>
	{'abstract': u'Permutations over with low differential uniform, high algebraic degree and high nonlinearity are of great cryptographical importance since they can be chosen as the substitution boxes (S-boxes) for many block ciphers. A well known example is that the Advanced Encryption Standard (AES) chooses a differentially 4-uniform permutation, the multiplicative inverse function, as its S-box. In this paper, we present a new construction of differentially 4-uniformity permutations over even characteristic finite fields and obtain many new CCZ-inequivalent functions. All the functions are switching neighbors in the narrow sense of the multiplicative inverse function and have the optimal algebraic degree and high nonlinearity.',
	 'authors': u'Jie Peng, Chik How Tan, Qichun Wang,',
	 'category': u'Computer Science ',
	 'date': '2014-7-18',
	 'pdflink': u'http://arxiv.org/pdf/1407.4884',
	 'subjects': u'Information Theory (cs.IT)',
	 'title': u'\nA new construction of differentially 4-uniform permutations over  $F_{2^{2k}}$',
	 'urllink': u'http://arxiv.org/abs/1407.4884'}
2015-03-23 20:38:48+0000 [xxu461000] INFO: Crawled 552 pages (at 12 pages/min), scraped 535 items (at 12 items/min)
2015-03-23 20:38:52+0000 [xxu461000] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1404.5029> (referer: http://arxiv.org/list/cs/14?skip=4000&show=1000)
2015-03-23 20:38:52+0000 [xxu461000] DEBUG: Scraped from <200 http://arxiv.org/abs/1404.5029>
	{'abstract': u"Accurate state estimation is of paramount importance to maintain the power system operating in a secure and efficient state. The recently identified coordinated data injection attacks to meter measurements can bypass the current security system and introduce errors to the state estimates. The conventional wisdom to mitigate such attacks is by securing meter measurements to evade malicious injections. In this paper, we provide a novel alternative to defend against false-data injection attacks using covert power network topological information. By keeping the exact reactance of a set of transmission lines from attackers, no false data injection attack can be launched to compromise any set of state variables. We first investigate from the attackers' perspective the necessary condition to perform injection attack. Based on the arguments, we characterize the optimal protection problem, which protects the state variables with minimum cost, as a well-studied Steiner tree problem in a graph. Besides, we also propose a mixed defending strategy that jointly considers the use of covert topological information and secure meter measurements when either method alone is costly or unable to achieve the protection objective. A mixed integer linear programming (MILP) formulation is introduced to obtain the optimal mixed defending strategy. To tackle the NP-hardness of the problem, a tree pruning-based heuristic is further presented to produce an approximate solution in polynomial time. The advantageous performance of the proposed defending mechanisms is verified in IEEE standard power system testcases.",
	 'authors': u'Suzhi Bi, Ying Jun, Zhang,',
	 'category': u'Computer Science ',
	 'date': '2014-4-20',
	 'pdflink': u'http://arxiv.org/pdf/1404.5029',
	 'subjects': u'Cryptography and Security (cs.CR)',
	 'title': u'\nUsing Covert Topological Information for Defense Against Malicious  Attacks on DC State Estimation',
	 'urllink': u'http://arxiv.org/abs/1404.5029'}
2015-03-23 20:38:57+0000 [xxu461000] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1409.1668> (referer: http://arxiv.org/list/cs/14?skip=10000&show=1000)
2015-03-23 20:38:57+0000 [xxu461000] DEBUG: Scraped from <200 http://arxiv.org/abs/1409.1668>
	{'abstract': u"Mrs. Gerber's Lemma (MGL) hinges on the convexity of , where is the binary entropy function. In this work, we prove that is convex in for every provided is convex in , where . Moreover, our result subsumes MGL and simplifies the original proof. We show that the generalized MGL can be applied in binary broadcast channel to simplify some discussion.",
	 'authors': u'Fan Cheng,',
	 'category': u'Computer Science ',
	 'date': '2014-9-5',
	 'pdflink': u'http://arxiv.org/pdf/1409.1668',
	 'subjects': u'Information Theory (cs.IT)',
	 'title': u"\nGeneralization of Mrs. Gerber's Lemma",
	 'urllink': u'http://arxiv.org/abs/1409.1668'}
2015-03-23 20:39:01+0000 [xxu461000] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1406.7549> (referer: http://arxiv.org/list/cs/14?skip=7000&show=1000)
2015-03-23 20:39:01+0000 [xxu461000] DEBUG: Scraped from <200 http://arxiv.org/abs/1406.7549>
	{'abstract': u'The scientific problem in our project is defined as a question: how social technologies could contribute to the development of smart and inclusive society? The subject of our research are networked projects (virtual CI systems) which include collective decision making tools and innovation mechanisms allowing and encouraging individual and team creativity, entrepreneurship, online collaboration, new forms of self-regulation and self-governance, self-configuration of communities by considering these projects as being catalyst for emergence of CI. The answers to these theoretical questions could have huge practical implications by influencing more reasonable and sophisticated application of social technologies in practice.',
	 'authors': u'Aelita Skarzauskiene, Birute Pitrenaite-Zileniene, Edgaras Leichteris, Zaneta Paunksniene, Monika Maciuliene,',
	 'category': u'Computer Science ',
	 'date': '2014-6-29',
	 'pdflink': u'http://arxiv.org/pdf/1406.7549',
	 'subjects': u'Social and Information Networks (cs.SI)',
	 'title': u'\nSocial Technologies for Developing Collective Intelligence in Networked  Society',
	 'urllink': u'http://arxiv.org/abs/1406.7549'}
2015-03-23 20:39:07+0000 [xxu461000] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1405.1967> (referer: http://arxiv.org/list/cs/14?skip=5000&show=1000)
2015-03-23 20:39:07+0000 [xxu461000] DEBUG: Scraped from <200 http://arxiv.org/abs/1405.1967>
	{'abstract': u'In this paper the technique for resolution and contrast enhancement of satellite geographical images based on discrete wavelet transform (DWT), stationary wavelet transform (SWT) and singular value decomposition (SVD) has been proposed. In this, the noise is added in the input low resolution and low contrast image. The median filter is used remove noise from the input image. This low resolution, low contrast image without noise is decomposed into four sub-bands by using DWT and SWT. The resolution enhancement technique is based on the interpolation of high frequency components obtained by DWT and input image. SWT is used to enhance input image. DWT is used to decompose an image into four frequency sub bands and these four sub-bands are interpolated using bicubic interpolation technique. All these sub-bands are reconstructed as high resolution image by using inverse DWT (IDWT). To increase the contrast the proposed technique uses DWT and SVD. GHE is used to equalize an image. The equalized image is decomposed into four sub-bands using DWT and new LL sub-band is reconstructed using SVD. All sub-bands are reconstructed using IDWT to generate high resolution and contrast image over conventional techniques. The experimental result shows superiority of the proposed technique over conventional techniques. Key words: Discrete wavelet transform (DWT), General histogram equalization (GHE), Median filter, Singular value decomposition (SVD), Stationary wavelet transform (SWT).',
	 'authors': u'Prajakta P. Khairnar, C. A. Manjare,',
	 'category': u'Computer Science ',
	 'date': '2014-5-8',
	 'pdflink': u'http://arxiv.org/pdf/1405.1967',
	 'subjects': u'Computer Vision and Pattern Recognition (cs.CV)',
	 'title': u'\nImage Resolution and Contrast Enhancement of Satellite Geographical  Images with Removal of Noise using Wavelet Transforms',
	 'urllink': u'http://arxiv.org/abs/1405.1967'}
2015-03-23 20:39:11+0000 [xxu461000] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1405.5505> (referer: http://arxiv.org/list/cs/14?skip=6000&show=1000)
2015-03-23 20:39:11+0000 [xxu461000] DEBUG: Scraped from <200 http://arxiv.org/abs/1405.5505>
	{'abstract': u'A mean function in a reproducing kernel Hilbert space (RKHS), or a kernel mean, is central to kernel methods in that it is used by many classical algorithms such as kernel principal component analysis, and it also forms the core inference step of modern kernel methods that rely on embedding probability distributions in RKHSs. Given a finite sample, an empirical average has been used commonly as a standard estimator of the true kernel mean. Despite a widespread use of this estimator, we show that it can be improved thanks to the well-known Stein phenomenon. We propose a new family of estimators called kernel mean shrinkage estimators (KMSEs), which benefit from both theoretical justifications and good empirical performance. The results demonstrate that the proposed estimators outperform the standard one, especially in a "large , small " paradigm.',
	 'authors': u'Krikamol Muandet, Bharath Sriperumbudur, Kenji Fukumizu, Arthur Gretton, Bernhard Sch\xf6lkopf,',
	 'category': u'Computer Science ',
	 'date': '2014-5-21',
	 'pdflink': u'http://arxiv.org/pdf/1405.5505',
	 'subjects': u'Machine Learning (stat.ML)',
	 'title': u'\nKernel Mean Shrinkage Estimators',
	 'urllink': u'http://arxiv.org/abs/1405.5505'}
2015-03-23 20:39:15+0000 [xxu461000] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1407.4879> (referer: http://arxiv.org/list/cs/14?skip=8000&show=1000)
2015-03-23 20:39:15+0000 [xxu461000] DEBUG: Scraped from <200 http://arxiv.org/abs/1407.4879>
	{'abstract': u'The use of photovoltaic (PV) sources is becoming very popular in smart grid for their ecological benefits, with higher scalability and utilization for local generation and delivery. PV can also potentially avoid the energy losses that are normally associated with long-range grid distribution. The increased penetration of solar panels, however, has introduced a need for solar energy models that are capable of producing realistic synthetic data with small error margins. Such models, for instance, can be used to design the appropriate size of energy storage devices or to determine the maximum charging rate of a PV-powered electric vehicle (EV) charging station. In this regard, this paper proposes a stochastic model for solar generation using a Markov chain approach. Based on real data, it is first shown that the solar states are inter-dependent, and thus suitable for modeling using a Markov model. Then, the probabilities of transition between states are shown to be heterogeneous over different time segments. A model is proposed that captures the inter temporal dependency of solar irradiance through segmentation of the Markov chain across different times of the day. In the studied model, different state transition matrices are constructed for different time segments, which the proposed algorithm then uses to generate the solar states for different times of the day. Numerical examples are provided to show the effectiveness of the proposed synthetic generator.',
	 'authors': u'Wayes Tushar, Shisheng Huang, Chau Yuen, Jian, Zhang, David B. Smith,',
	 'category': u'Computer Science ',
	 'date': '2014-7-18',
	 'pdflink': u'http://arxiv.org/pdf/1407.4879',
	 'subjects': u'Performance (cs.PF)',
	 'title': u'\nSynthetic Generation of Solar States for Smart Grid: A Multiple Segment  Markov Chain Apptoach',
	 'urllink': u'http://arxiv.org/abs/1407.4879'}
2015-03-23 20:39:19+0000 [xxu461000] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1404.5021> (referer: http://arxiv.org/list/cs/14?skip=4000&show=1000)
2015-03-23 20:39:19+0000 [xxu461000] DEBUG: Scraped from <200 http://arxiv.org/abs/1404.5021>
	{'abstract': u'Local rank modulation scheme was suggested recently for representing information in flash memories in order to overcome drawbacks of rank modulation. For with divides , an -LRM scheme is a local rank modulation scheme where the cells are locally viewed cyclically through a sliding window of size resulting in a sequence of small permutations which requires less comparisons and less distinct values. The gap between two such windows equals to . In this work, encoding, decoding, and asymptotic enumeration of the -LRM scheme is studied. The techniques which are suggested have some generalizations for -LRM, , but the proofs will become more complicated. The enumeration problem is presented also as a purely combinatorial problem. Finally, we prove the conjecture that the size of a constant weight -LRM Gray code with weight two is at most .',
	 'authors': u'Michal Horovitz, Tuvi Etzion,',
	 'category': u'Computer Science ',
	 'date': '2014-4-20',
	 'pdflink': u'http://arxiv.org/pdf/1404.5021',
	 'subjects': u'Information Theory (cs.IT)',
	 'title': u'\nLocal Rank Modulation for Flash Memories II',
	 'urllink': u'http://arxiv.org/abs/1404.5021'}
2015-03-23 20:39:24+0000 [xxu461000] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1409.1666> (referer: http://arxiv.org/list/cs/14?skip=10000&show=1000)
2015-03-23 20:39:24+0000 [xxu461000] DEBUG: Scraped from <200 http://arxiv.org/abs/1409.1666>
	{'abstract': u'In distributed storage systems, storage nodes intermittently go offline for numerous reasons. On coming back online, nodes need to update their contents to reflect any modifications to the data in the interim. In this paper, we consider a setting where no information regarding modified data needs to be logged in the system. In such a setting, a \'stale\' node needs to update its contents by downloading data from already updated nodes, while neither the stale node nor the updated nodes have any knowledge as to which data symbols are modified and what their value is. We investigate the fundamental limits on the amount of communication necessary for such an "oblivious" update process. We first present a generic lower bound on the amount of communication that is necessary under any storage code with a linear encoding (while allowing non-linear update protocols). This lower bound is derived under a set of extremely weak conditions, giving all updated nodes access to the entire modified data and the stale node access to the entire stale data as side information. We then present codes and update algorithms that are optimal in that they meet this lower bound. Next, we present a lower bound for an important subclass of codes, that of linear Maximum-Distance-Separable (MDS) codes. We then present an MDS code construction and an associated update algorithm that meets this lower bound. These results thus establish the capacity of oblivious updates in terms of the communication requirements under these settings.',
	 'authors': u'Preetum Nakkiran, Nihar B. Shah, K. V. Rashmi,',
	 'category': u'Computer Science ',
	 'date': '2014-9-5',
	 'pdflink': u'http://arxiv.org/pdf/1409.1666',
	 'subjects': u'Information Theory (cs.IT)',
	 'title': u'\nFundamental Limits on Communication for Oblivious Updates in Storage  Networks',
	 'urllink': u'http://arxiv.org/abs/1409.1666'}
2015-03-23 20:39:28+0000 [xxu461000] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1406.7547> (referer: http://arxiv.org/list/cs/14?skip=7000&show=1000)
2015-03-23 20:39:28+0000 [xxu461000] DEBUG: Scraped from <200 http://arxiv.org/abs/1406.7547>
	{'abstract': u'Recent work [Hazy 2012] has demonstrated computationally that collectives that are organized into networks which govern the flow of resources can learn to recognize newly emerging opportunities distributed in the environment. This paper argues that the system does this through a process analogous to neural network learning with relative status playing the role of synaptic weights. Hazy showed computationally that learning of this type can occur even when resource allocation decision makers have no direct visibility into the environment, have no direct understanding of the opportunity, and are not involved in their exploitation except to the extent that they evaluate the success or failure of funded projects. Effectively, the system of interactions learns which individuals have the best access to information and other resources within the ecosystem. Hazy [2012] calls this previously unidentified emergence phenomenon: Influence Process Structural Learning (IPSL). In the prior model of IPSL, a three-tiered organizational structure was predetermined in the model design [Hazy 2012]. These initial conditions delimit the extent to which the emergence of collective intelligence can be posited because the model itself assumes a defined structure. This work contributes to the field by extending the IPSL argument for collective intelligence to a holistic emergence argument. It begins by briefly reviewing previously published work. It continues the conversation by adding two additional steps: Firstly, it shows how a three-tier organizing structure might emerge through known complexity mechanisms. In this case the mechanism identified is preferential attachment [Barabasi 2002]. Secondly, the paper shows how collective intelligence can emerge within a system of agents when the influence structure among these agents is treated as a the genetic algorithm.',
	 'authors': u'James Hazy, Baran Curuklu,',
	 'category': u'Computer Science ',
	 'date': '2014-6-29',
	 'pdflink': u'http://arxiv.org/pdf/1406.7547',
	 'subjects': u'Social and Information Networks (cs.SI)',
	 'title': u'\nInfluence Process Structural Learning and the Emergence of Collective  Intelligence',
	 'urllink': u'http://arxiv.org/abs/1406.7547'}
2015-03-23 20:39:32+0000 [xxu461000] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1405.1966> (referer: http://arxiv.org/list/cs/14?skip=5000&show=1000)
2015-03-23 20:39:32+0000 [xxu461000] DEBUG: Scraped from <200 http://arxiv.org/abs/1405.1966>
	{'abstract': u"Texture segmentation is the process of partitioning an image into regions with different textures containing a similar group of pixels. Detecting the discontinuity of the filter's output and their statistical properties help in segmenting and classifying a given image with different texture regions. In this proposed paper, chili x-ray image texture segmentation is performed by using Gabor filter. The texture segmented result obtained from Gabor filter fed into three texture filters, namely Entropy, Standard Deviation and Range filter. After performing texture analysis, features can be extracted by using Statistical methods. In this paper Gray Level Co-occurrence Matrices and First order statistics are used as feature extraction methods. Features extracted from statistical methods are given to Support Vector Machine (SVM) classifier. Using this methodology, it is found that texture segmentation is followed by the Gray Level Co-occurrence Matrix feature extraction method gives a higher accuracy rate of 84% when compared with First order feature extraction method. Key Words: Texture segmentation, Texture filter, Gabor filter, Feature extraction methods, SVM classifier.",
	 'authors': u'M.Rajalakshmi, Dr. P.Subashini,',
	 'category': u'Computer Science ',
	 'date': '2014-4-15',
	 'pdflink': u'http://arxiv.org/pdf/1405.1966',
	 'subjects': u'Computer Vision and Pattern Recognition (cs.CV)',
	 'title': u'\nTexture Based Image Segmentation of Chili Pepper X-Ray Images Using  Gabor Filter',
	 'urllink': u'http://arxiv.org/abs/1405.1966'}
2015-03-23 20:39:36+0000 [xxu461000] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1405.5498> (referer: http://arxiv.org/list/cs/14?skip=6000&show=1000)
2015-03-23 20:39:36+0000 [xxu461000] DEBUG: Scraped from <200 http://arxiv.org/abs/1405.5498>
	{'abstract': u'Dynamic resource allocation (DRA) problems are an important class of dynamic stochastic optimization problems that arise in a variety of important real-world applications. DRA problems are notoriously difficult to solve to optimality since they frequently combine stochastic elements with intractably large state and action spaces. Although the artificial intelligence and operations research communities have independently proposed two successful frameworks for solving dynamic stochastic optimization problems---Monte Carlo tree search (MCTS) and mathematical optimization (MO), respectively---the relative merits of these two approaches are not well understood. In this paper, we adapt both MCTS and MO to a problem inspired by tactical wildfire and management and undertake an extensive computational study comparing the two methods on large scale instances in terms of both the state and the action spaces. We show that both methods are able to greatly improve on a baseline, problem-specific heuristic. On smaller instances, the MCTS and MO approaches perform comparably, but the MO approach outperforms MCTS as the size of the problem increases for a fixed computational budget.',
	 'authors': u'Dimitris Bertsimas, J. Daniel Griffith, Vishal Gupta, Mykel J. Kochenderfer, Velibor V. Mi\u0161i\u0107, Robert Moss,',
	 'category': u'Computer Science ',
	 'date': '2014-5-21',
	 'pdflink': u'http://arxiv.org/pdf/1405.5498',
	 'subjects': u'Optimization and Control (math.OC)',
	 'title': u'\nA Comparison of Monte Carlo Tree Search and Mathematical Optimization  for Large Scale Dynamic Resource Allocation',
	 'urllink': u'http://arxiv.org/abs/1405.5498'}
2015-03-23 20:39:41+0000 [xxu461000] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1407.4874> (referer: http://arxiv.org/list/cs/14?skip=8000&show=1000)
2015-03-23 20:39:41+0000 [xxu461000] DEBUG: Scraped from <200 http://arxiv.org/abs/1407.4874>
	{'abstract': u'This paper proposes a novel Affine Subspace Representation (ASR) descriptor to deal with affine distortions induced by viewpoint changes. Unlike the traditional local descriptors such as SIFT, ASR inherently encodes local information of multi-view patches, making it robust to affine distortions while maintaining a high discriminative ability. To this end, PCA is used to represent affine-warped patches as PCA-patch vectors for its compactness and efficiency. Then according to the subspace assumption, which implies that the PCA-patch vectors of various affine-warped patches of the same keypoint can be represented by a low-dimensional linear subspace, the ASR descriptor is obtained by using a simple subspace-to-point mapping. Such a linear subspace representation could accurately capture the underlying information of a keypoint (local structure) under multiple views without sacrificing its distinctiveness. To accelerate the computation of ASR descriptor, a fast approximate algorithm is proposed by moving the most computational part (ie, warp patch under various affine transformations) to an offline training stage. Experimental results show that ASR is not only better than the state-of-the-art descriptors under various image transformations, but also performs well without a dedicated affine invariant detector when dealing with viewpoint changes.',
	 'authors': u'Zhenhua Wang, Bin Fan, Fuchao Wu,',
	 'category': u'Computer Science ',
	 'date': '2014-7-18',
	 'pdflink': u'http://arxiv.org/pdf/1407.4874',
	 'subjects': u'Computer Vision and Pattern Recognition (cs.CV)',
	 'title': u'\nAffine Subspace Representation for Feature Description',
	 'urllink': u'http://arxiv.org/abs/1407.4874'}
2015-03-23 20:39:46+0000 [xxu461000] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1404.5020> (referer: http://arxiv.org/list/cs/14?skip=4000&show=1000)
2015-03-23 20:39:46+0000 [xxu461000] DEBUG: Scraped from <200 http://arxiv.org/abs/1404.5020>
	{'abstract': u'Non-intrusive load monitoring (NILM) is an important topic in smart-grid and smart-home. Many energy disaggregation algorithms have been proposed to detect various individual appliances from one aggregated signal observation. However, few works studied the energy disaggregation of plug-in electric vehicle (EV) charging in the residential environment since EVs charging at home has emerged only recently. Recent studies showed that EV charging has a large impact on smart-grid especially in summer. Therefore, EV charging monitoring has become a more important and urgent missing piece in energy disaggregation. In this paper, we present a novel method to disaggregate EV charging signals from aggregated real power signals. The proposed method can effectively mitigate interference coming from air-conditioner (AC), enabling accurate EV charging detection and energy estimation under the presence of AC power signals. Besides, the proposed algorithm requires no training, demands a light computational load, delivers high estimation accuracy, and works well for data recorded at the low sampling rate 1/60 Hz. When the algorithm is tested on real-world data recorded from 11 houses over about a whole year (total 125 months worth of data), the averaged error in estimating energy consumption of EV charging is 15.7 kwh/month (while the true averaged energy consumption of EV charging is 208.5 kwh/month), and the averaged normalized mean square error in disaggregating EV charging load signals is 0.19.',
	 'authors': u'Zhilin Zhang, Jae Hyun Son, Ying Li, Mark Trayer, Zhouyue Pi, Dong Yoon Hwang, Joong Ki Moon,',
	 'category': u'Computer Science ',
	 'date': '2014-4-20',
	 'pdflink': u'http://arxiv.org/pdf/1404.5020',
	 'subjects': u'Other Computer Science (cs.OH)',
	 'title': u'\nTraining-Free Non-Intrusive Load Monitoring of Electric Vehicle Charging  with Low Sampling Rate',
	 'urllink': u'http://arxiv.org/abs/1404.5020'}
2015-03-23 20:39:48+0000 [xxu461000] INFO: Crawled 565 pages (at 13 pages/min), scraped 548 items (at 13 items/min)
2015-03-23 20:39:50+0000 [xxu461000] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1409.1662> (referer: http://arxiv.org/list/cs/14?skip=10000&show=1000)
2015-03-23 20:39:50+0000 [xxu461000] DEBUG: Scraped from <200 http://arxiv.org/abs/1409.1662>
	{'abstract': u'This work studies the problem of separate random number generation from correlated sources with side information at the tester under the criterion of statistical distance. The complete characterization of the achievable rate region of this problem is obtained using the information-spectrum approach and the random-bin approach. A perfect duality between separate random number generation and Slepian-Wolf coding is therefore established. A refined analysis is further performed for two important random-bin maps. One is the pure-random-bin map that is uniformly distributed over the set of all maps (with the same domain and codomain). The other is the equal-random-bin map that is uniformly distributed over the set of all surjective maps that induce an equal or quasi-equal partition of the domain. Both of them are proved to have a doubly-exponential concentration of the performance of their sample maps. As an application, an open and transparent lottery scheme, using a random number generator on a public data source, is proposed to solve the social problem of scarce resource allocation. The core of the proposed framework of lottery algorithm is a permutation, a good rateless randomness extractor, whose existence is confirmed by the theoretical performance of equal-random-bin maps. This extractor, together with other important details of the scheme, ensures that the lottery scheme is immune to all kinds of fraud (under some reasonable assumptions).',
	 'authors': u'Shengtian Yang,',
	 'category': u'Computer Science ',
	 'date': '2014-9-5',
	 'pdflink': u'http://arxiv.org/pdf/1409.1662',
	 'subjects': u'Information Theory (cs.IT)',
	 'title': u'\nSeparate Random Number Generation from Correlated Sources',
	 'urllink': u'http://arxiv.org/abs/1409.1662'}
2015-03-23 20:39:56+0000 [xxu461000] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1406.7542> (referer: http://arxiv.org/list/cs/14?skip=7000&show=1000)
2015-03-23 20:39:56+0000 [xxu461000] DEBUG: Scraped from <200 http://arxiv.org/abs/1406.7542>
	{'abstract': u"We present theoretical and empirical results demonstrating the usefulness of voting rules for participatory democracies. We first give algorithms which efficiently elicit epsilon-approximations to two prominent voting rules: the Borda rule and the Condorcet winner. This result circumvents previous prohibitive lower bounds and is surprisingly strong: even if the number of ideas is as large as the number of participants, each participant will only have to make a logarithmic number of comparisons, an exponential improvement over the linear number of comparisons previously needed. We demonstrate the approach in an experiment in Finland's recent off-road traffic law reform, observing that the total number of comparisons needed to achieve a fixed epsilon approximation is linear in the number of ideas and that the constant is not large. Finally, we note a few other experimental observations which support the use of voting rules for aggregation. First, we observe that rating, one of the common alternatives to ranking, manifested effects of bias in our data. Second, we show that very few of the topics lacked a Condorcet winner, one of the prominent negative results in voting. Finally, we show data hinting at a potential future direction: the use of partial rankings as opposed to pairwise comparisons to further decrease the elicitation time.",
	 'authors': u'David Lee, Ashish Goel, Tanja Aitamurto, Helene Landemore,',
	 'category': u'Computer Science ',
	 'date': '2014-6-29',
	 'pdflink': u'http://arxiv.org/pdf/1406.7542',
	 'subjects': u'Multiagent Systems (cs.MA)',
	 'title': u'\nCrowdsourcing for Participatory Democracies: Efficient Elicitation of  Social Choice Functions',
	 'urllink': u'http://arxiv.org/abs/1406.7542'}
2015-03-23 20:39:59+0000 [xxu461000] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1405.1965> (referer: http://arxiv.org/list/cs/14?skip=5000&show=1000)
2015-03-23 20:39:59+0000 [xxu461000] DEBUG: Scraped from <200 http://arxiv.org/abs/1405.1965>
	{'abstract': u'Accurately estimating the wiring diagram of a brain, known as a connectome, at an ultrastructure level is an open research problem. Specifically, precisely tracking neural processes is difficult, especially across many image slices. Here, we propose a novel method to automatically identify and annotate small subcellular structures present in axons, known as axoplasmic reticula, through a 3D volume of high-resolution neural electron microscopy data. Our method produces high precision annotations, which can help improve automatic segmentation by using our results as seeds for segmentation, and as cues to aid segment merging.',
	 'authors': u'Ayushi Sinha, William Gray Roncal, Narayanan Kasthuri, Jeff W. Lichtman, Randal Burns, Michael Kazhdan,',
	 'category': u'Computer Science ',
	 'date': '2014-4-16',
	 'pdflink': u'http://arxiv.org/pdf/1405.1965',
	 'subjects': u'Computer Vision and Pattern Recognition (cs.CV)',
	 'title': u'\nAutomatic Annotation of Axoplasmic Reticula in Pursuit of Connectomes  using High-Resolution Neural EM Data',
	 'urllink': u'http://arxiv.org/abs/1405.1965'}
2015-03-23 20:40:05+0000 [xxu461000] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1405.5311> (referer: http://arxiv.org/list/cs/14?skip=6000&show=1000)
2015-03-23 20:40:05+0000 [xxu461000] DEBUG: Scraped from <200 http://arxiv.org/abs/1405.5311>
	{'abstract': u'Conventional approaches of sampling signals follow the celebrated theorem of Nyquist and Shannon. Compressive sampling, introduced by Donoho, Romberg and Tao, is a new paradigm that goes against the conventional methods in data acquisition and provides a way of recovering signals using fewer samples than the traditional methods use. Here we suggest an alternative way of reconstructing the original signals in compressive sampling using EM algorithm. We first propose a naive approach which has certain computational difficulties and subsequently modify it to a new approach which performs better than the conventional methods of compressive sampling. The comparison of the different approaches and the performance of the new approach has been studied using simulated data.',
	 'authors': u'Atanu Kumar Ghosh, Arnab Chakraborty,',
	 'category': u'Computer Science ',
	 'date': '2014-5-21',
	 'pdflink': u'http://arxiv.org/pdf/1405.5311',
	 'subjects': u'Methodology (stat.ME)',
	 'title': u'\nCompressive Sampling Using EM Algorithm',
	 'urllink': u'http://arxiv.org/abs/1405.5311'}
2015-03-23 20:40:09+0000 [xxu461000] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1407.4867> (referer: http://arxiv.org/list/cs/14?skip=8000&show=1000)
2015-03-23 20:40:09+0000 [xxu461000] DEBUG: Scraped from <200 http://arxiv.org/abs/1407.4867>
	{'abstract': u'Human activity recognition based on the computer vision is the process of labelling image sequences with action labels. Accurate systems for this problem are applied in areas such as visual surveillance, human computer interaction and video retrieval.',
	 'authors': u'Jay Prakash Gupta, Pushkar Dixit, Nishant Singh, Vijay Bhaskar Semwal,',
	 'category': u'Computer Science ',
	 'date': '2014-7-18',
	 'pdflink': u'http://arxiv.org/e-print/1407.4867',
	 'subjects': u'Computer Vision and Pattern Recognition (cs.CV)',
	 'title': u'\nAnalysis of Gait Pattern to Recognize the Human Activities',
	 'urllink': u'http://arxiv.org/abs/1407.4867'}
2015-03-23 20:40:14+0000 [xxu461000] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1404.5012> (referer: http://arxiv.org/list/cs/14?skip=4000&show=1000)
2015-03-23 20:40:14+0000 [xxu461000] DEBUG: Scraped from <200 http://arxiv.org/abs/1404.5012>
	{'abstract': u'The usual weight generating functions associating with a convolutional code (CC) are based on state space realizations or the weight adjacency matrices (WAMs). The MacWilliams identity for CCs on the WAMs was first established by Gluesing-Luerssen and Schneider in the case of minimal encoders, and generalized by Forney using the normal factor graph theorem. We define the dual of a convolutional code in the viewpoint of constraint codes and obtain a simple and direct proof of the MacWilliams identity for CCs. By considering the weight enumeration functions over infinite stages, i.e. all codewords of a CC, we establish additional relations between a CC and its dual. Relations between various notions of weight generating function are also clarified, and the reason that no MacWilliams identity exists for free-distance enumerators is clear now. Hence the MacWilliams theorem for CCs can be considered complete. For our purpose, we choose a different representation for the exact weight generating function (EWGF) of a block code, by defining it as a linear combination of orthonormal vectors in Dirac bra-ket notation, rather than the standard polynomial representation. Within this framework, the MacWilliams identity for the EWGFs can be derived with simply a Fourier transform. This representation provides great flexibility so that various notions of weight generating functions and their MacWilliams identities can be easily obtained from the MacWilliams identity for the EWGFs. As a result, we also obtain the MacWilliams identity for the input-output weight adjacency matrices (IOWAMs) of a systematic convolutional code and its dual, which cannot be obtained from previous approaches. Finally, paralleling the development of the classical case, we establish the MacWilliams identity for quantum convolutional codes.',
	 'authors': u'Ching-Yi Lai, Min-Hsiu Hsieh, Hsiao-feng Lu,',
	 'category': u'Computer Science ',
	 'date': '2014-4-20',
	 'pdflink': u'http://arxiv.org/pdf/1404.5012',
	 'subjects': u'Information Theory (cs.IT)',
	 'title': u'\nOn the MacWilliams Identity for Classical and Quantum Convolutional  Codes',
	 'urllink': u'http://arxiv.org/abs/1404.5012'}
2015-03-23 20:40:19+0000 [xxu461000] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1409.1661> (referer: http://arxiv.org/list/cs/14?skip=10000&show=1000)
2015-03-23 20:40:19+0000 [xxu461000] DEBUG: Scraped from <200 http://arxiv.org/abs/1409.1661>
	{'abstract': u'The penetration of wireless broadband services in remote areas has primarily been limited due to the lack of economic incentives that service providers encounter in sparsely populated areas. Besides, wireless backhaul links like satellite and microwave are either expensive or require strict line of sight communication making them unattractive. TV white space channels with their desirable radio propagation characteristics can provide an excellent alternative for engineering backhaul networks in areas that lack abundant infrastructure. Specifically, TV white space channels can provide "free wireless backhaul pipes" to transport aggregated traffic from broadband sources to fiber access points. In this paper, we investigate the feasibility of multi-hop wireless backhaul in the available white space channels by using noncontiguous Orthogonal Frequency Division Multiple Access (NC-OFDMA) transmissions between fixed backhaul towers. Specifically, we consider joint power control, scheduling and routing strategies to maximize the minimum rate across broadband towers in the network. Depending on the population density and traffic demands of the location under consideration, we discuss the suitable choice of cell size for the backhaul network. Using the example of available TV white space channels in Wichita, Kansas (a small city located in central USA), we provide illustrative numerical examples for designing such wireless backhaul network.',
	 'authors': u'Ratnesh Kumbhkar, Muhammad Nazmul Islam, Narayan B. Mandayam, Ivan Seskar,',
	 'category': u'Computer Science ',
	 'date': '2014-9-5',
	 'pdflink': u'http://arxiv.org/pdf/1409.1661',
	 'subjects': u'Networking and Internet Architecture (cs.NI)',
	 'title': u'\nRate Optimal design of a Wireless Backhaul Network using TV White Space',
	 'urllink': u'http://arxiv.org/abs/1409.1661'}
2015-03-23 20:40:23+0000 [xxu461000] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1406.7541> (referer: http://arxiv.org/list/cs/14?skip=7000&show=1000)
2015-03-23 20:40:23+0000 [xxu461000] DEBUG: Scraped from <200 http://arxiv.org/abs/1406.7541>
	{'abstract': u"The principles of open collaboration for innovation (and production), once distinctive to open source software, are now found in many other ventures. Some of these ventures are internet-based: Wikipedia, online forums and communities. Others are off-line: in medicine, science, and everyday life. Such ventures have been affecting traditional firms, and may represent a new organizational form. Despite the impact of such ventures, questions remain about their operating principles and performance. Here we define open collaboration (OC), the underlying set of principles, and propose that it is a robust engine for innovation and production. First, we review multiple OC ventures and identify four defining principles. In all instances, participants create goods and services of economic value, they exchange and reuse each other's work, they labor purposefully with just loose coordination, and they permit anyone to contribute and consume. These principles distinguish OC from other organizational forms, such as firms or cooperatives. Next, we turn to performance. To understand the performance of OC, we develop a computational model, combining innovation theory with recent evidence on human cooperation. We identify and investigate three elements that affect performance: the cooperativeness of participants, the diversity of their needs, and the degree to which the goods are rival (subtractable). Through computational experiments, we find that OC performs well even in seemingly harsh environments: when cooperators are a minority, free riders are present, diversity is lacking, or goods are rival. We conclude that OC is viable and likely to expand into new domains. The findings also inform the discussion on new organizational forms, collaborative and communal.",
	 'authors': u'Sheen Levine, Michael Prietula,',
	 'category': u'Computer Science ',
	 'date': '2014-6-29',
	 'pdflink': u'http://arxiv.org/pdf/1406.7541',
	 'subjects': u'Computers and Society (cs.CY)',
	 'title': u'\nOpen Collaboration for Innovation: Principles and Performance',
	 'urllink': u'http://arxiv.org/abs/1406.7541'}
2015-03-23 20:40:29+0000 [xxu461000] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1405.1964> (referer: http://arxiv.org/list/cs/14?skip=5000&show=1000)
2015-03-23 20:40:29+0000 [xxu461000] DEBUG: Scraped from <200 http://arxiv.org/abs/1405.1964>
	{'abstract': u'This paper proposes a fully distributed Demand-Side Management system for Smart Grid infrastructures, especially tailored to reduce the peak demand of residential users. In particular, we use a dynamic pricing strategy, where energy tariffs are function of the overall power demand of customers. We consider two practical cases: (1) a fully distributed approach, where each appliance decides autonomously its own scheduling, and (2) a hybrid approach, where each user must schedule all his appliances. We analyze numerically these two approaches, showing that they are characterized practically by the same performance level in all the considered grid scenarios. We model the proposed system using a non-cooperative game theoretical approach, and demonstrate that our game is a generalized ordinal potential one under general conditions. Furthermore, we propose a simple yet effective best response strategy that is proved to converge in a few steps to a pure Nash Equilibrium, thus demonstrating the robustness of the power scheduling plan obtained without any central coordination of the operator or the customers. Numerical results, obtained using real load profiles and appliance models, show that the system-wide peak absorption achieved in a completely distributed fashion can be reduced up to 55%, thus decreasing the capital expenditure (CAPEX) necessary to meet the growing energy demand.',
	 'authors': u'Antimo Barbato, Antonio Capone, Lin Chen, Fabio Martignon, Stefano Paris,',
	 'category': u'Computer Science ',
	 'date': '2014-5-8',
	 'pdflink': u'http://arxiv.org/pdf/1405.1964',
	 'subjects': u'Computer Science and Game Theory (cs.GT)',
	 'title': u'\nA Distributed Demand-Side Management Framework for the Smart Grid',
	 'urllink': u'http://arxiv.org/abs/1405.1964'}
2015-03-23 20:40:34+0000 [xxu461000] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1405.5300> (referer: http://arxiv.org/list/cs/14?skip=6000&show=1000)
2015-03-23 20:40:34+0000 [xxu461000] DEBUG: Scraped from <200 http://arxiv.org/abs/1405.5300>
	{'abstract': u'We propose an efficient distributed randomized coordinate descent method for minimizing regularized non-strongly convex loss functions. The method attains the optimal convergence rate, where is the iteration counter. The core of the work is the theoretical study of stepsize parameters. We have implemented the method on Archer - the largest supercomputer in the UK - and show that the method is capable of solving a (synthetic) LASSO optimization problem with 50 billion variables.',
	 'authors': u'Olivier Fercoq, Zheng Qu, Peter Richt\xe1rik, Martin Tak\xe1\u010d,',
	 'category': u'Computer Science ',
	 'date': '2014-5-21',
	 'pdflink': u'http://arxiv.org/pdf/1405.5300',
	 'subjects': u'Optimization and Control (math.OC)',
	 'title': u'\nFast Distributed Coordinate Descent for Non-Strongly Convex Losses',
	 'urllink': u'http://arxiv.org/abs/1405.5300'}
2015-03-23 20:40:40+0000 [xxu461000] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1407.4865> (referer: http://arxiv.org/list/cs/14?skip=8000&show=1000)
2015-03-23 20:40:40+0000 [xxu461000] DEBUG: Scraped from <200 http://arxiv.org/abs/1407.4865>
	{'abstract': u'Internet security finds it difficult to keep the information secure and to maintain the integrity of the data. Sending messages over the internet secretly is one of the major tasks as it is widely used for passing the message.',
	 'authors': u'Pushkar Dixit, Nishant Singh, Jay Prakash Gupta,',
	 'category': u'Computer Science ',
	 'date': '2014-7-18',
	 'pdflink': u'http://arxiv.org/e-print/1407.4865',
	 'subjects': u'Multimedia (cs.MM)',
	 'title': u'\nRobust Lossless Semi Fragile Information Protection in Images',
	 'urllink': u'http://arxiv.org/abs/1407.4865'}
2015-03-23 20:40:43+0000 [xxu461000] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1404.5009> (referer: http://arxiv.org/list/cs/14?skip=4000&show=1000)
2015-03-23 20:40:43+0000 [xxu461000] DEBUG: Scraped from <200 http://arxiv.org/abs/1404.5009>
	{'abstract': u'We propose a new Branch-and-Cut (BC) method for solving general MAP-MRF inference problems. The core of our method is a very efficient bounding procedure, which combines scalable semidefinite programming (SDP) and a cutting-plane method for seeking violated constraints. We analyze the performance of the proposed method under different settings, and demonstrate that our method outperforms state-of-the-art, especially when connectivity is high or the relative magnitudes of the unary costs are low. Experiments show that our method achieves better approximation than the state-of-the-art methods within a variety of time budgets on difficult MAP-MRF inference problems.',
	 'authors': u'Peng Wang, Chunhua Shen, Anton van den Hengel, Philip Torr,',
	 'category': u'Computer Science ',
	 'date': '2014-4-20',
	 'pdflink': u'http://arxiv.org/pdf/1404.5009',
	 'subjects': u'Computer Vision and Pattern Recognition (cs.CV)',
	 'title': u'\nEfficient Semidefinite Branch-and-Cut for MAP-MRF Inference',
	 'urllink': u'http://arxiv.org/abs/1404.5009'}
2015-03-23 20:40:48+0000 [xxu461000] INFO: Crawled 577 pages (at 12 pages/min), scraped 560 items (at 12 items/min)
2015-03-23 20:40:48+0000 [xxu461000] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1409.1660> (referer: http://arxiv.org/list/cs/14?skip=10000&show=1000)
2015-03-23 20:40:48+0000 [xxu461000] DEBUG: Scraped from <200 http://arxiv.org/abs/1409.1660>
	{'abstract': u"A building's environment has profound influence on occupant comfort and health. Continuous monitoring of building occupancy and environment is essential to fault detection, intelligent control, and building commissioning. Though many solutions for environmental measuring based on wireless sensor networks exist, they are not easily accessible to households and building owners who may lack time or technical expertise needed to set up a system and get quick and detailed overview of environmental conditions. Building-in-Briefcase (BiB) is a portable sensor network platform that is trivially easy to deploy in any building environment. Once the sensors are distributed, the environmental data is collected and communicated to the BiB router via TCP/IP protocol and WiFi technology which then forwards the data to the central database securely over the internet through a 3G radio. The user, with minimal effort, can access the aggregated data and visualize the trends in real time on the BiB web portal. Paramount to the adoption and continued operation of an indoor sensing platform is battery lifetime. This design has achieved a multi-year lifespan by careful selection of components, an efficient binary communications protocol and data compression. Our BiB sensor is capable of collecting a rich set of environmental parameters, and is expandable to measure others, such as CO2. This paper describes the power characteristics of BiB sensors and their occupancy estimation and activity recognition functionality. Our vision is large-scale deployment of BiB in thousands of buildings, which would provide ample research opportunities and opportunities to identify ways to improve the building environment and energy efficiency.",
	 'authors': u'Kevin Weekly, Ming Jin, Han Zou, Christopher Hsu, Alexandre Bayen, Costas Spanos,',
	 'category': u'Computer Science ',
	 'date': '2014-9-5',
	 'pdflink': u'http://arxiv.org/pdf/1409.1660',
	 'subjects': u'Computers and Society (cs.CY)',
	 'title': u'\nBuilding-in-Briefcase (BiB)',
	 'urllink': u'http://arxiv.org/abs/1409.1660'}
2015-03-23 20:40:52+0000 [xxu461000] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1406.7540> (referer: http://arxiv.org/list/cs/14?skip=7000&show=1000)
2015-03-23 20:40:52+0000 [xxu461000] DEBUG: Scraped from <200 http://arxiv.org/abs/1406.7540>
	{'abstract': u'The rise of worldwide Internet-scale services demands large distributed systems. Indeed, when handling several millions of users, it is common to operate thousands of servers spread across the globe. Here, replication plays a central role, as it contributes to improve the user experience by hiding failures and by providing acceptable latency. In this paper, we claim that atomic multicast, with strong and well-defined properties, is the appropriate abstraction to efficiently design and implement globally scalable distributed systems. We substantiate our claim with the design of two modern online services atop atomic multicast, a strongly consistent key-value store and a distributed log. In addition to presenting the design of these services, we experimentally assess their performance in a geographically distributed deployment.',
	 'authors': u'Samuel Benz, Parisa Jalili Marandi, Fernando Pedone, Beno\xeet Garbinato,',
	 'category': u'Computer Science ',
	 'date': '2014-6-29',
	 'pdflink': u'http://arxiv.org/pdf/1406.7540',
	 'subjects': u'Distributed, Parallel, and Cluster Computing (cs.DC)',
	 'title': u'\nBuilding global and scalable systems with Atomic Multicast',
	 'urllink': u'http://arxiv.org/abs/1406.7540'}
2015-03-23 20:40:56+0000 [xxu461000] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1405.1963> (referer: http://arxiv.org/list/cs/14?skip=5000&show=1000)
2015-03-23 20:40:56+0000 [xxu461000] DEBUG: Scraped from <200 http://arxiv.org/abs/1405.1963>
	{'abstract': u"The introduction of device-to-device (D2D) into cellular networks poses many new challenges in the resource allocation design due to the co-channel interference caused by spectrum reuse and limited battery life of user equipments (UEs). In this paper, we propose a distributed interference-aware energy-efficient resource allocation algorithm to maximize each UE's energy efficiency (EE) subject to its specific quality of service (QoS) and maximum transmission power constraints. We model the resource allocation problem as a noncooperative game, in which each player is self-interested and wants to maximize its own EE. The formulated EE maximization problem is a non-convex problem and is transformed into a convex optimization problem by exploiting the properties of the nonlinear fractional programming. An iterative optimization algorithm is proposed and verified through computer simulations.",
	 'authors': u'Zhenyu Zhou, Mianxiong Dong, Kaoru Ota, Jun Wu, Takuro Sato,',
	 'category': u'Computer Science ',
	 'date': '2014-5-3',
	 'pdflink': u'http://arxiv.org/pdf/1405.1963',
	 'subjects': u'Networking and Internet Architecture (cs.NI)',
	 'title': u'\nDistributed Interference-Aware Energy-Efficient Resource Allocation for  Device-to-Device Communications Underlaying Cellular Networks',
	 'urllink': u'http://arxiv.org/abs/1405.1963'}
2015-03-23 20:41:00+0000 [xxu461000] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1405.5293> (referer: http://arxiv.org/list/cs/14?skip=6000&show=1000)
2015-03-23 20:41:00+0000 [xxu461000] DEBUG: Scraped from <200 http://arxiv.org/abs/1405.5293>
	{'abstract': u'The NumericalHilbert package for Macaulay2 includes algorithms for computing local dual spaces of polynomial ideals, and related local combinatorial data about its scheme structure. These techniques are numerically stable, and can be used with floating point arithmetic over the complex numbers. They provide a viable alternative in this setting to purely symbolic methods such as standard bases. In particular, these methods can be used to compute initial ideals, local Hilbert functions and Hilbert regularity.',
	 'authors': u'Robert Krone,',
	 'category': u'Computer Science ',
	 'date': '2014-5-21',
	 'pdflink': u'http://arxiv.org/pdf/1405.5293',
	 'subjects': u'Commutative Algebra (math.AC)',
	 'title': u'\nNumerical Hilbert functions for Macaulay2',
	 'urllink': u'http://arxiv.org/abs/1405.5293'}
2015-03-23 20:41:03+0000 [xxu461000] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1407.4863> (referer: http://arxiv.org/list/cs/14?skip=8000&show=1000)
2015-03-23 20:41:03+0000 [xxu461000] DEBUG: Scraped from <200 http://arxiv.org/abs/1407.4863>
	{'abstract': u'Quadratic Assignment Problem (QAP) is an NP-hard combinatorial optimization problem, therefore, solving the QAP requires applying one or more of the meta-heuristic algorithms. This paper presents a comparative study between Meta-heuristic algorithms: Genetic Algorithm, Tabu Search, and Simulated annealing for solving a real-life (QAP) and analyze their performance in terms of both runtime efficiency and solution quality. The results show that Genetic Algorithm has a better solution quality while Tabu Search has a faster execution time in comparison with other Meta-heuristic algorithms for solving QAP.',
	 'authors': u'Gamal Abd El-Nasser A. Said, Abeer M. Mahmoud, El-Sayed M. El-Horbaty,',
	 'category': u'Computer Science ',
	 'date': '2014-7-18',
	 'pdflink': u'http://arxiv.org/pdf/1407.4863',
	 'subjects': u'Artificial Intelligence (cs.AI)',
	 'title': u'\nA Comparative Study of Meta-heuristic Algorithms for Solving Quadratic  Assignment Problem',
	 'urllink': u'http://arxiv.org/abs/1407.4863'}
2015-03-23 20:41:07+0000 [xxu461000] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1404.5007> (referer: http://arxiv.org/list/cs/14?skip=4000&show=1000)
2015-03-23 20:41:07+0000 [xxu461000] DEBUG: Scraped from <200 http://arxiv.org/abs/1404.5007>
	{'abstract': u'Two-transmitter Gaussian multiple access wiretap channel with multiple antennas at the transmitters, legitimate receiver and eavesdroppers is studied. The existence of unknown number of eavesdroppers is assumed but with maximum number of antennas at any eavesdropper limited to a known value . The channel matrices between the transmitters and the receiver is available everywhere, while the legitimate the legitimate transmitters and the legitimate receiver have no information about the eavesdroppers channels. A new upperbound is established and A new achievable DoF bound is provided and meets the upperbound. It is important to note that the same problem has been studied recently with arbitrarily varying eavesdropper channels and an upperbound has been derived. However, our achievable sum secure DoF exceeds their previously derived upperbound. Consequently, we revisited the uppperbound derivation and we re-derived a new mathematically robust upperbound.',
	 'authors': u'Mohamed Amir, Tamer Khattab, Tarek Elfouly, Amr Mohamed,',
	 'category': u'Computer Science ',
	 'date': '2014-4-20',
	 'pdflink': u'http://arxiv.org/pdf/1404.5007',
	 'subjects': u'Information Theory (cs.IT)',
	 'title': u'\nOn the degrees of freedom of MIMO Multiple access channel with multiple  eavesdroppers',
	 'urllink': u'http://arxiv.org/abs/1404.5007'}
2015-03-23 20:41:09+0000 [xxu461000] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1409.1657> (referer: http://arxiv.org/list/cs/14?skip=10000&show=1000)
2015-03-23 20:41:09+0000 [xxu461000] DEBUG: Scraped from <200 http://arxiv.org/abs/1409.1657>
	{'abstract': u'We further study the keyless authentication problem in a noisy model in our previous work, where no secret setup is available for sender Alice and receiver Bob while there is DMC from Alice to Bob and a two-way noiseless but insecure channel between them. We propose a construction such that the message length over DMC does not depend on the size of the source space. If the source space is and the number of channel uses is , then our protocol only has a round complexity of In addition, we show that the round complexity of any secure protocol in our model is lower bounded by . We also obtain a lower bound on the success probability when the message size on DMC is given. Finally, we derive the capacity for a non-interactive authentication protocol under general DMCs, which extends the result under BSCs in our previous work.',
	 'authors': u'Shaoquan Jiang,',
	 'category': u'Computer Science ',
	 'date': '2014-9-5',
	 'pdflink': u'http://arxiv.org/pdf/1409.1657',
	 'subjects': u'Information Theory (cs.IT)',
	 'title': u'\nOn the Optimality of Keyless Authentication in a Noisy Model',
	 'urllink': u'http://arxiv.org/abs/1409.1657'}
2015-03-23 20:41:15+0000 [xxu461000] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1406.7539> (referer: http://arxiv.org/list/cs/14?skip=7000&show=1000)
2015-03-23 20:41:15+0000 [xxu461000] DEBUG: Scraped from <200 http://arxiv.org/abs/1406.7539>
	{'abstract': u'Exploration of task mappings plays a crucial role in achieving high performance in heterogeneous multi-processor system-on-chip (MPSoC) platforms. The problem of optimally mapping a set of tasks onto a set of given heterogeneous processors for maximal throughput has been known, in general, to be NP-complete. The problem is further exacerbated when multiple applications (i.e., bigger task sets) and the communication between tasks are also considered. Previous research has shown that Genetic Algorithms (GA) typically are a good choice to solve this problem when the solution space is relatively small. However, when the size of the problem space increases, classic genetic algorithms still suffer from the problem of long evolution times. To address this problem, this paper proposes a novel bias-elitist genetic algorithm that is guided by domain-specific heuristics to speed up the evolution process. Experimental results reveal that our proposed algorithm is able to handle large scale task mapping problems and produces high-quality mapping solutions in only a short time period.',
	 'authors': u'Wei Quan, Andy D. Pimentel,',
	 'category': u'Computer Science ',
	 'date': '2014-6-29',
	 'pdflink': u'http://arxiv.org/pdf/1406.7539',
	 'subjects': u'Performance (cs.PF)',
	 'title': u'\nExploring Task Mappings on Heterogeneous MPSoCs using a Bias-Elitist  Genetic Algorithm',
	 'urllink': u'http://arxiv.org/abs/1406.7539'}
2015-03-23 20:41:19+0000 [xxu461000] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1405.1958> (referer: http://arxiv.org/list/cs/14?skip=5000&show=1000)
2015-03-23 20:41:19+0000 [xxu461000] DEBUG: Scraped from <200 http://arxiv.org/abs/1405.1958>
	{'abstract': u'The aim of this study is to conduct a profound research in biological-inspired computing, fuzzy logic, and linguistic control methods (rules based systems) under the context of soft computing in order to build a self-adaptive, and intelligent network threats assessment and protection system.',
	 'authors': u'Mohamed A. Hassan,',
	 'category': u'Computer Science ',
	 'date': '2014-5-8',
	 'pdflink': u'http://arxiv.org/pdf/1405.1958',
	 'subjects': u'Neural and Evolutionary Computing (cs.NE)',
	 'title': u'\nHybrid Intelligent System; A Self-Adaptive Network Protection System  "Using Soft Computing and Open-Source Tools"',
	 'urllink': u'http://arxiv.org/abs/1405.1958'}
2015-03-23 20:41:24+0000 [xxu461000] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1405.5254> (referer: http://arxiv.org/list/cs/14?skip=6000&show=1000)
2015-03-23 20:41:24+0000 [xxu461000] DEBUG: Scraped from <200 http://arxiv.org/abs/1405.5254>
	{'abstract': u"Alice and Bob receive a bipartite state (possibly entangled) from some finite collection or from some subspace. Alice sends a message to Bob through a noisy quantum channel such that Bob may determine the initial state, with zero chance of error. This framework encompasses, for example, teleportation, dense coding, entanglement assisted quantum channel capacity, and one-way communication complexity of function evaluation. With classical sources and channels, this problem can be analyzed using graph homomorphisms. We show this quantum version can be analyzed using homomorphisms on non-commutative graphs (an operator space generalization of graphs). Previously the Lov 'sz number has been generalized to non-commutative graphs; we show this to be a homomorphism monotone, thus providing bounds on quantum source-channel coding. We generalize the Schrijver and Szegedy numbers, and show these to be monotones as well. As an application we construct a quantum channel whose entanglement assisted zero-error one-shot capacity can only be unlocked by using a non-maximally entangled state. These homomorphisms allow definition of a chromatic number for non-commutative graphs. Many open questions are presented regarding the possibility of a more fully developed theory.",
	 'authors': u'Dan Stahlke,',
	 'category': u'Computer Science ',
	 'date': '2014-5-20',
	 'pdflink': u'http://arxiv.org/pdf/1405.5254',
	 'subjects': u'Quantum Physics (quant-ph)',
	 'title': u'\nQuantum source-channel coding and non-commutative graph theory',
	 'urllink': u'http://arxiv.org/abs/1405.5254'}
2015-03-23 20:41:27+0000 [xxu461000] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1407.4859> (referer: http://arxiv.org/list/cs/14?skip=8000&show=1000)
2015-03-23 20:41:27+0000 [xxu461000] DEBUG: Scraped from <200 http://arxiv.org/abs/1407.4859>
	{'abstract': u'Data layouts play a crucial role in determining the performance of a given application running on a given architecture. Existing parallel programming frameworks for both multicore and heterogeneous systems leave the onus of selecting a data layout to the programmer. Therefore, shifting the burden of data layout selection to optimizing compilers can greatly enhance programmer productivity and application performance. In this work, we introduce : a two-level hierarchal formulation of the data layout problem for modern heterogeneous architectures. We have created a reference implementation of ADHA in the Heterogeneous Habanero-C (H2C) parallel programming system. ADHA shows significant performance benefits of up to 6.92 compared to manually specified layouts for two benchmark programs running on a CPU+GPU heterogeneous platform.',
	 'authors': u'Deepak Majeti, Kuldeep S. Meel, Rajkishore Barik, Vivek Sarkar,',
	 'category': u'Computer Science ',
	 'date': '2014-7-18',
	 'pdflink': u'http://arxiv.org/pdf/1407.4859',
	 'subjects': u'Distributed, Parallel, and Cluster Computing (cs.DC)',
	 'title': u'\nADHA: Automatic Data layout framework for Heterogeneous Architectures',
	 'urllink': u'http://arxiv.org/abs/1407.4859'}
2015-03-23 20:41:31+0000 [xxu461000] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1404.5002> (referer: http://arxiv.org/list/cs/14?skip=4000&show=1000)
2015-03-23 20:41:31+0000 [xxu461000] DEBUG: Scraped from <200 http://arxiv.org/abs/1404.5002>
	{'abstract': u'Many graph processing algorithms require determination of shortest-path distances between arbitrary numbers of node pairs. Since computation of exact distances between all node-pairs of a large graph, e.g., 10M nodes and up, is prohibitively expensive both in computational time and storage space, distance approximation is often used in place of exact computation. In this paper, we present a novel and scalable distance oracle that leverages the hyperbolic core of real-world large graphs for fast and scalable distance approximation. We show empirically that the proposed oracle significantly outperforms prior oracles on a random set of test cases drawn from public domain graph libraries. There are two sets of prior work against which we benchmark our approach. The first set, which often outperforms other oracles, employs embedding of the graph into low dimensional Euclidean spaces with carefully constructed hyperbolic distances, but provides no guarantees on the distance estimation error. The second set leverages Gromov-type tree contraction of the graph with the additive error guaranteed not to exceed , where is the hyperbolic constant of the graph. We show that our proposed oracle 1) is significantly faster than those oracles that use hyperbolic embedding (first set) with similar approximation error and, perhaps surprisingly, 2) exhibits substantially lower average estimation error compared to Gromov-like tree contractions (second set). We substantiate our claims through numerical computations on a collection of a dozen real world networks and synthetic test cases from multiple domains, ranging in size from 10s of thousand to 10s of millions of nodes.',
	 'authors': u'Deepak Ajwani, W. Sean Kennedy, Alessandra Sala, Iraj Saniee,',
	 'category': u'Computer Science ',
	 'date': '2014-4-20',
	 'pdflink': u'http://arxiv.org/pdf/1404.5002',
	 'subjects': u'Social and Information Networks (cs.SI)',
	 'title': u'\nA Geometric Distance Oracle for Large Real-World Graphs',
	 'urllink': u'http://arxiv.org/abs/1404.5002'}
2015-03-23 20:41:34+0000 [xxu461000] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1409.1656> (referer: http://arxiv.org/list/cs/14?skip=10000&show=1000)
2015-03-23 20:41:34+0000 [xxu461000] DEBUG: Scraped from <200 http://arxiv.org/abs/1409.1656>
	{'abstract': u"Multi-tenancy is one of the most important concepts for any Software as a Service (SaaS) application. Multi-tenant SaaS application serves a large number of tenants with one single application instance. Complex SaaS application that serves significant number of tenants could have a huge number of customizations with complicated relationships, which increases the customization complexity and reduces the customization understandability. Modeling such customizations, validating each tenant's customization, and adapting SaaS applications on the fly based on each tenant's requirements become very complex tasks. To mitigate these challenges, we propose an aspect-oriented approach that makes use of the Orthogonal Variability Model (OVM) and Metagraphs. The OVM is used to provide the tenants with simple and understandable customization model. A Metagraph-based algorithm has been developed to validate tenants' customizations. On the other hand, the aspect-oriented approach offers a high level of runtime adaptability.",
	 'authors': u'Ashraf A. Shahin, Areeg Samir, Abdelaziz Khamis,',
	 'category': u'Computer Science ',
	 'date': '2014-9-5',
	 'pdflink': u'http://arxiv.org/pdf/1409.1656',
	 'subjects': u'Software Engineering (cs.SE)',
	 'title': u'\nAn Aspect-Oriented Approach for SaaS Application Customization',
	 'urllink': u'http://arxiv.org/abs/1409.1656'}
2015-03-23 20:41:40+0000 [xxu461000] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1406.7538> (referer: http://arxiv.org/list/cs/14?skip=7000&show=1000)
2015-03-23 20:41:40+0000 [xxu461000] DEBUG: Scraped from <200 http://arxiv.org/abs/1406.7538>
	{'abstract': u'In the following work, we compare the spread of information by word-of-mouth (WOM) to the spread of information through search engines. We assume that the initial acknowledgement of new information derives from social interactions but that solid opinions are only formed after further evaluation through search engines. Search engines can be viewed as central hubs that connect information presented in relevant websites to searchers. Since they construct new connections between searchers and information in every query performed, the network structure is less relevant. Although models of viral spread of ideas have been inspected in many previous works [1], [2], [3], [4], [5], [6], [7], [8], [9], [10], [11], [12], [13], only few assume the acceptance of a novel concept to be solely based on the evaluation of the opinions of others [8], [5]. Following this approach, combined with that of models of information spread with threshold [1] that claim the propagation in a network to occur only if a threshold of neighbors hold an opinion, the proposed work adds a new theoretical perspective that is relevant to the daily use of search engines as a major information search tool. We continue by presenting some justifications based on experimentations. Last we discuss possible outcomes of over use of search engines vs. WOM, and suggest an hypothesis that such overuse might actually narrow the collective information set.',
	 'authors': u'Alon Sela, Hila Oved, Irad Ben-Gal,',
	 'category': u'Computer Science ',
	 'date': '2014-6-29',
	 'pdflink': u'http://arxiv.org/pdf/1406.7538',
	 'subjects': u'Social and Information Networks (cs.SI)',
	 'title': u'\nInformation Spread in a Connected World',
	 'urllink': u'http://arxiv.org/abs/1406.7538'}
2015-03-23 20:41:43+0000 [xxu461000] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1405.1932> (referer: http://arxiv.org/list/cs/14?skip=5000&show=1000)
2015-03-23 20:41:43+0000 [xxu461000] DEBUG: Scraped from <200 http://arxiv.org/abs/1405.1932>
	{'abstract': u'Over the past few years, emergence of cloud computing has notably made an evolution in the IT industry by putting forward an "everything as a service" idea .Cloud Computing is of growing interest to companies throughout the world, but there are many barriers associated with its adoption which should be eliminated. This paper aims to investigate Cloud Computing and discusses the drivers and inhibitors of its adoption. Moreover, an attempt has been made to identify the key stakeholders of Cloud Computing and outline the current security challenges. A SWOT analysis which consists of strengths, weaknesses, opportunities and threats has also carried out in which Cloud Computing adoption for SMEs (Small and Medium-sized Enterprises) is evaluated. Finally, the paper concludes with some further research areas in the field of Cloud Computing.',
	 'authors': u'Kimia Ghaffari, Mohammad Soltani Delgosha, Neda Abdolvand,',
	 'category': u'Computer Science ',
	 'date': '2014-5-8',
	 'pdflink': u'http://arxiv.org/pdf/1405.1932',
	 'subjects': u'Distributed, Parallel, and Cluster Computing (cs.DC)',
	 'title': u'\nTowards Cloud Computing: A SWOT Analysis on its Adoption in SMEs',
	 'urllink': u'http://arxiv.org/abs/1405.1932'}
2015-03-23 20:41:48+0000 [xxu461000] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1405.5245> (referer: http://arxiv.org/list/cs/14?skip=6000&show=1000)
2015-03-23 20:41:48+0000 [xxu461000] DEBUG: Scraped from <200 http://arxiv.org/abs/1405.5245>
	{'abstract': u'We present an accurate and efficient discretization approach for the adaptive discretization of typical model equations employed in numerical weather prediction. A semi-Lagrangian approach is combined with the TR-BDF2 semi-implicit time discretization method and with a spatial discretization based on adaptive discontinuous finite elements. The resulting method has full second order accuracy in time and can employ polynomial bases of arbitrarily high degree in space, is unconditionally stable and can effectively adapt the number of degrees of freedom employed in each element, in order to balance accuracy and computational cost. The p-adaptivity approach employed does not require remeshing, therefore it is especially suitable for applications, such as numerical weather prediction, in which a large number of physical quantities are associated with a given mesh. Furthermore, although the proposed method can be implemented on arbitrary unstructured and nonconforming meshes, even its application on simple Cartesian meshes in spherical coordinates can cure effectively the pole problem by reducing the polynomial degree used in the polar elements. Numerical simulations of classical benchmarks for the shallow water and for the fully compressible Euler equations validate the method and demonstrate its capability to achieve accurate results also at large Courant numbers, with time steps up to 100 times larger than those of typical explicit discretizations of the same problems, while reducing the computational cost thanks to the adaptivity algorithm.',
	 'authors': u'G. Tumolo, L. Bonaventura,',
	 'category': u'Computer Science ',
	 'date': '2014-5-20',
	 'pdflink': u'http://arxiv.org/pdf/1405.5245',
	 'subjects': u'Numerical Analysis (math.NA)',
	 'title': u'\nAn accurate and efficient numerical framework for adaptive numerical  weather prediction',
	 'urllink': u'http://arxiv.org/abs/1405.5245'}
2015-03-23 20:41:48+0000 [xxu461000] INFO: Crawled 593 pages (at 16 pages/min), scraped 576 items (at 16 items/min)
2015-03-23 20:41:53+0000 [xxu461000] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1407.4835> (referer: http://arxiv.org/list/cs/14?skip=8000&show=1000)
2015-03-23 20:41:53+0000 [xxu461000] DEBUG: Scraped from <200 http://arxiv.org/abs/1407.4835>
	{'abstract': u'We consider the problem of Partial Quantifier Elimination (PQE). Given formula exists(X)[F(X,Y) &amp; G(X,Y)], where F, G are in conjunctive normal form, the PQE problem is to find a formula F*(Y) such that F* &amp; exists(X)[G] is logically equivalent to exists(X)[F &amp; G]. We solve the PQE problem by generating and adding to F clauses over the free variables that make the clauses of F with quantified variables redundant. The traditional Quantifier Elimination problem (QE) is a special case of PQE where G is empty so all clauses of the input formula with quantified variables need to be made redundant. The importance of PQE is twofold. First, many problems are more naturally formulated in terms of PQE rather than QE. Second, in many cases PQE can be solved more efficiently than QE. We describe a PQE algorithm based on the machinery of dependency sequents and give experimental results showing the promise of PQE.',
	 'authors': u'Eugene Goldberg, Panagiotis Manolios,',
	 'category': u'Computer Science ',
	 'date': '2014-7-17',
	 'pdflink': u'http://arxiv.org/pdf/1407.4835',
	 'subjects': u'Logic in Computer Science (cs.LO)',
	 'title': u'\nPartial Quantifier Elimination',
	 'urllink': u'http://arxiv.org/abs/1407.4835'}
2015-03-23 20:41:57+0000 [xxu461000] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1404.5000> (referer: http://arxiv.org/list/cs/14?skip=4000&show=1000)
2015-03-23 20:41:57+0000 [xxu461000] DEBUG: Scraped from <200 http://arxiv.org/abs/1404.5000>
	{'abstract': u"Constraints on agent's ability to pay play a major role in auction design for any setting where the magnitude of financial transactions is sufficiently large. Those constraints have been traditionally modeled in mechanism design as emph, i.e., mechanism is not allowed to charge agents more than a certain amount. Yet, real auction systems (such as Google AdWords) allow more sophisticated constraints on agents' ability to pay, such as emph. In this work, we investigate the design of Pareto optimal and incentive compatible auctions for agents with emph, which captures more realistic models of liquidity constraints that the agents may have. Our result applies to a very general class of allocation constraints known as polymatroidal environments, encompassing many settings of interest such as multi-unit auctions, matching markets, video-on-demand and advertisement systems. Our design is based Ausubel's emph. Incentive compatibility and feasibility with respect to ability-to-pay constraints are direct consequences of the clinching framework. Pareto-optimality, on the other hand, is considerably more challenging, since the no-trade condition that characterizes it depends not only on whether agents have their budgets exhausted or not, but also on prices which the goods are allocated. In order to get a handle on those prices, we introduce novel concepts of dropping prices and saturation. These concepts lead to our main structural result which is a characterization of the tight sets in the clinching auction outcome and its relation to dropping prices.",
	 'authors': u'Gagan Goel, Vahab Mirrokni, Renato Paes Leme,',
	 'category': u'Computer Science ',
	 'date': '2014-4-20',
	 'pdflink': u'http://arxiv.org/pdf/1404.5000',
	 'subjects': u'Computer Science and Game Theory (cs.GT)',
	 'title': u'\nClinching Auctions Beyond Hard Budget Constraints',
	 'urllink': u'http://arxiv.org/abs/1404.5000'}
2015-03-23 20:42:01+0000 [xxu461000] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1409.1654> (referer: http://arxiv.org/list/cs/14?skip=10000&show=1000)
2015-03-23 20:42:01+0000 [xxu461000] DEBUG: Scraped from <200 http://arxiv.org/abs/1409.1654>
	{'abstract': u"In the past few years, computer worms are seen as one of significant challenges of cloud computing. Worms are rapidly changing and getting more sophisticated to evade detection. One major issue to defend against computer worms is collecting worms' payloads to generate their signature and study their behavior. To collect worms' payloads, we identified challenges for detecting and collecting worms' payloads and proposed high-interactive honeypot to collect payloads of zero-day polymorphic worms in homogeneous and heterogeneous cloud computing platforms. Virtual machine (VM) memory and VM disk image are inspected from outside using open-source forensics tools and VMWare Virtual Disk Development Kit. Our experiments show that the proposed approach overcomes the identified challenges.",
	 'authors': u'Ashraf A. Shahin,',
	 'category': u'Computer Science ',
	 'date': '2014-9-5',
	 'pdflink': u'http://arxiv.org/pdf/1409.1654',
	 'subjects': u'Distributed, Parallel, and Cluster Computing (cs.DC)',
	 'title': u'\nPolymorphic Worms Collection in Cloud Computing',
	 'urllink': u'http://arxiv.org/abs/1409.1654'}
2015-03-23 20:42:06+0000 [xxu461000] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1406.7537> (referer: http://arxiv.org/list/cs/14?skip=7000&show=1000)
2015-03-23 20:42:06+0000 [xxu461000] DEBUG: Scraped from <200 http://arxiv.org/abs/1406.7537>
	{'abstract': u'In this study, groups who could not reach a consensus were investigated using the group polarization paradigm. The purpose was to explore the conditions leading to intragroup disagreement and attitude change following disagreement among 269 participants. Analysis indicated that the probability of consensus was low when the group means differed from the grand mean of the entire sample. When small differences among group members were found, depolarization (reverse direction of polarization) followed disagreement. These results suggested the groups which deviated most from the population tendency were the most likely to cause within-group disagreement, while within-group variances determined the direction of attitude change following disagreement within the group.',
	 'authors': u'Yoshiko Arima,',
	 'category': u'Computer Science ',
	 'date': '2014-6-29',
	 'pdflink': u'http://arxiv.org/pdf/1406.7537',
	 'subjects': u'Social and Information Networks (cs.SI)',
	 'title': u'\nEffect of Group Means on the Probability of Consensus',
	 'urllink': u'http://arxiv.org/abs/1406.7537'}
2015-03-23 20:42:10+0000 [xxu461000] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1405.1924> (referer: http://arxiv.org/list/cs/14?skip=5000&show=1000)
2015-03-23 20:42:10+0000 [xxu461000] DEBUG: Scraped from <200 http://arxiv.org/abs/1405.1924>
	{'abstract': u'In this work we present our expert system of Automatic reading or speech synthesis based on a text written in Standard Arabic, our work is carried out in two great stages: the creation of the sound data base, and the transformation of the written text into speech (Text To Speech TTS). This transformation is done firstly by a Phonetic Orthographical Transcription (POT) of any written Standard Arabic text with the aim of transforming it into his corresponding phonetics sequence, and secondly by the generation of the voice signal which corresponds to the chain transcribed. We spread out the different of conception of the system, as well as the results obtained compared to others works studied to realize TTS based on Standard Arabic.',
	 'authors': u'Tebbi Hanane, Azzoune Hamid,',
	 'category': u'Computer Science ',
	 'date': '2014-5-8',
	 'pdflink': u'http://arxiv.org/pdf/1405.1924',
	 'subjects': u'Computation and Language (cs.CL)',
	 'title': u'\nAn Expert System for Automatic Reading of A Text Written in Standard  Arabic',
	 'urllink': u'http://arxiv.org/abs/1405.1924'}
2015-03-23 20:42:16+0000 [xxu461000] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1405.5210> (referer: http://arxiv.org/list/cs/14?skip=6000&show=1000)
2015-03-23 20:42:16+0000 [xxu461000] DEBUG: Scraped from <200 http://arxiv.org/abs/1405.5210>
	{'abstract': u'Given an cost array we consider the problem -P3AP which consists in finding pairwise disjoint permutations of such that is minimized. For the case the planar 3-dimensional assignment problem P3AP results. Our main result concerns the -P3AP on cost arrays that are layered Monge arrays. In a layered Monge array all matrices that result from fixing the third index are Monge matrices. We prove that the -P3AP and the P3AP remain NP-hard for layered Monge arrays. Furthermore, we show that in the layered Monge case there always exists an optimal solution of the -3PAP which can be represented as matrix with bandwidth . This structural result allows us to provide a dynamic programming algorithm that solves the -P3AP in polynomial time on layered Monge arrays when is fixed.',
	 'authors': u'Ante \u0106usti\u0107, Bettina Klinz, Gerhard J. Woeginger,',
	 'category': u'Computer Science ',
	 'date': '2014-5-20',
	 'pdflink': u'http://arxiv.org/pdf/1405.5210',
	 'subjects': u'Optimization and Control (math.OC)',
	 'title': u'\nPlanar 3-dimensional assignment problems with Monge-like cost arrays',
	 'urllink': u'http://arxiv.org/abs/1405.5210'}
2015-03-23 20:42:21+0000 [xxu461000] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1407.4833> (referer: http://arxiv.org/list/cs/14?skip=8000&show=1000)
2015-03-23 20:42:21+0000 [xxu461000] DEBUG: Scraped from <200 http://arxiv.org/abs/1407.4833>
	{'abstract': u'In this paper, we present an ontology of mathematical knowledge concepts that covers a wide range of the fields of mathematics and introduces a balanced representation between comprehensive and sensible models. We demonstrate the applications of this representation in information extraction, semantic search, and education. We argue that the ontology can be a core of future integration of math-aware data sets in the Web of Data and, therefore, provide mappings onto relevant datasets, such as DBpedia and ScienceWISE.',
	 'authors': u'Olga Nevzorova, Nikita Zhiltsov, Alexander Kirillovich, Evgeny Lipachev,',
	 'category': u'Computer Science ',
	 'date': '2014-7-17',
	 'pdflink': u'http://arxiv.org/pdf/1407.4833',
	 'subjects': u'Artificial Intelligence (cs.AI)',
	 'title': u'\n$OntoMath^{PRO}$ Ontology: A Linked Data Hub for Mathematics',
	 'urllink': u'http://arxiv.org/abs/1407.4833'}
2015-03-23 20:42:25+0000 [xxu461000] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1404.4997> (referer: http://arxiv.org/list/cs/14?skip=4000&show=1000)
2015-03-23 20:42:25+0000 [xxu461000] DEBUG: Scraped from <200 http://arxiv.org/abs/1404.4997>
	{'abstract': u'We consider the problem of identifying the parameters of an unknown mixture of two arbitrary -dimensional gaussians from a sequence of independent random samples. Our main results are upper and lower bounds giving a computationally efficient moment-based estimator with an optimal convergence rate, thus resolving a problem introduced by Pearson (1894). Denoting by the variance of the unknown mixture, we prove that samples are necessary and sufficient to estimate each parameter up to constant additive error when Our upper bound extends to arbitrary dimension up to a (provably necessary) logarithmic loss in using a novel---yet simple---dimensionality reduction technique. We further identify several interesting special cases where the sample complexity is notably smaller than our optimal worst-case bound. For instance, if the means of the two components are separated by the sample complexity reduces to and this is again optimal. Our results also apply to learning each component of the mixture up to small error in total variation distance. Here our algorithm is optimal up to logarithmic factors in the near-isotropic case, while giving strong improvements in sample complexity over previous work in the general case.',
	 'authors': u'Moritz Hardt, Eric Price,',
	 'category': u'Computer Science ',
	 'date': '2014-4-19',
	 'pdflink': u'http://arxiv.org/pdf/1404.4997',
	 'subjects': u'Learning (cs.LG)',
	 'title': u'\nTight bounds for learning a mixture of two gaussians',
	 'urllink': u'http://arxiv.org/abs/1404.4997'}
2015-03-23 20:42:31+0000 [xxu461000] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1409.1639> (referer: http://arxiv.org/list/cs/14?skip=10000&show=1000)
2015-03-23 20:42:31+0000 [xxu461000] DEBUG: Scraped from <200 http://arxiv.org/abs/1409.1639>
	{'abstract': u'Extract-Transform-Load (ETL) handles large amount of data and manages workload through dataflows. ETL dataflows are widely regarded as complex and expensive operations in terms of time and system resources. In order to minimize the time and the resources required by ETL dataflows, this paper presents a framework to optimize dataflows using shared cache and parallelization techniques. The framework classifies the components in an ETL dataflow into different categories based on their data operation properties. The framework then partitions the dataflow based on the classification at different granularities. Furthermore, the framework applies optimization techniques such as cache re-using, pipelining and multi-threading to the already-partitioned dataflows. The proposed techniques reduce system memory footprint and the frequency of copying data between different components, and also take full advantage of the computing power of multi-core processors. The experimental results show that the proposed optimization framework is 4.7 times faster than the ordinary ETL dataflows (without using the proposed optimization techniques), and outperforms the similar tool (Kettle).',
	 'authors': u'Xiufeng Liu,',
	 'category': u'Computer Science ',
	 'date': '2014-9-5',
	 'pdflink': u'http://arxiv.org/pdf/1409.1639',
	 'subjects': u'Databases (cs.DB)',
	 'title': u'\nOptimizing ETL Dataflow Using Shared Caching and Parallelization Methods',
	 'urllink': u'http://arxiv.org/abs/1409.1639'}
2015-03-23 20:42:37+0000 [xxu461000] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1406.7535> (referer: http://arxiv.org/list/cs/14?skip=7000&show=1000)
2015-03-23 20:42:37+0000 [xxu461000] DEBUG: Scraped from <200 http://arxiv.org/abs/1406.7535>
	{'abstract': u'We give a -time ( is the input size) blackbox polynomial identity testing algorithm for unknown-order read-once oblivious algebraic branching programs (ROABP). The best result known for this class was due to Forbes-Saptharishi-Shpilka (STOC 2014), and that too only for multilinear ROABP. We get rid of their exponential dependence on the individual degree. With this, we match the time-complexity for the unknown order ROABP with the known order ROABP (due to Forbes-Shpilka (FOCS 2013)) and also with the depth- set-multilinear circuits (due to Agrawal-Saha-Saxena (STOC 2013)). Our proof is simpler and involves a new technique called basis isolation. The depth- model has recently gained much importance, as it has become a stepping-stone to understanding general arithmetic circuits. Its restriction to multilinearity has known exponential lower bounds but no nontrivial blackbox identity tests. In this paper, we take a step towards designing such hitting-sets. We give the first subexponential whitebox PIT for the sum of constantly many set-multilinear depth- circuits. To achieve this, we define notions of distance and base sets. Distance, for a multilinear depth- circuit, measures how far are the partitions from a mere refinement. We design a hitting-set in time for -distance. Further, we give an extension of our result to models where the distance is large but it is small when restricted to certain base sets (of variables). We also explore a new model of ROABP where the factor-matrices are invertible (called invertible-factor ROABP). We design a hitting-set in time poly() for width- invertible-factor ROABP. Further, we could do without the invertibility restriction when . Previously, the best result for width- ROABP was quasi-polynomial time (Forbes-Saptharishi-Shpilka, STOC 2014).',
	 'authors': u'Manindra Agrawal, Rohit Gurjar, Arpita Korwar, Nitin Saxena,',
	 'category': u'Computer Science ',
	 'date': '2014-6-29',
	 'pdflink': u'http://arxiv.org/pdf/1406.7535',
	 'subjects': u'Computational Complexity (cs.CC)',
	 'title': u'\nHitting-sets for ROABP and Sum of Set-Multilinear circuits',
	 'urllink': u'http://arxiv.org/abs/1406.7535'}
2015-03-23 20:42:41+0000 [xxu461000] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1405.1916> (referer: http://arxiv.org/list/cs/14?skip=5000&show=1000)
2015-03-23 20:42:41+0000 [xxu461000] DEBUG: Scraped from <200 http://arxiv.org/abs/1405.1916>
	{'abstract': u'This paper considers a retrial queueing model for a base station in cellular networks where fresh calls and handover calls are available. Fresh calls are initiated from the cell of the base station. On the other hand, a handover call has been connecting to a base station and moves to another one. In order to keep the continuation of the communication, it is desired that an available channel in the new base station is immediately assigned to the handover call. To this end, a channel is reserved as the guard channel for handover calls in base stations. Blocked fresh and handover calls join a virtual orbit and repeat their attempts in a later time. We assume that a base station can recognize retrial calls and give them the same priority as that of handover calls. We model a base station by a multiserver retrial queue with priority customers for which a level-dependent QBD process is formulated. We obtain Taylor series expansion for the nonzero elements of the rate matrices of the level-dependent QBD. Using the expansion results, we obtain an asymptotic upper bound for the joint stationary distribution of the number of busy channels and that of customers in the orbit. Furthermore, we derive an efficient numerical algorithm to calculate the joint stationary distribution.',
	 'authors': u'Kazuki Kajiwara, Tuan Phung-Duc,',
	 'category': u'Computer Science ',
	 'date': '2014-5-8',
	 'pdflink': u'http://arxiv.org/pdf/1405.1916',
	 'subjects': u'Performance (cs.PF)',
	 'title': u'\nAsymptotic and Numerical Analysis of Multiserver Retrial Queue with  Guard Channel for Cellular Networks',
	 'urllink': u'http://arxiv.org/abs/1405.1916'}
2015-03-23 20:42:46+0000 [xxu461000] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1405.5070> (referer: http://arxiv.org/list/cs/14?skip=6000&show=1000)
2015-03-23 20:42:46+0000 [xxu461000] DEBUG: Scraped from <200 http://arxiv.org/abs/1405.5070>
	{'abstract': u'The pervasiveness of mobile devices, which is increasing daily, is generating a vast amount of geo-located data allowing us to gain further insights into human behaviors. In particular, this new technology enables users to communicate through mobile social media applications, such as Twitter, anytime and anywhere. Thus, geo-located tweets offer the possibility to carry out in-depth studies on human mobility. In this paper, we study the use of Twitter in transportation by identifying tweets posted from roads and rails in Europe between September 2012 and November 2013. We compute the percentage of highway and railway segments covered by tweets in 39 countries. The coverages are very different from country to country and their variability can be partially explained by differences in Twitter penetration rates. Still, some of these differences might be related to cultural factors regarding mobility habits and interacting socially online. Analyzing particular road sectors, our results show a positive correlation between the number of tweets on the road and the Average Annual Daily Traffic on highways in France and in the UK. Transport modality can be studied with these data as well, for which we discover very heterogeneous usage patterns across the continent.',
	 'authors': u'Maxime Lenormand, Ant\xf2nia Tugores, Pere Colet, Jos\xe9 J. Ramasco,',
	 'category': u'Computer Science ',
	 'date': '2014-5-20',
	 'pdflink': u'http://arxiv.org/pdf/1405.5070',
	 'subjects': u'Physics and Society (physics.soc-ph)',
	 'title': u'\nTweets on the road',
	 'urllink': u'http://arxiv.org/abs/1405.5070'}
2015-03-23 20:42:48+0000 [xxu461000] INFO: Crawled 605 pages (at 12 pages/min), scraped 588 items (at 12 items/min)
2015-03-23 20:42:52+0000 [xxu461000] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1407.4832> (referer: http://arxiv.org/list/cs/14?skip=8000&show=1000)
2015-03-23 20:42:52+0000 [xxu461000] DEBUG: Scraped from <200 http://arxiv.org/abs/1407.4832>
	{'abstract': u"Out of thousands of names to choose from, picking the right one for your child is a daunting task. In this work, our objective is to help parents making an informed decision while choosing a name for their baby. We follow a recommender system approach and combine, in an ensemble, the individual rankings produced by simple collaborative filtering algorithms in order to produce a personalized list of names that meets the individual parents' taste. Our experiments were conducted using real-world data collected from the query logs of 'nameling' (nameling.net), an online portal for searching and exploring names, which corresponds to the dataset released in the context of the ECML PKDD Discover Challenge 2013. Our approach is intuitive, easy to implement, and features fast training and prediction steps.",
	 'authors': u'Bernat Coma-Puig, Ernesto Diaz-Aviles, Wolfgang Nejdl,',
	 'category': u'Computer Science ',
	 'date': '2014-7-16',
	 'pdflink': u'http://arxiv.org/pdf/1407.4832',
	 'subjects': u'Information Retrieval (cs.IR)',
	 'title': u'\nCollaborative Filtering Ensemble for Personalized Name Recommendation',
	 'urllink': u'http://arxiv.org/abs/1407.4832'}
2015-03-23 20:42:57+0000 [xxu461000] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1404.4995> (referer: http://arxiv.org/list/cs/14?skip=4000&show=1000)
2015-03-23 20:42:57+0000 [xxu461000] DEBUG: Scraped from <200 http://arxiv.org/abs/1404.4995>
	{'abstract': u'We present a new outer bound for the sum capacity of general multi-unicast deterministic networks. Intuitively, this bound can be understood as applying the cut-set bound to concatenated copies of the original network with a special restriction on the allowed transmit signal distributions. We first study applications to finite-field networks, where we obtain a general outer-bound expression in terms of ranks of the transfer matrices. We then show that, even though our outer bound is for deterministic networks, a recent result relating the capacity of AWGN KxKxK networks and the capacity of a deterministic counterpart allows us to establish an outer bound to the DoF of KxKxK wireless networks with general connectivity. This bound is tight in the case of the "adjacent-cell interference" topology, and yields graph-theoretic necessary and sufficient conditions for K DoF to be achievable in general topologies.',
	 'authors': u'Ilan Shomorony, A. Salman Avestimehr,',
	 'category': u'Computer Science ',
	 'date': '2014-4-19',
	 'pdflink': u'http://arxiv.org/pdf/1404.4995',
	 'subjects': u'Information Theory (cs.IT)',
	 'title': u'\nA Generalized Cut-Set Bound for Deterministic Multi-Flow Networks and  its Applications',
	 'urllink': u'http://arxiv.org/abs/1404.4995'}
2015-03-23 20:43:02+0000 [xxu461000] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1409.1636> (referer: http://arxiv.org/list/cs/14?skip=10000&show=1000)
2015-03-23 20:43:02+0000 [xxu461000] DEBUG: Scraped from <200 http://arxiv.org/abs/1409.1636>
	{'abstract': u'In data warehousing, Extract-Transform-Load (ETL) extracts the data from data sources into a central data warehouse regularly for the support of business decision-makings. The data from transaction processing systems are featured with the high frequent changes of insertion, update, and deletion. It is challenging for ETL to propagate the changes to the data warehouse, and maintain the change history. Moreover, ETL jobs typically run in a sequential order when processing the data with dependencies, which is not optimal, eg, when processing early-arriving data. In this paper, we propose a two-level data staging ETL for handling transaction data. The proposed method detects the changes of the data from transactional processing systems, identifies the corresponding operation codes for the changes, and uses two staging databases to facilitate the data processing in an ETL process. The proposed ETL provides the "one-stop" method for fast-changing, slowly-changing and early-arriving data processing.',
	 'authors': u'Xiufeng Liu,',
	 'category': u'Computer Science ',
	 'date': '2014-9-5',
	 'pdflink': u'http://arxiv.org/pdf/1409.1636',
	 'subjects': u'Databases (cs.DB)',
	 'title': u'\nTwo-level Data Staging ETL for Transaction Data',
	 'urllink': u'http://arxiv.org/abs/1409.1636'}
2015-03-23 20:43:07+0000 [xxu461000] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1406.7527> (referer: http://arxiv.org/list/cs/14?skip=7000&show=1000)
2015-03-23 20:43:07+0000 [xxu461000] DEBUG: Scraped from <200 http://arxiv.org/abs/1406.7527>
	{'abstract': u'Every probability distribution can be approximated up to a given precision by a phase-type distribution, i.e. a distribution encoded by a continuous time Markov chain (CTMC). However, an excessive number of states in the corresponding CTMC is needed for some standard distributions, in particular most distributions with regions of zero density such as uniform or shifted distributions. Addressing this class of distributions, we suggest an alternative representation by CTMC extended with discrete-time transitions. Using discrete-time transitions we split the density function into multiple intervals. Within each interval, we then approximate the density with standard phase-type fitting. We provide an experimental evidence that our method requires only a moderate number of states to approximate such distributions with regions of zero density. Furthermore, the usage of CTMC with discrete-time transitions is supported by a number of techniques for their analysis. Thus, our results promise an efficient approach to the transient analysis of a class of non-Markovian models.',
	 'authors': u'\u013dubo\u0161 Koren\u010diak, Jan Kr\u010d\xe1l, Vojt\u011bch \u0158eh\xe1k,',
	 'category': u'Computer Science ',
	 'date': '2014-6-29',
	 'pdflink': u'http://arxiv.org/pdf/1406.7527',
	 'subjects': u'Performance (cs.PF)',
	 'title': u'\nDealing with Zero Density Using Piecewise Phase-type Approximation',
	 'urllink': u'http://arxiv.org/abs/1406.7527'}
2015-03-23 20:43:12+0000 [xxu461000] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1405.1912> (referer: http://arxiv.org/list/cs/14?skip=5000&show=1000)
2015-03-23 20:43:13+0000 [xxu461000] DEBUG: Scraped from <200 http://arxiv.org/abs/1405.1912>
	{'abstract': u'Normalization is an important database design method, in the course of the teaching of data modeling the understanding and applying of this method cause problems for students the most. For improving the efficiency of learning normalization we looked for alternative normalization methods and introduced them into education. We made a survey among engineer students how efficient could they execute the normalization with different methods. We executed statistical and data mining examinations to decide whether any of the methods resulted significantly better solutions.',
	 'authors': u'M\xe1rta Czenky,',
	 'category': u'Computer Science ',
	 'date': '2014-5-8',
	 'pdflink': u'http://arxiv.org/pdf/1405.1912',
	 'subjects': u'Databases (cs.DB)',
	 'title': u'\nThe Efficiency Examination of Teaching of Different Normalization  Methods',
	 'urllink': u'http://arxiv.org/abs/1405.1912'}
2015-03-23 20:43:17+0000 [xxu461000] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1405.5057> (referer: http://arxiv.org/list/cs/14?skip=6000&show=1000)
2015-03-23 20:43:17+0000 [xxu461000] DEBUG: Scraped from <200 http://arxiv.org/abs/1405.5057>
	{'abstract': u'Listening habits are strongly influenced by two opposing aspects, the desire for variety and the demand for uniformity in music. In this work we quantify these two notions in terms of musical instrumentation and production technologies that are typically involved in crafting popular music. We assign a "complexity value" to each music style. A style is complex if it shows the property of having both high variety and low uniformity in instrumentation. We find a strong inverse relation between variety and uniformity of music styles that is remarkably stable over the last half century. Individual styles, however, show dramatic changes in their "complexity" during that period. Styles like "new wave" or "disco" quickly climbed towards higher complexity in the 70s and fell back to low complexity levels shortly afterwards, whereas styles like "folk rock" remained at constant high complexity levels. We show that changes in the complexity of a style are related to its number of sales and to the number of artists contributing to that style. As a style attracts a growing number of artists, its instrumentational variety usually increases. At the same time the instrumentational uniformity of a style decreases, i.e. a unique stylistic and increasingly complex expression pattern emerges. In contrast, album sales of a given style typically increase with decreasing complexity. This can be interpreted as music becoming increasingly formulaic once commercial or mainstream success sets in.',
	 'authors': u'Gamaliel Percino, Peter Klimek, Stefan Thurner,',
	 'category': u'Computer Science ',
	 'date': '2014-5-20',
	 'pdflink': u'http://arxiv.org/pdf/1405.5057',
	 'subjects': u'Physics and Society (physics.soc-ph)',
	 'title': u'\nInstrumentational complexity of music genres and why simplicity sells',
	 'urllink': u'http://arxiv.org/abs/1405.5057'}
2015-03-23 20:43:21+0000 [xxu461000] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1407.4831> (referer: http://arxiv.org/list/cs/14?skip=8000&show=1000)
2015-03-23 20:43:21+0000 [xxu461000] DEBUG: Scraped from <200 http://arxiv.org/abs/1407.4831>
	{'abstract': u'Information services are an inherent part of our everyday life. Especially since ubiquitous cities are being developed all over the world their number is increasing even faster. They aim at facilitating the production of information and the access to the needed information and are supposed to make life easier. Until today many different evaluation models (among others, TAM, TAM 2, TAM 3, UTAUT and MATH) have been developed to measure the quality and acceptance of these services. Still, they only consider subareas of the whole concept that represents an information service. As a holistic and comprehensive approach, the ISE Model studies five dimensions that influence adoption, use, impact and diffusion of the information service: information service quality, information user, information acceptance, information environment and time. All these aspects have a great impact on the final grading and of the success (or failure) of the service. Our model combines approaches, which study subjective impressions of users (e.g., the perceived service quality), and user-independent, more objective approaches (e.g., the degree of gamification of a system). Furthermore, we adopt results of network economics, especially the Success breeds success-principle.',
	 'authors': u'Laura Schumann, Wolfgang G. Stock,',
	 'category': u'Computer Science ',
	 'date': '2014-7-16',
	 'pdflink': u'http://arxiv.org/pdf/1407.4831',
	 'subjects': u'Computers and Society (cs.CY)',
	 'title': u'\nThe Information Service Evaluation (ISE) Model',
	 'urllink': u'http://arxiv.org/abs/1407.4831'}
2015-03-23 20:43:26+0000 [xxu461000] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1404.4985> (referer: http://arxiv.org/list/cs/14?skip=4000&show=1000)
2015-03-23 20:43:26+0000 [xxu461000] DEBUG: Scraped from <200 http://arxiv.org/abs/1404.4985>
	{'abstract': u'While Artificial Intelligence has successfully outperformed humans in complex combinatorial games (such as chess and checkers), humans have retained their supremacy in social interactions that require intuition and adaptation, such as cooperation and coordination games. Despite significant advances in learning algorithms, most algorithms adapt at times scales which are not relevant for interactions with humans, and therefore the advances in AI on this front have remained of a more theoretical nature. This has also hindered the experimental evaluation of how these algorithms perform against humans, as the length of experiments needed to evaluate them is beyond what humans are reasonably expected to endure (max 100 repetitions). This scenario is rapidly changing, as recent algorithms are able to converge to their functional regimes in shorter time-scales. Additionally, this shift opens up possibilities for experimental investigation: where do humans stand compared with these new algorithms? We evaluate humans experimentally against a representative element of these fast-converging algorithms. Our results indicate that the performance of at least one of these algorithms is comparable to, and even exceeds, the performance of people.',
	 'authors': u'Fatimah Ishowo-Oloko, Jacob Crandall, Manuel Cebrian, Sherief Abdallah, Iyad Rahwan,',
	 'category': u'Computer Science ',
	 'date': '2014-4-19',
	 'pdflink': u'http://arxiv.org/pdf/1404.4985',
	 'subjects': u'Computers and Society (cs.CY)',
	 'title': u'\nLearning in Repeated Games: Human Versus Machine',
	 'urllink': u'http://arxiv.org/abs/1404.4985'}
2015-03-23 20:43:31+0000 [xxu461000] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1409.1628> (referer: http://arxiv.org/list/cs/14?skip=10000&show=1000)
2015-03-23 20:43:31+0000 [xxu461000] DEBUG: Scraped from <200 http://arxiv.org/abs/1409.1628>
	{'abstract': u"Cognitive radio transceiver can opportunistically access the underutilized spectrum resource of primary systems for new wireless services. With interleave implementation, the secondary transmission may be interrupted by the primary user's transmission. To facilitate the delay analysis of such secondary transmission for a fixed-size secondary packet, we study the resulting extended delivery time that includes both transmission time and waiting time. In particular, we derive the exact distribution function of extended delivery time of secondary transmission for both continuous sensing and periodic sensing cases. Selected numerical and simulation results are presented for illustrating the mathematical formulation. Finally, we consider a generalized M/G/1 queue set-up at the secondary user and formulate the closed-form expressions for the expected delay with Poisson traffic. The analytical results will greatly facilitate the design of the secondary system for particular target application.",
	 'authors': u'Muneer Usman, Hong-Chuan Yang, Mohamed-Slim Alouini,',
	 'category': u'Computer Science ',
	 'date': '2014-9-4',
	 'pdflink': u'http://arxiv.org/pdf/1409.1628',
	 'subjects': u'Information Theory (cs.IT)',
	 'title': u'\nExtended Delivery Time Analysis for Cognitive Packet Transmission with  Application to Secondary Queuing Analysis',
	 'urllink': u'http://arxiv.org/abs/1409.1628'}
2015-03-23 20:43:36+0000 [xxu461000] DEBUG: Crawled (200) <GET http://arxiv.org/abs/1406.7525> (referer: http://arxiv.org/list/cs/14?skip=7000&show=1000)
2015-03-23 20:43:36+0000 [xxu461000] DEBUG: Scraped from <200 http://arxiv.org/abs/1406.7525>
	{'abstract': u'This paper addresses the problem of holistic road scene understanding based on the integration of visual and range data. To achieve the grand goal, we propose an approach that jointly tackles object-level image segmentation and semantic region labeling within a conditional random field (CRF) framework. Specifically, we first generate semantic object hypotheses by clustering 3D points, learning their prior appearance models, and using a deep learning method for reasoning their semantic categories. The learned priors, together with spatial and geometric contexts, are incorporated in CRF. With this formulation, visual and range data are fused thoroughly, and moreover, the coupled segmentation and semantic labeling problem can be inferred via Graph Cuts. Our approach is validated on the challenging KITTI dataset that contains diverse complicated road scenarios. Both quantitative and qualitative evaluations demonstrate its effectiveness.',
	 'authors': u'Wenqi Huang, Xiaojin Gong,',
	 'category': u'Computer Science ',
	 'date': '2014-6-29',
	 'pdflink': u'http://arxiv.org/pdf/1406.7525',
	 'subjects': u'Computer Vision and Pattern Recognition (cs.CV)',
	 'title': u'\nFusion Based Holistic Road Scene Understanding',
	 'urllink': u'http://arxiv.org/abs/1406.7525'}
2015-03-23 20:43:42+0000 [xxu461000] DEBUG: Crawled (403) <GET http://arxiv.org/abs/1408.2858> (referer: http://arxiv.org/list/cs/14?skip=9000&show=1000)
2015-03-23 20:43:42+0000 [xxu461000] DEBUG: Ignoring response <403 http://arxiv.org/abs/1408.2858>: HTTP status code is not handled or not allowed
2015-03-23 20:43:47+0000 [xxu461000] DEBUG: Crawled (403) <GET http://arxiv.org/abs/1405.1902> (referer: http://arxiv.org/list/cs/14?skip=5000&show=1000)
2015-03-23 20:43:47+0000 [xxu461000] DEBUG: Ignoring response <403 http://arxiv.org/abs/1405.1902>: HTTP status code is not handled or not allowed
2015-03-23 20:43:48+0000 [xxu461000] INFO: Crawled 617 pages (at 12 pages/min), scraped 598 items (at 10 items/min)
2015-03-23 20:43:51+0000 [xxu461000] DEBUG: Crawled (403) <GET http://arxiv.org/abs/1405.4980> (referer: http://arxiv.org/list/cs/14?skip=6000&show=1000)
2015-03-23 20:43:51+0000 [xxu461000] DEBUG: Ignoring response <403 http://arxiv.org/abs/1405.4980>: HTTP status code is not handled or not allowed
2015-03-23 20:43:56+0000 [xxu461000] DEBUG: Crawled (403) <GET http://arxiv.org/abs/1407.4798> (referer: http://arxiv.org/list/cs/14?skip=8000&show=1000)
2015-03-23 20:43:56+0000 [xxu461000] DEBUG: Ignoring response <403 http://arxiv.org/abs/1407.4798>: HTTP status code is not handled or not allowed
2015-03-23 20:44:00+0000 [xxu461000] DEBUG: Crawled (403) <GET http://arxiv.org/abs/1404.4984> (referer: http://arxiv.org/list/cs/14?skip=4000&show=1000)
2015-03-23 20:44:00+0000 [xxu461000] DEBUG: Ignoring response <403 http://arxiv.org/abs/1404.4984>: HTTP status code is not handled or not allowed
2015-03-23 20:44:04+0000 [xxu461000] DEBUG: Crawled (403) <GET http://arxiv.org/abs/1409.1619> (referer: http://arxiv.org/list/cs/14?skip=10000&show=1000)
2015-03-23 20:44:04+0000 [xxu461000] DEBUG: Ignoring response <403 http://arxiv.org/abs/1409.1619>: HTTP status code is not handled or not allowed
2015-03-23 20:44:09+0000 [xxu461000] DEBUG: Crawled (403) <GET http://arxiv.org/abs/1406.7524> (referer: http://arxiv.org/list/cs/14?skip=7000&show=1000)
2015-03-23 20:44:09+0000 [xxu461000] DEBUG: Ignoring response <403 http://arxiv.org/abs/1406.7524>: HTTP status code is not handled or not allowed
2015-03-23 20:44:15+0000 [xxu461000] DEBUG: Crawled (403) <GET http://arxiv.org/abs/1408.2855> (referer: http://arxiv.org/list/cs/14?skip=9000&show=1000)
2015-03-23 20:44:15+0000 [xxu461000] DEBUG: Ignoring response <403 http://arxiv.org/abs/1408.2855>: HTTP status code is not handled or not allowed
2015-03-23 20:44:19+0000 [xxu461000] DEBUG: Crawled (403) <GET http://arxiv.org/abs/1405.1894> (referer: http://arxiv.org/list/cs/14?skip=5000&show=1000)
2015-03-23 20:44:19+0000 [xxu461000] DEBUG: Ignoring response <403 http://arxiv.org/abs/1405.1894>: HTTP status code is not handled or not allowed
2015-03-23 20:44:25+0000 [xxu461000] DEBUG: Crawled (403) <GET http://arxiv.org/abs/1405.4921> (referer: http://arxiv.org/list/cs/14?skip=6000&show=1000)
2015-03-23 20:44:25+0000 [xxu461000] DEBUG: Ignoring response <403 http://arxiv.org/abs/1405.4921>: HTTP status code is not handled or not allowed
2015-03-23 20:44:30+0000 [xxu461000] DEBUG: Crawled (403) <GET http://arxiv.org/abs/1407.4777> (referer: http://arxiv.org/list/cs/14?skip=8000&show=1000)
2015-03-23 20:44:30+0000 [xxu461000] DEBUG: Ignoring response <403 http://arxiv.org/abs/1407.4777>: HTTP status code is not handled or not allowed
2015-03-23 20:44:34+0000 [xxu461000] DEBUG: Crawled (403) <GET http://arxiv.org/abs/1404.4983> (referer: http://arxiv.org/list/cs/14?skip=4000&show=1000)
2015-03-23 20:44:35+0000 [xxu461000] DEBUG: Ignoring response <403 http://arxiv.org/abs/1404.4983>: HTTP status code is not handled or not allowed
2015-03-23 20:44:40+0000 [xxu461000] DEBUG: Crawled (403) <GET http://arxiv.org/abs/1409.1612> (referer: http://arxiv.org/list/cs/14?skip=10000&show=1000)
2015-03-23 20:44:40+0000 [xxu461000] DEBUG: Ignoring response <403 http://arxiv.org/abs/1409.1612>: HTTP status code is not handled or not allowed
2015-03-23 20:44:45+0000 [xxu461000] DEBUG: Crawled (403) <GET http://arxiv.org/abs/1406.7515> (referer: http://arxiv.org/list/cs/14?skip=7000&show=1000)
2015-03-23 20:44:45+0000 [xxu461000] DEBUG: Ignoring response <403 http://arxiv.org/abs/1406.7515>: HTTP status code is not handled or not allowed
2015-03-23 20:44:48+0000 [xxu461000] INFO: Crawled 629 pages (at 12 pages/min), scraped 598 items (at 0 items/min)
2015-03-23 20:44:49+0000 [xxu461000] DEBUG: Crawled (403) <GET http://arxiv.org/abs/1408.2854> (referer: http://arxiv.org/list/cs/14?skip=9000&show=1000)
2015-03-23 20:44:49+0000 [xxu461000] DEBUG: Ignoring response <403 http://arxiv.org/abs/1408.2854>: HTTP status code is not handled or not allowed
2015-03-23 20:44:54+0000 [xxu461000] DEBUG: Crawled (403) <GET http://arxiv.org/abs/1410.6937> (referer: http://arxiv.org/list/cs/14?skip=12000&show=1000)
2015-03-23 20:44:54+0000 [xxu461000] DEBUG: Ignoring response <403 http://arxiv.org/abs/1410.6937>: HTTP status code is not handled or not allowed
2015-03-23 20:45:00+0000 [xxu461000] DEBUG: Crawled (403) <GET http://arxiv.org/abs/1405.1893> (referer: http://arxiv.org/list/cs/14?skip=5000&show=1000)
2015-03-23 20:45:00+0000 [xxu461000] DEBUG: Ignoring response <403 http://arxiv.org/abs/1405.1893>: HTTP status code is not handled or not allowed
2015-03-23 20:45:05+0000 [xxu461000] DEBUG: Crawled (403) <GET http://arxiv.org/abs/1405.4647> (referer: http://arxiv.org/list/cs/14?skip=6000&show=1000)
2015-03-23 20:45:05+0000 [xxu461000] DEBUG: Ignoring response <403 http://arxiv.org/abs/1405.4647>: HTTP status code is not handled or not allowed
2015-03-23 20:45:08+0000 [xxu461000] DEBUG: Crawled (403) <GET http://arxiv.org/abs/1407.4765> (referer: http://arxiv.org/list/cs/14?skip=8000&show=1000)
2015-03-23 20:45:08+0000 [xxu461000] DEBUG: Ignoring response <403 http://arxiv.org/abs/1407.4765>: HTTP status code is not handled or not allowed
2015-03-23 20:45:12+0000 [xxu461000] DEBUG: Crawled (403) <GET http://arxiv.org/abs/1404.4982> (referer: http://arxiv.org/list/cs/14?skip=4000&show=1000)
2015-03-23 20:45:12+0000 [xxu461000] DEBUG: Ignoring response <403 http://arxiv.org/abs/1404.4982>: HTTP status code is not handled or not allowed
2015-03-23 20:45:17+0000 [xxu461000] DEBUG: Crawled (403) <GET http://arxiv.org/abs/1409.1606> (referer: http://arxiv.org/list/cs/14?skip=10000&show=1000)
2015-03-23 20:45:17+0000 [xxu461000] DEBUG: Ignoring response <403 http://arxiv.org/abs/1409.1606>: HTTP status code is not handled or not allowed
2015-03-23 20:45:20+0000 [xxu461000] DEBUG: Crawled (403) <GET http://arxiv.org/abs/1406.7497> (referer: http://arxiv.org/list/cs/14?skip=7000&show=1000)
2015-03-23 20:45:20+0000 [xxu461000] DEBUG: Ignoring response <403 http://arxiv.org/abs/1406.7497>: HTTP status code is not handled or not allowed
2015-03-23 20:45:26+0000 [xxu461000] DEBUG: Crawled (403) <GET http://arxiv.org/abs/1408.2852> (referer: http://arxiv.org/list/cs/14?skip=9000&show=1000)
2015-03-23 20:45:26+0000 [xxu461000] DEBUG: Ignoring response <403 http://arxiv.org/abs/1408.2852>: HTTP status code is not handled or not allowed
2015-03-23 20:45:30+0000 [xxu461000] DEBUG: Crawled (403) <GET http://arxiv.org/abs/1410.6935> (referer: http://arxiv.org/list/cs/14?skip=12000&show=1000)
2015-03-23 20:45:30+0000 [xxu461000] DEBUG: Ignoring response <403 http://arxiv.org/abs/1410.6935>: HTTP status code is not handled or not allowed
2015-03-23 20:45:35+0000 [xxu461000] DEBUG: Crawled (403) <GET http://arxiv.org/abs/1405.1891> (referer: http://arxiv.org/list/cs/14?skip=5000&show=1000)
2015-03-23 20:45:35+0000 [xxu461000] DEBUG: Ignoring response <403 http://arxiv.org/abs/1405.1891>: HTTP status code is not handled or not allowed
2015-03-23 20:45:40+0000 [xxu461000] DEBUG: Crawled (403) <GET http://arxiv.org/abs/1405.4480> (referer: http://arxiv.org/list/cs/14?skip=6000&show=1000)
2015-03-23 20:45:40+0000 [xxu461000] DEBUG: Ignoring response <403 http://arxiv.org/abs/1405.4480>: HTTP status code is not handled or not allowed
2015-03-23 20:45:45+0000 [xxu461000] DEBUG: Crawled (403) <GET http://arxiv.org/abs/1407.4764> (referer: http://arxiv.org/list/cs/14?skip=8000&show=1000)
2015-03-23 20:45:45+0000 [xxu461000] DEBUG: Ignoring response <403 http://arxiv.org/abs/1407.4764>: HTTP status code is not handled or not allowed
2015-03-23 20:45:48+0000 [xxu461000] INFO: Crawled 642 pages (at 13 pages/min), scraped 598 items (at 0 items/min)
2015-03-23 20:45:50+0000 [xxu461000] DEBUG: Crawled (403) <GET http://arxiv.org/abs/1404.4975> (referer: http://arxiv.org/list/cs/14?skip=4000&show=1000)
2015-03-23 20:45:50+0000 [xxu461000] DEBUG: Ignoring response <403 http://arxiv.org/abs/1404.4975>: HTTP status code is not handled or not allowed
2015-03-23 20:45:56+0000 [xxu461000] DEBUG: Crawled (403) <GET http://arxiv.org/abs/1409.1564> (referer: http://arxiv.org/list/cs/14?skip=10000&show=1000)
2015-03-23 20:45:56+0000 [xxu461000] DEBUG: Ignoring response <403 http://arxiv.org/abs/1409.1564>: HTTP status code is not handled or not allowed
2015-03-23 20:46:01+0000 [xxu461000] DEBUG: Crawled (403) <GET http://arxiv.org/abs/1406.7496> (referer: http://arxiv.org/list/cs/14?skip=7000&show=1000)
2015-03-23 20:46:01+0000 [xxu461000] DEBUG: Ignoring response <403 http://arxiv.org/abs/1406.7496>: HTTP status code is not handled or not allowed
2015-03-23 20:46:06+0000 [xxu461000] DEBUG: Crawled (403) <GET http://arxiv.org/abs/1408.2842> (referer: http://arxiv.org/list/cs/14?skip=9000&show=1000)
2015-03-23 20:46:06+0000 [xxu461000] DEBUG: Ignoring response <403 http://arxiv.org/abs/1408.2842>: HTTP status code is not handled or not allowed
2015-03-23 20:46:12+0000 [xxu461000] DEBUG: Crawled (403) <GET http://arxiv.org/abs/1410.6915> (referer: http://arxiv.org/list/cs/14?skip=12000&show=1000)
2015-03-23 20:46:12+0000 [xxu461000] DEBUG: Ignoring response <403 http://arxiv.org/abs/1410.6915>: HTTP status code is not handled or not allowed
2015-03-23 20:46:17+0000 [xxu461000] DEBUG: Crawled (403) <GET http://arxiv.org/abs/1405.1885> (referer: http://arxiv.org/list/cs/14?skip=5000&show=1000)
2015-03-23 20:46:17+0000 [xxu461000] DEBUG: Ignoring response <403 http://arxiv.org/abs/1405.1885>: HTTP status code is not handled or not allowed
2015-03-23 20:46:23+0000 [xxu461000] DEBUG: Crawled (403) <GET http://arxiv.org/abs/1405.4443> (referer: http://arxiv.org/list/cs/14?skip=6000&show=1000)
2015-03-23 20:46:23+0000 [xxu461000] DEBUG: Ignoring response <403 http://arxiv.org/abs/1405.4443>: HTTP status code is not handled or not allowed
2015-03-23 20:46:28+0000 [xxu461000] DEBUG: Crawled (403) <GET http://arxiv.org/abs/1407.4755> (referer: http://arxiv.org/list/cs/14?skip=8000&show=1000)
2015-03-23 20:46:28+0000 [xxu461000] DEBUG: Ignoring response <403 http://arxiv.org/abs/1407.4755>: HTTP status code is not handled or not allowed
2015-03-23 20:46:33+0000 [xxu461000] DEBUG: Crawled (403) <GET http://arxiv.org/abs/1404.4972> (referer: http://arxiv.org/list/cs/14?skip=4000&show=1000)
2015-03-23 20:46:33+0000 [xxu461000] DEBUG: Ignoring response <403 http://arxiv.org/abs/1404.4972>: HTTP status code is not handled or not allowed
2015-03-23 20:46:39+0000 [xxu461000] DEBUG: Crawled (403) <GET http://arxiv.org/abs/1409.1556> (referer: http://arxiv.org/list/cs/14?skip=10000&show=1000)
2015-03-23 20:46:39+0000 [xxu461000] DEBUG: Ignoring response <403 http://arxiv.org/abs/1409.1556>: HTTP status code is not handled or not allowed
2015-03-23 20:46:43+0000 [xxu461000] DEBUG: Crawled (403) <GET http://arxiv.org/abs/1406.7487> (referer: http://arxiv.org/list/cs/14?skip=7000&show=1000)
2015-03-23 20:46:43+0000 [xxu461000] DEBUG: Ignoring response <403 http://arxiv.org/abs/1406.7487>: HTTP status code is not handled or not allowed
2015-03-23 20:46:45+0000 [xxu461000] DEBUG: Crawled (403) <GET http://arxiv.org/abs/1408.2824> (referer: http://arxiv.org/list/cs/14?skip=9000&show=1000)
2015-03-23 20:46:45+0000 [xxu461000] DEBUG: Ignoring response <403 http://arxiv.org/abs/1408.2824>: HTTP status code is not handled or not allowed
2015-03-23 20:46:48+0000 [xxu461000] INFO: Crawled 654 pages (at 12 pages/min), scraped 598 items (at 0 items/min)
2015-03-23 20:46:50+0000 [xxu461000] DEBUG: Crawled (403) <GET http://arxiv.org/abs/1410.6913> (referer: http://arxiv.org/list/cs/14?skip=12000&show=1000)
2015-03-23 20:46:50+0000 [xxu461000] DEBUG: Ignoring response <403 http://arxiv.org/abs/1410.6913>: HTTP status code is not handled or not allowed
2015-03-23 20:46:55+0000 [xxu461000] DEBUG: Crawled (403) <GET http://arxiv.org/abs/1405.1879> (referer: http://arxiv.org/list/cs/14?skip=5000&show=1000)
2015-03-23 20:46:55+0000 [xxu461000] DEBUG: Ignoring response <403 http://arxiv.org/abs/1405.1879>: HTTP status code is not handled or not allowed
2015-03-23 20:46:59+0000 [xxu461000] DEBUG: Crawled (403) <GET http://arxiv.org/abs/1405.4342> (referer: http://arxiv.org/list/cs/14?skip=6000&show=1000)
2015-03-23 20:46:59+0000 [xxu461000] DEBUG: Ignoring response <403 http://arxiv.org/abs/1405.4342>: HTTP status code is not handled or not allowed
2015-03-23 20:47:05+0000 [xxu461000] DEBUG: Crawled (403) <GET http://arxiv.org/abs/1407.4739> (referer: http://arxiv.org/list/cs/14?skip=8000&show=1000)
2015-03-23 20:47:05+0000 [xxu461000] DEBUG: Ignoring response <403 http://arxiv.org/abs/1407.4739>: HTTP status code is not handled or not allowed
2015-03-23 20:47:11+0000 [xxu461000] DEBUG: Crawled (403) <GET http://arxiv.org/abs/1404.4970> (referer: http://arxiv.org/list/cs/14?skip=4000&show=1000)
2015-03-23 20:47:11+0000 [xxu461000] DEBUG: Ignoring response <403 http://arxiv.org/abs/1404.4970>: HTTP status code is not handled or not allowed
2015-03-23 20:47:17+0000 [xxu461000] DEBUG: Crawled (403) <GET http://arxiv.org/abs/1409.1551> (referer: http://arxiv.org/list/cs/14?skip=10000&show=1000)
2015-03-23 20:47:17+0000 [xxu461000] DEBUG: Ignoring response <403 http://arxiv.org/abs/1409.1551>: HTTP status code is not handled or not allowed
2015-03-23 20:47:21+0000 [xxu461000] DEBUG: Crawled (403) <GET http://arxiv.org/abs/1406.7486> (referer: http://arxiv.org/list/cs/14?skip=7000&show=1000)
2015-03-23 20:47:21+0000 [xxu461000] DEBUG: Ignoring response <403 http://arxiv.org/abs/1406.7486>: HTTP status code is not handled or not allowed
2015-03-23 20:47:27+0000 [xxu461000] DEBUG: Crawled (403) <GET http://arxiv.org/abs/1408.2813> (referer: http://arxiv.org/list/cs/14?skip=9000&show=1000)
2015-03-23 20:47:27+0000 [xxu461000] DEBUG: Ignoring response <403 http://arxiv.org/abs/1408.2813>: HTTP status code is not handled or not allowed
2015-03-23 20:47:32+0000 [xxu461000] DEBUG: Crawled (403) <GET http://arxiv.org/abs/1410.6909> (referer: http://arxiv.org/list/cs/14?skip=12000&show=1000)
2015-03-23 20:47:32+0000 [xxu461000] DEBUG: Ignoring response <403 http://arxiv.org/abs/1410.6909>: HTTP status code is not handled or not allowed
2015-03-23 20:47:36+0000 [xxu461000] DEBUG: Crawled (403) <GET http://arxiv.org/abs/1405.1873> (referer: http://arxiv.org/list/cs/14?skip=5000&show=1000)
2015-03-23 20:47:36+0000 [xxu461000] DEBUG: Ignoring response <403 http://arxiv.org/abs/1405.1873>: HTTP status code is not handled or not allowed
2015-03-23 20:47:41+0000 [xxu461000] DEBUG: Crawled (403) <GET http://arxiv.org/abs/1405.4301> (referer: http://arxiv.org/list/cs/14?skip=6000&show=1000)
2015-03-23 20:47:41+0000 [xxu461000] DEBUG: Ignoring response <403 http://arxiv.org/abs/1405.4301>: HTTP status code is not handled or not allowed
2015-03-23 20:47:46+0000 [xxu461000] DEBUG: Crawled (403) <GET http://arxiv.org/abs/1407.4738> (referer: http://arxiv.org/list/cs/14?skip=8000&show=1000)
2015-03-23 20:47:46+0000 [xxu461000] DEBUG: Ignoring response <403 http://arxiv.org/abs/1407.4738>: HTTP status code is not handled or not allowed
2015-03-23 20:47:48+0000 [xxu461000] INFO: Crawled 666 pages (at 12 pages/min), scraped 598 items (at 0 items/min)
2015-03-23 20:47:52+0000 [xxu461000] DEBUG: Crawled (403) <GET http://arxiv.org/abs/1404.4963> (referer: http://arxiv.org/list/cs/14?skip=4000&show=1000)
2015-03-23 20:47:52+0000 [xxu461000] DEBUG: Ignoring response <403 http://arxiv.org/abs/1404.4963>: HTTP status code is not handled or not allowed
2015-03-23 20:47:57+0000 [xxu461000] DEBUG: Crawled (403) <GET http://arxiv.org/abs/1409.1544> (referer: http://arxiv.org/list/cs/14?skip=10000&show=1000)
2015-03-23 20:47:57+0000 [xxu461000] DEBUG: Ignoring response <403 http://arxiv.org/abs/1409.1544>: HTTP status code is not handled or not allowed
2015-03-23 20:48:01+0000 [xxu461000] DEBUG: Crawled (403) <GET http://arxiv.org/abs/1406.7483> (referer: http://arxiv.org/list/cs/14?skip=7000&show=1000)
2015-03-23 20:48:01+0000 [xxu461000] DEBUG: Ignoring response <403 http://arxiv.org/abs/1406.7483>: HTTP status code is not handled or not allowed
2015-03-23 20:48:06+0000 [xxu461000] DEBUG: Crawled (403) <GET http://arxiv.org/abs/1408.2812> (referer: http://arxiv.org/list/cs/14?skip=9000&show=1000)
2015-03-23 20:48:06+0000 [xxu461000] DEBUG: Ignoring response <403 http://arxiv.org/abs/1408.2812>: HTTP status code is not handled or not allowed
2015-03-23 20:48:11+0000 [xxu461000] DEBUG: Crawled (403) <GET http://arxiv.org/abs/1410.6905> (referer: http://arxiv.org/list/cs/14?skip=12000&show=1000)
2015-03-23 20:48:11+0000 [xxu461000] DEBUG: Ignoring response <403 http://arxiv.org/abs/1410.6905>: HTTP status code is not handled or not allowed
2015-03-23 20:48:16+0000 [xxu461000] DEBUG: Crawled (403) <GET http://arxiv.org/abs/1405.1861> (referer: http://arxiv.org/list/cs/14?skip=5000&show=1000)
2015-03-23 20:48:16+0000 [xxu461000] DEBUG: Ignoring response <403 http://arxiv.org/abs/1405.1861>: HTTP status code is not handled or not allowed
2015-03-23 20:48:22+0000 [xxu461000] DEBUG: Crawled (403) <GET http://arxiv.org/abs/1405.4298> (referer: http://arxiv.org/list/cs/14?skip=6000&show=1000)
2015-03-23 20:48:22+0000 [xxu461000] DEBUG: Ignoring response <403 http://arxiv.org/abs/1405.4298>: HTTP status code is not handled or not allowed
2015-03-23 20:48:27+0000 [xxu461000] DEBUG: Crawled (403) <GET http://arxiv.org/abs/1407.4735> (referer: http://arxiv.org/list/cs/14?skip=8000&show=1000)
2015-03-23 20:48:27+0000 [xxu461000] DEBUG: Ignoring response <403 http://arxiv.org/abs/1407.4735>: HTTP status code is not handled or not allowed
2015-03-23 20:48:31+0000 [xxu461000] DEBUG: Crawled (403) <GET http://arxiv.org/abs/1404.4960> (referer: http://arxiv.org/list/cs/14?skip=4000&show=1000)
2015-03-23 20:48:31+0000 [xxu461000] DEBUG: Ignoring response <403 http://arxiv.org/abs/1404.4960>: HTTP status code is not handled or not allowed
2015-03-23 20:48:37+0000 [xxu461000] DEBUG: Crawled (403) <GET http://arxiv.org/abs/1409.1533> (referer: http://arxiv.org/list/cs/14?skip=10000&show=1000)
2015-03-23 20:48:37+0000 [xxu461000] DEBUG: Ignoring response <403 http://arxiv.org/abs/1409.1533>: HTTP status code is not handled or not allowed
2015-03-23 20:48:40+0000 [xxu461000] DEBUG: Crawled (403) <GET http://arxiv.org/abs/1406.7459> (referer: http://arxiv.org/list/cs/14?skip=7000&show=1000)
2015-03-23 20:48:40+0000 [xxu461000] DEBUG: Ignoring response <403 http://arxiv.org/abs/1406.7459>: HTTP status code is not handled or not allowed
2015-03-23 20:48:46+0000 [xxu461000] DEBUG: Crawled (403) <GET http://arxiv.org/abs/1408.2810> (referer: http://arxiv.org/list/cs/14?skip=9000&show=1000)
2015-03-23 20:48:46+0000 [xxu461000] DEBUG: Ignoring response <403 http://arxiv.org/abs/1408.2810>: HTTP status code is not handled or not allowed
2015-03-23 20:48:48+0000 [xxu461000] INFO: Crawled 678 pages (at 12 pages/min), scraped 598 items (at 0 items/min)
2015-03-23 20:48:50+0000 [xxu461000] DEBUG: Crawled (403) <GET http://arxiv.org/abs/1410.6903> (referer: http://arxiv.org/list/cs/14?skip=12000&show=1000)
2015-03-23 20:48:50+0000 [xxu461000] DEBUG: Ignoring response <403 http://arxiv.org/abs/1410.6903>: HTTP status code is not handled or not allowed
2015-03-23 20:48:56+0000 [xxu461000] DEBUG: Crawled (403) <GET http://arxiv.org/abs/1405.1857> (referer: http://arxiv.org/list/cs/14?skip=5000&show=1000)
2015-03-23 20:48:56+0000 [xxu461000] DEBUG: Ignoring response <403 http://arxiv.org/abs/1405.1857>: HTTP status code is not handled or not allowed
2015-03-23 20:49:02+0000 [xxu461000] DEBUG: Crawled (403) <GET http://arxiv.org/abs/1405.4070> (referer: http://arxiv.org/list/cs/14?skip=6000&show=1000)
2015-03-23 20:49:02+0000 [xxu461000] DEBUG: Ignoring response <403 http://arxiv.org/abs/1405.4070>: HTTP status code is not handled or not allowed
2015-03-23 20:49:07+0000 [xxu461000] DEBUG: Crawled (403) <GET http://arxiv.org/abs/1407.4723> (referer: http://arxiv.org/list/cs/14?skip=8000&show=1000)
2015-03-23 20:49:07+0000 [xxu461000] DEBUG: Ignoring response <403 http://arxiv.org/abs/1407.4723>: HTTP status code is not handled or not allowed
2015-03-23 20:49:12+0000 [xxu461000] DEBUG: Crawled (403) <GET http://arxiv.org/abs/1404.4942> (referer: http://arxiv.org/list/cs/14?skip=4000&show=1000)
2015-03-23 20:49:12+0000 [xxu461000] DEBUG: Ignoring response <403 http://arxiv.org/abs/1404.4942>: HTTP status code is not handled or not allowed
2015-03-23 20:49:17+0000 [xxu461000] DEBUG: Crawled (403) <GET http://arxiv.org/abs/1409.1515> (referer: http://arxiv.org/list/cs/14?skip=10000&show=1000)
2015-03-23 20:49:17+0000 [xxu461000] DEBUG: Ignoring response <403 http://arxiv.org/abs/1409.1515>: HTTP status code is not handled or not allowed
2015-03-23 20:49:22+0000 [xxu461000] DEBUG: Crawled (403) <GET http://arxiv.org/abs/1406.7447> (referer: http://arxiv.org/list/cs/14?skip=7000&show=1000)
2015-03-23 20:49:22+0000 [xxu461000] DEBUG: Ignoring response <403 http://arxiv.org/abs/1406.7447>: HTTP status code is not handled or not allowed
2015-03-23 20:49:26+0000 [xxu461000] DEBUG: Crawled (403) <GET http://arxiv.org/abs/1408.2803> (referer: http://arxiv.org/list/cs/14?skip=9000&show=1000)
2015-03-23 20:49:26+0000 [xxu461000] DEBUG: Ignoring response <403 http://arxiv.org/abs/1408.2803>: HTTP status code is not handled or not allowed
2015-03-23 20:49:31+0000 [xxu461000] DEBUG: Crawled (403) <GET http://arxiv.org/abs/1410.6902> (referer: http://arxiv.org/list/cs/14?skip=12000&show=1000)
2015-03-23 20:49:31+0000 [xxu461000] DEBUG: Ignoring response <403 http://arxiv.org/abs/1410.6902>: HTTP status code is not handled or not allowed
2015-03-23 20:49:35+0000 [xxu461000] DEBUG: Crawled (403) <GET http://arxiv.org/abs/1405.1853> (referer: http://arxiv.org/list/cs/14?skip=5000&show=1000)
2015-03-23 20:49:35+0000 [xxu461000] DEBUG: Ignoring response <403 http://arxiv.org/abs/1405.1853>: HTTP status code is not handled or not allowed
2015-03-23 20:49:39+0000 [xxu461000] DEBUG: Crawled (403) <GET http://arxiv.org/abs/1405.4047> (referer: http://arxiv.org/list/cs/14?skip=6000&show=1000)
2015-03-23 20:49:39+0000 [xxu461000] DEBUG: Ignoring response <403 http://arxiv.org/abs/1405.4047>: HTTP status code is not handled or not allowed
2015-03-23 20:49:44+0000 [xxu461000] DEBUG: Crawled (403) <GET http://arxiv.org/abs/1407.4709> (referer: http://arxiv.org/list/cs/14?skip=8000&show=1000)
2015-03-23 20:49:44+0000 [xxu461000] DEBUG: Ignoring response <403 http://arxiv.org/abs/1407.4709>: HTTP status code is not handled or not allowed
2015-03-23 20:49:48+0000 [xxu461000] INFO: Crawled 690 pages (at 12 pages/min), scraped 598 items (at 0 items/min)
2015-03-23 20:49:50+0000 [xxu461000] DEBUG: Crawled (403) <GET http://arxiv.org/abs/1404.4939> (referer: http://arxiv.org/list/cs/14?skip=4000&show=1000)
2015-03-23 20:49:50+0000 [xxu461000] DEBUG: Ignoring response <403 http://arxiv.org/abs/1404.4939>: HTTP status code is not handled or not allowed
2015-03-23 20:49:55+0000 [xxu461000] DEBUG: Crawled (403) <GET http://arxiv.org/abs/1409.1513> (referer: http://arxiv.org/list/cs/14?skip=10000&show=1000)
2015-03-23 20:49:55+0000 [xxu461000] DEBUG: Ignoring response <403 http://arxiv.org/abs/1409.1513>: HTTP status code is not handled or not allowed
2015-03-23 20:49:59+0000 [xxu461000] DEBUG: Crawled (403) <GET http://arxiv.org/abs/1406.7445> (referer: http://arxiv.org/list/cs/14?skip=7000&show=1000)
2015-03-23 20:49:59+0000 [xxu461000] DEBUG: Ignoring response <403 http://arxiv.org/abs/1406.7445>: HTTP status code is not handled or not allowed
2015-03-23 20:50:05+0000 [xxu461000] DEBUG: Crawled (403) <GET http://arxiv.org/abs/1408.2800> (referer: http://arxiv.org/list/cs/14?skip=9000&show=1000)
2015-03-23 20:50:05+0000 [xxu461000] DEBUG: Ignoring response <403 http://arxiv.org/abs/1408.2800>: HTTP status code is not handled or not allowed
2015-03-23 20:50:09+0000 [xxu461000] DEBUG: Crawled (403) <GET http://arxiv.org/abs/1410.6890> (referer: http://arxiv.org/list/cs/14?skip=12000&show=1000)
2015-03-23 20:50:09+0000 [xxu461000] DEBUG: Ignoring response <403 http://arxiv.org/abs/1410.6890>: HTTP status code is not handled or not allowed
2015-03-23 20:50:13+0000 [xxu461000] DEBUG: Crawled (403) <GET http://arxiv.org/abs/1405.1851> (referer: http://arxiv.org/list/cs/14?skip=5000&show=1000)
2015-03-23 20:50:13+0000 [xxu461000] DEBUG: Ignoring response <403 http://arxiv.org/abs/1405.1851>: HTTP status code is not handled or not allowed
2015-03-23 20:50:18+0000 [xxu461000] DEBUG: Crawled (403) <GET http://arxiv.org/abs/1405.3848> (referer: http://arxiv.org/list/cs/14?skip=6000&show=1000)
2015-03-23 20:50:18+0000 [xxu461000] DEBUG: Ignoring response <403 http://arxiv.org/abs/1405.3848>: HTTP status code is not handled or not allowed
2015-03-23 20:50:24+0000 [xxu461000] DEBUG: Crawled (403) <GET http://arxiv.org/abs/1407.4705> (referer: http://arxiv.org/list/cs/14?skip=8000&show=1000)
2015-03-23 20:50:24+0000 [xxu461000] DEBUG: Ignoring response <403 http://arxiv.org/abs/1407.4705>: HTTP status code is not handled or not allowed
2015-03-23 20:50:30+0000 [xxu461000] DEBUG: Crawled (403) <GET http://arxiv.org/abs/1404.4936> (referer: http://arxiv.org/list/cs/14?skip=4000&show=1000)
2015-03-23 20:50:30+0000 [xxu461000] DEBUG: Ignoring response <403 http://arxiv.org/abs/1404.4936>: HTTP status code is not handled or not allowed
2015-03-23 20:50:34+0000 [xxu461000] DEBUG: Crawled (403) <GET http://arxiv.org/abs/1409.1510> (referer: http://arxiv.org/list/cs/14?skip=10000&show=1000)
2015-03-23 20:50:34+0000 [xxu461000] DEBUG: Ignoring response <403 http://arxiv.org/abs/1409.1510>: HTTP status code is not handled or not allowed
2015-03-23 20:50:40+0000 [xxu461000] DEBUG: Crawled (403) <GET http://arxiv.org/abs/1406.7444> (referer: http://arxiv.org/list/cs/14?skip=7000&show=1000)
2015-03-23 20:50:40+0000 [xxu461000] DEBUG: Ignoring response <403 http://arxiv.org/abs/1406.7444>: HTTP status code is not handled or not allowed
2015-03-23 20:50:43+0000 [xxu461000] DEBUG: Crawled (403) <GET http://arxiv.org/abs/1408.2790> (referer: http://arxiv.org/list/cs/14?skip=9000&show=1000)
2015-03-23 20:50:43+0000 [xxu461000] DEBUG: Ignoring response <403 http://arxiv.org/abs/1408.2790>: HTTP status code is not handled or not allowed
2015-03-23 20:50:48+0000 [xxu461000] DEBUG: Crawled (403) <GET http://arxiv.org/abs/1410.6885> (referer: http://arxiv.org/list/cs/14?skip=12000&show=1000)
2015-03-23 20:50:48+0000 [xxu461000] DEBUG: Ignoring response <403 http://arxiv.org/abs/1410.6885>: HTTP status code is not handled or not allowed
2015-03-23 20:50:48+0000 [xxu461000] INFO: Crawled 703 pages (at 13 pages/min), scraped 598 items (at 0 items/min)
2015-03-23 20:50:53+0000 [xxu461000] DEBUG: Crawled (403) <GET http://arxiv.org/abs/1405.1842> (referer: http://arxiv.org/list/cs/14?skip=5000&show=1000)
2015-03-23 20:50:53+0000 [xxu461000] DEBUG: Ignoring response <403 http://arxiv.org/abs/1405.1842>: HTTP status code is not handled or not allowed
2015-03-23 20:50:59+0000 [xxu461000] DEBUG: Crawled (403) <GET http://arxiv.org/abs/1405.3805> (referer: http://arxiv.org/list/cs/14?skip=6000&show=1000)
2015-03-23 20:50:59+0000 [xxu461000] DEBUG: Ignoring response <403 http://arxiv.org/abs/1405.3805>: HTTP status code is not handled or not allowed
2015-03-23 20:51:01+0000 [xxu461000] DEBUG: Crawled (403) <GET http://arxiv.org/abs/1407.4694> (referer: http://arxiv.org/list/cs/14?skip=8000&show=1000)
2015-03-23 20:51:01+0000 [xxu461000] DEBUG: Ignoring response <403 http://arxiv.org/abs/1407.4694>: HTTP status code is not handled or not allowed
2015-03-23 20:51:07+0000 [xxu461000] DEBUG: Crawled (403) <GET http://arxiv.org/abs/1404.4935> (referer: http://arxiv.org/list/cs/14?skip=4000&show=1000)
2015-03-23 20:51:07+0000 [xxu461000] DEBUG: Ignoring response <403 http://arxiv.org/abs/1404.4935>: HTTP status code is not handled or not allowed
2015-03-23 20:51:12+0000 [xxu461000] DEBUG: Crawled (403) <GET http://arxiv.org/abs/1409.1496> (referer: http://arxiv.org/list/cs/14?skip=10000&show=1000)
2015-03-23 20:51:12+0000 [xxu461000] DEBUG: Ignoring response <403 http://arxiv.org/abs/1409.1496>: HTTP status code is not handled or not allowed
2015-03-23 20:51:18+0000 [xxu461000] DEBUG: Crawled (403) <GET http://arxiv.org/abs/1406.7443> (referer: http://arxiv.org/list/cs/14?skip=7000&show=1000)
2015-03-23 20:51:18+0000 [xxu461000] DEBUG: Ignoring response <403 http://arxiv.org/abs/1406.7443>: HTTP status code is not handled or not allowed
2015-03-23 20:51:24+0000 [xxu461000] DEBUG: Crawled (403) <GET http://arxiv.org/abs/1408.2782> (referer: http://arxiv.org/list/cs/14?skip=9000&show=1000)
2015-03-23 20:51:24+0000 [xxu461000] DEBUG: Ignoring response <403 http://arxiv.org/abs/1408.2782>: HTTP status code is not handled or not allowed
2015-03-23 20:51:28+0000 [xxu461000] DEBUG: Crawled (403) <GET http://arxiv.org/abs/1410.6863> (referer: http://arxiv.org/list/cs/14?skip=12000&show=1000)
2015-03-23 20:51:28+0000 [xxu461000] DEBUG: Ignoring response <403 http://arxiv.org/abs/1410.6863>: HTTP status code is not handled or not allowed
2015-03-23 20:51:33+0000 [xxu461000] DEBUG: Crawled (403) <GET http://arxiv.org/abs/1405.1841> (referer: http://arxiv.org/list/cs/14?skip=5000&show=1000)
2015-03-23 20:51:33+0000 [xxu461000] DEBUG: Ignoring response <403 http://arxiv.org/abs/1405.1841>: HTTP status code is not handled or not allowed
2015-03-23 20:51:38+0000 [xxu461000] DEBUG: Crawled (403) <GET http://arxiv.org/abs/1405.3681> (referer: http://arxiv.org/list/cs/14?skip=6000&show=1000)
2015-03-23 20:51:38+0000 [xxu461000] DEBUG: Ignoring response <403 http://arxiv.org/abs/1405.3681>: HTTP status code is not handled or not allowed
2015-03-23 20:51:41+0000 [xxu461000] DEBUG: Crawled (403) <GET http://arxiv.org/abs/1407.4692> (referer: http://arxiv.org/list/cs/14?skip=8000&show=1000)
2015-03-23 20:51:41+0000 [xxu461000] DEBUG: Ignoring response <403 http://arxiv.org/abs/1407.4692>: HTTP status code is not handled or not allowed
2015-03-23 20:51:45+0000 [xxu461000] DEBUG: Crawled (403) <GET http://arxiv.org/abs/1404.4927> (referer: http://arxiv.org/list/cs/14?skip=4000&show=1000)
2015-03-23 20:51:45+0000 [xxu461000] DEBUG: Ignoring response <403 http://arxiv.org/abs/1404.4927>: HTTP status code is not handled or not allowed
2015-03-23 20:51:48+0000 [xxu461000] INFO: Crawled 715 pages (at 12 pages/min), scraped 598 items (at 0 items/min)
2015-03-23 20:51:50+0000 [xxu461000] DEBUG: Crawled (403) <GET http://arxiv.org/abs/1409.1487> (referer: http://arxiv.org/list/cs/14?skip=10000&show=1000)
2015-03-23 20:51:50+0000 [xxu461000] DEBUG: Ignoring response <403 http://arxiv.org/abs/1409.1487>: HTTP status code is not handled or not allowed
2015-03-23 20:51:55+0000 [xxu461000] DEBUG: Crawled (403) <GET http://arxiv.org/abs/1406.7438> (referer: http://arxiv.org/list/cs/14?skip=7000&show=1000)
2015-03-23 20:51:55+0000 [xxu461000] DEBUG: Ignoring response <403 http://arxiv.org/abs/1406.7438>: HTTP status code is not handled or not allowed
2015-03-23 20:51:59+0000 [xxu461000] DEBUG: Crawled (403) <GET http://arxiv.org/abs/1408.2779> (referer: http://arxiv.org/list/cs/14?skip=9000&show=1000)
2015-03-23 20:51:59+0000 [xxu461000] DEBUG: Ignoring response <403 http://arxiv.org/abs/1408.2779>: HTTP status code is not handled or not allowed
2015-03-23 20:52:04+0000 [xxu461000] DEBUG: Crawled (403) <GET http://arxiv.org/abs/1410.6858> (referer: http://arxiv.org/list/cs/14?skip=12000&show=1000)
2015-03-23 20:52:04+0000 [xxu461000] DEBUG: Ignoring response <403 http://arxiv.org/abs/1410.6858>: HTTP status code is not handled or not allowed
2015-03-23 20:52:08+0000 [xxu461000] DEBUG: Crawled (403) <GET http://arxiv.org/abs/1405.1837> (referer: http://arxiv.org/list/cs/14?skip=5000&show=1000)
2015-03-23 20:52:08+0000 [xxu461000] DEBUG: Ignoring response <403 http://arxiv.org/abs/1405.1837>: HTTP status code is not handled or not allowed
2015-03-23 20:52:14+0000 [xxu461000] DEBUG: Crawled (403) <GET http://arxiv.org/abs/1405.3630> (referer: http://arxiv.org/list/cs/14?skip=6000&show=1000)
2015-03-23 20:52:14+0000 [xxu461000] DEBUG: Ignoring response <403 http://arxiv.org/abs/1405.3630>: HTTP status code is not handled or not allowed
2015-03-23 20:52:19+0000 [xxu461000] DEBUG: Crawled (403) <GET http://arxiv.org/abs/1407.4668> (referer: http://arxiv.org/list/cs/14?skip=8000&show=1000)
2015-03-23 20:52:19+0000 [xxu461000] DEBUG: Ignoring response <403 http://arxiv.org/abs/1407.4668>: HTTP status code is not handled or not allowed
2015-03-23 20:52:24+0000 [xxu461000] DEBUG: Crawled (403) <GET http://arxiv.org/abs/1404.4925> (referer: http://arxiv.org/list/cs/14?skip=4000&show=1000)
2015-03-23 20:52:24+0000 [xxu461000] DEBUG: Ignoring response <403 http://arxiv.org/abs/1404.4925>: HTTP status code is not handled or not allowed
2015-03-23 20:52:30+0000 [xxu461000] DEBUG: Crawled (403) <GET http://arxiv.org/abs/1409.1484> (referer: http://arxiv.org/list/cs/14?skip=10000&show=1000)
2015-03-23 20:52:30+0000 [xxu461000] DEBUG: Ignoring response <403 http://arxiv.org/abs/1409.1484>: HTTP status code is not handled or not allowed
2015-03-23 20:52:36+0000 [xxu461000] DEBUG: Crawled (403) <GET http://arxiv.org/abs/1406.7437> (referer: http://arxiv.org/list/cs/14?skip=7000&show=1000)
2015-03-23 20:52:36+0000 [xxu461000] DEBUG: Ignoring response <403 http://arxiv.org/abs/1406.7437>: HTTP status code is not handled or not allowed
2015-03-23 20:52:40+0000 [xxu461000] DEBUG: Crawled (403) <GET http://arxiv.org/abs/1408.2776> (referer: http://arxiv.org/list/cs/14?skip=9000&show=1000)
2015-03-23 20:52:40+0000 [xxu461000] DEBUG: Ignoring response <403 http://arxiv.org/abs/1408.2776>: HTTP status code is not handled or not allowed
2015-03-23 20:52:46+0000 [xxu461000] DEBUG: Crawled (403) <GET http://arxiv.org/abs/1410.6854> (referer: http://arxiv.org/list/cs/14?skip=12000&show=1000)
2015-03-23 20:52:46+0000 [xxu461000] DEBUG: Ignoring response <403 http://arxiv.org/abs/1410.6854>: HTTP status code is not handled or not allowed
2015-03-23 20:52:48+0000 [xxu461000] INFO: Crawled 727 pages (at 12 pages/min), scraped 598 items (at 0 items/min)
2015-03-23 20:52:49+0000 [xxu461000] DEBUG: Crawled (403) <GET http://arxiv.org/abs/1405.1836> (referer: http://arxiv.org/list/cs/14?skip=5000&show=1000)
2015-03-23 20:52:49+0000 [xxu461000] DEBUG: Ignoring response <403 http://arxiv.org/abs/1405.1836>: HTTP status code is not handled or not allowed
2015-03-23 20:52:55+0000 [xxu461000] DEBUG: Crawled (403) <GET http://arxiv.org/abs/1405.3574> (referer: http://arxiv.org/list/cs/14?skip=6000&show=1000)
2015-03-23 20:52:55+0000 [xxu461000] DEBUG: Ignoring response <403 http://arxiv.org/abs/1405.3574>: HTTP status code is not handled or not allowed
2015-03-23 20:53:00+0000 [xxu461000] DEBUG: Crawled (403) <GET http://arxiv.org/abs/1407.4650> (referer: http://arxiv.org/list/cs/14?skip=8000&show=1000)
2015-03-23 20:53:00+0000 [xxu461000] DEBUG: Ignoring response <403 http://arxiv.org/abs/1407.4650>: HTTP status code is not handled or not allowed
2015-03-23 20:53:04+0000 [xxu461000] DEBUG: Crawled (403) <GET http://arxiv.org/abs/1404.4923> (referer: http://arxiv.org/list/cs/14?skip=4000&show=1000)
2015-03-23 20:53:04+0000 [xxu461000] DEBUG: Ignoring response <403 http://arxiv.org/abs/1404.4923>: HTTP status code is not handled or not allowed
2015-03-23 20:53:10+0000 [xxu461000] DEBUG: Crawled (403) <GET http://arxiv.org/abs/1409.1467> (referer: http://arxiv.org/list/cs/14?skip=10000&show=1000)
2015-03-23 20:53:10+0000 [xxu461000] DEBUG: Ignoring response <403 http://arxiv.org/abs/1409.1467>: HTTP status code is not handled or not allowed
2015-03-23 20:53:15+0000 [xxu461000] DEBUG: Crawled (403) <GET http://arxiv.org/abs/1406.7435> (referer: http://arxiv.org/list/cs/14?skip=7000&show=1000)
2015-03-23 20:53:15+0000 [xxu461000] DEBUG: Ignoring response <403 http://arxiv.org/abs/1406.7435>: HTTP status code is not handled or not allowed
2015-03-23 20:53:20+0000 [xxu461000] DEBUG: Crawled (403) <GET http://arxiv.org/abs/1408.2774> (referer: http://arxiv.org/list/cs/14?skip=9000&show=1000)
2015-03-23 20:53:20+0000 [xxu461000] DEBUG: Ignoring response <403 http://arxiv.org/abs/1408.2774>: HTTP status code is not handled or not allowed
2015-03-23 20:53:24+0000 [xxu461000] DEBUG: Crawled (403) <GET http://arxiv.org/abs/1410.6831> (referer: http://arxiv.org/list/cs/14?skip=12000&show=1000)
2015-03-23 20:53:24+0000 [xxu461000] DEBUG: Ignoring response <403 http://arxiv.org/abs/1410.6831>: HTTP status code is not handled or not allowed
2015-03-23 20:53:29+0000 [xxu461000] DEBUG: Crawled (403) <GET http://arxiv.org/abs/1405.1833> (referer: http://arxiv.org/list/cs/14?skip=5000&show=1000)
2015-03-23 20:53:29+0000 [xxu461000] DEBUG: Ignoring response <403 http://arxiv.org/abs/1405.1833>: HTTP status code is not handled or not allowed
2015-03-23 20:53:34+0000 [xxu461000] DEBUG: Crawled (403) <GET http://arxiv.org/abs/1405.3559> (referer: http://arxiv.org/list/cs/14?skip=6000&show=1000)
2015-03-23 20:53:34+0000 [xxu461000] DEBUG: Ignoring response <403 http://arxiv.org/abs/1405.3559>: HTTP status code is not handled or not allowed
2015-03-23 20:53:40+0000 [xxu461000] DEBUG: Crawled (403) <GET http://arxiv.org/abs/1407.4645> (referer: http://arxiv.org/list/cs/14?skip=8000&show=1000)
2015-03-23 20:53:40+0000 [xxu461000] DEBUG: Ignoring response <403 http://arxiv.org/abs/1407.4645>: HTTP status code is not handled or not allowed
2015-03-23 20:53:45+0000 [xxu461000] DEBUG: Crawled (403) <GET http://arxiv.org/abs/1404.4910> (referer: http://arxiv.org/list/cs/14?skip=4000&show=1000)
2015-03-23 20:53:45+0000 [xxu461000] DEBUG: Ignoring response <403 http://arxiv.org/abs/1404.4910>: HTTP status code is not handled or not allowed
2015-03-23 20:53:48+0000 [xxu461000] INFO: Crawled 739 pages (at 12 pages/min), scraped 598 items (at 0 items/min)
2015-03-23 20:53:50+0000 [xxu461000] DEBUG: Crawled (403) <GET http://arxiv.org/abs/1409.1466> (referer: http://arxiv.org/list/cs/14?skip=10000&show=1000)
2015-03-23 20:53:50+0000 [xxu461000] DEBUG: Ignoring response <403 http://arxiv.org/abs/1409.1466>: HTTP status code is not handled or not allowed
2015-03-23 20:53:56+0000 [xxu461000] DEBUG: Crawled (403) <GET http://arxiv.org/abs/1406.7429> (referer: http://arxiv.org/list/cs/14?skip=7000&show=1000)
2015-03-23 20:53:56+0000 [xxu461000] DEBUG: Ignoring response <403 http://arxiv.org/abs/1406.7429>: HTTP status code is not handled or not allowed
2015-03-23 20:54:01+0000 [xxu461000] DEBUG: Crawled (403) <GET http://arxiv.org/abs/1408.2770> (referer: http://arxiv.org/list/cs/14?skip=9000&show=1000)
2015-03-23 20:54:01+0000 [xxu461000] DEBUG: Ignoring response <403 http://arxiv.org/abs/1408.2770>: HTTP status code is not handled or not allowed
2015-03-23 20:54:05+0000 [xxu461000] DEBUG: Crawled (403) <GET http://arxiv.org/abs/1410.6830> (referer: http://arxiv.org/list/cs/14?skip=12000&show=1000)
2015-03-23 20:54:05+0000 [xxu461000] DEBUG: Ignoring response <403 http://arxiv.org/abs/1410.6830>: HTTP status code is not handled or not allowed
2015-03-23 20:54:09+0000 [xxu461000] DEBUG: Crawled (403) <GET http://arxiv.org/abs/1405.1827> (referer: http://arxiv.org/list/cs/14?skip=5000&show=1000)
2015-03-23 20:54:09+0000 [xxu461000] DEBUG: Ignoring response <403 http://arxiv.org/abs/1405.1827>: HTTP status code is not handled or not allowed
2015-03-23 20:54:15+0000 [xxu461000] DEBUG: Crawled (403) <GET http://arxiv.org/abs/1405.3536> (referer: http://arxiv.org/list/cs/14?skip=6000&show=1000)
2015-03-23 20:54:15+0000 [xxu461000] DEBUG: Ignoring response <403 http://arxiv.org/abs/1405.3536>: HTTP status code is not handled or not allowed
2015-03-23 20:54:19+0000 [xxu461000] DEBUG: Crawled (403) <GET http://arxiv.org/abs/1407.4640> (referer: http://arxiv.org/list/cs/14?skip=8000&show=1000)
2015-03-23 20:54:19+0000 [xxu461000] DEBUG: Ignoring response <403 http://arxiv.org/abs/1407.4640>: HTTP status code is not handled or not allowed
2015-03-23 20:54:23+0000 [xxu461000] DEBUG: Crawled (403) <GET http://arxiv.org/abs/1404.4909> (referer: http://arxiv.org/list/cs/14?skip=4000&show=1000)
2015-03-23 20:54:23+0000 [xxu461000] DEBUG: Ignoring response <403 http://arxiv.org/abs/1404.4909>: HTTP status code is not handled or not allowed
2015-03-23 20:54:29+0000 [xxu461000] DEBUG: Crawled (403) <GET http://arxiv.org/abs/1409.1465> (referer: http://arxiv.org/list/cs/14?skip=10000&show=1000)
2015-03-23 20:54:29+0000 [xxu461000] DEBUG: Ignoring response <403 http://arxiv.org/abs/1409.1465>: HTTP status code is not handled or not allowed
2015-03-23 20:54:35+0000 [xxu461000] DEBUG: Crawled (403) <GET http://arxiv.org/abs/1406.7424> (referer: http://arxiv.org/list/cs/14?skip=7000&show=1000)
2015-03-23 20:54:35+0000 [xxu461000] DEBUG: Ignoring response <403 http://arxiv.org/abs/1406.7424>: HTTP status code is not handled or not allowed
2015-03-23 20:54:41+0000 [xxu461000] DEBUG: Crawled (403) <GET http://arxiv.org/abs/1408.2764> (referer: http://arxiv.org/list/cs/14?skip=9000&show=1000)
2015-03-23 20:54:41+0000 [xxu461000] DEBUG: Ignoring response <403 http://arxiv.org/abs/1408.2764>: HTTP status code is not handled or not allowed
2015-03-23 20:54:45+0000 [xxu461000] DEBUG: Crawled (403) <GET http://arxiv.org/abs/1410.6824> (referer: http://arxiv.org/list/cs/14?skip=12000&show=1000)
2015-03-23 20:54:45+0000 [xxu461000] DEBUG: Ignoring response <403 http://arxiv.org/abs/1410.6824>: HTTP status code is not handled or not allowed
2015-03-23 20:54:48+0000 [xxu461000] INFO: Crawled 751 pages (at 12 pages/min), scraped 598 items (at 0 items/min)
2015-03-23 20:54:49+0000 [xxu461000] DEBUG: Crawled (403) <GET http://arxiv.org/abs/1405.1823> (referer: http://arxiv.org/list/cs/14?skip=5000&show=1000)
2015-03-23 20:54:49+0000 [xxu461000] DEBUG: Ignoring response <403 http://arxiv.org/abs/1405.1823>: HTTP status code is not handled or not allowed
2015-03-23 20:54:55+0000 [xxu461000] DEBUG: Crawled (403) <GET http://arxiv.org/abs/1405.3505> (referer: http://arxiv.org/list/cs/14?skip=6000&show=1000)
2015-03-23 20:54:55+0000 [xxu461000] DEBUG: Ignoring response <403 http://arxiv.org/abs/1405.3505>: HTTP status code is not handled or not allowed
2015-03-23 20:55:01+0000 [xxu461000] DEBUG: Crawled (403) <GET http://arxiv.org/abs/1407.4626> (referer: http://arxiv.org/list/cs/14?skip=8000&show=1000)
2015-03-23 20:55:01+0000 [xxu461000] DEBUG: Ignoring response <403 http://arxiv.org/abs/1407.4626>: HTTP status code is not handled or not allowed
2015-03-23 20:55:04+0000 [xxu461000] DEBUG: Crawled (403) <GET http://arxiv.org/abs/1404.4904> (referer: http://arxiv.org/list/cs/14?skip=4000&show=1000)
2015-03-23 20:55:04+0000 [xxu461000] DEBUG: Ignoring response <403 http://arxiv.org/abs/1404.4904>: HTTP status code is not handled or not allowed
2015-03-23 20:55:09+0000 [xxu461000] DEBUG: Crawled (403) <GET http://arxiv.org/abs/1409.1461> (referer: http://arxiv.org/list/cs/14?skip=10000&show=1000)
2015-03-23 20:55:09+0000 [xxu461000] DEBUG: Ignoring response <403 http://arxiv.org/abs/1409.1461>: HTTP status code is not handled or not allowed
2015-03-23 20:55:15+0000 [xxu461000] DEBUG: Crawled (403) <GET http://arxiv.org/abs/1406.7423> (referer: http://arxiv.org/list/cs/14?skip=7000&show=1000)
2015-03-23 20:55:15+0000 [xxu461000] DEBUG: Ignoring response <403 http://arxiv.org/abs/1406.7423>: HTTP status code is not handled or not allowed
2015-03-23 20:55:19+0000 [xxu461000] DEBUG: Crawled (403) <GET http://arxiv.org/abs/1408.2758> (referer: http://arxiv.org/list/cs/14?skip=9000&show=1000)
2015-03-23 20:55:19+0000 [xxu461000] DEBUG: Ignoring response <403 http://arxiv.org/abs/1408.2758>: HTTP status code is not handled or not allowed
2015-03-23 20:55:23+0000 [xxu461000] DEBUG: Crawled (403) <GET http://arxiv.org/abs/1410.6806> (referer: http://arxiv.org/list/cs/14?skip=12000&show=1000)
2015-03-23 20:55:23+0000 [xxu461000] DEBUG: Ignoring response <403 http://arxiv.org/abs/1410.6806>: HTTP status code is not handled or not allowed
2015-03-23 20:55:28+0000 [xxu461000] DEBUG: Crawled (403) <GET http://arxiv.org/abs/1405.1821> (referer: http://arxiv.org/list/cs/14?skip=5000&show=1000)
2015-03-23 20:55:28+0000 [xxu461000] DEBUG: Ignoring response <403 http://arxiv.org/abs/1405.1821>: HTTP status code is not handled or not allowed
